# Personalized Daily ArXiv Papers 12/25/2025
Total relevant papers: 25

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](#link0)
**Authors:** Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess

1. [TongSIM: A General Platform for Simulating Intelligent Machines](#link1)
**Authors:** Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang

2. [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](#link2)
**Authors:** Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu

3. [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](#link3)
**Authors:** Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou

4. [Latent Implicit Visual Reasoning](#link4)
**Authors:** Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig

5. [Fast SAM2 with Text-Driven Token Pruning](#link5)
**Authors:** Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen

6. [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](#link6)
**Authors:** Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu

7. [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](#link7)
**Authors:** Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu

8. [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](#link8)
**Authors:** Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang

9. [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](#link9)
**Authors:** Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan

10. [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](#link10)
**Authors:** Hongxing Fan, Shuyu Zhao, Jiayang Ao, Lu Sheng

11. [LongVideoAgent: Multi-Agent Reasoning with Long Videos](#link11)
**Authors:** Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen

12. [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](#link12)
**Authors:** Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu

13. [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](#link13)
**Authors:** Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan

14. [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](#link14)
**Authors:** Xiangzuo Wu, Chengwei Ren, Jun Zhou, Xiu Li, Yuan Liu

15. [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](#link15)
**Authors:** Tianchen Deng, Xun Chen, Ziming Li, Hongming Shen, Danwei Wang, Javier Civera, Hesheng Wang

16. [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](#link16)
**Authors:** Zeqing Song, Zhongmiao Yan, Junyuan Deng, Songpengcheng Xia, Xiang Mu, Jingyi Xu, Qi Wu, Ling Pei

17. [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](#link17)
**Authors:** Vidit Agrawal (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), John Peters (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Tyler N. Thompson (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Mohammad Vali Sanian (Institute for Molecular Medicine Finland, University of Helsinki, Helsinki, Finland), Chau Pham (Boston University, Boston, MA, USA), Nikita Moshkov (Institute of Computational Biology, Helmholtz Munich, Neuherberg, Germany), Arshad Kazi (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Aditya Pillai (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Jack Freeman (Morgridge Institute for Research, Madison, WI, USA), Byunguk Kang (Massachusetts Institute of Technology, Cambridge, MA, USA, Broad Institute of MIT and Harvard, Cambridge, MA, USA), Samouil L. Farhi (Broad Institute of MIT and Harvard, Cambridge, MA, USA), Ernest Fraenkel (Massachusetts Institute of Technology, Cambridge, MA, USA), Ron Stewart (Morgridge Institute for Research, Madison, WI, USA), Lassi Paavolainen (Institute for Molecular Medicine Finland, University of Helsinki, Helsinki, Finland), Bryan A. Plummer (Boston University, Boston, MA, USA), Juan C. Caicedo (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA)

18. [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](#link18)
**Authors:** Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong, Beining Wu, Jie Dong, Feiwei Qin, Yunkang Cao, Qiyuan Tian

19. [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](#link19)
**Authors:** Zhuo Yang, Yeyun Chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li

20. [Scaling Reinforcement Learning for Content Moderation with Large Language Models](#link20)
**Authors:** Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal

21. [Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising](#link21)
**Authors:** Yiwen Shan, Haiyu Zhao, Peng Hu, Xi Peng, Yuanbiao Gou

22. [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](#link22)
**Authors:** Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao

23. [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](#link23)
**Authors:** Chao Gong, Dong Li, Yingwei Pan, Jingjing Chen, Ting Yao, Tao Mei

24. [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](#link24)
**Authors:** Zibin Liu, Banglei Guan, Yang Shang, Shunkun Liang, Zhenbao Yu, Qifeng Yu

---
## 0. [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770) <a id="link0"></a>
**ArXiv ID:** 2512.20770
**Authors:** Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess

**Abstract:**  Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

**Comment:** OccuFly introduces the first real-world, camera-based aerial Semantic Scene Completion (SSC) benchmark, with a novel LiDAR-free data generation framework. This is a direct match for criterion 3 (embodied AI papers on building new benchmarks/simulators with novel angles), as it addresses aerial 3D scene understanding, a previously underexplored domain.
**Relevance:** 10
**Novelty:** 8

---

## 1. [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206) <a id="link1"></a>
**ArXiv ID:** 2512.20206
**Authors:** Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang

**Abstract:**  As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

**Comment:** Matches criterion 3: Introduces TongSIM, a new general-purpose, high-fidelity simulation platform for embodied AI, supporting a wide range of tasks and providing comprehensive benchmarks for spatial and social reasoning. This is a new benchmark/simulator for embodied agents.
**Relevance:** 10
**Novelty:** 8

---

## 2. [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004) <a id="link2"></a>
**ArXiv ID:** 2512.21004
**Authors:** Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu

**Abstract:**  Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

**Comment:** NExT-Vid proposes a novel autoregressive visual generative pretraining framework for images and videos, focusing on next-frame prediction and context-isolated flow-matching. This is highly relevant to criterion 4 (vision foundation models and applications), and also touches on generative modeling in multi-modal learning.
**Relevance:** 9
**Novelty:** 8

---

## 3. [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276) <a id="link3"></a>
**ArXiv ID:** 2512.20276
**Authors:** Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou

**Abstract:**  Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

**Comment:** ActionFlow introduces a system-level inference framework for accelerating Vision-Language-Action (VLA) models on edge devices, with a novel pipelining strategy and memory optimizations. This is a direct match for criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (embodied AI methods for real-world deployment), as it enables real-time VLA inference for robotics.
**Relevance:** 9
**Novelty:** 8

---

## 4. [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218) <a id="link4"></a>
**ArXiv ID:** 2512.21218
**Authors:** Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig

**Abstract:**  While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

**Comment:** Matches criterion 2: Proposes a new mechanism for LMMs (Large Multimodal Models) to perform visual reasoning using unsupervised visual tokens, improving vision-centric reasoning in MLLMs.
**Relevance:** 9
**Novelty:** 8

---

## 5. [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333) <a id="link5"></a>
**ArXiv ID:** 2512.21333
**Authors:** Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen

**Abstract:**  Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

**Comment:** Fast SAM2 with Text-Driven Token Pruning proposes a text-guided token pruning framework for the Segment Anything Model 2 (SAM2), a vision foundation model, to improve efficiency in video object segmentation. This is a strong match for criterion 4 (vision foundation models and applications), and also involves prompt-driven segmentation, which is relevant to your friend's interests.
**Relevance:** 9
**Novelty:** 7

---

## 6. [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252) <a id="link6"></a>
**ArXiv ID:** 2512.21252
**Authors:** Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu

**Abstract:**  The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

**Comment:** DreaMontage introduces a framework for arbitrary frame-guided one-shot video generation, with novel mechanisms for visual fidelity, cinematic expressiveness, and memory-efficient long-sequence generation. This is relevant to criterion 4 (vision foundation models and applications), especially in generative modeling for video.
**Relevance:** 7
**Novelty:** 8

---

## 7. [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135) <a id="link7"></a>
**ArXiv ID:** 2512.21135
**Authors:** Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu

**Abstract:**  Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

**Comment:** TGC-Net is a CLIP-based framework for text-guided medical image segmentation, introducing a structure-aware and semantically-aligned approach. This matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), as it adapts CLIP for medical imaging and enhances cross-modal alignment.
**Relevance:** 8
**Novelty:** 7

---

## 8. [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](https://arxiv.org/abs/2512.21268) <a id="link8"></a>
**ArXiv ID:** 2512.21268
**Authors:** Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang

**Abstract:**  Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.

**Comment:** ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision introduces a new framework for controllable video synthesis using attention supervision and a 3D-aware object layout. This is a strong match for criterion 4 (vision foundation models and applications), and is also relevant to generative modeling in multi-modal learning.
**Relevance:** 8
**Novelty:** 7

---

## 9. [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185) <a id="link9"></a>
**ArXiv ID:** 2512.21185
**Authors:** Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan

**Abstract:**  In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

**Comment:** UltraShape 1.0 introduces a scalable 3D diffusion framework for high-fidelity 3D geometry generation, with a novel data processing pipeline and a decoupling of spatial localization from geometric detail synthesis. This is a strong match for criterion 4 (vision foundation models and applications), as it presents a new 3D generative model and data pipeline, and is also relevant to generative modeling in multi-modal learning.
**Relevance:** 8
**Novelty:** 7

---

## 10. [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936) <a id="link10"></a>
**ArXiv ID:** 2512.20936
**Authors:** Hongxing Fan, Shuyu Zhao, Jiayang Ao, Lu Sheng

**Abstract:**  Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

**Comment:** Matches criterion 2: Introduces a collaborative multi-agent reasoning framework for amodal completion, with a new MLLM-based evaluation metric (MAC-Score). Focuses on vision-language reasoning and evaluation.
**Relevance:** 8
**Novelty:** 7

---

## 11. [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618) <a id="link11"></a>
**ArXiv ID:** 2512.20618
**Authors:** Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen

**Abstract:**  Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

**Comment:** Matches criterion 2: Proposes a multi-agent system for long video reasoning using multimodal LLMs, with reinforcement learning for improved planning and reasoning. Also relevant to criterion 3 as it introduces new datasets and a novel agent-based approach.
**Relevance:** 8
**Novelty:** 7

---

## 12. [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094) <a id="link12"></a>
**ArXiv ID:** 2512.21094
**Authors:** Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu

**Abstract:**  Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

**Comment:** Matches criterion 3: Proposes T2AV-Compass, a unified benchmark for text-to-audio-video generation, including a novel evaluation protocol using MLLMs as judges. This is a new benchmark for multi-modal generative models.
**Relevance:** 8
**Novelty:** 7

---

## 13. [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284) <a id="link13"></a>
**ArXiv ID:** 2512.21284
**Authors:** Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan

**Abstract:**  Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

**Comment:** SpikeSurgSeg proposes a spike-driven video Transformer for surgical scene segmentation, with a masked autoencoding pretraining strategy for SNNs. This is a novel method for efficient spatial understanding in a real-time, resource-constrained setting, which aligns with criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications).
**Relevance:** 7
**Novelty:** 7

---

## 14. [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003) <a id="link14"></a>
**ArXiv ID:** 2512.21003
**Authors:** Xiangzuo Wu, Chengwei Ren, Jun Zhou, Xiu Li, Yuan Liu

**Abstract:**  Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

**Comment:** Matches criterion 4: Proposes a feed-forward multi-view inverse rendering framework with attention mechanisms for vision foundation models, and a consistency-based finetuning strategy for real-world generalization.
**Relevance:** 7
**Novelty:** 6

---

## 15. [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078) <a id="link15"></a>
**ArXiv ID:** 2512.21078
**Authors:** Tianchen Deng, Xun Chen, Ziming Li, Hongming Shen, Danwei Wang, Javier Civera, Hesheng Wang

**Abstract:**  Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

**Comment:** Matches criterion 1: UniPR-3D introduces a new architecture for visual place recognition that integrates multi-view 3D representations, improving spatial understanding and generalization.
**Relevance:** 7
**Novelty:** 6

---

## 16. [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976) <a id="link16"></a>
**ArXiv ID:** 2512.20976
**Authors:** Zeqing Song, Zhongmiao Yan, Junyuan Deng, Songpengcheng Xia, Xiang Mu, Jingyi Xu, Qi Wu, Ling Pei

**Abstract:**  Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

**Comment:** Matches criterion 1: Proposes XGrid-Mapping, a new hybrid explicit-implicit grid framework for efficient neural LiDAR mapping, improving spatial understanding for embodied agents. Also relevant to spatial intelligence.
**Relevance:** 7
**Novelty:** 6

---

## 17. [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833) <a id="link17"></a>
**ArXiv ID:** 2512.20833
**Authors:** Vidit Agrawal (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), John Peters (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Tyler N. Thompson (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Mohammad Vali Sanian (Institute for Molecular Medicine Finland, University of Helsinki, Helsinki, Finland), Chau Pham (Boston University, Boston, MA, USA), Nikita Moshkov (Institute of Computational Biology, Helmholtz Munich, Neuherberg, Germany), Arshad Kazi (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Aditya Pillai (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA), Jack Freeman (Morgridge Institute for Research, Madison, WI, USA), Byunguk Kang (Massachusetts Institute of Technology, Cambridge, MA, USA, Broad Institute of MIT and Harvard, Cambridge, MA, USA), Samouil L. Farhi (Broad Institute of MIT and Harvard, Cambridge, MA, USA), Ernest Fraenkel (Massachusetts Institute of Technology, Cambridge, MA, USA), Ron Stewart (Morgridge Institute for Research, Madison, WI, USA), Lassi Paavolainen (Institute for Molecular Medicine Finland, University of Helsinki, Helsinki, Finland), Bryan A. Plummer (Boston University, Boston, MA, USA), Juan C. Caicedo (Morgridge Institute for Research, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA)

**Abstract:**  Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

**Comment:** This paper introduces CHAMMI-75, a new open-access dataset for multi-channel microscopy images, enabling channel-adaptive models for cellular morphology. This is relevant to criterion 4 (vision foundation models and applications), as it supports the development of more generalizable vision models in bioimaging.
**Relevance:** 5
**Novelty:** 6

---

## 18. [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264) <a id="link18"></a>
**ArXiv ID:** 2512.21264
**Authors:** Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong, Beining Wu, Jie Dong, Feiwei Qin, Yunkang Cao, Qiyuan Tian

**Abstract:**  Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

**Comment:** This paper presents a unified anomaly detection framework for incomplete multi-sequence MRI using a dual-pathway DINOv2 encoder and feature distribution alignment. While it uses a vision foundation model (DINOv2), the focus is on medical anomaly detection, not on spatial intelligence for embodied agents or new VLLMs/MLLMs. Some relevance to criterion 4.
**Relevance:** 5
**Novelty:** 6

---

## 19. [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135) <a id="link19"></a>
**ArXiv ID:** 2512.20135
**Authors:** Zhuo Yang, Yeyun Chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li

**Abstract:**  Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

**Comment:** This paper presents MolAct, an agentic RL framework for molecular editing and optimization, formalizing molecular design as an agentic RL problem with LLM agents. While it is an interesting application of RL and LLMs, it does not directly match any of the four criteria, as it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 7

---

## 20. [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061) <a id="link20"></a>
**ArXiv ID:** 2512.20061
**Authors:** Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal

**Abstract:**  Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

**Comment:** This paper investigates scaling RL for content moderation with LLMs, focusing on RL training and reward shaping for policy-aligned classifiers. While it is an interesting application of RL and LLMs, it does not directly match any of the four criteria, as it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 21. [Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising](https://arxiv.org/abs/2512.21038) <a id="link21"></a>
**ArXiv ID:** 2512.21038
**Authors:** Yiwen Shan, Haiyu Zhao, Peng Hu, Xi Peng, Yuanbiao Gou

**Abstract:**  Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.

**Comment:** Next-Scale Prediction (NSP) introduces a self-supervised paradigm for real-world image denoising, decoupling noise decorrelation from detail preservation. While it is a clever statistical trick in image processing, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 22. [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019) <a id="link22"></a>
**ArXiv ID:** 2512.21019
**Authors:** Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao

**Abstract:**  State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

**Comment:** This paper proposes a novel and efficient video defense framework against 3D-field Talking Face Generation (TFG) methods, introducing a similarity-guided parameter sharing mechanism and a multi-scale dual-domain attention module. While it involves spatial perturbations and attention mechanisms, it is primarily focused on privacy and defense, not on spatial understanding or embodied agents. No direct match to the listed criteria.
**Relevance:** 3
**Novelty:** 6

---

## 23. [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104) <a id="link23"></a>
**ArXiv ID:** 2512.21104
**Authors:** Chao Gong, Dong Li, Yingwei Pan, Jingjing Chen, Ting Yao, Tao Mei

**Abstract:**  Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

**Comment:** Does not directly match any specific criterion. Focuses on image inpainting with prompt alignment, which is more about generative image editing than spatial intelligence, embodied AI, or VLLMs.
**Relevance:** 3
**Novelty:** 5

---

## 24. [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053) <a id="link24"></a>
**ArXiv ID:** 2512.21053
**Authors:** Zibin Liu, Banglei Guan, Yang Shang, Shunkun Liang, Zhenbao Yu, Qifeng Yu

**Abstract:**  Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

**Comment:** Does not directly match any specific criterion. Focuses on 6DoF object pose tracking with event cameras, which is more about low-level vision and sensor fusion than spatial intelligence or embodied AI benchmarks.
**Relevance:** 3
**Novelty:** 5

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.