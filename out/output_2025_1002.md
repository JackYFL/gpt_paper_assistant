# Personalized Daily ArXiv Papers 10/02/2025
Total relevant papers: 33

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory](#link0)
**Authors:** Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, Jieneng Chen

1. [BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](#link1)
**Authors:** Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, Zehuan Yuan

2. [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](#link2)
**Authors:** Xianjie Liu, Yiman Hu, Yixiong Zou, Liang Wu, Jian Xu, Bo Zheng

3. [EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations](#link3)
**Authors:** Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, Junwei Liang

4. [OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding](#link4)
**Authors:** Jiancong Xie, Wenjin Wang, Zhuomeng Zhang, Zihan Liu, Qi Liu, Ke Feng, Zixun Sun, Yuedong Yang

5. [PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset](#link5)
**Authors:** Thomas Campagnolo, Ezio Malis, Philippe Martinet, Gaetan Bahl

6. [A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models](#link6)
**Authors:** Leah Bar, Liron Mor Yosef, Shai Zucker, Neta Shoham, Inbar Seroussi, Nir Sochen

7. [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](#link7)
**Authors:** Diego Ortiz Barbosa, Mohit Agrawal, Yash Malegaonkar, Luis Burbano, Axel Andersson, Gy\"orgy D\'an, Henrik Sandberg, Alvaro A. Cardenas

8. [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](#link8)
**Authors:** Junjie Li, Ziao Wang, Jianghong Ma, Xiaofeng Zhang

9. [Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning](#link9)
**Authors:** Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong

10. [Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation](#link10)
**Authors:** Yunbo Xu, Xuesong Zhang, Jia Li, Zhenzhen Hu, Richang Hong

11. [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](#link11)
**Authors:** Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin

12. [A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features](#link12)
**Authors:** Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann

13. [Multi-Objective Task-Aware Predictor for Image-Text Alignment](#link13)
**Authors:** Eunki Kim, Na Min An, James Thorne, Hyunjung Shim

14. [VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors](#link14)
**Authors:** Atif Belal, Heitor R. Medeiros, Marco Pedersoli, Eric Granger

15. [From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation](#link15)
**Authors:** Fan Yang, Zhiyang Chen, Yousong Zhu, Xin Li, Jinqiao Wang

16. [Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model](#link16)
**Authors:** Hyun-kyu Ko, Youbin Kim, Jihyeon Park, Dongheok Park, Gyeongjin Kang, Wonjun Cho, Hyung Yi, Eunbyung Park

17. [Multi-level Dynamic Style Transfer for NeRFs](#link17)
**Authors:** Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha

18. [Semantic-Driven AI Agent Communications: Challenges and Solutions](#link18)
**Authors:** Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu

19. [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](#link19)
**Authors:** Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo

20. [Hierarchical Reasoning Model: A Critical Supplementary Material](#link20)
**Authors:** Renee Ge, Qianli Liao, Tomaso Poggio

21. [MOLM: Mixture of LoRA Markers](#link21)
**Authors:** Samar Fares, Nurbek Tastan, Noor Hussein, Karthik Nandakumar

22. [Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning](#link22)
**Authors:** Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama

23. [Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation](#link23)
**Authors:** Taeyun Woo, Jinah Park, Tae-Kyun Kim

24. [UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs](#link24)
**Authors:** Mengfei Xia, Nan Xue, Jiapeng Zhu, Yujun Shen

25. [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](#link25)
**Authors:** Federico Tiblias, Irina Bigoulaeva, Jingcheng Niu, Simone Balloccu, Iryna Gurevych

26. [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](#link26)
**Authors:** Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng

27. [Equivariant Splitting: Self-supervised learning from incomplete data](#link27)
**Authors:** Victor Sechaud, J\'er\'emy Scanvic, Quentin Barth\'elemy, Patrice Abry, Juli\'an Tachella

28. [Feature Identification for Hierarchical Contrastive Learning](#link28)
**Authors:** Julius Ott, Nastassia Vysotskaya, Huawei Sun, Lorenzo Servadei, Robert Wille

29. [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](#link29)
**Authors:** Wei Liu, Haomei Xu, Bingqing Liu, Zhiying Deng, Haozhao Wang, Jun Wang, Ruixuan Li, Yee Whye Teh, Wee Sun Lee

30. [Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network](#link30)
**Authors:** Shaocong Zhu, Zhiwei Li, Xinghua Li, Huanfeng Shen

31. [Beyond one-hot encoding? Journey into compact encoding for large multi-class segmentation](#link31)
**Authors:** Aaron Kujawa, Thomas Booth, Tom Vercauteren

32. [ZQBA: Zero Query Black-box Adversarial Attack](#link32)
**Authors:** Joana C. Costa, Tiago Roxo, Hugo Proen\c{c}a, Pedro R. M. In\'acio

---
## 0. [EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory](https://arxiv.org/abs/2510.01183) <a id="link0"></a>
**ArXiv ID:** 2510.01183
**Authors:** Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, Jieneng Chen

**Abstract:**  Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.

**Comment:** Matches criterion 1 (spatial intelligence for embodied agents), criterion 3 (new benchmark for embodied AI), and criterion 4 (vision foundation models and applications). EvoWorld introduces a world model with evolving 3D memory for panoramic video generation and spatially consistent long-horizon exploration, plus a new benchmark.
**Relevance:** 10
**Novelty:** 9

---

## 1. [BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](https://arxiv.org/abs/2510.00438) <a id="link1"></a>
**ArXiv ID:** 2510.00438
**Authors:** Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, Zehuan Yuan

**Abstract:**  Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.

**Comment:** Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). The paper introduces BindWeave, a framework for subject-consistent video generation using a pretrained MLLM for cross-modal reasoning, addressing complex spatial and temporal relationships in video generation.
**Relevance:** 10
**Novelty:** 8

---

## 2. [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054) <a id="link2"></a>
**ArXiv ID:** 2510.00054
**Authors:** Xianjie Liu, Yiman Hu, Yixiong Zou, Liang Wu, Jian Xu, Bo Zheng

**Abstract:**  Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use "zoom in" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this "zoom in" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.

**Comment:** Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). The paper proposes HiDe, a new training-free framework for improving high-resolution image understanding in MLLMs, with a novel hierarchical decoupling approach and strong empirical results.
**Relevance:** 10
**Novelty:** 8

---

## 3. [EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations](https://arxiv.org/abs/2510.00405) <a id="link3"></a>
**ArXiv ID:** 2510.00405
**Authors:** Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, Junwei Liang

**Abstract:**  Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.

**Comment:** Matches criterion 3 (embodied AI, new benchmark for trajectory prediction under realistic, noisy ego-view observations). Introduces EgoTraj-Bench and a new model (BiFlow) for robust trajectory prediction in ego-centric settings.
**Relevance:** 10
**Novelty:** 8

---

## 4. [OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding](https://arxiv.org/abs/2510.00069) <a id="link4"></a>
**ArXiv ID:** 2510.00069
**Authors:** Jiancong Xie, Wenjin Wang, Zhuomeng Zhang, Zihan Liu, Qi Liu, Ke Feng, Zixun Sun, Yuedong Yang

**Abstract:**  Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities. However, evaluating their capacity for human-like understanding in One-Image Guides remains insufficiently explored. One-Image Guides are a visual format combining text, imagery, and symbols to present reorganized and structured information for easier comprehension, which are specifically designed for human viewing and inherently embody the characteristics of human perception and understanding. Here, we present OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding across diverse domains. To reduce the cost of manual annotation, we developed a semi-automated annotation pipeline in which multiple intelligent agents collaborate to generate preliminary image descriptions, assisting humans in constructing image-text pairs. With OIG-Bench, we have conducted a comprehensive evaluation of 29 state-of-the-art MLLMs, including both proprietary and open-source models. The results show that Qwen2.5-VL-72B performs the best among the evaluated models, with an overall accuracy of 77%. Nevertheless, all models exhibit notable weaknesses in semantic understanding and logical reasoning, indicating that current MLLMs still struggle to accurately interpret complex visual-text relationships. In addition, we also demonstrate that the proposed multi-agent annotation system outperforms all MLLMs in image captioning, highlighting its potential as both a high-quality image description generator and a valuable tool for future dataset construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.

**Comment:** Matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for MLLMs with a novel annotation pipeline). Presents a new benchmark (OIG-Bench) for evaluating MLLMs on One-Image Guides and a multi-agent annotation system.
**Relevance:** 10
**Novelty:** 8

---

## 5. [PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset](https://arxiv.org/abs/2510.00818) <a id="link5"></a>
**ArXiv ID:** 2510.00818
**Authors:** Thomas Campagnolo, Ezio Malis, Philippe Martinet, Gaetan Bahl

**Abstract:**  Understanding how natural language phrases correspond to specific regions in images is a key challenge in multimodal semantic segmentation. Recent advances in phrase grounding are largely limited to single-view images, neglecting the rich geometric cues available in stereo vision. For this, we introduce PhraseStereo, the first novel dataset that brings phrase-region segmentation to stereo image pairs. PhraseStereo builds upon the PhraseCut dataset by leveraging GenStereo to generate accurate right-view images from existing single-view data, enabling the extension of phrase grounding into the stereo domain. This new setting introduces unique challenges and opportunities for multimodal learning, particularly in leveraging depth cues for more precise and context-aware grounding. By providing stereo image pairs with aligned segmentation masks and phrase annotations, PhraseStereo lays the foundation for future research at the intersection of language, vision, and 3D perception, encouraging the development of models that can reason jointly over semantics and geometry. The PhraseStereo dataset will be released online upon acceptance of this work.

**Comment:** Matches criterion 3 (new benchmark for embodied AI/simulator-related, novel angle). Introduces the first open-vocabulary stereo image segmentation dataset, enabling phrase grounding in stereo pairs and leveraging depth cues for multimodal learning.
**Relevance:** 9
**Novelty:** 8

---

## 6. [A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models](https://arxiv.org/abs/2510.00666) <a id="link6"></a>
**ArXiv ID:** 2510.00666
**Authors:** Leah Bar, Liron Mor Yosef, Shai Zucker, Neta Shoham, Inbar Seroussi, Nir Sochen

**Abstract:**  The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models, the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold coordinate space is considered uninteresting and is predefined or considered uniform. This study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.

**Comment:** Matches criterion 4 (vision foundation models and generative modeling). Proposes a geometric-probabilistic unification for generative AI, interprets diffusion models, and introduces a new deterministic model (MPPM) that outperforms LDMs.
**Relevance:** 9
**Novelty:** 8

---

## 7. [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167) <a id="link7"></a>
**ArXiv ID:** 2510.00167
**Authors:** Diego Ortiz Barbosa, Mohit Agrawal, Yash Malegaonkar, Luis Burbano, Axel Andersson, Gy\"orgy D\'an, Henrik Sandberg, Alvaro A. Cardenas

**Abstract:**  Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous aerial systems.

**Comment:** Matches criterion 3 (embodied AI, new method, simulator-related, novel angle). Demonstrates embodied AI for adaptive drone landing in a simulated urban environment, leveraging VLMs for real-time decision-making.
**Relevance:** 9
**Novelty:** 7

---

## 8. [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](https://arxiv.org/abs/2510.00040) <a id="link8"></a>
**ArXiv ID:** 2510.00040
**Authors:** Junjie Li, Ziao Wang, Jianghong Ma, Xiaofeng Zhang

**Abstract:**  Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.

**Comment:** Matches criterion 2 (VLLM/MLLM) and criterion 4 (vision foundation models and applications). Proposes a new data curation paradigm for VLMs based on intrinsic capability analysis, showing strong empirical results with less data.
**Relevance:** 8
**Novelty:** 8

---

## 9. [Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning](https://arxiv.org/abs/2510.00072) <a id="link9"></a>
**ArXiv ID:** 2510.00072
**Authors:** Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong

**Abstract:**  We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.

**Comment:** Matches criterion 2 (VLLMs/MLLMs): Introduces a post-training framework for geospatial reasoning in vision-language models. Also relevant to criterion 4 (vision foundation models and applications).
**Relevance:** 8
**Novelty:** 8

---

## 10. [Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation](https://arxiv.org/abs/2510.00604) <a id="link10"></a>
**ArXiv ID:** 2510.00604
**Authors:** Yunbo Xu, Xuesong Zhang, Jia Li, Zhenzhen Hu, Richang Hong

**Abstract:**  Following language instructions, vision-language navigation (VLN) agents are tasked with navigating unseen environments. While augmenting multifaceted visual representations has propelled advancements in VLN, the significance of foreground and background in visual observations remains underexplored. Intuitively, foreground regions provide semantic cues, whereas the background encompasses spatial connectivity information. Inspired on this insight, we propose a Consensus-driven Online Feature Augmentation strategy (COFA) with alternative foreground and background features to facilitate the navigable generalization. Specifically, we first leverage semantically-enhanced landmark identification to disentangle foreground and background as candidate augmented features. Subsequently, a consensus-driven online augmentation strategy encourages the agent to consolidate two-stage voting results on feature preferences according to diverse instructions and navigational locations. Experiments on REVERIE and R2R demonstrate that our online foreground-background augmentation boosts the generalization of baseline and attains state-of-the-art performance.

**Comment:** Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (embodied AI, new method): Proposes a novel method for vision-language navigation agents by disentangling foreground and background features for better spatial generalization.
**Relevance:** 9
**Novelty:** 7

---

## 11. [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](https://arxiv.org/abs/2510.00690) <a id="link11"></a>
**ArXiv ID:** 2510.00690
**Authors:** Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin

**Abstract:**  Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.

**Comment:** Matches criterion 2 (new VLLM/MLLM methods) and criterion 4 (vision foundation models and applications). Proposes a new RL-based optimization framework for aligning VLMs in complex reasoning, with adaptive curriculum and clipping.
**Relevance:** 8
**Novelty:** 7

---

## 12. [A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features](https://arxiv.org/abs/2510.00978) <a id="link12"></a>
**ArXiv ID:** 2510.00978
**Authors:** Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann

**Abstract:**  Visually localizing an image, i.e., estimating its camera pose, requires building a scene representation that serves as a visual map. The representation we choose has direct consequences towards the practicability of our system. Even when starting from mapping images with known camera poses, state-of-the-art approaches still require hours of mapping time in the worst case, and several minutes in the best. This work raises the question whether we can achieve competitive accuracy much faster. We introduce FastForward, a method that creates a map representation and relocalizes a query image on-the-fly in a single feed-forward pass. At the core, we represent multiple mapping images as a collection of features anchored in 3D space. FastForward utilizes these mapping features to predict image-to-scene correspondences for the query image, enabling the estimation of its camera pose. We couple FastForward with image retrieval and achieve state-of-the-art accuracy when compared to other approaches with minimal map preparation time. Furthermore, FastForward demonstrates robust generalization to unseen domains, including challenging large-scale outdoor environments.

**Comment:** Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents) and criterion 4 (vision foundation models and applications). FastForward introduces a feed-forward, feature-based approach to camera localization, enabling rapid mapping and robust generalization, which is a significant spatial intelligence advance.
**Relevance:** 8
**Novelty:** 7

---

## 13. [Multi-Objective Task-Aware Predictor for Image-Text Alignment](https://arxiv.org/abs/2510.00766) <a id="link13"></a>
**ArXiv ID:** 2510.00766
**Authors:** Eunki Kim, Na Min An, James Thorne, Hyunjung Shim

**Abstract:**  Evaluating image-text alignment while reflecting human preferences across multiple aspects is a significant issue for the development of reliable vision-language applications. It becomes especially crucial in real-world scenarios where multiple valid descriptions exist depending on contexts or user needs. However, research progress is hindered by the lack of comprehensive benchmarks and existing evaluation predictors lacking at least one of these key properties: (1) Alignment with human judgments, (2) Long-sequence processing, (3) Inference efficiency, and (4) Applicability to multi-objective scoring. To address these challenges, we propose a plug-and-play architecture to build a robust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of both multi and single-objective scoring. MULTI-TAP can produce a single overall score, utilizing a reward head built on top of a large vision-language model (LVLMs). We show that MULTI-TAP is robust in terms of application to different LVLM architectures, achieving significantly higher performance than existing metrics and even on par with the GPT-4o-based predictor, G-VEval, with a smaller size (7-8B). By training a lightweight ridge regression layer on the frozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained scores for multiple human-interpretable objectives. MULTI-TAP performs better than VisionREWARD, a high-performing multi-objective reward model, in both performance and efficiency on multi-objective benchmarks and our newly released text-image-to-text dataset, EYE4ALL. Our new dataset, consisting of chosen/rejected human preferences (EYE4ALLPref) and human-annotated fine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a foundation for developing more accessible AI systems by capturing the underlying preferences of users, including blind and low-vision (BLV) individuals.

**Comment:** Matches criterion 2 (VLLMs/MLLMs): Proposes a multi-objective task-aware predictor for image-text alignment, built on top of large vision-language models. Also relevant to criterion 4 (vision foundation models and applications).
**Relevance:** 8
**Novelty:** 7

---

## 14. [VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors](https://arxiv.org/abs/2510.00458) <a id="link14"></a>
**ArXiv ID:** 2510.00458
**Authors:** Atif Belal, Heitor R. Medeiros, Marco Pedersoli, Eric Granger

**Abstract:**  Vision-language object detectors (VLODs) such as YOLO-World and Grounding DINO achieve impressive zero-shot recognition by aligning region proposals with text representations. However, their performance often degrades under domain shift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs that leverages dense proposal overlap and image-conditioned prompt scores. First, an IoU-weighted entropy objective is proposed that concentrates adaptation on spatially coherent proposal clusters and reduces confirmation bias from isolated boxes. Second, image-conditioned prompt selection is introduced, which ranks prompts by image-level compatibility and fuses the most informative prompts with the detector logits. Our benchmarking across diverse distribution shifts -- including stylized domains, driving scenes, low-light conditions, and common corruptions -- shows the effectiveness of our method on two state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent improvements over the zero-shot and TTA baselines. Code : https://github.com/imatif17/VLOD-TTA

**Comment:** Matches criterion 2 (VLLMs/MLLMs): Proposes a test-time adaptation framework for vision-language object detectors (VLODs), specifically for models like YOLO-World and Grounding DINO. Also relevant to criterion 4 (vision foundation models and applications).
**Relevance:** 8
**Novelty:** 7

---

## 15. [From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation](https://arxiv.org/abs/2510.00806) <a id="link15"></a>
**ArXiv ID:** 2510.00806
**Authors:** Fan Yang, Zhiyang Chen, Yousong Zhu, Xin Li, Jinqiao Wang

**Abstract:**  Current video generation models produce physically inconsistent motion that violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for physics-aware image-to-video generation. First, we employ a Vision Language Model to predict coarse-grained motion trajectories that maintain consistency with real-world physics. Second, these trajectories guide video generation through attention-based mechanisms for fine-grained motion refinement. We build a trajectory prediction dataset based on video tracking data with realistic motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of 545 on UCF-101 and 539 on MSR-VTT.

**Comment:** Matches criterion 2 (VLLMs/MLLMs): Proposes a vision-language model for trajectory forecasting and controlled video generation, using VLMs for physics-aware motion prediction. Also relevant to criterion 4 (vision foundation models and applications).
**Relevance:** 8
**Novelty:** 7

---

## 16. [Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model](https://arxiv.org/abs/2510.00862) <a id="link16"></a>
**ArXiv ID:** 2510.00862
**Authors:** Hyun-kyu Ko, Youbin Kim, Jihyeon Park, Dongheok Park, Gyeongjin Kang, Wonjun Cho, Hyung Yi, Eunbyung Park

**Abstract:**  State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.

**Comment:** Matches criterion 1 (methodological improvements to spatial understanding) and is relevant to criterion 4 (vision foundation models and applications). Proposes a hybrid SSM/attention architecture for video super-resolution, with a novel Gather-Scatter mechanism for spatial-temporal feature alignment.
**Relevance:** 7
**Novelty:** 7

---

## 17. [Multi-level Dynamic Style Transfer for NeRFs](https://arxiv.org/abs/2510.00592) <a id="link17"></a>
**ArXiv ID:** 2510.00592
**Authors:** Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha

**Abstract:**  As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed. However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization. In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module. Particularly, we propose a multi-level feature adaptor that helps generate a multi-level feature grid representation from the content radiance field, effectively capturing the multi-scale spatial structure of the scene. In addition, we present a dynamic style injection module that learns to extract relevant style features and adaptively integrates them into the content patterns. The stylized multi-level features are then transformed into the final stylized view through our proposed multi-level cascade decoder. Furthermore, we extend our 3D style transfer method to support omni-view style transfer using 3D style references. Extensive experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D style transfer, preserving multi-scale spatial structures while effectively transferring stylistic characteristics.

**Comment:** Partially matches criterion 1 (spatial understanding in NeRFs) and criterion 4 (vision foundation models and their application to 3D style transfer). Proposes a new method for multi-level dynamic style transfer in NeRFs, improving spatial structure preservation.
**Relevance:** 7
**Novelty:** 7

---

## 18. [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381) <a id="link18"></a>
**ArXiv ID:** 2510.00381
**Authors:** Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu

**Abstract:**  With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication, which conveys task-relevant meaning rather than raw data, offers a promising solution. However, its practical deployment remains constrained by dynamic environments and limited resources. To address these issues, this article proposes a semantic-driven AI agent communication framework and develops three enabling techniques. First, semantic adaptation transmission applies fine-tuning with real or generative samples to efficiently adapt models to varying environments. Second, semantic lightweight transmission incorporates pruning, quantization, and perception-aware sampling to reduce model complexity and alleviate computational burden on edge agents. Third, semantic self-evolution control employs distributed hierarchical decision-making to optimize multi-dimensional resources, enabling robust multi-agent collaboration in dynamic environments. Simulation results show that the proposed solutions achieve faster convergence and stronger robustness, while the proposed distributed hierarchical optimization method significantly outperforms conventional decision-making schemes, highlighting its potential for AI agent communication networks.

**Comment:** Partially matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (multi-agent communication framework for AI agents, with new methods for semantic communication). Focuses on semantic-driven communication and distributed decision-making for AI agents.
**Relevance:** 6
**Novelty:** 7

---

## 19. [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](https://arxiv.org/abs/2510.00778) <a id="link19"></a>
**ArXiv ID:** 2510.00778
**Authors:** Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo

**Abstract:**  Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance.In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.

**Comment:** Some relevance to criterion 4 (vision foundation models and applications) as it deals with diffusion models and adversarial defenses, but not a direct match to any criterion. Focuses on security/privacy in generative models.
**Relevance:** 4
**Novelty:** 6

---

## 20. [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355) <a id="link20"></a>
**ArXiv ID:** 2510.00355
**Authors:** Renee Ge, Qianli Liao, Tomaso Poggio

**Abstract:**  Transformers have demonstrated remarkable performance in natural language processing and related domains, as they largely focus on sequential, autoregressive next-token prediction tasks. Yet, they struggle in logical reasoning, not necessarily because of a fundamental limitation of these models, but possibly due to the lack of exploration of more creative uses, such as latent space and recurrent reasoning. An emerging exploration in this direction is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a novel type of recurrent reasoning in the latent space of transformers, achieving remarkable performance on a wide range of 2D reasoning tasks. Despite the promising results, this line of models is still at an early stage and calls for in-depth investigation. In this work, we perform a critical review on this class of models, examine key design choices and present intriguing variants that achieve significantly better performance on the Sudoku-Extreme and Maze-Hard tasks than previously reported. Our results also raise surprising observations and intriguing directions for further research.

**Comment:** Some relevance to criterion 2 (reasoning in transformer-based models), but not directly about VLLMs/MLLMs or vision. Focuses on logical reasoning in transformers with hierarchical latent space reasoning.
**Relevance:** 4
**Novelty:** 6

---

## 21. [MOLM: Mixture of LoRA Markers](https://arxiv.org/abs/2510.00293) <a id="link21"></a>
**ArXiv ID:** 2510.00293
**Authors:** Samar Fares, Nurbek Tastan, Noor Hussein, Karthik Nandakumar

**Abstract:**  Generative models can generate photorealistic images at scale. This raises urgent concerns about the ability to detect synthetically generated images and attribute these images to specific sources. While watermarking has emerged as a possible solution, existing methods remain fragile to realistic distortions, susceptible to adaptive removal, and expensive to update when the underlying watermarking key changes. We propose a general watermarking framework that formulates the encoding problem as key-dependent perturbation of the parameters of a generative model. Within this framework, we introduce Mixture of LoRA Markers (MOLM), a routing-based instantiation in which binary keys activate lightweight LoRA adapters inside residual and attention blocks. This design avoids key-specific re-training and achieves the desired properties such as imperceptibility, fidelity, verifiability, and robustness. Experiments on Stable Diffusion and FLUX show that MOLM preserves image quality while achieving robust key recovery against distortions, compression and regeneration, averaging attacks, and black-box adversarial attacks on the extractor.

**Comment:** Does not directly match any of the four criteria. The paper is about watermarking for generative models, which is tangential to vision foundation models but not a direct methodological or empirical advance in spatial understanding, VLLMs/MLLMs, or embodied AI.
**Relevance:** 3
**Novelty:** 6

---

## 22. [Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning](https://arxiv.org/abs/2510.00570) <a id="link22"></a>
**ArXiv ID:** 2510.00570
**Authors:** Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama

**Abstract:**  Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.

**Comment:** Some relevance to criterion 4 (vision foundation models and applications) via multi-task learning with MoE, but not a direct match to any criterion. Focuses on LoRA-based MoE for multi-task learning.
**Relevance:** 4
**Novelty:** 5

---

## 23. [Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation](https://arxiv.org/abs/2510.00527) <a id="link23"></a>
**ArXiv ID:** 2510.00527
**Authors:** Taeyun Woo, Jinah Park, Tae-Kyun Kim

**Abstract:**  Deterministic models for 3D hand pose reconstruction, whether single-staged or cascaded, struggle with pose ambiguities caused by self-occlusions and complex hand articulations. Existing cascaded approaches refine predictions in a coarse-to-fine manner but remain deterministic and cannot capture pose uncertainties. Recent probabilistic methods model pose distributions yet are restricted to single-stage estimation, which often fails to produce accurate 3D reconstructions without refinement. To address these limitations, we propose a coarse-to-fine cascaded diffusion framework that combines probabilistic modeling with cascaded refinement. The first stage is a joint diffusion model that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a joint sample. By training Mesh LDM with diverse joint hypotheses in a learned latent space, our framework learns distribution-aware joint-mesh relationships and robust hand priors. Furthermore, the cascaded design mitigates the difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate that our method achieves state-of-the-art performance while effectively modeling pose distributions.

**Comment:** Does not match any specific criteria. Focuses on hand pose estimation using diffusion models, not spatial intelligence on embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 24. [UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs](https://arxiv.org/abs/2510.00624) <a id="link24"></a>
**ArXiv ID:** 2510.00624
**Authors:** Mengfei Xia, Nan Xue, Jiapeng Zhu, Yujun Shen

**Abstract:**  Adversarial training turns out to be the key to one-step generation, especially for Generative Adversarial Network (GAN) and diffusion model distillation. Yet in practice, GAN training hardly converges properly and struggles in mode collapse. In this work, we quantitatively analyze the extent of Nash equilibrium in GAN training, and conclude that redundant shortcuts by inputting condition in $D$ disables meaningful knowledge extraction. We thereby propose to employ an unconditional discriminator (UCD), in which $D$ is enforced to extract more comprehensive and robust features with no condition injection. In this way, $D$ is able to leverage better knowledge to supervise $G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee on compatibility with vanilla GAN theory indicates that UCD can be implemented in a plug-in manner. Extensive experiments confirm the significant performance improvements with high efficiency. For instance, we achieved \textbf{1.47 FID} on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art one-step diffusion models. The code will be made publicly available.

**Comment:** Does not match any specific criteria. Focuses on GAN training and discriminator design, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 25. [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025) <a id="link25"></a>
**ArXiv ID:** 2510.01025
**Authors:** Federico Tiblias, Irina Bigoulaeva, Jingcheng Niu, Simone Balloccu, Iryna Gurevych

**Abstract:**  The linear representation hypothesis states that language models (LMs) encode concepts as directions in their latent space, forming organized, multidimensional manifolds. Prior efforts focus on discovering specific geometries for specific features, and thus lack generalization. We introduce Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to automatically discover feature manifolds. We apply SMDS to temporal reasoning as a case study, finding that different features form various geometric structures such as circles, lines, and clusters. SMDS reveals many insights on these structures: they consistently reflect the properties of the concepts they represent; are stable across model families and sizes; actively support reasoning in models; and dynamically reshape in response to context changes. Together, our findings shed light on the functional role of feature manifolds, supporting a model of entity-based reasoning in which LMs encode and transform structured representations.

**Comment:** Does not match any specific criteria. Focuses on feature manifold discovery in LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 26. [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088) <a id="link26"></a>
**ArXiv ID:** 2510.01088
**Authors:** Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng

**Abstract:**  Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal--models intrinsically "know" when to refuse. We introduce Safety Instincts Reinforcement Learning (SIRL), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. SIRL teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to adaptive attacks. Using only 15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.

**Comment:** Does not match any specific criteria. Focuses on LLM safety and internal reward signals, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 27. [Equivariant Splitting: Self-supervised learning from incomplete data](https://arxiv.org/abs/2510.00929) <a id="link27"></a>
**ArXiv ID:** 2510.00929
**Authors:** Victor Sechaud, J\'er\'emy Scanvic, Quentin Barth\'elemy, Patrice Abry, Juli\'an Tachella

**Abstract:**  Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models.

**Comment:** Does not match any specific criteria. Focuses on self-supervised learning for inverse problems, which is relevant to computer vision but not directly to the listed criteria.
**Relevance:** 3
**Novelty:** 6

---

## 28. [Feature Identification for Hierarchical Contrastive Learning](https://arxiv.org/abs/2510.00837) <a id="link28"></a>
**ArXiv ID:** 2510.00837
**Authors:** Julius Ott, Nastassia Vysotskaya, Huawei Sun, Lorenzo Servadei, Robert Wille

**Abstract:**  Hierarchical classification is a crucial task in many applications, where objects are organized into multiple levels of categories. However, conventional classification approaches often neglect inherent inter-class relationships at different hierarchy levels, thus missing important supervisory signals. Thus, we propose two novel hierarchical contrastive learning (HMLC) methods. The first, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an attention mechanism to capture hierarchy-specific features (A-HMLC), imitating human processing. Our approach explicitly models inter-class relationships and imbalanced class distribution at higher hierarchy levels, enabling fine-grained clustering across all hierarchy levels. On the competitive CIFAR100 and ModelNet40 datasets, our method achieves state-of-the-art performance in linear evaluation, outperforming existing hierarchical contrastive learning methods by 2 percentage points in terms of accuracy. The effectiveness of our approach is backed by both quantitative and qualitative results, highlighting its potential for applications in computer vision and beyond.

**Comment:** Does not match any specific criteria. Focuses on hierarchical contrastive learning, which is relevant to computer vision but not directly to the listed criteria.
**Relevance:** 3
**Novelty:** 6

---

## 29. [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](https://arxiv.org/abs/2510.00625) <a id="link29"></a>
**ArXiv ID:** 2510.00625
**Authors:** Wei Liu, Haomei Xu, Bingqing Liu, Zhiying Deng, Haozhao Wang, Jun Wang, Ruixuan Li, Yee Whye Teh, Wee Sun Lee

**Abstract:**  Large language models (LLMs) inevitably encode outdated or incorrect knowledge. Updating, deleting, and forgetting such knowledge is important for alignment, safety, and other issues. To address this issue, model editing has emerged as a promising paradigm: by precisely editing a small subset of parameters such that a specific fact is updated while preserving other knowledge. Despite its great success reported in previous papers, we find the apparent reliability of editing rests on a fragile foundation and the current literature is largely driven by illusory success. The fundamental goal of steering the model's output toward a target with minimal modification would encourage exploiting hidden shortcuts, rather than utilizing real semantics. This problem directly challenges the feasibility of the current model editing literature at its very foundation, as shortcuts are inherently at odds with robust knowledge integration. Coincidentally, this issue has long been obscured by evaluation frameworks that lack the design of negative examples. To uncover it, we systematically develop a suite of new evaluation methods. Strikingly, we find that state-of-the-art approaches collapse even under the simplest negation queries. Our empirical evidence shows that editing is likely to be based on shortcuts rather than full semantics, calling for an urgent reconsideration of the very basis of model editing before further advancements can be meaningfully pursued.

**Comment:** Does not match any specific criteria. Focuses on model editing in LLMs, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 30. [Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network](https://arxiv.org/abs/2510.00654) <a id="link30"></a>
**ArXiv ID:** 2510.00654
**Authors:** Shaocong Zhu, Zhiwei Li, Xinghua Li, Huanfeng Shen

**Abstract:**  Clouds significantly affect the quality of optical satellite images, which seriously limits their precise application. Recently, deep learning has been widely applied to cloud detection and has achieved satisfactory results. However, the lack of distinctive features in thin clouds and the low quality of training samples limit the cloud detection accuracy of deep learning methods, leaving space for further improvements. In this paper, we propose a weakly supervised cloud detection method that combines spectral features and multi-scale scene-level deep network (SpecMCD) to obtain highly accurate pixel-level cloud masks. The method first utilizes a progressive training framework with a multi-scale scene-level dataset to train the multi-scale scene-level cloud detection network. Pixel-level cloud probability maps are then obtained by combining the multi-scale probability maps and cloud thickness map based on the characteristics of clouds in dense cloud coverage and large cloud-area coverage images. Finally, adaptive thresholds are generated based on the differentiated regions of the scene-level cloud masks at different scales and combined with distance-weighted optimization to obtain binary cloud masks. Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1 multispectral (GF1-MS) images, were used to verify the effectiveness of the proposed method. Compared to the other weakly supervised cloud detection methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method shows an improvement of over 7.82%, highlighting the superiority and potential of the SpecMCD method for cloud detection under different cloud coverage conditions.

**Comment:** Does not directly match any of the four criteria. The paper is about weakly supervised cloud detection using spectral features and deep networks, which is an application of computer vision but not a new method for spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---

## 31. [Beyond one-hot encoding? Journey into compact encoding for large multi-class segmentation](https://arxiv.org/abs/2510.00667) <a id="link31"></a>
**ArXiv ID:** 2510.00667
**Authors:** Aaron Kujawa, Thomas Booth, Tom Vercauteren

**Abstract:**  This work presents novel methods to reduce computational and memory requirements for medical image segmentation with a large number of classes. We curiously observe challenges in maintaining state-of-the-art segmentation performance with all of the explored options. Standard learning-based methods typically employ one-hot encoding of class labels. The computational complexity and memory requirements thus increase linearly with the number of classes. We propose a family of binary encoding approaches instead of one-hot encoding to reduce the computational complexity and memory requirements to logarithmic in the number of classes. In addition to vanilla binary encoding, we investigate the effects of error-correcting output codes (ECOCs), class weighting, hard/soft decoding, class-to-codeword assignment, and label embedding trees. We apply the methods to the use case of whole brain parcellation with 108 classes based on 3D MRI images. While binary encodings have proven efficient in so-called extreme classification problems in computer vision, we faced challenges in reaching state-of-the-art segmentation quality with binary encodings. Compared to one-hot encoding (Dice Similarity Coefficient (DSC) = 82.4 (2.8)), we report reduced segmentation performance with the binary segmentation approaches, achieving DSCs in the range from 39.3 to 73.8. Informative negative results all too often go unpublished. We hope that this work inspires future research of compact encoding strategies for large multi-class segmentation tasks.

**Comment:** Does not match any specific criteria. Focuses on encoding strategies for large multi-class segmentation, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---

## 32. [ZQBA: Zero Query Black-box Adversarial Attack](https://arxiv.org/abs/2510.00769) <a id="link32"></a>
**ArXiv ID:** 2510.00769
**Authors:** Joana C. Costa, Tiago Roxo, Hugo Proen\c{c}a, Pedro R. M. In\'acio

**Abstract:**  Current black-box adversarial attacks either require multiple queries or diffusion models to produce adversarial samples that can impair the target model performance. However, these methods require training a surrogate loss or diffusion models to produce adversarial samples, which limits their applicability in real-world settings. Thus, we propose a Zero Query Black-box Adversarial (ZQBA) attack that exploits the representations of Deep Neural Networks (DNNs) to fool other networks. Instead of requiring thousands of queries to produce deceiving adversarial samples, we use the feature maps obtained from a DNN and add them to clean images to impair the classification of a target model. The results suggest that ZQBA can transfer the adversarial samples to different models and across various datasets, namely CIFAR and Tiny ImageNet. The experiments also show that ZQBA is more effective than state-of-the-art black-box attacks with a single query, while maintaining the imperceptibility of perturbations, evaluated both quantitatively (SSIM) and qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world contexts. All the source code is available at https://github.com/Joana-Cabral/ZQBA.

**Comment:** Does not match any specific criteria. Focuses on adversarial attacks, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.