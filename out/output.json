{
    "2507.09491": {
        "authors": [
            "Yiyang Zhou",
            "Linjie Li",
            "Shi Qiu",
            "Zhengyuan Yang",
            "Yuyang Zhao",
            "Siwei Han",
            "Yangfan He",
            "Kangqi Li",
            "Haonian Ji",
            "Zihao Zhao",
            "Haibo Tong",
            "Lijuan Wang",
            "Huaxiu Yao"
        ],
        "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?",
        "abstract": "arXiv:2507.09491v1 Announce Type: new  Abstract: Existing video benchmarks often resemble image-based benchmarks, with question types like \"What actions does the person perform throughout the video?\" or \"What color is the woman's dress in the video?\" For these, models can often answer by scanning just a few key frames, without deep temporal reasoning. This limits our ability to assess whether large vision-language models (LVLMs) can truly think with videos rather than perform superficial frame-level analysis. To address this, we introduce GLIMPSE, a benchmark specifically designed to evaluate whether LVLMs can genuinely think with videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video understanding beyond static image cues. It consists of 3,269 videos and over 4,342 highly visual-centric questions across 11 categories, including Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions are carefully crafted by human annotators and require watching the entire video and reasoning over full video context-this is what we mean by thinking with video. These questions cannot be answered by scanning selected frames or relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy, but current LVLMs face significant challenges. Even the best-performing model, GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move beyond surface-level reasoning to truly think with videos.",
        "arxiv_id": "2507.09491",
        "ARXIVID": "2507.09491",
        "COMMENT": "This paper introduces GLIMPSE, a new benchmark specifically designed to test whether large vision-language models (LVLMs) can perform deep temporal and spatial reasoning with videos, not just frame-level analysis. This directly matches criterion 3 (new embodied AI benchmarks with novel angles) and criterion 2 (VLLMs/MLLMs), as it evaluates and exposes limitations in current VLLMs for video understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.09876": {
        "authors": [
            "Yongheng Zhang",
            "Xu Liu",
            "Ruihan Tao",
            "Qiguang Chen",
            "Hao Fei",
            "Wanxiang Che",
            "Libo Qin"
        ],
        "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models",
        "abstract": "arXiv:2507.09876v1 Announce Type: new  Abstract: Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.",
        "arxiv_id": "2507.09876",
        "ARXIVID": "2507.09876",
        "COMMENT": "This paper introduces a new video reasoning paradigm (ViTCoT) for MLLMs, a new benchmark (ViTIB), and demonstrates improved video understanding in large language models. It directly matches criterion 2 (new MLLMs/VLLMs) and criterion 3 (new benchmarks for embodied/video AI), and is highly relevant for multi-modal generative modeling and surprising empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.09291": {
        "authors": [
            "Yuval Grader",
            "Hadar Averbuch-Elor"
        ],
        "title": "Supercharging Floorplan Localization with Semantic Rays",
        "abstract": "arXiv:2507.09291v1 Announce Type: new  Abstract: Floorplans provide a compact representation of the building's structure, revealing not only layout information but also detailed semantics such as the locations of windows and doors. However, contemporary floorplan localization techniques mostly focus on matching depth-based structural cues, ignoring the rich semantics communicated within floorplans. In this work, we introduce a semantic-aware localization framework that jointly estimates depth and semantic rays, consolidating over both for predicting a structural-semantic probability volume. Our probability volume is constructed in a coarse-to-fine manner: We first sample a small set of rays to obtain an initial low-resolution probability volume. We then refine these probabilities by performing a denser sampling only in high-probability regions and process the refined values for predicting a 2D location and orientation angle. We conduct an evaluation on two standard floorplan localization benchmarks. Our experiments demonstrate that our approach substantially outperforms state-of-the-art methods, achieving significant improvements in recall metrics compared to prior works. Moreover, we show that our framework can easily incorporate additional metadata such as room labels, enabling additional gains in both accuracy and efficiency.",
        "arxiv_id": "2507.09291",
        "ARXIVID": "2507.09291",
        "COMMENT": "Proposes a semantic-aware localization framework for floorplan localization, jointly estimating depth and semantic rays. This is a strong match for criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel method for embodied AI with a new angle: semantic rays).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.09374": {
        "authors": [
            "Chenglin Zhu",
            "Tao Zhang",
            "Chong Li",
            "Mingan Lin",
            "Zenan Zhou",
            "Jian Xie"
        ],
        "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique",
        "abstract": "arXiv:2507.09374v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific tasks, particularly those requiring multi-step and interpretable reasoning. Their limitations include insufficient scientific reasoning patterns, lack of global coherence in multi-step inference, and the absence of reflective self-correction, making them unreliable in structured scientific contexts. We introduce EduFlow, the first end-to-end framework that covers the full pipeline of educational scientific reasoning, including data selection, MCTS-based trajectory construction, model training, and output optimization. At its core is EduPRM, a process-aware reward model that critiques reasoning steps with tags and justifications. EduPRM is trained via curriculum learning on three complementary supervision sources: MCTS-guided trajectories, error-injected critiques, and teacher-student dialogues, enabling dynamic adaptation to multi-stage problem solving and iterative refinement during inference. We further propose EduMCTS, a domain-adapted search framework that introduces bootstrapping actions specifically designed for educational reasoning, such as a self-reflection mechanism that promotes reflective error correction. It further leverages EduPRM's fine-grained feedback to guide the search toward higher-quality reasoning trajectories. By applying self-consistency and rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of educational reasoning trajectories. Extensive experiments demonstrate that EduFlow enhances reasoning consistency and coherence. Code, data, and models will be released.",
        "arxiv_id": "2507.09374",
        "ARXIVID": "2507.09374",
        "COMMENT": "Presents EduFlow, a new framework for improving MLLMs' problem-solving proficiency with multi-stage, multi-perspective critique. This is a direct match for criterion 2 (new MLLMs) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.10318": {
        "authors": [
            "Yuhan Liu",
            "Jingwen Fu",
            "Yang Wu",
            "Kangyi Wu",
            "Pengna Li",
            "Jiayi Wu",
            "Sanping Zhou",
            "Jingmin Xin"
        ],
        "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching",
        "abstract": "arXiv:2507.10318v1 Announce Type: new  Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment.",
        "arxiv_id": "2507.10318",
        "ARXIVID": "2507.10318",
        "COMMENT": "Strongly matches criterion 4 (vision foundation models and applications) and is also relevant to criterion 1 (spatial understanding) as it addresses the misalignment between vision foundation models and feature matching, introducing a new framework and benchmark for multi-instance matching.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.09180": {
        "authors": [
            "Zichun Xu",
            "Yuntao Li",
            "Zhaomin Wang",
            "Lei Zhuang",
            "Guocai Yang",
            "Jingdong Zhao"
        ],
        "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning",
        "abstract": "arXiv:2507.09180v1 Announce Type: new  Abstract: Depth information is robust to scene appearance variations and inherently carries 3D spatial details. In this paper, a visual backbone based on the vision transformer is proposed to fuse RGB and depth modalities for enhancing generalization. Different modalities are first processed by separate CNN stems, and the combined convolutional features are delivered to the scalable vision transformer to obtain visual representations. Moreover, a contrastive unsupervised learning scheme is designed with masked and unmasked tokens to accelerate the sample efficiency during the reinforcement learning progress. For sim2real transfer, a flexible curriculum learning schedule is developed to deploy domain randomization over training processes.",
        "arxiv_id": "2507.09180",
        "ARXIVID": "2507.09180",
        "COMMENT": "This paper proposes a new method for fusing RGB and depth modalities in visual reinforcement learning, with a focus on generalization and sim2real transfer. It directly matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (new methods for embodied AI with novel angles, e.g., depth fusion and curriculum learning).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2507.09500": {
        "authors": [
            "Yiwen Liang",
            "Hui Chen",
            "Yizhe Xiong",
            "Zihan Zhou",
            "Mengyao Lyu",
            "Zijia Lin",
            "Shuaicheng Niu",
            "Sicheng Zhao",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations",
        "abstract": "arXiv:2507.09500v1 Announce Type: new  Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.",
        "arxiv_id": "2507.09500",
        "ARXIVID": "2507.09500",
        "COMMENT": "This paper proposes a new method for reliable test-time adaptation of vision-language models under distribution shift, directly addressing criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). It introduces new statistical tricks for adaptation and calibration.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2507.10118": {
        "authors": [
            "Ivan Martinovi\\'c",
            "Josip \\v{S}ari\\'c",
            "Marin Or\\v{s}i\\'c",
            "Matej Kristan",
            "Sini\\v{s}a \\v{S}egvi\\'c"
        ],
        "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation",
        "abstract": "arXiv:2507.10118v1 Announce Type: new  Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised segmentation methods address this challenge by learning models on few labeled images alongside a large corpus of unlabeled images. Although foundation models could further account for label scarcity, effective mechanisms for their exploitation remain underexplored. We address this by devising a novel semi-supervised panoptic approach fueled by two dedicated foundation models. We enhance recognition by complementing unsupervised mask-transformer consistency with zero-shot classification of CLIP features. We enhance localization by class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting decoupled enhancement of recognition and localization (DEARLi) particularly excels in the most challenging semi-supervised scenarios with large taxonomies and limited labeled data. Moreover, DEARLi outperforms the state of the art in semi-supervised semantic segmentation by a large margin while requiring 8x less GPU memory, in spite of being trained only for the panoptic objective. We observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The source code is available at https://github.com/helen1c/DEARLi.",
        "arxiv_id": "2507.10118",
        "ARXIVID": "2507.10118",
        "COMMENT": "Proposes a new semi-supervised panoptic segmentation method leveraging two foundation models (CLIP and SAM), with decoupled recognition and localization. This is a strong match for criterion 4 (vision foundation models and applications), and also relevant to criterion 1 (spatial understanding in segmentation).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.10496": {
        "authors": [
            "Ruilong Li",
            "Brent Yi",
            "Junchen Liu",
            "Hang Gao",
            "Yi Ma",
            "Angjoo Kanazawa"
        ],
        "title": "Cameras as Relative Positional Encoding",
        "abstract": "arXiv:2507.10496v1 Announce Type: new  Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks, where geometric relationships between viewpoints are critical for 3D perception. To leverage these relationships, multi-view transformers must use camera geometry to ground visual tokens in 3D space. In this work, we compare techniques for conditioning transformers on cameras: token-level raymap encodings, attention-level relative pose encodings, and a new relative encoding we propose -- Projective Positional Encoding (PRoPE) -- that captures complete camera frustums, both intrinsics and extrinsics, as a relative positional encoding. Our experiments begin by showing how relative camera conditioning improves performance in feedforward novel view synthesis, with further gains from PRoPE. This holds across settings: scenes with both shared and varying intrinsics, when combining token- and attention-level conditioning, and for generalization to inputs with out-of-distribution sequence lengths and camera intrinsics. We then verify that these benefits persist for different tasks, stereo depth estimation and discriminative spatial cognition, as well as larger model sizes.",
        "arxiv_id": "2507.10496",
        "ARXIVID": "2507.10496",
        "COMMENT": "Matches criterion 1: Proposes a new method (Projective Positional Encoding) for spatial understanding in multi-view transformers, with applications to 3D perception and spatial cognition. Also relevant to criterion 4 as it deals with transformer-based vision models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.10202": {
        "authors": [
            "Jaeseong Lee",
            "Yeeun Choi",
            "Heechan Choi",
            "Hanjung Kim",
            "Seonjoo Kim"
        ],
        "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images",
        "abstract": "arXiv:2507.10202v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding, reasoning, and generation. However, they struggle with tasks requiring fine-grained localization and reasoning in high-resolution images. This constraint stems from the fact that MLLMs are fine-tuned with fixed image resolution to align with the pre-trained image encoder used in MLLM. Consequently, feeding high-resolution images directly into MLLMs leads to poor generalization due to a train-test resolution discrepancy, while downsampling these images-although ensuring consistency-compromises fine-grained visual details and ultimately degrades performance. To address this challenge, we propose Extract Candidate then Predict (ECP), a novel training-free, task-agnostic two-stage framework designed to enhance MLLM performance on high-resolution images. The key intuition behind ECP is that while MLLMs struggle with high-resolution images, their predictions on downsampled images still contain implicit localization cues. By first identifying candidate region using the coarse prediction and then predicting the final output based on candidate region, ECP effectively preserves fine-grained details while mitigating the challenges posed by high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared to baseline respectively, demonstrating its effectiveness. Code is available at https://github.com/yenncye/ECP.",
        "arxiv_id": "2507.10202",
        "ARXIVID": "2507.10202",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Proposes a training-free, task-agnostic framework to enhance MLLM performance on high-resolution images, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.09880": {
        "authors": [
            "Keito Suzuki",
            "Bang Du",
            "Runfa Blark Li",
            "Kunyao Chen",
            "Lei Wang",
            "Peng Liu",
            "Ning Bi",
            "Truong Nguyen"
        ],
        "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing",
        "abstract": "arXiv:2507.09880v1 Announce Type: new  Abstract: Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.",
        "arxiv_id": "2507.09880",
        "ARXIVID": "2507.09880",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is tangentially related to criterion 1 (spatial understanding) as it introduces the first open-vocabulary 4D human parsing framework, with innovations in spatial-temporal correspondence and mask fusion.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.09574": {
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Liang Chen",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Junjie Hu"
        ],
        "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models",
        "abstract": "arXiv:2507.09574v1 Announce Type: new  Abstract: Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR",
        "arxiv_id": "2507.09574",
        "ARXIVID": "2507.09574",
        "COMMENT": "Directly matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications) as it proposes a new autoregressive multimodal image generation framework with efficient multimodal-conditioned tuning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.09279": {
        "authors": [
            "Anita Kriz",
            "Elizabeth Laura Janes",
            "Xing Shen",
            "Tal Arbel"
        ],
        "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models",
        "abstract": "arXiv:2507.09279v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/vccrl-llm.",
        "arxiv_id": "2507.09279",
        "ARXIVID": "2507.09279",
        "COMMENT": "Directly matches criterion 2 (new MLLMs) as it proposes a reinforcement learning prompt augmentation framework for confidence calibration in multimodal large language models, with empirical results in medical VQA.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.09082": {
        "authors": [
            "Seungwoo Kim",
            "Khai Loong Aw",
            "Klemen Kotar",
            "Cristobal Eyzaguirre",
            "Wanhee Lee",
            "Yunong Liu",
            "Jared Watrous",
            "Stefan Stojanov",
            "Juan Carlos Niebles",
            "Jiajun Wu",
            "Daniel L. K. Yamins"
        ],
        "title": "Taming generative video models for zero-shot optical flow extraction",
        "abstract": "arXiv:2507.09082v1 Announce Type: new  Abstract: Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.",
        "arxiv_id": "2507.09082",
        "ARXIVID": "2507.09082",
        "COMMENT": "This paper introduces a novel zero-shot method for extracting optical flow from generative video models using counterfactual prompting, without fine-tuning. This is relevant to criterion 4 (vision foundation models and applications), as it leverages generative video models for a classic vision task in a new way.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2507.09216": {
        "authors": [
            "Jingguo Liu",
            "Han Yu",
            "Shigang Li",
            "Jianfeng Li"
        ],
        "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models",
        "abstract": "arXiv:2507.09216v1 Announce Type: new  Abstract: Due to the current lack of large-scale datasets at the million-scale level, tasks involving panoramic images predominantly rely on existing two-dimensional pre-trained image benchmark models as backbone networks. However, these networks are not equipped to recognize the distortions and discontinuities inherent in panoramic images, which adversely affects their performance in such tasks. In this paper, we introduce a novel spherical sampling method for panoramic images that enables the direct utilization of existing pre-trained models developed for two-dimensional images. Our method employs spherical discrete sampling based on the weights of the pre-trained models, effectively mitigating distortions while achieving favorable initial training values. Additionally, we apply the proposed sampling method to panoramic image segmentation, utilizing features obtained from the spherical model as masks for specific channel attentions, which yields commendable results on commonly used indoor datasets, Stanford2D3D.",
        "arxiv_id": "2507.09216",
        "ARXIVID": "2507.09216",
        "COMMENT": "This paper proposes a novel spherical sampling method to adapt large-scale 2D pre-trained models for 360-degree panoramic image segmentation, addressing spatial distortions. This matches criterion 4 (vision foundation models and applications) and is also relevant to criterion 1 (spatial understanding improvements for embodied agents).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10095": {
        "authors": [
            "Bingchao Wang",
            "Zhiwei Ning",
            "Jianyu Ding",
            "Xuanang Gao",
            "Yin Li",
            "Dongsheng Jiang",
            "Jie Yang",
            "Wei Liu"
        ],
        "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text",
        "abstract": "arXiv:2507.10095v1 Announce Type: new  Abstract: CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To remedy this issue, we propose FIX-CLIP which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIP's text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input.",
        "arxiv_id": "2507.10095",
        "ARXIVID": "2507.10095",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (improving CLIP for long-text, using synthetic captions from MLLMs). Introduces new modules and a large synthetic dataset for long-text understanding in CLIP.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10547": {
        "authors": [
            "Borui Zhang",
            "Qihang Rao",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Quantize-then-Rectify: Efficient VQ-VAE Training",
        "abstract": "arXiv:2507.10547v1 Announce Type: new  Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel multi-group quantization} to enlarge codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.",
        "arxiv_id": "2507.10547",
        "ARXIVID": "2507.10547",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a new, efficient method for training visual tokenizers (VQ-VAE) that are key components in multimodal large models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09612": {
        "authors": [
            "You Huang",
            "Lichao Chen",
            "Jiayi Ji",
            "Liujuan Cao",
            "Shengchuan Zhang",
            "Rongrong Ji"
        ],
        "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive",
        "abstract": "arXiv:2507.09612v1 Announce Type: new  Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices.",
        "arxiv_id": "2507.09612",
        "ARXIVID": "2507.09612",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in interactive segmentation) and criterion 4 (vision foundation models and applications, as it builds on SAM). The dynamic hybrid attention and adaptive computation for efficient, high-precision segmentation is a novel method for spatial intelligence in vision tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10034": {
        "authors": [
            "Xianghong Zou",
            "Jianping Li",
            "Zhe Chen",
            "Zhen Cao",
            "Zhen Dong",
            "Qiegen Liu",
            "Bisheng Yang"
        ],
        "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning",
        "abstract": "arXiv:2507.10034v1 Announce Type: new  Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry and robotics applications such as autonomous driving, intelligent transportation, and augmented reality. In real-world large-scale deployments of a positioning system, PCPR models must continuously acquire, update, and accumulate knowledge to adapt to diverse and dynamic environments, i.e., the ability known as continual learning (CL). However, existing PCPR models often suffer from catastrophic forgetting, leading to significant performance degradation in previously learned scenes when adapting to new environments or sensor types. This results in poor model scalability, increased maintenance costs, and system deployment difficulties, undermining the practicality of PCPR. To address these issues, we propose LifelongPR, a novel continual learning framework for PCPR, which effectively extracts and fuses knowledge from sequential point cloud data. First, to alleviate the knowledge loss, we propose a replay sample selection method that dynamically allocates sample sizes according to each dataset's information quantity and selects spatially diverse samples for maximal representativeness. Second, to handle domain shifts, we design a prompt learning-based CL framework with a lightweight prompt module and a two-stage training strategy, enabling domain-specific feature adaptation while minimizing forgetting. Comprehensive experiments on large-scale public and self-collected datasets are conducted to validate the effectiveness of the proposed method. Compared with state-of-the-art (SOTA) methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly available at https://github.com/zouxianghong/LifelongPR.",
        "arxiv_id": "2507.10034",
        "ARXIVID": "2507.10034",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI, continual learning for point cloud place recognition). Proposes a continual learning framework with replay and prompt learning for spatial understanding in robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10223": {
        "authors": [
            "Xiangyu Yin",
            "Boyuan Yang",
            "Weichen Liu",
            "Qiyao Xue",
            "Abrar Alamri",
            "Goeran Fiedler",
            "Wei Gao"
        ],
        "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users",
        "abstract": "arXiv:2507.10223v1 Announce Type: new  Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at https://github.com/pittisl/ProGait and dataset at https://huggingface.co/datasets/ericyxy98/ProGait.",
        "arxiv_id": "2507.10223",
        "ARXIVID": "2507.10223",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI, simulator-related). Introduces a new video dataset and benchmark for prosthesis users, supporting multiple vision tasks and providing baseline models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09672": {
        "authors": [
            "Xinyu Zhang",
            "Zhonghao Ye",
            "Jingwei Zhang",
            "Xiang Tian",
            "Zhisheng Liang",
            "Shipeng Yu"
        ],
        "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation",
        "abstract": "arXiv:2507.09672v1 Announce Type: new  Abstract: WiFi-based human pose estimation has emerged as a promising non-visual alternative approaches due to its pene-trability and privacy advantages. This paper presents VST-Pose, a novel deep learning framework for accurate and continuous pose estimation using WiFi channel state information. The proposed method introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture that adopts a dual-stream architecture to separately capture temporal dependencies and structural relationships among body joints. To enhance sensitivity to subtle human motions, a velocity modeling branch is integrated into the framework, which learns short-term keypoint dis-placement patterns and improves fine-grained motion representation. We construct a 2D pose dataset specifically designed for smart home care scenarios and demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset. Further evaluation on the public MMFi dataset confirms the model's robustness and effectiveness in 3D pose estimation tasks. The proposed system provides a reliable and privacy-aware solution for continuous human motion analysis in indoor environments. Our codes are available in https://github.com/CarmenQing/VST-Pose.",
        "arxiv_id": "2507.09672",
        "ARXIVID": "2507.09672",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents). Proposes a novel spatiotemporal attention network for WiFi-based human pose estimation, with a velocity modeling branch for fine-grained motion.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09008": {
        "authors": [
            "Xiwei Xuan",
            "Xiaoqi Wang",
            "Wenbin He",
            "Jorge Piazentin Ono",
            "Liang Gou",
            "Kwan-Liu Ma",
            "Liu Ren"
        ],
        "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels",
        "abstract": "arXiv:2507.09008v1 Announce Type: new  Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA's effectiveness from both quantitative and qualitative perspectives.",
        "arxiv_id": "2507.09008",
        "ARXIVID": "2507.09008",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it presents a visual analytics framework to improve the quality of data labels generated by multi-modal foundation models like CLIP and LLaVA.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.10065": {
        "authors": [
            "Chenguo Lin",
            "Yuchen Lin",
            "Panwang Pan",
            "Yifan Yu",
            "Honglei Yan",
            "Katerina Fragkiadaki",
            "Yadong Mu"
        ],
        "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
        "abstract": "arXiv:2507.10065v1 Announce Type: new  Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.",
        "arxiv_id": "2507.10065",
        "ARXIVID": "2507.10065",
        "COMMENT": "This paper presents a new method for fast 4D dynamic view synthesis from monocular videos, enabling unified modeling of appearance, geometry, and motion. It is relevant to spatial understanding and intelligence (criterion 1), and also to vision foundation models and their applications (criterion 4), as it enables new capabilities in dynamic scene modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.09209": {
        "authors": [
            "Xiao Liang",
            "Di Wang",
            "Zhicheng Jiao",
            "Ronghan Li",
            "Pengfei Yang",
            "Quan Wang",
            "Tat-Seng Chua"
        ],
        "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models",
        "abstract": "arXiv:2507.09209v1 Announce Type: new  Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the development of multi-modal medical assistant systems. Despite this progress, current models still have inherent probabilistic uncertainties, often producing erroneous or unverified responses-an issue with serious implications in medical applications. Existing methods aim to enhance the performance of Medical Vision Language Model (MedVLM) by adjusting model structure, fine-tuning with high-quality data, or through preference fine-tuning. However, these training-dependent strategies are costly and still lack sufficient alignment with clinical expertise. To address these issues, we propose an expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance (Expert-CFG) to align MedVLM with clinical expertise without additional training. This framework introduces an uncertainty estimation strategy to identify unreliable outputs. It then retrieves relevant references to assist experts in highlighting key terms and applies classifier-free guidance to refine the token embeddings of MedVLM, ensuring that the adjusted outputs are correct and align with expert highlights. Evaluations across three medical visual question answering benchmarks demonstrate that the proposed Expert-CFG, with 4.2B parameters and limited expert annotations, outperforms state-of-the-art models with 13B parameters. The results demonstrate the feasibility of deploying such a system in resource-limited settings for clinical use.",
        "arxiv_id": "2507.09209",
        "ARXIVID": "2507.09209",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes an expert-in-the-loop framework for medical vision-language models, with uncertainty estimation and classifier-free guidance, improving alignment with clinical expertise.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.09266": {
        "authors": [
            "JianHe Low",
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation",
        "abstract": "arXiv:2507.09266v1 Announce Type: new  Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving strong performances without relying on gloss annotations. However, these gains have often come with increased model complexity and high computational demands, raising concerns about scalability, especially as large-scale sign language datasets become more common. We propose a segment-aware visual tokenization framework that leverages sign segmentation to convert continuous video into discrete, sign-informed visual tokens. This reduces input sequence length by up to 50% compared to prior methods, resulting in up to 2.67x lower memory usage and better scalability on larger datasets. To bridge the visual and linguistic modalities, we introduce a token-to-token contrastive alignment objective, along with a dual-level supervision that aligns both language embeddings and intermediate hidden states. This improves fine-grained cross-modal alignment without relying on gloss-level supervision. Our approach notably exceeds the performance of state-of-the-art methods on the PHOENIX14T benchmark, while significantly reducing sequence length. Further experiments also demonstrate our improved performance over prior work under comparable sequence-lengths, validating the potential of our tokenization and alignment strategies.",
        "arxiv_id": "2507.09266",
        "ARXIVID": "2507.09266",
        "COMMENT": "Proposes a segment-aware visual tokenization framework for sign language translation, reducing sequence length and improving cross-modal alignment. This is relevant to criterion 4 (vision foundation models and applications) and also touches on multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.09256": {
        "authors": [
            "Junyu Chen",
            "Yihua Gao",
            "Mingyuan Ge",
            "Mingyong Li"
        ],
        "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching",
        "abstract": "arXiv:2507.09256v1 Announce Type: new  Abstract: Image-text matching is crucial for bridging the semantic gap between computer vision and natural language processing. However, existing methods still face challenges in handling high-order associations and semantic ambiguities among similar instances. These ambiguities arise from subtle differences between soft positive samples (semantically similar but incorrectly labeled) and soft negative samples (locally matched but globally inconsistent), creating matching uncertainties. Furthermore, current methods fail to fully utilize the neighborhood relationships among semantically similar instances within training batches, limiting the model's ability to learn high-order shared knowledge. This paper proposes the Ambiguity-Aware and High-order Relation learning framework (AAHR) to address these issues. AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, effectively mitigating the soft positive sample problem. The framework introduces global and local feature extraction mechanisms and an adaptive aggregation network, significantly enhancing full-grained semantic understanding capabilities. Additionally, AAHR employs intra-modal and inter-modal correlation matrices to investigate neighborhood relationships among sample instances thoroughly. It incorporates GNN to enhance semantic interactions between instances. Furthermore, AAHR integrates momentum contrastive learning to expand the negative sample set. These combined strategies significantly improve the model's ability to discriminate between features. Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching. The code and model checkpoints for this research are available at https://github.com/Image-Text-Matching/AAHR .",
        "arxiv_id": "2507.09256",
        "ARXIVID": "2507.09256",
        "COMMENT": "Presents a new framework for image-text matching with ambiguity-aware and high-order relation learning. This is relevant to multi-modal learning and vision-language models (criterion 4), with novel techniques for semantic understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.09168": {
        "authors": [
            "Haiming Zhu",
            "Yangyang Xu",
            "Chenshu Xu",
            "Tingrui Shen",
            "Wenxi Liu",
            "Yong Du",
            "Jun Yu",
            "Shengfeng He"
        ],
        "title": "Stable Score Distillation",
        "abstract": "arXiv:2507.09168v1 Announce Type: new  Abstract: Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.",
        "arxiv_id": "2507.09168",
        "ARXIVID": "2507.09168",
        "COMMENT": "This paper presents a new method (Stable Score Distillation) for text-guided image and 3D editing, improving stability and spatial control in diffusion-based models. This is relevant to criterion 4 (vision foundation models and applications), as it advances generative modeling for spatially-aware editing tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.09294": {
        "authors": [
            "Rui Tang",
            "Haochen Yin",
            "Guankun Wang",
            "Long Bai",
            "An Wang",
            "Huxin Gao",
            "Jiazheng Wang",
            "Hongliang Ren"
        ],
        "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection",
        "abstract": "arXiv:2507.09294v1 Announce Type: new  Abstract: Surgical phase recognition plays a critical role in developing intelligent assistance systems for minimally invasive procedures such as Endoscopic Submucosal Dissection (ESD). However, the high visual similarity across different phases and the lack of structural cues in RGB images pose significant challenges. Depth information offers valuable geometric cues that can complement appearance features by providing insights into spatial relationships and anatomical structures. In this paper, we pioneer the use of depth information for surgical phase recognition and propose Geo-RepNet, a geometry-aware convolutional framework that integrates RGB image and depth information to enhance recognition performance in complex surgical scenes. Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention (GEMA) to inject spatial guidance through geometry-aware cross-attention and efficient multi-scale aggregation. To evaluate the effectiveness of our approach, we construct a nine-phase ESD dataset with dense frame-level annotations from real-world ESD videos. Extensive experiments on the proposed dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while maintaining robustness and high computational efficiency under complex and low-texture surgical environments.",
        "arxiv_id": "2507.09294",
        "ARXIVID": "2507.09294",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications). Proposes a geometry-aware representation learning method for surgical phase recognition, integrating depth and RGB.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.09885": {
        "authors": [
            "Zhanjiang Yang",
            "Lijun Sun",
            "Jiawei Dong",
            "Xiaoxin An",
            "Yang Liu",
            "Meng Li"
        ],
        "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention",
        "abstract": "arXiv:2507.09885v1 Announce Type: new  Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective solution for various vision-based applications. However, most existing learning-based hyperspectral reconstruction methods directly learn the RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent challenge of transitioning from low-dimensional to high-dimensional information. To address this limitation, we propose a two-stage approach, MCGA, which first learns spectral patterns before estimating the mapping. In the first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the RGB-to-HSI mapping is refined by querying features from the MoC to replace latent HSI representations, incorporating prior knowledge rather than forcing a direct high-dimensional transformation. To further enhance reconstruction quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention, which adaptively adjust feature map intensities to meet hyperspectral reconstruction requirements. This physically motivated attention mechanism ensures lightweight and efficient HSI recovery. Moreover, we propose an entropy-based Test-Time Adaptation strategy to improve robustness in real-world scenarios. Extensive experiments demonstrate that our method, MCGA, achieves state-of-the-art performance. The code and models will be released at https://github.com/Fibonaccirabbit/MCGA",
        "arxiv_id": "2507.09885",
        "ARXIVID": "2507.09885",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel two-stage approach for hyperspectral reconstruction using VQ-VAE and attention mechanisms, with strong empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.09524": {
        "authors": [
            "Yunwei Lan",
            "Zhigao Cui",
            "Xin Luo",
            "Chang Liu",
            "Nian Wang",
            "Menglin Zhang",
            "Yanzhao Su",
            "Dong Liu"
        ],
        "title": "When Schr\\\"odinger Bridge Meets Real-World Image Dehazing with Unpaired Training",
        "abstract": "arXiv:2507.09524v1 Announce Type: new  Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator's limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method's superiority. Code: https://github.com/ywxjm/DehazeSB.",
        "arxiv_id": "2507.09524",
        "ARXIVID": "2507.09524",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel unpaired dehazing framework using Schr\u00f6dinger Bridge and prompt learning with CLIP, leveraging vision-language alignment for image restoration.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2507.09619": {
        "authors": [
            "Yilin Lu",
            "Jianghang Lin",
            "Linhuang Xie",
            "Kai Zhao",
            "Yansong Qu",
            "Shengchuan Zhang",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection",
        "abstract": "arXiv:2507.09619v1 Announce Type: new  Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.",
        "arxiv_id": "2507.09619",
        "ARXIVID": "2507.09619",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Uses pretrained latent diffusion models for anomaly synthesis, and introduces new methods for generating aligned anomaly image-mask pairs for industrial inspection.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2507.09915": {
        "authors": [
            "Siyue Yao",
            "Mingjie Sun",
            "Eng Gee Lim",
            "Ran Yi",
            "Baojiang Zhong",
            "Moncef Gabbouj"
        ],
        "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios",
        "abstract": "arXiv:2507.09915v1 Announce Type: new  Abstract: The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide \"crucial information\" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.",
        "arxiv_id": "2507.09915",
        "ARXIVID": "2507.09915",
        "COMMENT": "Crucial-Diff is a unified diffusion model for generating crucial image samples and annotations in data-scarce scenarios, with a focus on improving downstream detection and segmentation. This is relevant to criterion 4 (vision foundation models and applications), as it applies diffusion models to data augmentation for vision tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.10306": {
        "authors": [
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation",
        "abstract": "arXiv:2507.10306v1 Announce Type: new  Abstract: Sign Language Translation (SLT) aims to convert sign language videos into spoken or written text. While early systems relied on gloss annotations as an intermediate supervision, such annotations are costly to obtain and often fail to capture the full complexity of continuous signing. In this work, we propose a two-phase, dual visual encoder framework for gloss-free SLT, leveraging contrastive visual-language pretraining. During pretraining, our approach employs two complementary visual backbones whose outputs are jointly aligned with each other and with sentence-level text embeddings via a contrastive objective. During the downstream SLT task, we fuse the visual features and input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our dual encoder architecture consistently outperforms its single stream variants and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.",
        "arxiv_id": "2507.10306",
        "ARXIVID": "2507.10306",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (dual visual encoders, contrastive pretraining for sign language translation). Uses contrastive visual-language pretraining and dual visual encoders, which is a methodological improvement in vision-language modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.09068": {
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "title": "Infinite Video Understanding",
        "abstract": "arXiv:2507.09068v1 Announce Type: new  Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
        "arxiv_id": "2507.09068",
        "ARXIVID": "2507.09068",
        "COMMENT": "A position paper outlining the challenges and research directions for 'Infinite Video Understanding' with MLLMs. While it does not introduce a new model or method, it is highly relevant to criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models), especially for long-form video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.09861": {
        "authors": [
            "Yihao Ding",
            "Siwen Luo",
            "Yue Dai",
            "Yanbei Jiang",
            "Zechuan Li",
            "Geoffrey Martin",
            "Yifan Peng"
        ],
        "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends",
        "abstract": "arXiv:2507.09861v1 Announce Type: new  Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.",
        "arxiv_id": "2507.09861",
        "ARXIVID": "2507.09861",
        "COMMENT": "Matches criterion 2 (survey on MLLM-based visually rich document understanding). Provides a comprehensive review of methods, training paradigms, and datasets for MLLMs in document understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 4
    },
    "2507.10143": {
        "authors": [
            "David Calhas",
            "Arlindo L. Oliveira"
        ],
        "title": "Deep Recurrence for Dynamical Segmentation Models",
        "abstract": "arXiv:2507.10143v1 Announce Type: new  Abstract: While biological vision systems rely heavily on feedback connections to iteratively refine perception, most artificial neural networks remain purely feedforward, processing input in a single static pass. In this work, we propose a predictive coding inspired feedback mechanism that introduces a recurrent loop from output to input, allowing the model to refine its internal state over time. We implement this mechanism within a standard U-Net architecture and introduce two biologically motivated operations, softmax projection and exponential decay, to ensure stability of the feedback loop. Through controlled experiments on a synthetic segmentation task, we show that the feedback model significantly outperforms its feedforward counterpart in noisy conditions and generalizes more effectively with limited supervision. Notably, feedback achieves above random performance with just two training examples, while the feedforward model requires at least four. Our findings demonstrate that feedback enhances robustness and data efficiency, and offer a path toward more adaptive and biologically inspired neural architectures. Code is available at: github.com/DCalhas/feedback_segmentation.",
        "arxiv_id": "2507.10143",
        "ARXIVID": "2507.10143",
        "COMMENT": "Related to criterion 1 (methodological improvements for spatial understanding) as it introduces a biologically inspired feedback mechanism for segmentation models, improving robustness and data efficiency. Not directly about embodied agents, but relevant for spatial intelligence in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09183": {
        "authors": [
            "Yongwei Jiang",
            "Yixiong Zou",
            "Yuhua Li",
            "Ruixuan Li"
        ],
        "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning",
        "abstract": "arXiv:2507.09183v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data scarcity and incremental learning in real-world scenarios. While pool-based prompting methods have demonstrated success in traditional incremental learning, their effectiveness in FSCIL settings remains unexplored. This paper presents the first study of current prompt pool methods in FSCIL tasks, revealing an unanticipated performance degradation in incremental sessions. Through comprehensive analysis, we identify that this phenomenon stems from token-dimension saturation: with limited data, excessive prompts compete for task-relevant information, leading to model overfitting. Based on this finding, we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively shifts pool-based prompt learning from the token dimension to the spatial dimension. LGSP-Prompt generates spatial prompts by synergistically combining local spatial features and global frequency-domain representations to highlight key patterns in input images. We construct two spatial prompt pools enabling dynamic prompt selection to maintain acquired knowledge while effectively learning novel sessions. Extensive experiments demonstrate that our approach achieves state-of-the-art performance across multiple FSCIL benchmarks, showing significant advantages in both base knowledge preservation and incremental learning. Our implementation is available at https://github.com/Jywsuperman/LGSP.",
        "arxiv_id": "2507.09183",
        "ARXIVID": "2507.09183",
        "COMMENT": "This paper revisits prompt learning for few-shot class-incremental learning, introducing a spatial prompt pool and new analysis. While it is relevant to vision foundation models and prompt learning, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or new benchmarks. It is most relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09242": {
        "authors": [
            "Shiqi Jiang",
            "Xinpeng Li",
            "Xi Mao",
            "Changbo Wang",
            "Chenhui Li"
        ],
        "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process",
        "abstract": "arXiv:2507.09242v1 Announce Type: new  Abstract: Artistic image assessment has become a prominent research area in computer vision. In recent years, the field has witnessed a proliferation of datasets and methods designed to evaluate the aesthetic quality of paintings. However, most existing approaches focus solely on static final images, overlooking the dynamic and multi-stage nature of the artistic painting process. To address this gap, we propose a novel framework for human-aligned assessment of painting processes. Specifically, we introduce the Painting Process Assessment Dataset (PPAD), the first large-scale dataset comprising real and synthetic painting process images, annotated by domain experts across eight detailed attributes. Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based model enhanced with temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture, enabling effective assessment of the painting process. Experimental results demonstrate that our method outperforms existing baselines in accuracy, robustness, and alignment with human judgment, offering new insights into computational creativity and art education.",
        "arxiv_id": "2507.09242",
        "ARXIVID": "2507.09242",
        "COMMENT": "This paper introduces a new dataset and model for assessing artistic painting processes, focusing on temporal and process-based evaluation. It is relevant to vision foundation models and their applications (criterion 4), as it proposes a new benchmark and transformer-based model for a novel task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09308": {
        "authors": [
            "Zile Wang",
            "Hao Yu",
            "Jiabo Zhan",
            "Chun Yuan"
        ],
        "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning",
        "abstract": "arXiv:2507.09308v1 Announce Type: new  Abstract: Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on https://github.com/o0o0o00o0/AlphaVAE for reproducibility.",
        "arxiv_id": "2507.09308",
        "ARXIVID": "2507.09308",
        "COMMENT": "This paper introduces a new benchmark and method for RGBA image generation and reconstruction, extending VAEs to handle transparency. It is relevant to vision foundation models and their applications (criterion 4), as it proposes a new benchmark and method for a previously underexplored image type.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09649": {
        "authors": [
            "Zhengyuan Peng",
            "Jianqing Xu",
            "Shen Li",
            "Jiazhen Ji",
            "Yuge Huang",
            "Jingyun Zhang",
            "Jinmin Li",
            "Shouhong Ding",
            "Rizen Guo",
            "Xin Tan",
            "Lizhuang Ma"
        ],
        "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR",
        "abstract": "arXiv:2507.09649v1 Announce Type: new  Abstract: Human-machine interaction through augmented reality (AR) and virtual reality (VR) is increasingly prevalent, requiring accurate and efficient gaze estimation which hinges on the accuracy of eye segmentation to enable smooth user experiences. We introduce EyeSeg, a novel eye segmentation framework designed to overcome key challenges that existing approaches struggle with: motion blur, eyelid occlusion, and train-test domain gaps. In these situations, existing models struggle to extract robust features, leading to suboptimal performance. Noting that these challenges can be generally quantified by uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation framework for AR/VR wherein we explicitly model the uncertainties by performing Bayesian uncertainty learning of a posterior under the closed set prior. Theoretically, we prove that a statistic of the learned posterior indicates segmentation uncertainty levels and empirically outperforms existing methods in downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score and the segmentation result, weighting and fusing multiple gaze estimates for robustness, which proves to be effective especially under motion blur, eyelid occlusion and cross-domain challenges. Moreover, empirical results suggest that EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing previous approaches. The code is publicly available at https://github.com/JethroPeng/EyeSeg.",
        "arxiv_id": "2507.09649",
        "ARXIVID": "2507.09649",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes an uncertainty-aware eye segmentation framework for AR/VR, with Bayesian uncertainty learning and robust segmentation for downstream tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10236": {
        "authors": [
            "Despina Konstantinidou",
            "Dimitrios Karageorgiou",
            "Christos Koutlis",
            "Olga Papadopoulou",
            "Emmanouil Schinas",
            "Symeon Papadopoulos"
        ],
        "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?",
        "abstract": "arXiv:2507.10236v1 Announce Type: new  Abstract: The rapid advancement of generative technologies presents both unprecedented creative opportunities and significant challenges, particularly in maintaining social trust and ensuring the integrity of digital information. Following these concerns, the challenge of AI-Generated Image Detection (AID) becomes increasingly critical. As these technologies become more sophisticated, the quality of AI-generated images has reached a level that can easily deceive even the most discerning observers. Our systematic evaluation highlights a critical weakness in current AI-Generated Image Detection models: while they perform exceptionally well on controlled benchmark datasets, they struggle significantly with real-world variations. To assess this, we introduce ITW-SM, a new dataset of real and AI-generated images collected from major social media platforms. In this paper, we identify four key factors that influence AID performance in real-world scenarios: backbone architecture, training data composition, pre-processing strategies and data augmentation combinations. By systematically analyzing these components, we shed light on their impact on detection efficacy. Our modifications result in an average AUC improvement of 26.87% across various AID models under real-world conditions.",
        "arxiv_id": "2507.10236",
        "ARXIVID": "2507.10236",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a new real-world dataset for AI-generated image detection and analyzes backbone architectures, data, and augmentation strategies for detection models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09862": {
        "authors": [
            "Youliang Zhang",
            "Zhaoyang Li",
            "Duomin Wang",
            "Jiahe Zhang",
            "Deyu Zhou",
            "Zixin Yin",
            "Xili Dai",
            "Gang Yu",
            "Xiu Li"
        ],
        "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation",
        "abstract": "arXiv:2507.09862v1 Announce Type: new  Abstract: The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
        "arxiv_id": "2507.09862",
        "ARXIVID": "2507.09862",
        "COMMENT": "Presents a large-scale dataset for audio-visual dyadic human generation, which is relevant to multi-modal learning but does not introduce a new VLLM/MLLM or a new method for spatial intelligence or embodied AI. Some relevance to criterion 4 (vision foundation models applications) as it provides a benchmark and baseline.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10127": {
        "authors": [
            "Md Abulkalam Azad",
            "John Nyberg",
            "H{\\aa}vard Dalen",
            "Bj{\\o}rnar Grenne",
            "Lasse Lovstakken",
            "Andreas {\\O}stvik"
        ],
        "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion",
        "abstract": "arXiv:2507.10127v1 Announce Type: new  Abstract: Accurate motion estimation for tracking deformable tissues in echocardiography is essential for precise cardiac function measurements. While traditional methods like block matching or optical flow struggle with intricate cardiac motion, modern point tracking approaches remain largely underexplored in this domain. This work investigates the potential of state-of-the-art (SOTA) point tracking methods for ultrasound, with a focus on echocardiography. Although these novel approaches demonstrate strong performance in general videos, their effectiveness and generalizability in echocardiography remain limited. By analyzing cardiac motion throughout the heart cycle in real B-mode ultrasound videos, we identify that a directional motion bias across different views is affecting the existing training strategies. To mitigate this, we refine the training procedure and incorporate a set of tailored augmentations to reduce the bias and enhance tracking robustness and generalization through impartial cardiac motion. We also propose a lightweight network leveraging multi-scale cost volumes from spatial context alone to challenge the advanced spatiotemporal point tracking models. Experiments demonstrate that fine-tuning with our strategies significantly improves models' performances over their baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker boosts overall position accuracy by 60.7% and reduces median trajectory error by 61.5% across heart cycle phases. Interestingly, several point tracking models fail to outperform our proposed simple model in terms of tracking accuracy and generalization, reflecting their limitations when applied to echocardiography. Nevertheless, clinical evaluation reveals that these methods improve GLS measurements, aligning more closely with expert-validated, semi-automated tools and thus demonstrating better reproducibility in real-world applications.",
        "arxiv_id": "2507.10127",
        "ARXIVID": "2507.10127",
        "COMMENT": "Partially matches criterion 1 (methodological improvements to spatial understanding in embodied agents) as it proposes new training strategies and a lightweight network for point tracking in echocardiography, but is focused on medical imaging rather than embodied agents or general spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.09323": {
        "authors": [
            "Kaixuan Cong",
            "Yifan Wang",
            "Rongkun Xue",
            "Yuyang Jiang",
            "Yiming Feng",
            "Jing Yang"
        ],
        "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition",
        "abstract": "arXiv:2507.09323v1 Announce Type: new  Abstract: Humans do not understand individual events in isolation; rather, they generalize concepts within classes and compare them to others. Existing audio-video pre-training paradigms only focus on the alignment of the overall audio-video modalities, without considering the reinforcement of distinguishing easily confused classes through cognitive induction and contrast during training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder (DICCAE), an encoder that aligns audio-video representations at a fine-grained, category-level. DICCAE addresses category confusion by dynamically adjusting the confusion loss based on inter-class confusion degrees, thereby enhancing the model's ability to distinguish between similar activities. To further extend the application of DICCAE, we also introduce a novel training framework that incorporates both audio and video modalities, as well as their fusion. To mitigate the scarcity of audio-video data in the human activity recognition task, we propose a cluster-guided audio-video self-supervised pre-training strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its feature representation quality through extensive ablation studies, validating the necessity of each module.",
        "arxiv_id": "2507.09323",
        "ARXIVID": "2507.09323",
        "COMMENT": "This paper proposes a new encoder for audio-visual fusion in human activity recognition, focusing on inter-class confusion. While it is multi-modal and relevant to computer vision, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models as per the criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.10343": {
        "authors": [
            "Hugo Norrby",
            "Gabriel F\\\"arm",
            "Kevin Hernandez-Diaz",
            "Fernando Alonso-Fernandez"
        ],
        "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans",
        "abstract": "arXiv:2507.10343v1 Announce Type: new  Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic segmentation (FGSS) architecture designed to improve the generalization ability of wall segmentation on floorplans. FGSSNet features a U-Net segmentation backbone with a multi-headed dedicated feature extractor used to extract domain-specific feature maps which are injected into the latent space of U-Net to guide the segmentation process. This dedicated feature extractor is trained as an encoder-decoder with selected wall patches, representative of the walls present in the input floorplan, to produce a compressed latent representation of wall patches while jointly trained to predict the wall width. In doing so, we expect that the feature extractor encodes texture and width features of wall patches that are useful to guide the wall segmentation process. Our experiments show increased performance by the use of such injected features in comparison to the vanilla U-Net, highlighting the validity of the proposed approach.",
        "arxiv_id": "2507.10343",
        "ARXIVID": "2507.10343",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1) as it proposes a new architecture for semantic segmentation of floorplans, but the focus is on wall segmentation and feature extraction rather than embodied agents or general spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.08806": {
        "authors": [
            "Daewon Choi",
            "Jimin Lee",
            "Jihoon Tack",
            "Woomin Song",
            "Saket Dingliwal",
            "Sai Muralidhar Jayanthi",
            "Bhavana Ganesh",
            "Jinwoo Shin",
            "Aram Galstyan",
            "Sravan Babu Bodapati"
        ],
        "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning",
        "abstract": "arXiv:2507.08806v1 Announce Type: new  Abstract: Recent large language models have shown promising capabilities in long-form reasoning, following structured chains of thought before arriving at a final answer. However, we observe that these reasoning paths tend to include substantial redundancy; analyzing attention patterns reveals that attention scores are widely scattered, particularly incorrect answers exhibit greater attention sparsity. In this paper, we demonstrate that deliberately removing this redundancy in the reasoning process significantly improves performance through clear thinking, i.e., removing distraction. Specifically, we systematically identify reasoning redundancy by measuring token-level attention scores to a special end-of-thinking token, which is appended to an explicit instruction inserted to conclude each intermediate reasoning step. Furthermore, we propose structure-aware pruning that prioritizes removing tokens in low-contributing reasoning chunks over individual tokens. After evicting redundant tokens, we remove the injected end-of-thinking instruction, then resume the reasoning generation. We demonstrate that our method significantly improves overall accuracy across reasoning-intensive benchmarks without any training involved. In particular, our method shows strong performance on challenging mathematical competition benchmarks such as AIME and AMC, where reasoning redundancy is more prevalent.",
        "arxiv_id": "2507.08806",
        "ARXIVID": "2507.08806",
        "COMMENT": "Proposes a method for improving reasoning in LLMs by pruning redundant tokens. While interesting for language modeling and reasoning, it does not match the specific criteria (no spatial, vision, or multi-modal focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09514": {
        "authors": [
            "Tien-Yu Chi",
            "Hung-Yueh Chiang",
            "Diana Marculescu",
            "Kai-Chiang Wu"
        ],
        "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models",
        "abstract": "arXiv:2507.09514v1 Announce Type: new  Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.",
        "arxiv_id": "2507.09514",
        "ARXIVID": "2507.09514",
        "COMMENT": "QuarterMap is a post-training token pruning method for vision state space models, improving efficiency in models like VMamba. While relevant to computer vision and model efficiency, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in the sense of criteria 1-4.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10265": {
        "authors": [
            "Xinlong Ding",
            "Hongwei Yu",
            "Jiawei Li",
            "Feifan Li",
            "Yu Shang",
            "Bochao Zou",
            "Huimin Ma",
            "Jiansheng Chen"
        ],
        "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
        "abstract": "arXiv:2507.10265v1 Announce Type: new  Abstract: Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.",
        "arxiv_id": "2507.10265",
        "ARXIVID": "2507.10265",
        "COMMENT": "Focuses on adversarial attacks for pose estimation, which is a vision task, but does not match any specific criterion (not about embodied agents, VLLMs/MLLMs, benchmarks, or foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09980": {
        "authors": [
            "Zhipeng Xue",
            "Yan Zhang",
            "Ming Li",
            "Chun Li",
            "Yue Liu",
            "Fei Yu"
        ],
        "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures",
        "abstract": "arXiv:2507.09980v1 Announce Type: new  Abstract: Existing multi-view classification and clustering methods typically improve task accuracy by leveraging and fusing information from different views. However, ensuring the reliability of multi-view integration and final decisions is crucial, particularly when dealing with noisy or corrupted data. Current methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty of network predictions, ignoring domain gaps between different modalities. To address this issue, KPHD-Net, based on H\\\"older divergence, is proposed for multi-view classification and clustering tasks. Generally, our KPHD-Net employs a variational Dirichlet distribution to represent class probability distributions, models evidences from different views, and then integrates it with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation effects. Our theoretical analysis demonstrates that Proper H\\\"older divergence offers a more effective measure of distribution discrepancies, ensuring enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence theory, recognized for its superior performance in multi-view fusion tasks, is introduced and combined with the Kalman filter to provide future state estimations. This integration further enhances the reliability of the final fusion results. Extensive experiments show that the proposed KPHD-Net outperforms the current state-of-the-art methods in both classification and clustering tasks regarding accuracy, robustness, and reliability, with theoretical guarantees.",
        "arxiv_id": "2507.09980",
        "ARXIVID": "2507.09980",
        "COMMENT": "This paper proposes a new method for uncertainty quantification in multi-view data using divergence measures and evidence theory. While it is a clever statistical approach for multi-modal learning, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is most relevant to general multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10293": {
        "authors": [
            "Wenkang Han",
            "Wang Lin",
            "Yiyun Zhou",
            "Qi Liu",
            "Shulei Wang",
            "Chang Yao",
            "Jingyuan Chen"
        ],
        "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration",
        "abstract": "arXiv:2507.10293v1 Announce Type: new  Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from degraded versions. Traditional methods struggle to preserve fine-grained, identity-specific features when degradation is severe, often producing average-looking faces that lack individual characteristics. To address these challenges, we introduce IP-FVR, a novel method that leverages a high-quality reference face image as a visual prompt to provide identity conditioning during the denoising process. IP-FVR incorporates semantically rich identity information from the reference image using decoupled cross-attention mechanisms, ensuring detailed and identity consistent results. For intra-clip identity drift (within 24 frames), we introduce an identity-preserving feedback learning method that combines cosine similarity-based reward signals with suffix-weighted temporal aggregation. This approach effectively minimizes drift within sequences of frames. For inter-clip identity drift, we develop an exponential blending strategy that aligns identities across clips by iteratively blending frames from previous clips during the denoising process. This method ensures consistent identity representation across different clips. Additionally, we enhance the restoration process with a multi-stream negative prompt, guiding the model's attention to relevant facial attributes and minimizing the generation of low-quality or incorrect features. Extensive experiments on both synthetic and real-world datasets demonstrate that IP-FVR outperforms existing methods in both quality and identity preservation, showcasing its substantial potential for practical applications in face video restoration.",
        "arxiv_id": "2507.10293",
        "ARXIVID": "2507.10293",
        "COMMENT": "This paper introduces a new method for face video restoration with identity preservation using reference images and novel attention mechanisms. While it is a strong generative modeling paper in vision, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10470": {
        "authors": [
            "Zhicun Yin",
            "Junjie Chen",
            "Ming Liu",
            "Zhixin Wang",
            "Fan Li",
            "Renjing Pei",
            "Xiaoming Li",
            "Rynson W. H. Lau",
            "Wangmeng Zuo"
        ],
        "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction",
        "abstract": "arXiv:2507.10470v1 Announce Type: new  Abstract: Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at https://github.com/yinzhicun/RefSTAR.",
        "arxiv_id": "2507.10470",
        "ARXIVID": "2507.10470",
        "COMMENT": "This paper proposes a new method for blind facial image restoration using reference selection, transfer, and reconstruction. While it is a novel generative modeling approach in computer vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is most relevant to general computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09207": {
        "authors": [
            "Alexander C. Ogren",
            "Berthy T. Feng",
            "Jihoon Ahn",
            "Katherine L. Bouman",
            "Chiara Daraio"
        ],
        "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves",
        "abstract": "arXiv:2507.09207v1 Announce Type: new  Abstract: Wave propagation on the surface of a material contains information about physical properties beneath its surface. We propose a method for inferring the thickness and stiffness of a structure from just a video of waves on its surface. Our method works by extracting a dispersion relation from the video and then solving a physics-based optimization problem to find the best-fitting thickness and stiffness parameters. We validate our method on both simulated and real data, in both cases showing strong agreement with ground-truth measurements. Our technique provides a proof-of-concept for at-home health monitoring of medically-informative tissue properties, and it is further applicable to fields such as human-computer interaction.",
        "arxiv_id": "2507.09207",
        "ARXIVID": "2507.09207",
        "COMMENT": "Does not directly match any criterion. Focuses on inferring subsurface properties from video, which is an application of computer vision but not directly related to spatial intelligence in embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09560": {
        "authors": [
            "Bolun Zheng",
            "Xinjie Liu",
            "Qianyu Zhang",
            "Canjin Wang",
            "Fangni Chen",
            "Mingen Xu"
        ],
        "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation",
        "abstract": "arXiv:2507.09560v1 Announce Type: new  Abstract: 3D hand pose estimation has garnered great attention in recent years due to its critical applications in human-computer interaction, virtual reality, and related fields. The accurate estimation of hand joints is essential for high-quality hand pose estimation. However, existing methods neglect the importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints overall and often fail to account for the phenomenon of error accumulation for distal joints in gesture estimation, which can cause certain joints to incur larger errors, resulting in misalignments and artifacts in the pose estimation and degrading the overall reconstruction quality. To address this challenge, we propose a novel segmented architecture for enhanced hand pose estimation (EHPE). We perform local extraction of TIP and wrist, thus alleviating the effect of error accumulation on TIP prediction and further reduce the predictive errors for all joints on this basis. EHPE consists of two key stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions of the TIP and wrist joints are estimated to provide an initial accurate joint configuration; In the Prior Guided Joints Estimation stage (PG-stage), a dual-branch interaction network is employed to refine the positions of the remaining joints. Extensive experiments on two widely used benchmarks demonstrate that EHPE achieves state-of-the-arts performance. Code is available at https://github.com/SereinNout/EHPE.",
        "arxiv_id": "2507.09560",
        "ARXIVID": "2507.09560",
        "COMMENT": "Relevant to computer vision and pose estimation, but does not directly match any specific criterion (focuses on hand pose estimation, not embodied agents or spatial intelligence in a broader sense).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}