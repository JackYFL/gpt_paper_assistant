{
    "2510.15870": {
        "authors": [
            "Hanrong Ye",
            "Chao-Han Huck Yang",
            "Arushi Goel",
            "Wei Huang",
            "Ligeng Zhu",
            "Yuanhang Su",
            "Sean Lin",
            "An-Chieh Cheng",
            "Zhen Wan",
            "Jinchuan Tian",
            "Yuming Lou",
            "Dong Yang",
            "Zhijian Liu",
            "Yukang Chen",
            "Ambrish Dantrey",
            "Ehsan Jahangiri",
            "Sreyan Ghosh",
            "Daguang Xu",
            "Ehsan Hosseini-Asl",
            "Danial Mohseni Taheri",
            "Vidya Murali",
            "Sifei Liu",
            "Jason Lu",
            "Oluwatobi Olabiyi",
            "Frank Wang",
            "Rafael Valle",
            "Bryan Catanzaro",
            "Andrew Tao",
            "Song Han",
            "Jan Kautz",
            "Hongxu Yin",
            "Pavlo Molchanov"
        ],
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
        "abstract": "arXiv:2510.15870v1 Announce Type: new  Abstract: Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
        "arxiv_id": "2510.15870",
        "ARXIVID": "2510.15870",
        "COMMENT": "Matches criterion 2 and 4: Introduces OmniVinci, a new open-source omni-modal LLM with architectural innovations for multi-modal (vision, audio, etc.) alignment and reasoning. Strong empirical results and applications in robotics and medical AI.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2510.15018": {
        "authors": [
            "Mingxuan Liu",
            "Honglin He",
            "Elisa Ricci",
            "Wayne Wu",
            "Bolei Zhou"
        ],
        "title": "UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos",
        "abstract": "arXiv:2510.15018v1 Announce Type: new  Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.",
        "arxiv_id": "2510.15018",
        "ARXIVID": "2510.15018",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator) and criterion 1 (spatial understanding for embodied agents). UrbanVerse is a new real-to-sim system for urban simulation, using city-tour videos to generate interactive, physics-aware environments for embodied agent training.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2510.15841": {
        "authors": [
            "Jiayi Lin",
            "Jiabo Huang",
            "Shaogang Gong"
        ],
        "title": "Neuro-Symbolic Spatial Reasoning in Segmentation",
        "abstract": "arXiv:2510.15841v1 Announce Type: new  Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from an open set of categories, requiring generalization to unseen and unlabelled objects. Using vision-language models (VLMs) to correlate local image patches with potential unseen object categories suffers from a lack of understanding of spatial relations of objects in a scene. To solve this problem, we introduce neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg) to impose explicit spatial relational constraints by first order logic (FOL) formulated in a neural network architecture. This is the first attempt to explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically extracts spatial relations, e.g., , and encodes them as first-order logic formulas using our proposed pseudo categories. Each pixel learns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo category (e.g., \"right of person\") simultaneously, enforcing relational constraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally, these logic constraints are formulated in a deep network architecture by fuzzy logic relaxation, enabling end-to-end learning of spatial-relationally consistent segmentation. RelateSeg achieves state-of-the-art performance in terms of average mIoU across four benchmark datasets and particularly shows clear advantages on images containing multiple categories, with the cost of only introducing a single auxiliary loss function and no additional parameters, validating the effectiveness of NeSy spatial reasoning in OVSS.",
        "arxiv_id": "2510.15841",
        "ARXIVID": "2510.15841",
        "COMMENT": "Directly matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a neuro-symbolic spatial reasoning approach for open-vocabulary semantic segmentation, introducing explicit spatial relational constraints via first-order logic in a deep network.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.15148": {
        "authors": [
            "Xingrui Wang",
            "Jiang Liu",
            "Chao Huang",
            "Xiaodong Yu",
            "Ze Wang",
            "Ximeng Sun",
            "Jialian Wu",
            "Alan Yuille",
            "Emad Barsoum",
            "Zicheng Liu"
        ],
        "title": "XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models",
        "abstract": "arXiv:2510.15148v1 Announce Type: new  Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text understanding within a single framework. While existing benchmarks primarily evaluate general cross-modal question-answering ability, it remains unclear whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly designed to measure cross-modal consistency. XModBench comprises 60,828 multiple-choice questions spanning five task families and systematically covers all six modality compositions in question-answer pairs, enabling fine-grained diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and directional imbalance. Experiments show that even the strongest model, Gemini 2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than 60% accuracy, (ii) reveals persistent modality disparities, with performance dropping substantially when the same semantic content is conveyed through audio rather than text, and (iii) shows systematic directional imbalance, exhibiting lower consistency when vision serves as context compared to text. These findings indicate that current OLLMs remain far from truly modality-invariant reasoning and position XModBench as a fundamental diagnostic tool for evaluating and improving cross-modal competence. All data and evaluation tools will be available at https://xingruiwang.github.io/projects/XModBench/.",
        "arxiv_id": "2510.15148",
        "ARXIVID": "2510.15148",
        "COMMENT": "Introduces XModBench, a new large-scale tri-modal benchmark for evaluating cross-modal consistency in omni-modal large language models (OLLMs). This directly matches criterion 2 (new VLLMs/MLLMs) and criterion 3 (new benchmark for embodied/cross-modal AI).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.15050": {
        "authors": [
            "Chao Huang",
            "Zeliang Zhang",
            "Jiang Liu",
            "Ximeng Sun",
            "Jialian Wu",
            "Xiaodong Yu",
            "Ze Wang",
            "Chenliang Xu",
            "Emad Barsoum",
            "Zicheng Liu"
        ],
        "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
        "abstract": "arXiv:2510.15050v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their reasoning ability often lags behind that of strong text-only counterparts. Existing methods to bridge this gap rely on supervised fine-tuning over large-scale multimodal reasoning data or reinforcement learning, both of which are resource-intensive. A promising alternative is model merging, which interpolates parameters between reasoning-enhanced LLMs and multimodal variants. However, our analysis shows that naive merging is not always a \"free lunch\": its effectiveness varies drastically across model families, with some (e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance degradation. To address this, we propose Directional Reasoning Injection for Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning knowledge in the gradient space, without destabilizing multimodal alignment. DRIFT precomputes a reasoning prior as the parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning. This approach preserves the simplicity of standard supervised fine-tuning pipelines while enabling efficient reasoning transfer. Extensive experiments on multimodal reasoning benchmarks, including MathVista and MathVerse, demonstrate that DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning, while matching or surpassing training-heavy methods at a fraction of the cost.",
        "arxiv_id": "2510.15050",
        "ARXIVID": "2510.15050",
        "COMMENT": "Matches criterion 2: This paper proposes a new method (DRIFT) for improving reasoning in MLLMs (multi-modal large language models) by transferring reasoning knowledge in the gradient space. It is directly about MLLMs and introduces a novel fine-tuning approach for reasoning transfer.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.15259": {
        "authors": [
            "Chenwei Tang",
            "Jingyu Xing",
            "Xinyu Liu",
            "Zizhou Wang",
            "Jiawei Du",
            "Liangli Zhen",
            "Jiancheng Lv"
        ],
        "title": "Experience-Driven Exploration for Efficient API-Free AI Agents",
        "abstract": "arXiv:2510.15259v1 Announce Type: new  Abstract: Most existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.",
        "arxiv_id": "2510.15259",
        "ARXIVID": "2510.15259",
        "COMMENT": "Matches criterion 1 and 3: Proposes a new method (KG-Agent) for spatial/strategic exploration in embodied agents operating in GUI environments, using a knowledge graph for state-action representation and hybrid intrinsic rewards. Evaluated in complex, open-ended environments (simulator-like).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.15041": {
        "authors": [
            "Yichen Li",
            "Zhiyi Li",
            "Brandon Feng",
            "Dinghuai Zhang",
            "Antonio Torralba"
        ],
        "title": "Generalized Dynamics Generation towards Scannable Physical World Model",
        "abstract": "arXiv:2510.15041v1 Announce Type: new  Abstract: Digital twin worlds with realistic interactive dynamics presents a new opportunity to develop generalist embodied agents in scannable environments with complex physical behaviors. To this end, we present GDGen (Generalized Representation for Generalized Dynamics Generation), a framework that takes a potential energy perspective to seamlessly integrate rigid body, articulated body, and soft body dynamics into a unified, geometry-agnostic system. GDGen operates from the governing principle that the potential energy for any stable physical system should be low. This fresh perspective allows us to treat the world as one holistic entity and infer underlying physical properties from simple motion observations. We extend classic elastodynamics by introducing directional stiffness to capture a broad spectrum of physical behaviors, covering soft elastic, articulated, and rigid body systems. We propose a specialized network to model the extended material property and employ a neural field to represent deformation in a geometry-agnostic manner. Extensive experiments demonstrate that GDGen robustly unifies diverse simulation paradigms, offering a versatile foundation for creating interactive virtual environments and training robotic agents in complex, dynamically rich scenarios.",
        "arxiv_id": "2510.15041",
        "ARXIVID": "2510.15041",
        "COMMENT": "Matches criterion 3 (embodied AI, new simulator methods) and criterion 1 (spatial understanding in embodied agents). Proposes a unified framework for simulating diverse physical dynamics, enabling more realistic digital twins for embodied agent training.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.15846": {
        "authors": [
            "Pramod Rao",
            "Abhimitra Meka",
            "Xilong Zhou",
            "Gereon Fox",
            "Mallikarjun B R",
            "Fangneng Zhan",
            "Tim Weyrich",
            "Bernd Bickel",
            "Hanspeter Pfister",
            "Wojciech Matusik",
            "Thabo Beeler",
            "Mohamed Elgharib",
            "Marc Habermann",
            "Christian Theobalt"
        ],
        "title": "3DPR: Single Image 3D Portrait Relight using Generative Priors",
        "abstract": "arXiv:2510.15846v1 Announce Type: new  Abstract: Rendering novel, relit views of a human head, given a monocular portrait image as input, is an inherently underconstrained problem. The traditional graphics solution is to explicitly decompose the input image into geometry, material and lighting via differentiable rendering; but this is constrained by the multiple assumptions and approximations of the underlying models and parameterizations of these scene components. We propose 3DPR, an image-based relighting model that leverages generative priors learnt from multi-view One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a high-quality prior over the distribution of high-frequency face reflectance. We leverage the latent space of a pre-trained generative head model that provides a rich prior over face geometry learnt from in-the-wild image datasets. The input portrait is first embedded in the latent manifold of such a model through an encoder-based inversion process. Then a novel triplane-based reflectance network trained on our lightstage data is used to synthesize high-fidelity OLAT images to enable image-based relighting. Our reflectance network operates in the latent space of the generative head model, crucially enabling a relatively small number of lightstage images to train the reflectance model. Combining the generated OLATs according to a given HDRI environment maps yields physically accurate environmental relighting results. Through quantitative and qualitative evaluations, we demonstrate that 3DPR outperforms previous methods, particularly in preserving identity and in capturing lighting effects such as specularities, self-shadows, and subsurface scattering. Project Page: https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "arxiv_id": "2510.15846",
        "ARXIVID": "2510.15846",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is highly relevant to generative modeling in multi-modal learning. Proposes a generative prior-based method for 3D portrait relighting from a single image, with a new dataset and clever use of generative latent spaces.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.15072": {
        "authors": [
            "Jiaxin Guo",
            "Tongfan Guan",
            "Wenzhen Dong",
            "Wenzhao Zheng",
            "Wenting Wang",
            "Yue Wang",
            "Yeung Yam",
            "Yun-Hui Liu"
        ],
        "title": "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images",
        "abstract": "arXiv:2510.15072v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Project Page: https://wrld.github.io/SaLon3R/.",
        "arxiv_id": "2510.15072",
        "ARXIVID": "2510.15072",
        "COMMENT": "Matches criterion 3 (embodied AI papers on new benchmarks or methods, simulator related, novel angles). Proposes a new structure-aware, long-term 3D Gaussian Splatting method for generalizable 3D reconstruction from unposed images, with a novel redundancy removal and transformer-based refinement.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.15040": {
        "authors": [
            "Xinyi Gu",
            "Jiayuan Mao",
            "Zhang-Wei Hong",
            "Zhuoran Yu",
            "Pengyuan Li",
            "Dhiraj Joshi",
            "Rogerio Feris",
            "Zexue He"
        ],
        "title": "Composition-Grounded Instruction Synthesis for Visual Reasoning",
        "abstract": "arXiv:2510.15040v1 Announce Type: new  Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong performance on diverse multimodal tasks, but remain limited in reasoning capabilities for domains where annotations are difficult to collect. In this work, we focus on artificial image domains such as charts, rendered documents, and webpages, which are abundant in practice yet lack large-scale human annotated reasoning datasets. We introduce COGS (COmposition-Grounded instruction Synthesis), a data-efficient framework for equipping MLLMs with advanced reasoning abilities from a small set of seed questions. The key idea is to decompose each seed question into primitive perception and reasoning factors, which can then be systematically recomposed with new images to generate large collections of synthetic question-answer pairs. Each generated question is paired with subquestions and intermediate answers, enabling reinforcement learning with factor-level process rewards. Experiments on chart reasoning show that COGS substantially improves performance on unseen questions, with the largest gains on reasoning-heavy and compositional questions. Moreover, training with a factor-level mixture of different seed data yields better transfer across multiple datasets, suggesting that COGS induces generalizable capabilities rather than dataset-specific overfitting. We further demonstrate that the framework extends beyond charts to other domains such as webpages.",
        "arxiv_id": "2510.15040",
        "ARXIVID": "2510.15040",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new data-efficient framework (COGS) for improving reasoning in MLLMs, especially in artificial image domains, with a novel compositional instruction synthesis approach.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.15301": {
        "authors": [
            "Minglei Shi",
            "Haolin Wang",
            "Wenzhao Zheng",
            "Ziyang Yuan",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Latent Diffusion Model without Variational Autoencoder",
        "abstract": "arXiv:2510.15301v1 Announce Type: new  Abstract: Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations.",
        "arxiv_id": "2510.15301",
        "ARXIVID": "2510.15301",
        "COMMENT": "Matches criterion 4: Proposes SVG, a latent diffusion model without VAEs, leveraging self-supervised representations (DINO) for more efficient and semantically meaningful generative modeling. Relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.15742": {
        "authors": [
            "Qingyan Bai",
            "Qiuyu Wang",
            "Hao Ouyang",
            "Yue Yu",
            "Hanlin Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Shuailei Ma",
            "Yanhong Zeng",
            "Zichen Liu",
            "Yinghao Xu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset",
        "abstract": "arXiv:2510.15742v1 Announce Type: new  Abstract: Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.",
        "arxiv_id": "2510.15742",
        "ARXIVID": "2510.15742",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (scaling instruction-based video editing, which is a multi-modal generative task). Introduces a large-scale synthetic dataset and a new model for instruction-based video editing.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.15849": {
        "authors": [
            "Joongwon Chae",
            "Lihui Luo",
            "Xi Yuan",
            "Dongmei Yu",
            "Zhenglin Chen",
            "Lian Zhang",
            "Peiwu Qin"
        ],
        "title": "Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt",
        "abstract": "arXiv:2510.15849v1 Announce Type: new  Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised models require large annotated datasets, while SAM-family models remain prompt-driven. We present Memory-SAM, a training-free, human-prompt-free pipeline that automatically generates effective prompts from a small memory of prior cases via dense DINOv3 features and FAISS retrieval. Given a query image, mask-constrained correspondences to the retrieved exemplar are distilled into foreground/background point prompts that guide SAM2 without manual clicks or model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled, 300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863, surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On controlled data, ceiling effects above 0.98 make small differences less meaningful given annotation variability, while our method shows clear gains under real-world conditions. Results indicate that retrieval-to-prompt enables data-efficient, robust segmentation of irregular boundaries in tongue imaging. The code is publicly available at https://github.com/jw-chae/memory-sam.",
        "arxiv_id": "2510.15849",
        "ARXIVID": "2510.15849",
        "COMMENT": "This paper proposes Memory-SAM, a training-free, human-prompt-free segmentation pipeline using retrieval-to-prompt for tongue segmentation. It leverages vision foundation models (SAM2, DINOv3) and applies them in a novel way for robust segmentation. This matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15261": {
        "authors": [
            "Jitesh Jain",
            "Shubham Maheshwari",
            "Ning Yu",
            "Wen-mei Hwu",
            "Humphrey Shi"
        ],
        "title": "AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory",
        "abstract": "arXiv:2510.15261v1 Announce Type: new  Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG), there has been a growing interest in augmenting agent systems with external memory databases. However, the existing systems focus on storing text information in their memory, ignoring the importance of multimodal signals. Motivated by the multimodal nature of human memory, we present AUGUSTUS, a multimodal agent system aligned with the ideas of human memory in cognitive science. Technically, our system consists of 4 stages connected in a loop: (i) encode: understanding the inputs; (ii) store in memory: saving important information; (iii) retrieve: searching for relevant context from memory; and (iv) act: perform the task. Unlike existing systems that use vector databases, we propose conceptualizing information into semantic tags and associating the tags with their context to store them in a graph-structured multimodal contextual memory for efficient concept-driven retrieval. Our system outperforms the traditional multimodal RAG approach while being 3.5 times faster for ImageNet classification and outperforming MemGPT on the MSC benchmark.",
        "arxiv_id": "2510.15261",
        "ARXIVID": "2510.15261",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new multimodal agent system (AUGUSTUS) with graph-structured contextual memory, outperforming existing multimodal RAG systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15104": {
        "authors": [
            "Guofeng Zhang",
            "Angtian Wang",
            "Jacob Zhiyuan Fang",
            "Liming Jiang",
            "Haotian Yang",
            "Bo Liu",
            "Yiding Yang",
            "Guang Chen",
            "Longyin Wen",
            "Alan Yuille",
            "Chongyang Ma"
        ],
        "title": "TGT: Text-Grounded Trajectories for Locally Controlled Video Generation",
        "abstract": "arXiv:2510.15104v1 Announce Type: new  Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas standard methods still have limited ability to control the subject composition of generated scenes. Prior work shows that adding localized text control signals, such as bounding boxes or segmentation masks, can help. However, these methods struggle in complex scenarios and degrade in multi-object settings, offering limited precision and lacking a clear correspondence between individual trajectories and visual entities as the number of controllable objects increases. We introduce Text-Grounded Trajectories (TGT), a framework that conditions video generation on trajectories paired with localized text descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate these signals and adopt a dual-CFG scheme to separately modulate local and global text guidance. In addition, we develop a data processing pipeline that produces trajectories with localized descriptions of tracked entities, and we annotate two million high quality video clips to train TGT. Together, these components enable TGT to use point trajectories as intuitive motion handles, pairing each trajectory with text to control both appearance and motion. Extensive experiments show that TGT achieves higher visual quality, more accurate text alignment, and improved motion controllability compared with prior approaches. Website: https://textgroundedtraj.github.io.",
        "arxiv_id": "2510.15104",
        "ARXIVID": "2510.15104",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is also relevant to generative modeling in multi-modal learning. Proposes a new framework (TGT) for locally controlled text-to-video generation with novel trajectory-based conditioning and a large annotated dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15015": {
        "authors": [
            "Mor Ventura",
            "Michael Toker",
            "Or Patashnik",
            "Yonatan Belinkov",
            "Roi Reichart"
        ],
        "title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models",
        "abstract": "arXiv:2510.15015v1 Announce Type: new  Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable to semantic leakage, the unintended transfer of semantically related features between distinct entities. Existing mitigation strategies are often optimization-based or dependent on external inputs. We introduce DeLeaker, a lightweight, optimization-free inference-time approach that mitigates leakage by directly intervening on the model's attention maps. Throughout the diffusion process, DeLeaker dynamically reweights attention maps to suppress excessive cross-entity interactions while strengthening the identity of each entity. To support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages), the first dataset dedicated to semantic leakage, comprising 1,130 human-verified samples spanning diverse scenarios, together with a novel automatic evaluation framework. Experiments demonstrate that DeLeaker consistently outperforms all baselines, even when they are provided with external information, achieving effective leakage mitigation without compromising fidelity or quality. These results underscore the value of attention control and pave the way for more semantically precise T2I models.",
        "arxiv_id": "2510.15015",
        "ARXIVID": "2510.15015",
        "COMMENT": "Matches criterion 4: Proposes DeLeaker, a novel inference-time method for mitigating semantic leakage in text-to-image diffusion models, with a new dataset and evaluation framework. Focuses on vision foundation models and their generative quality.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15271": {
        "authors": [
            "Jingrui Yu",
            "Jun Liu",
            "Kefei Ren",
            "Joydeep Biswas",
            "Rurui Ye",
            "Keqiang Wu",
            "Chirag Majithia",
            "Di Zeng"
        ],
        "title": "CuSfM: CUDA-Accelerated Structure-from-Motion",
        "abstract": "arXiv:2510.15271v1 Announce Type: new  Abstract: Efficient and accurate camera pose estimation forms the foundational requirement for dense reconstruction in autonomous navigation, robotic perception, and virtual simulation systems. This paper addresses the challenge via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that leverages GPU parallelization to efficiently employ computationally intensive yet highly accurate feature extractors, generating comprehensive and non-redundant data associations for precise camera pose estimation and globally consistent mapping. The system supports pose optimization, mapping, prior-map localization, and extrinsic refinement. It is designed for offline processing, where computational resources can be fully utilized to maximize accuracy. Experimental results demonstrate that cuSfM achieves significantly improved accuracy and processing speed compared to the widely used COLMAP method across various testing scenarios, while maintaining the high precision and global consistency essential for offline SfM applications. The system is released as an open-source Python wrapper implementation, PyCuSfM, available at https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and applications in computer vision and robotics.",
        "arxiv_id": "2510.15271",
        "ARXIVID": "2510.15271",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents) and criterion 3 (embodied AI, simulator-related, new methods). cuSfM is a CUDA-accelerated Structure-from-Motion system for camera pose estimation, with open-source code and strong empirical results, relevant for spatial intelligence in robotics and simulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15510": {
        "authors": [
            "Heeseong Shin",
            "Byeongho Heo",
            "Dongyoon Han",
            "Seungryong Kim",
            "Taekyung Kim"
        ],
        "title": "Exploring Conditions for Diffusion models in Robotic Control",
        "abstract": "arXiv:2510.15510v1 Announce Type: new  Abstract: While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.",
        "arxiv_id": "2510.15510",
        "ARXIVID": "2510.15510",
        "COMMENT": "Explores the use of pre-trained diffusion models for robotic control, introducing ORCA for task-adaptive visual representations. This is a novel method for embodied AI (criterion 3), and leverages vision foundation models (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.15595": {
        "authors": [
            "Zhen Sun",
            "Lei Tan",
            "Yunhang Shen",
            "Chengmao Cai",
            "Xing Sun",
            "Pingyang Dai",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification",
        "abstract": "arXiv:2510.15595v1 Announce Type: new  Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images across different modalities. However, most existing methods focus on limited cross-modal settings and fail to support arbitrary query-retrieval combinations, hindering practical deployment. We propose FlexiReID, a flexible framework that supports seven retrieval modes across four modalities: rgb, infrared, sketches, and text. FlexiReID introduces an adaptive mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality features and a cross-modal query fusion module to enhance multimodal feature extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a unified dataset extending four popular Re-ID datasets to include all four modalities. Extensive experiments demonstrate that FlexiReID achieves state-of-the-art performance and offers strong generalization in complex scenarios.",
        "arxiv_id": "2510.15595",
        "ARXIVID": "2510.15595",
        "COMMENT": "Matches criterion 4: Proposes FlexiReID, a multi-modal person re-identification framework using an adaptive mixture-of-experts and a new dataset, relevant to vision foundation models and multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.15449": {
        "authors": [
            "Zhiqiang Zhu",
            "Xinbo Gao",
            "Wen Lu",
            "Jie Li",
            "Zhaoyang Wang",
            "Mingqian Ge"
        ],
        "title": "DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking",
        "abstract": "arXiv:2510.15449v1 Announce Type: new  Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on spatial localization supervision, which fails to provide fine-grained cues that point to target features and inevitably produces vague prompts. This limitation impairs the tracker's ability to accurately focus on the object features and results in trackers still performing poorly. To address this issue, we propose DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by encoding the given object's attribute features into the directional kernel enriched with fine-grained cues to generate precise prompts. Specifically, drawing inspiration from visual bionics, DPTrack first hierarchically captures the object's topological structure, leveraging topological attributes to enrich the feature representation. Subsequently, an encoder condenses these topology-aware features into the directional kernel, which serves as the core guidance signal that explicitly encapsulates the object's fine-grained attribute cues. Finally, a kernel-guided prompt module built on channel-category correspondence attributes propagates the kernel across the features of the search region to pinpoint the positions of target features and convert them into precise prompts, integrating spatial gating for robust nighttime tracking. Extensive evaluations on established benchmarks demonstrate DPTrack's superior performance. Our code will be available at https://github.com/zzq-vipsl/DPTrack.",
        "arxiv_id": "2510.15449",
        "ARXIVID": "2510.15449",
        "COMMENT": "Matches criterion 4: Proposes DPTrack, a prompt-based aerial tracker for nighttime scenarios, using kernel-guided prompt learning and topological cues for robust tracking. Applies vision foundation model concepts to a challenging vision task.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.15725": {
        "authors": [
            "Tingyu Lin",
            "Armin Dadras",
            "Florian Kleber",
            "Robert Sablatnig"
        ],
        "title": "DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification",
        "abstract": "arXiv:2510.15725v1 Announce Type: new  Abstract: Camera movement classification (CMC) models trained on contemporary, high-quality footage often degrade when applied to archival film, where noise, missing frames, and low contrast obscure motion cues. We bridge this gap by assembling a unified benchmark that consolidates two modern corpora into four canonical classes and restructures the HISTORIAN collection into five balanced categories. Building on this benchmark, we introduce DGME-T, a lightweight extension to the Video Swin Transformer that injects directional grid motion encoding, derived from optical flow, via a learnable and normalised late-fusion layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and its macro F1 from 82.08% to 87.81% on modern clips, while still improving the demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72% to 82.63% macro F1. A cross-domain study further shows that an intermediate fine-tuning stage on modern data increases historical performance by more than five percentage points. These results demonstrate that structured motion priors and transformer representations are complementary and that even a small, carefully calibrated motion head can substantially enhance robustness in degraded film analysis. Related resources are available at https://github.com/linty5/DGME-T.",
        "arxiv_id": "2510.15725",
        "ARXIVID": "2510.15725",
        "COMMENT": "Matches criterion 1 (methodological improvement for spatial/motion understanding) and criterion 4 (vision foundation models, as it extends Video Swin Transformer). Introduces a new motion encoding for transformers and a benchmark for historical camera movement classification.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.15386": {
        "authors": [
            "Ting-Yu Yen",
            "Yu-Sheng Chiu",
            "Shih-Hsuan Hung",
            "Peter Wonka",
            "Hung-Kuo Chu"
        ],
        "title": "PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction",
        "abstract": "arXiv:2510.15386v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality, real-time novel-view synthesis from multi-view images. However, most existing methods assume the object is captured in a single, static pose, resulting in incomplete reconstructions that miss occluded or self-occluded regions. We introduce PFGS, a pose-aware 3DGS framework that addresses the practical challenge of reconstructing complete objects from multi-pose image captures. Given images of an object in one main pose and several auxiliary poses, PFGS iteratively fuses each auxiliary set into a unified 3DGS representation of the main pose. Our pose-aware fusion strategy combines global and local registration to merge views effectively and refine the 3DGS model. While recent advances in 3D foundation models have improved registration robustness and efficiency, they remain limited by high memory demands and suboptimal accuracy. PFGS overcomes these challenges by incorporating them more intelligently into the registration process: it leverages background features for per-pose camera pose estimation and employs foundation models for cross-pose registration. This design captures the best of both approaches while resolving background inconsistency issues. Experimental results demonstrate that PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.",
        "arxiv_id": "2510.15386",
        "ARXIVID": "2510.15386",
        "COMMENT": "PFGS is a pose-aware 3D Gaussian Splatting framework for multi-pose object reconstruction, using foundation models for cross-pose registration. This is a methodological improvement in spatial understanding (criterion 1) and applies vision foundation models (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.15564": {
        "authors": [
            "Xiaoming Zhu",
            "Xu Huang",
            "Qinghongbing Xie",
            "Zhi Deng",
            "Junsheng Yu",
            "Yirui Guan",
            "Zhongyuan Liu",
            "Lin Zhu",
            "Qijun Zhao",
            "Ligang Liu",
            "Long Zeng"
        ],
        "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
        "abstract": "arXiv:2510.15564v1 Announce Type: new  Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium.",
        "arxiv_id": "2510.15564",
        "ARXIVID": "2510.15564",
        "COMMENT": "Proposes a vision-guided 3D scene layout generation system with a new asset library, image parsing, and scene graph optimization. This is a novel method for spatial understanding and 3D scene generation, matching criterion 1. Also relevant to vision foundation model applications (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.15019": {
        "authors": [
            "Junliang Ye",
            "Shenghao Xie",
            "Ruowen Zhao",
            "Zhengyi Wang",
            "Hongyu Yan",
            "Wenqiang Zu",
            "Lei Ma",
            "Jun Zhu"
        ],
        "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
        "abstract": "arXiv:2510.15019v1 Announce Type: new  Abstract: 3D object editing is essential for interactive content creation in gaming, animation, and robotics, yet current approaches remain inefficient, inconsistent, and often fail to preserve unedited regions. Most methods rely on editing multi-view renderings followed by reconstruction, which introduces artifacts and limits practicality. To address these challenges, we propose Nano3D, a training-free framework for precise and coherent 3D object editing without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized edits guided by front-view renderings, and further introduces region-aware merging strategies, Voxel/Slat-Merge, which adaptively preserve structural fidelity by ensuring consistency between edited and unedited areas. Experiments demonstrate that Nano3D achieves superior 3D consistency and visual quality compared with existing methods. Based on this framework, we construct the first large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000 high-quality 3D editing pairs. This work addresses long-standing challenges in both algorithm design and data availability, significantly improving the generality and reliability of 3D editing, and laying the groundwork for the development of feed-forward 3D editing models. Project Page:https://jamesyjl.github.io/Nano3D",
        "arxiv_id": "2510.15019",
        "ARXIVID": "2510.15019",
        "COMMENT": "Nano3D is a training-free framework for efficient 3D object editing without masks, introducing new merging strategies and a large-scale 3D editing dataset. This is a methodological improvement in 3D spatial understanding (criterion 1), and relevant to vision foundation model applications (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.15467": {
        "authors": [
            "Lingfeng Xuan",
            "Chang Nie",
            "Yiqing Xu",
            "Zhe Liu",
            "Yanzi Miao",
            "Hesheng Wang"
        ],
        "title": "MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes",
        "abstract": "arXiv:2510.15467v1 Announce Type: new  Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point clouds, forming a foundation for various tasks. However, applying SfM to driving scenes captured by multi-camera systems presents significant difficulties, including unreliable pose estimation, excessive outliers in road surface reconstruction, and low reconstruction efficiency. To address these limitations, we propose a Multi-camera Reconstruction and Aggregation Structure-from-Motion (MRASfM) framework specifically designed for driving scenes. MRASfM enhances the reliability of camera pose estimation by leveraging the fixed spatial relationships within the multi-camera system during the registration process. To improve the quality of road surface reconstruction, our framework employs a plane model to effectively remove erroneous points from the triangulated road surface. Moreover, treating the multi-camera set as a single unit in Bundle Adjustment (BA) helps reduce optimization variables to boost efficiency. In addition, MRASfM achieves multi-scene aggregation through scene association and assembly modules in a coarse-to-fine fashion. We deployed multi-camera systems on actual vehicles to validate the generalizability of MRASfM across various scenes and its robustness in challenging conditions through real-world applications. Furthermore, large-scale validation results on public datasets show the state-of-the-art performance of MRASfM, achieving 0.124 absolute pose error on the nuScenes dataset.",
        "arxiv_id": "2510.15467",
        "ARXIVID": "2510.15467",
        "COMMENT": "Presents MRASfM, a new structure-from-motion framework for multi-camera driving scenes, with improvements in pose estimation and road surface reconstruction. This is a methodological improvement in spatial understanding for embodied agents (criterion 1), and relevant to embodied AI (criterion 3).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.15194": {
        "authors": [
            "Tianchen Zhao",
            "Xuanbai Chen",
            "Zhihua Li",
            "Jun Fang",
            "Dongsheng An",
            "Xiang Xu",
            "Zhuowen Tu",
            "Yifan Xing"
        ],
        "title": "Salient Concept-Aware Generative Data Augmentation",
        "abstract": "arXiv:2510.15194v1 Announce Type: new  Abstract: Recent generative data augmentation methods conditioned on both image and text prompts struggle to balance between fidelity and diversity, as it is challenging to preserve essential image details while aligning with varied text prompts. This challenge arises because representations in the synthesis process often become entangled with non-essential input image attributes such as environmental contexts, creating conflicts with text prompts intended to modify these elements. To address this, we propose a personalized image generation framework that uses a salient concept-aware image embedding model to reduce the influence of irrelevant visual details during the synthesis process, thereby maintaining intuitive alignment between image and text inputs. By generating images that better preserve class-discriminative features with additional controlled variations, our framework effectively enhances the diversity of training datasets and thereby improves the robustness of downstream models. Our approach demonstrates superior performance across eight fine-grained vision datasets, outperforming state-of-the-art augmentation methods with averaged classification accuracy improvements by 0.73% and 6.5% under conventional and long-tail settings, respectively.",
        "arxiv_id": "2510.15194",
        "ARXIVID": "2510.15194",
        "COMMENT": "This paper proposes a salient concept-aware generative data augmentation framework for vision tasks, using a novel image embedding model to disentangle relevant features. It is relevant to generative modeling in vision and data augmentation, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is a clever generative augmentation method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.15831": {
        "authors": [
            "Do Xuan Long",
            "Xingchen Wan",
            "Hootan Nakhost",
            "Chen-Yu Lee",
            "Tomas Pfister",
            "Sercan \\\"O. Ar{\\i}k"
        ],
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "abstract": "arXiv:2510.15831v1 Announce Type: new  Abstract: Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "arxiv_id": "2510.15831",
        "ARXIVID": "2510.15831",
        "COMMENT": "VISTA is a novel multi-agent system for test-time self-improving video generation, using iterative prompt refinement and agent-based critique. While it is a generative model for video, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models, but is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.15138": {
        "authors": [
            "Anthony Bilic",
            "Guangyu Sun",
            "Ming Li",
            "Md Sanzid Bin Hossain",
            "Yu Tian",
            "Wei Zhang",
            "Laura Brattain",
            "Dexter Hadley",
            "Chen Chen"
        ],
        "title": "Fourier Transform Multiple Instance Learning for Whole Slide Image Classification",
        "abstract": "arXiv:2510.15138v1 Announce Type: new  Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning (MIL) with spatial patch features, yet existing methods struggle to capture global dependencies due to the immense size of WSIs and the local nature of patch embeddings. This limitation hinders the modeling of coarse structures essential for robust diagnostic prediction.   We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a framework that augments MIL with a frequency-domain branch to provide compact global context. Low-frequency crops are extracted from WSIs via the Fast Fourier Transform and processed through a modular FFT-Block composed of convolutional layers and Min-Max normalization to mitigate the high variance of frequency data. The learned global frequency feature is fused with spatial patch features through lightweight integration strategies, enabling compatibility with diverse MIL architectures.   FFT-MIL was evaluated across six state-of-the-art MIL methods on three public datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1 scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains across architectures and datasets. These results establish frequency-domain learning as an effective and efficient mechanism for capturing global dependencies in WSI classification, complementing spatial features and advancing the scalability and accuracy of MIL-based computational pathology.",
        "arxiv_id": "2510.15138",
        "ARXIVID": "2510.15138",
        "COMMENT": "Presents a new method (FFT-MIL) for WSI classification by integrating frequency-domain features to capture global spatial dependencies. This is a methodological improvement in spatial understanding, matching criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.15296": {
        "authors": [
            "Yiming Lin",
            "Shang Wang",
            "Junkai Zhou",
            "Qiufeng Wang",
            "Xiao-Bo Jin",
            "Kaizhu Huang"
        ],
        "title": "Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning",
        "abstract": "arXiv:2510.15296v1 Announce Type: new  Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging scenario where each training sample is annotated with only one positive label despite potentially belonging to multiple categories, making it difficult to capture complex label relationships and hierarchical structures. While existing methods implicitly model label relationships through distance-based similarity, lacking explicit geometric definitions for different relationship types. To address these limitations, we propose the first hyperbolic classification framework for SPMLL that represents each label as a hyperbolic ball rather than a point or vector, enabling rich inter-label relationship modeling through geometric ball interactions. Our ball-based approach naturally captures multiple relationship types simultaneously: inclusion for hierarchical structures, overlap for co-occurrence patterns, and separation for semantic independence. Further, we introduce two key component innovations: a temperature-adaptive hyperbolic ball classifier and a physics-inspired double-well regularization that guides balls toward meaningful configurations. To validate our approach, extensive experiments on four benchmark datasets (MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive performance with superior interpretability compared to existing methods. Furthermore, statistical analysis reveals strong correlation between learned embeddings and real-world co-occurrence patterns, establishing hyperbolic geometry as a more robust paradigm for structured classification under incomplete supervision.",
        "arxiv_id": "2510.15296",
        "ARXIVID": "2510.15296",
        "COMMENT": "This paper introduces a hyperbolic classification framework for single positive multi-label learning, modeling label relationships with hyperbolic balls. While it is a novel approach to structured classification and label relationships, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is relevant to machine learning and structured prediction.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2510.15752": {
        "authors": [
            "Yitong Sun",
            "Yao Huang",
            "Ruochen Zhang",
            "Huanran Chen",
            "Shouwei Ruan",
            "Ranjie Duan",
            "Xingxing Wei"
        ],
        "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation",
        "abstract": "arXiv:2510.15752v1 Announce Type: new  Abstract: Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at https://github.com/lorraine021/NDM.",
        "arxiv_id": "2510.15752",
        "ARXIVID": "2510.15752",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it addresses safety in text-to-image diffusion models, but focuses on detection/mitigation of implicit sexual content rather than core vision-language modeling or spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.15749": {
        "authors": [
            "Haoran Wang",
            "Bo Zhao",
            "Jinghui Wang",
            "Hanzhang Wang",
            "Huan Yang",
            "Wei Ji",
            "Hao Liu",
            "Xinyan Xiao"
        ],
        "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior",
        "abstract": "arXiv:2510.15749v1 Announce Type: new  Abstract: In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/",
        "arxiv_id": "2510.15749",
        "ARXIVID": "2510.15749",
        "COMMENT": "Does not match any specific criterion. Focuses on content-aware layout generation with a hierarchical reasoning framework, not directly on spatial intelligence for embodied agents, VLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.15400": {
        "authors": [
            "Chen Qian",
            "Haoyu Zhang",
            "Junnan Ma",
            "Liuhong Zhu",
            "Qingrui Cai",
            "Yu Wang",
            "Ruibo Song",
            "Lv Li",
            "Lin Mei",
            "Xianwang Jiang",
            "Qin Xu",
            "Boyu Jiang",
            "Ran Tao",
            "Chunmiao Chen",
            "Shufang Chen",
            "Dongyun Liang",
            "Qiu Guo",
            "Jianzhong Lin",
            "Taishan Kang",
            "Mengtian Lu",
            "Liyuan Fu",
            "Ruibin Huang",
            "Huijuan Wan",
            "Xu Huang",
            "Jianhua Wang",
            "Di Guo",
            "Hai Zhong",
            "Jianjun Zhou",
            "Xiaobo Qu"
        ],
        "title": "Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning",
        "abstract": "arXiv:2510.15400v1 Announce Type: new  Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging (multi-shot DWI) for body-wide tumor diagnostics is limited by severe motion-induced phase artifacts from respiration, peristalsis, and so on, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that overcomes these challenges through physics-informed modeling and synthetic-data-driven prompt learning. We model inter-shot phase variations as a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel matrix reconstruction. Crucially, the algorithm's rank parameter is automatically set via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion. Validated across 10,000+ clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1) Achieved twice the spatial resolution of clinical single-shot DWI, enhancing liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions (liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single model; (3) Outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (11 radiologists' evaluations on a 5-point scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points (good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points (good) on knee and tumor brain. The approach eliminates navigator signals and realistic data supervision, providing an interpretable, robust solution for high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance signifies transformative potential for precision oncology.",
        "arxiv_id": "2510.15400",
        "ARXIVID": "2510.15400",
        "COMMENT": "Related to vision foundation models (criterion 4) in the medical imaging domain, but not directly about general-purpose vision-language or multi-modal models. Focuses on MRI reconstruction using prompt learning and synthetic data.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.15748": {
        "authors": [
            "Minlin Zeng",
            "Zhipeng Zhou",
            "Yang Qiu",
            "Zhiqi Shen"
        ],
        "title": "Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment",
        "abstract": "arXiv:2510.15748v1 Announce Type: new  Abstract: Parkinson's disease assessment has garnered growing interest in recent years, particularly with the advent of sensor data and machine learning techniques. Among these, multimodal approaches have demonstrated strong performance by effectively integrating complementary information from various data sources. However, two major limitations hinder their practical application: (1) the need to synchronize all modalities during training, and (2) the dependence on all modalities during inference. To address these issues, we propose the first Parkinson's assessment system that formulates multimodal learning as a multi-objective optimization (MOO) problem. This not only allows for more flexible modality requirements during both training and inference, but also handles modality collapse issue during multimodal information fusion. In addition, to mitigate the imbalance within individual modalities, we introduce a margin-based class rebalancing strategy to enhance category learning. We conduct extensive experiments on three public datasets under both synchronous and asynchronous settings. The results show that our framework-Towards Relaxed InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous setting, and by 4.86 and 2.30 percentage points in the synchronous setting, highlighting its effectiveness and adaptability.",
        "arxiv_id": "2510.15748",
        "ARXIVID": "2510.15748",
        "COMMENT": "Somewhat related to criterion 2 (multimodal learning), but the focus is on medical assessment (Parkinson's disease) rather than general-purpose VLLMs/MLLMs or vision-language models. Uses multi-objective optimization for multimodal sensor data.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.15392": {
        "authors": [
            "Peng Ren",
            "Hai Yang"
        ],
        "title": "LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding",
        "abstract": "arXiv:2510.15392v1 Announce Type: new  Abstract: Generating long and stylized human motions in real time is critical for applications that demand continuous and responsive character control. Despite its importance, existing streaming approaches often operate directly in the raw motion space, leading to substantial computational overhead and making it difficult to maintain temporal stability. In contrast, latent-space VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality stylization, but they are generally confined to offline processing. To bridge this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a recent high-performing offline framework for arbitrary motion stylization and extends it to an online setting through a latent-space streaming architecture with a sliding-window causal design and the injection of decoded motion features to ensure smooth motion transitions. This architecture enables long-sequence real-time arbitrary stylization without relying on future frames or modifying the diffusion model architecture, achieving a favorable balance between stylization quality and responsiveness as demonstrated by experiments on benchmark datasets. Supplementary video and examples are available at the project page: https://pren1.github.io/lilac/",
        "arxiv_id": "2510.15392",
        "ARXIVID": "2510.15392",
        "COMMENT": "LILAC proposes a streaming VAE-diffusion framework for real-time, long-sequence motion stylization. While it is a generative model for motion, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models, but is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.15547": {
        "authors": [
            "Usman Ali",
            "Ali Zia",
            "Waqas Ali",
            "Umer Ramzan",
            "Abdul Rehman",
            "Muhammad Tayyab Chaudhry",
            "Wei Xiang"
        ],
        "title": "Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors",
        "abstract": "arXiv:2510.15547v1 Announce Type: new  Abstract: Reliable induction motor (IM) fault diagnosis is vital for industrial safety and operational continuity, mitigating costly unplanned downtime. Conventional approaches often struggle to capture complex multimodal signal relationships, are constrained to unimodal data or single fault types, and exhibit performance degradation under noisy or cross-domain conditions. This paper proposes the Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is the first to integrate contrastive learning within a hypergraph topology specifically designed for multimodal sensor fusion, enabling the joint modelling of intra- and inter-modal dependencies and enhancing generalisation beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis of bearing, stator, and rotor faults, addressing the engineering need for consolidated di- agnostic capabilities. Evaluated on three real-world benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain generalisation and resilience to noise, demonstrating its suitability for real-world deployment. An ablation study validates the contribution of each component. MM-HCAN provides a scalable and robust solution for comprehensive multi-fault diagnosis, supporting predictive maintenance and extended asset longevity in industrial environments.",
        "arxiv_id": "2510.15547",
        "ARXIVID": "2510.15547",
        "COMMENT": "This paper introduces a hypergraph contrastive attention network for multimodal sensor fusion in fault diagnosis. While it is a multimodal method, it is not about VLLMs/MLLMs, embodied AI, or vision foundation models. It is relevant to multimodal learning and sensor fusion, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15514": {
        "authors": [
            "Boyin Liu",
            "Zhuo Zhang",
            "Sen Huang",
            "Lipeng Xie",
            "Qingxu Fu",
            "Haoran Chen",
            "LI YU",
            "Tianyi Hu",
            "Zhaoyang Liu",
            "Bolin Ding",
            "Dongbin Zhao"
        ],
        "title": "Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning",
        "abstract": "arXiv:2510.15514v1 Announce Type: new  Abstract: However, this method often faces judgment inconsistencies that can destabilize reinforcement learning. While prior research has focused on the accuracy of judgments, the critical issue of logical coherence especially issues such as preference cycles hasn't been fully addressed. To fill this gap, we introduce a comprehensive framework designed to systematically detect and resolve these inconsistencies during the reinforcement learning training process. Our framework includes two main contributions: first, the Conflict Detection Rate (CDR), a new metric that quantifies judgment conflicts, and second, Deconflicted Graph Rewards (DGR), a framework that purifies signals by removing cycles before policy optimization. DGR constructs preference graphs from the initial judgments, transforms them into conflict-free Directed Acyclic Graphs (DAGs), and generates a logically coherent reward signal that is compatible with any policy optimizer. Experimental results show that our framework significantly enhances training stability and model performance compared to strong baselines, establishing logical consistency as a crucial and now manageable dimension of AI feedback.",
        "arxiv_id": "2510.15514",
        "ARXIVID": "2510.15514",
        "COMMENT": "This paper introduces a framework for resolving judgment inconsistencies in reinforcement learning from AI feedback. While it is an RL paper with a novel metric and method for logical consistency, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is relevant to machine learning and RL, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15119": {
        "authors": [
            "Ana Lawry Aguila",
            "Dina Zemlyanker",
            "You Cheng",
            "Sudeshna Das",
            "Daniel C. Alexander",
            "Oula Puonti",
            "Annabel Sorby-Adams",
            "W. Taylor Kimberly",
            "Juan Eugenio Iglesias"
        ],
        "title": "Deep generative priors for 3D brain analysis",
        "abstract": "arXiv:2510.15119v1 Announce Type: new  Abstract: Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.",
        "arxiv_id": "2510.15119",
        "ARXIVID": "2510.15119",
        "COMMENT": "This paper uses diffusion models as priors for 3D brain MRI analysis, which is a generative modeling approach in medical imaging. While it is a novel application of diffusion models, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is relevant to generative modeling in vision, but not a direct match to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15862": {
        "authors": [
            "Yi Wan",
            "Jiuqi Wang",
            "Liam Li",
            "Jinsong Liu",
            "Ruihao Zhu",
            "Zheqing Zhu"
        ],
        "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
        "abstract": "arXiv:2510.15862v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.",
        "arxiv_id": "2510.15862",
        "ARXIVID": "2510.15862",
        "COMMENT": "Somewhat relevant to criterion 2 (new LLM-based agent for research tasks), but not focused on vision or multi-modal learning. More about LLMs and tool use for research, not vision-language or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15557": {
        "authors": [
            "Tingyu Lin",
            "Marco Peer",
            "Florian Kleber",
            "Robert Sablatnig"
        ],
        "title": "ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents",
        "abstract": "arXiv:2510.15557v1 Announce Type: new  Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and printed text recognition in visually degraded and low-resource settings. The dataset is derived from 127 World War II-era archival video segments containing clapperboards that record structured production metadata such as date, location, and camera-operator identity. ClapperText includes 9,813 annotated frames and 94,573 word-level text instances, 67% of which are handwritten and 1,566 are partially occluded. Each instance includes transcription, semantic category, text type, and occlusion status, with annotations available as rotated bounding boxes represented as 4-point polygons to support spatially precise OCR applications. Recognizing clapperboard text poses significant challenges, including motion blur, handwriting variation, exposure fluctuations, and cluttered backgrounds, mirroring broader challenges in historical document analysis where structured content appears in degraded, non-standard forms. We provide both full-frame annotations and cropped word images to support downstream tasks. Using a consistent per-video evaluation protocol, we benchmark six representative recognition and seven detection models under zero-shot and fine-tuned conditions. Despite the small training set (18 videos), fine-tuning leads to substantial performance gains, highlighting ClapperText's suitability for few-shot learning scenarios. The dataset offers a realistic and culturally grounded resource for advancing robust OCR and document understanding in low-resource archival contexts. The dataset and evaluation code are available at https://github.com/linty5/ClapperText.",
        "arxiv_id": "2510.15557",
        "ARXIVID": "2510.15557",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it provides a new benchmark for OCR in low-resource archival documents, but does not propose a new method or model.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.15600": {
        "authors": [
            "Haoran Sun",
            "Yankai Jiang",
            "Zhenyu Tang",
            "Yaning Pan",
            "Shuang Gu",
            "Zekai Lin",
            "Lilong Wang",
            "Wenjie Lou",
            "Lei Liu",
            "Lei Bai",
            "Xiaosong Wang"
        ],
        "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
        "abstract": "arXiv:2510.15600v1 Announce Type: new  Abstract: The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.",
        "arxiv_id": "2510.15600",
        "ARXIVID": "2510.15600",
        "COMMENT": "Does not match any specific criterion. Focuses on protocol generation for bio-experiments using LLMs, not on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15666": {
        "authors": [
            "Lei Shi",
            "Gang Li",
            "Junxing Zhang"
        ],
        "title": "Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation",
        "abstract": "arXiv:2510.15666v1 Announce Type: new  Abstract: Automatic medical image segmentation is a fundamental step in computer-aided diagnosis, yet fully supervised approaches demand extensive pixel-level annotations that are costly and time-consuming. To alleviate this burden, we propose a weakly supervised segmentation framework that leverages only four extreme points as annotation. Specifically, bounding boxes derived from the extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to generate reliable initial pseudo labels. These pseudo labels are progressively refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm, which incorporates Monte Carlo dropout-based uncertainty estimation to construct a unified gradient uncertainty cost map for boundary tracing. Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a box alignment loss are introduced to ensure spatial consistency and precise boundary alignment during training. Extensive experiments on two public ultrasound datasets, BUSI and UNS, demonstrate that our method achieves performance comparable to, and even surpassing fully supervised counterparts while significantly reducing annotation cost. These results validate the effectiveness and practicality of the proposed weakly supervised framework for ultrasound image segmentation.",
        "arxiv_id": "2510.15666",
        "ARXIVID": "2510.15666",
        "COMMENT": "This paper proposes a weakly supervised segmentation framework for ultrasound images using extreme points and uncertainty estimation. While it is a vision paper with some spatial reasoning (boundary tracing), it does not focus on embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is relevant to computer vision and clever statistical tricks (uncertainty estimation), but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.15338": {
        "authors": [
            "Shengkai Hu",
            "Haozhe Qi",
            "Jun Wan",
            "Jiaxing Huang",
            "Lefei Zhang",
            "Hang Sun",
            "Dacheng Tao"
        ],
        "title": "Proto-Former: Unified Facial Landmark Detection by Prototype Transformer",
        "abstract": "arXiv:2510.15338v1 Announce Type: new  Abstract: Recent advances in deep learning have significantly improved facial landmark detection. However, existing facial landmark detection datasets often define different numbers of landmarks, and most mainstream methods can only be trained on a single dataset. This limits the model generalization to different datasets and hinders the development of a unified model. To address this issue, we propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection framework that explicitly enhances dataset-specific facial structural representations (i.e., prototype). Proto-Former overcomes the limitations of single-dataset training by enabling joint training across multiple datasets within a unified architecture. Specifically, Proto-Former comprises two key components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive feature extraction and learns prototype representations, and a Progressive Prototype-Aware Decoder (PPAD) that refines these prototypes to generate prompts that guide the model's attention to key facial regions. Furthermore, we introduce a novel Prototype-Aware (PA) loss, which achieves optimal path finding by constraining the selection weights of prototype experts. This loss function effectively resolves the problem of prototype expert addressing instability during multi-dataset training, alleviates gradient conflicts, and enables the extraction of more accurate facial structure features. Extensive experiments on widely used benchmark datasets demonstrate that our Proto-Former achieves superior performance compared to existing state-of-the-art methods. The code is publicly available at: https://github.com/Husk021118/Proto-Former.",
        "arxiv_id": "2510.15338",
        "ARXIVID": "2510.15338",
        "COMMENT": "Relevant to general computer vision and deep learning, but does not match any specific criterion (focuses on facial landmark detection, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in a broad sense).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.15258": {
        "authors": [
            "Xi Wang",
            "Xianyao Ling",
            "Kun Li",
            "Gang Yin",
            "Liang Zhang",
            "Jiang Wu",
            "Jun Xu",
            "Fu Zhang",
            "Wenbo Lei",
            "Annie Wang",
            "Peng Gong"
        ],
        "title": "Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions",
        "abstract": "arXiv:2510.15258v1 Announce Type: new  Abstract: In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from \"hallucination\" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.",
        "arxiv_id": "2510.15258",
        "ARXIVID": "2510.15258",
        "COMMENT": "Does not match any specific criterion. Focuses on LLM agents and knowledge graphs for multi-dimensional data analysis, not on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}