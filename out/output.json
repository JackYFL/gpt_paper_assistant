{
    "2508.15144": {
        "authors": [
            "Jiabo Ye",
            "Xi Zhang",
            "Haiyang Xu",
            "Haowei Liu",
            "Junyang Wang",
            "Zhaoqing Zhu",
            "Ziwei Zheng",
            "Feiyu Gao",
            "Junjie Cao",
            "Zhengxi Lu",
            "Jitong Liao",
            "Qi Zheng",
            "Fei Huang",
            "Jingren Zhou",
            "Ming Yan"
        ],
        "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
        "abstract": "arXiv:2508.15144v1 Announce Type: new  Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "arxiv_id": "2508.15144",
        "ARXIVID": "2508.15144",
        "COMMENT": "Matches criteria 1 and 3: introduces a new foundational agent for GUI automation with a novel cloud-based environment infrastructure, self-evolving data generation, and a scalable RL framework. Also presents a new open-source benchmark and method for embodied agents in GUI environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.15641": {
        "authors": [
            "Pengcheng Fang",
            "Yuxia Chen",
            "Rui Guo"
        ],
        "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding",
        "abstract": "arXiv:2508.15641v1 Announce Type: new  Abstract: Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.",
        "arxiv_id": "2508.15641",
        "ARXIVID": "2508.15641",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). The paper introduces a new Video LLM (Grounded VideoDiT) with innovations in temporal and entity grounding for long video understanding, and demonstrates state-of-the-art results on multiple benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.15761": {
        "authors": [
            "Yifu Zhang",
            "Hao Yang",
            "Yuqi Zhang",
            "Yifei Hu",
            "Fengda Zhu",
            "Chuang Lin",
            "Xiaofeng Mei",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng"
        ],
        "title": "Waver: Wave Your Way to Lifelike Video Generation",
        "abstract": "arXiv:2508.15761v1 Announce Type: new  Abstract: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.",
        "arxiv_id": "2508.15761",
        "ARXIVID": "2508.15761",
        "COMMENT": "Waver is a new vision foundation model for unified image and video generation, supporting text-to-video, image-to-video, and text-to-image in a single framework. It introduces a new architecture and training pipeline, and is directly relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.15360": {
        "authors": [
            "Chenhui Gou",
            "Ziyu Ma",
            "Zicheng Duan",
            "Haoyu He",
            "Feng Chen",
            "Akide Liu",
            "Bohan Zhuang",
            "Jianfei Cai",
            "Hamid Rezatofighi"
        ],
        "title": "An Empirical Study on How Video-LLMs Answer Video Questions",
        "abstract": "arXiv:2508.15360v1 Announce Type: new  Abstract: Taking advantage of large-scale data and pretrained language models, Video Large Language Models (Video-LLMs) have shown strong capabilities in answering video questions. However, most existing efforts focus on improving performance, with limited attention to understanding their internal mechanisms. This paper aims to bridge this gap through a systematic empirical study. To interpret existing VideoLLMs, we adopt attention knockouts as our primary analytical tool and design three variants: Video Temporal Knockout, Video Spatial Knockout, and Language-to-Video Knockout. Then, we apply these three knockouts on different numbers of layers (window of layers). By carefully controlling the window of layers and types of knockouts, we provide two settings: a global setting and a fine-grained setting. Our study reveals three key findings: (1) Global setting indicates Video information extraction primarily occurs in early layers, forming a clear two-stage process -- lower layers focus on perceptual encoding, while higher layers handle abstract reasoning; (2) In the fine-grained setting, certain intermediate layers exert an outsized impact on video question answering, acting as critical outliers, whereas most other layers contribute minimally; (3) In both settings, we observe that spatial-temporal modeling relies more on language-guided retrieval than on intra- and inter-frame self-attention among video tokens, despite the latter's high computational cost. Finally, we demonstrate that these insights can be leveraged to reduce attention computation in Video-LLMs. To our knowledge, this is the first work to systematically uncover how Video-LLMs internally process and understand video content, offering interpretability and efficiency perspectives for future research.",
        "arxiv_id": "2508.15360",
        "ARXIVID": "2508.15360",
        "COMMENT": "Matches criterion 2: provides a systematic empirical study of Video-LLMs, revealing new insights into their internal mechanisms and suggesting efficiency improvements. Also of interest for surprising empirical results in vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.15169": {
        "authors": [
            "Xuyang Chen",
            "Zhijun Zhai",
            "Kaixuan Zhou",
            "Zengmao Wang",
            "Jianan He",
            "Dong Wang",
            "Yanfeng Zhang",
            "mingwei Sun",
            "R\\\"udiger Westermann",
            "Konrad Schindler",
            "Liqiu Meng"
        ],
        "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion",
        "abstract": "arXiv:2508.15169v1 Announce Type: new  Abstract: Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.",
        "arxiv_id": "2508.15169",
        "ARXIVID": "2508.15169",
        "COMMENT": "Matches criterion 1 (spatial understanding for scene generation with mesh priors) and criterion 4 (vision foundation models and applications, as it leverages diffusion models and ControlNet for 3D scene synthesis). The method introduces a novel pipeline for cross-view consistent outdoor scene generation using city mesh models, which is highly relevant for spatial intelligence in embodied agents and virtual navigation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.15119": {
        "authors": [
            "Rachel Ma",
            "Jingyi Qu",
            "Andreea Bobu",
            "Dylan Hadfield-Menell"
        ],
        "title": "Open-Universe Assistance Games",
        "abstract": "arXiv:2508.15119v1 Announce Type: new  Abstract: Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.",
        "arxiv_id": "2508.15119",
        "ARXIVID": "2508.15119",
        "COMMENT": "Matches criterion 3: proposes a new framework (Open-Universe Assistance Games) for embodied AI agents to reason over unbounded goal spaces, and introduces a novel method (GOOD) for online goal inference in interactive environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.15568": {
        "authors": [
            "Youjia Zhang",
            "Youngeun Kim",
            "Young-Geun Choi",
            "Hongyeob Kim",
            "Huiling Liu",
            "Sungeun Hong"
        ],
        "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment",
        "abstract": "arXiv:2508.15568v1 Announce Type: new  Abstract: Test-time adaptation (TTA) enhances the zero-shot robustness under distribution shifts by leveraging unlabeled test data during inference. Despite notable advances, several challenges still limit its broader applicability. First, most methods rely on backpropagation or iterative optimization, which limits scalability and hinders real-time deployment. Second, they lack explicit modeling of class-conditional feature distributions. This modeling is crucial for producing reliable decision boundaries and calibrated predictions, but it remains underexplored due to the lack of both source data and supervision at test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and backPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian probabilistic inference task by modeling class-conditional likelihoods using gradually updated class means and a shared covariance matrix. This enables closed-form, training-free inference. To correct potential likelihood bias, we introduce lightweight regularization guided by CLIP priors and a historical knowledge bank. ADAPT requires no source data, no gradient updates, and no full access to target data, supporting both online and transductive settings. Extensive experiments across diverse benchmarks demonstrate that our method achieves state-of-the-art performance under a wide range of distribution shifts with superior scalability and robustness.",
        "arxiv_id": "2508.15568",
        "ARXIVID": "2508.15568",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to clever statistical tricks (test-time adaptation without backpropagation, using probabilistic Gaussian alignment and CLIP priors). The method is novel in its backpropagation-free, training-free approach for TTA.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.15653": {
        "authors": [
            "Ziyang Yan",
            "Ruikai Li",
            "Zhiyong Cui",
            "Bohan Li",
            "Han Jiang",
            "Yilong Ren",
            "Aoyong Li",
            "Zhenning Li",
            "Sijia Wen",
            "Haiyang Yu"
        ],
        "title": "MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction",
        "abstract": "arXiv:2508.15653v1 Announce Type: new  Abstract: Online HD map construction is a fundamental task in autonomous driving systems, aiming to acquire semantic information of map elements around the ego vehicle based on real-time sensor inputs. Recently, several approaches have achieved promising results by incorporating offline priors such as SD maps and HD maps or by fusing multi-modal data. However, these methods depend on stale offline maps and multi-modal sensor suites, resulting in avoidable computational overhead at inference. To address these limitations, we employ a knowledge distillation strategy to transfer knowledge from multimodal models with prior knowledge to an efficient, low-cost, and vision-centric student model. Specifically, we propose MapKD, a novel multi-level cross-modal knowledge distillation framework with an innovative Teacher-Coach-Student (TCS) paradigm. This framework consists of: (1) a camera-LiDAR fusion model with SD/HD map priors serving as the teacher; (2) a vision-centric coach model with prior knowledge and simulated LiDAR to bridge the cross-modal knowledge transfer gap; and (3) a lightweight vision-based student model. Additionally, we introduce two targeted knowledge distillation strategies: Token-Guided 2D Patch Distillation (TGPD) for bird's eye view feature alignment and Masked Semantic Response Distillation (MSRD) for semantic learning guidance. Extensive experiments on the challenging nuScenes dataset demonstrate that MapKD improves the student model by +6.68 mIoU and +10.94 mAP while simultaneously accelerating inference speed. The code is available at:https://github.com/2004yan/MapKD2026.",
        "arxiv_id": "2508.15653",
        "ARXIVID": "2508.15653",
        "COMMENT": "MapKD introduces a cross-modal knowledge distillation framework for efficient online HD map construction, using a teacher-coach-student paradigm and novel distillation strategies. This is highly relevant to spatial understanding and vision foundation model applications in autonomous driving (criteria 1 and 4).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.15769": {
        "authors": [
            "Yanxu Meng",
            "Haoning Wu",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
        "abstract": "arXiv:2508.15769v1 Announce Type: new  Abstract: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
        "arxiv_id": "2508.15769",
        "ARXIVID": "2508.15769",
        "COMMENT": "SceneGen proposes a novel framework for generating multiple 3D assets from a single scene image in one feedforward pass, with a new feature aggregation module for spatial reasoning. This is a methodological improvement for spatial understanding and 3D content generation, relevant to embodied AI and spatial intelligence (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.15376": {
        "authors": [
            "Cong Wang",
            "Xianda Guo",
            "Wenbo Xu",
            "Wei Tian",
            "Ruiqi Song",
            "Chenming Zhang",
            "Lingxi Li",
            "Long Chen"
        ],
        "title": "DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians",
        "abstract": "arXiv:2508.15376v1 Announce Type: new  Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.",
        "arxiv_id": "2508.15376",
        "ARXIVID": "2508.15376",
        "COMMENT": "DriveSplat presents a new method for 3D driving scene reconstruction using neural Gaussians, with dynamic-static decoupling and geometry-enhanced partitioning. This is a novel method for spatial understanding in complex, dynamic environments, and is highly relevant to embodied AI and spatial intelligence (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.15751": {
        "authors": [
            "Xueyuan Li",
            "Can Cui",
            "Ruining Deng",
            "Yucheng Tang",
            "Quan Liu",
            "Tianyuan Yao",
            "Shunxing Bao",
            "Naweed Chowdhury",
            "Haichun Yang",
            "Yuankai Huo"
        ],
        "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model",
        "abstract": "arXiv:2508.15751v1 Announce Type: new  Abstract: Purpose: Recent developments in computational pathology have been driven by advances in Vision Foundation Models, particularly the Segment Anything Model (SAM). This model facilitates nuclei segmentation through two primary methods: prompt-based zero-shot segmentation and the use of cell-specific SAM models for direct segmentation. These approaches enable effective segmentation across a range of nuclei and cells. However, general vision foundation models often face challenges with fine-grained semantic segmentation, such as identifying specific nuclei subtypes or particular cells. Approach: In this paper, we propose the molecular-empowered All-in-SAM Model to advance computational pathology by leveraging the capabilities of vision foundation models. This model incorporates a full-stack approach, focusing on: (1) annotation-engaging lay annotators through molecular-empowered learning to reduce the need for detailed pixel-level annotations, (2) learning-adapting the SAM model to emphasize specific semantics, which utilizes its strong generalizability with SAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating Molecular-Oriented Corrective Learning (MOCL). Results: Experimental results from both in-house and public datasets show that the All-in-SAM model significantly improves cell classification performance, even when faced with varying annotation quality. Conclusions: Our approach not only reduces the workload for annotators but also extends the accessibility of precise biomedical image analysis to resource-limited settings, thereby advancing medical diagnostics and automating pathology image analysis.",
        "arxiv_id": "2508.15751",
        "ARXIVID": "2508.15751",
        "COMMENT": "Matches criterion 4: applies and extends vision foundation models (SAM) for fine-grained biomedical image segmentation, introducing a molecular-empowered approach and new learning/refinement strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.15505": {
        "authors": [
            "Mengyu Wang",
            "Zhenyu Liu",
            "Kun Li",
            "Yu Wang",
            "Yuwei Wang",
            "Yanyan Wei",
            "Fei Wang"
        ],
        "title": "Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion",
        "abstract": "arXiv:2508.15505v1 Announce Type: new  Abstract: Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF) -- demonstrate AdaSFFuse's superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.",
        "arxiv_id": "2508.15505",
        "ARXIVID": "2508.15505",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to multi-modal learning. The paper proposes a new framework for multimodal image fusion with adaptive cross-domain learning, introducing novel modules for frequency decoupling and fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.15652": {
        "authors": [
            "Ardian Selmonaj",
            "Miroslav Strupl",
            "Oleg Szehr",
            "Alessandro Antonucci"
        ],
        "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2508.15652v1 Announce Type: new  Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.",
        "arxiv_id": "2508.15652",
        "ARXIVID": "2508.15652",
        "COMMENT": "This paper proposes a new method for understanding agent action effects in multi-agent reinforcement learning using information-theoretic Shapley values. It provides novel insights into cooperation dynamics and explainability in MARL, which is relevant to embodied AI and spatial intelligence (criterion 1).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.15767": {
        "authors": [
            "Jinhyung Park",
            "Javier Romero",
            "Shunsuke Saito",
            "Fabian Prada",
            "Takaaki Shiratori",
            "Yichen Xu",
            "Federica Bogo",
            "Shoou-I Yu",
            "Kris Kitani",
            "Rawal Khirodkar"
        ],
        "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling",
        "abstract": "arXiv:2508.15767v1 Announce Type: new  Abstract: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.",
        "arxiv_id": "2508.15767",
        "ARXIVID": "2508.15767",
        "COMMENT": "This paper introduces ATLAS, a new parametric human body model that decouples skeletal and shape parameters, improving expressivity and control. This is a methodological improvement in 3D human modeling, which is relevant to spatial understanding (criterion 1), especially for embodied agents and applications in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.15537": {
        "authors": [
            "Chang Liu",
            "Yang Xu",
            "Tamas Sziranyi"
        ],
        "title": "D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems",
        "abstract": "arXiv:2508.15537v1 Announce Type: new  Abstract: Extracting narrow roads from high-resolution remote sensing imagery remains a significant challenge due to their limited width, fragmented topology, and frequent occlusions. To address these issues, we propose D3FNet, a Dilated Dual-Stream Differential Attention Fusion Network designed for fine-grained road structure segmentation in remote perception systems. Built upon the encoder-decoder backbone of D-LinkNet, D3FNet introduces three key innovations:(1) a Differential Attention Dilation Extraction (DADE) module that enhances subtle road features while suppressing background noise at the bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates original and attention-modulated features to balance spatial precision with semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9) that mitigates gridding artifacts and improves continuity in narrow road prediction. Unlike conventional models that overfit to generic road widths, D3FNet specifically targets fine-grained, occluded, and low-contrast road segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show that D3FNet achieves superior IoU and recall on challenging road regions, outperforming state-of-the-art baselines. Ablation studies further verify the complementary synergy of attention-guided encoding and dual-path decoding. These results confirm D3FNet as a robust solution for fine-grained narrow road extraction in complex remote and cooperative perception scenarios.",
        "arxiv_id": "2508.15537",
        "ARXIVID": "2508.15537",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is tangentially related to criterion 1 (spatial understanding in remote perception). The paper introduces a new network for fine-grained road extraction from remote sensing imagery, with novel attention and fusion mechanisms.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.15314": {
        "authors": [
            "Naen Xu",
            "Jinghuai Zhang",
            "Changjiang Li",
            "Zhi Chen",
            "Chunyi Zhou",
            "Qingming Li",
            "Tianyu Du",
            "Shouling Ji"
        ],
        "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models",
        "abstract": "arXiv:2508.15314v1 Announce Type: new  Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines.",
        "arxiv_id": "2508.15314",
        "ARXIVID": "2508.15314",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and their applications), as it proposes a plug-and-play module for text-to-video diffusion models, but the focus is on concept erasure rather than spatial intelligence or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.15207": {
        "authors": [
            "Arjun Srinivasan",
            "Anubhav Paras",
            "Aniket Bera"
        ],
        "title": "Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning",
        "abstract": "arXiv:2508.15207v1 Announce Type: new  Abstract: Existing approaches in reinforcement learning train an agent to learn desired optimal behavior in an environment with rule based surrounding agents. In safety critical applications such as autonomous driving it is crucial that the rule based agents are modelled properly. Several behavior modelling strategies and IDM models are used currently to model the surrounding agents. We present a learning based method to derive the adversarial behavior for the rule based agents to cause failure scenarios. We evaluate our adversarial agent against all the rule based agents and show the decrease in cumulative reward.",
        "arxiv_id": "2508.15207",
        "ARXIVID": "2508.15207",
        "COMMENT": "Somewhat related to criterion 3 (embodied AI, new methods for agent behavior in simulators), as it proposes a learning-based method for adversarial agent behavior in autonomous driving, but does not introduce a new benchmark or simulator.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.15610": {
        "authors": [
            "Alfio Gliozzo",
            "Naweed Khan",
            "Christodoulos Constantinides",
            "Nandana Mihindukulasooriya",
            "Nahuel Defosse",
            "Junkyu Lee"
        ],
        "title": "Transduction is All You Need for Structured Data Workflows",
        "abstract": "arXiv:2508.15610v1 Announce Type: new  Abstract: This paper introduces Agentics, a modular framework for building agent-based systems capable of structured reasoning and compositional generalization over complex data. Designed with research and practical applications in mind, Agentics offers a novel perspective on working with data and AI workflows. In this framework, agents are abstracted from the logical flow and they are used internally to the data type to enable logical transduction among data. Agentics encourages AI developers to focus on modeling data rather than crafting prompts, enabling a declarative language in which data types are provided by LLMs and composed through logical transduction, which is executed by LLMs when types are connected. We provide empirical evidence demonstrating the applicability of this framework across domain-specific multiple-choice question answering, semantic parsing for text-to-SQL, and automated prompt optimization tasks, achieving state-of-the-art accuracy or improved scalability without sacrificing performance. The open-source implementation is available at \\texttt{https://github.com/IBM/agentics}.",
        "arxiv_id": "2508.15610",
        "ARXIVID": "2508.15610",
        "COMMENT": "Agentics is a modular framework for agent-based structured reasoning over data, using LLMs for logical transduction. While it is about agent-based systems and compositional reasoning, it does not focus on spatial intelligence, embodied agents, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.14958": {
        "authors": [
            "Mustafa Mohammadi Gharasuie",
            "Luis Rueda"
        ],
        "title": "Fast Graph Neural Network for Image Classification",
        "abstract": "arXiv:2508.14958v1 Announce Type: new  Abstract: The rapid progress in image classification has been largely driven by the adoption of Graph Convolutional Networks (GCNs), which offer a robust framework for handling complex data structures. This study introduces a novel approach that integrates GCNs with Voronoi diagrams to enhance image classification by leveraging their ability to effectively model relational data. Unlike conventional convolutional neural networks (CNNs), our method represents images as graphs, where pixels or regions function as vertices. These graphs are then refined using corresponding Delaunay triangulations, optimizing their representation. The proposed model achieves significant improvements in both preprocessing efficiency and classification accuracy across various benchmark datasets, surpassing state-of-the-art approaches, particularly in challenging scenarios involving intricate scenes and fine-grained categories. Experimental results, validated through cross-validation, underscore the effectiveness of combining GCNs with Voronoi diagrams for advancing image classification. This research not only presents a novel perspective on image classification but also expands the potential applications of graph-based learning paradigms in computer vision and unstructured data analysis.",
        "arxiv_id": "2508.14958",
        "ARXIVID": "2508.14958",
        "COMMENT": "Somewhat related to computer vision and graph-based learning, but does not match any specific criterion closely. No direct connection to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.15629": {
        "authors": [
            "Hao Chen",
            "Fang Qiu",
            "Li An",
            "Douglas Stow",
            "Eve Bohnett",
            "Haitao Lyu",
            "Shuang Tian"
        ],
        "title": "Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models",
        "abstract": "arXiv:2508.15629v1 Announce Type: new  Abstract: Wildlife and human activities are key components of landscape systems. Understanding their spatial distribution is essential for evaluating human wildlife interactions and informing effective conservation planning. Multiperspective monitoring of wildlife and human activities by combining camera traps and drone imagery. Capturing the spatial patterns of their distributions, which allows the identification of the overlap of their activity zones and the assessment of the degree of human wildlife conflict. The study was conducted in Chitwan National Park (CNP), Nepal, and adjacent regions. Images collected by visible and nearinfrared camera traps and thermal infrared drones from February to July 2022 were processed to create training and testing datasets, which were used to build deep learning models to automatic identify wildlife and human activities. Drone collected thermal imagery was used for detecting targets to provide a multiple monitoring perspective. Spatial pattern analysis was performed to identify animal and resident activity hotspots and delineation potential human wildlife conflict zones. Among the deep learning models tested, YOLOv11s achieved the highest performance with a precision of 96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most effective for detecting objects in camera trap imagery. Drone based thermal imagery, analyzed with an enhanced Faster RCNN model, added a complementary aerial viewpoint for camera trap detections. Spatial pattern analysis identified clear hotspots for both wildlife and human activities and their overlapping patterns within certain areas in the CNP and buffer zones indicating potential conflict. This study reveals human wildlife conflicts within the conserved landscape. Integrating multiperspective monitoring with automated object detection enhances wildlife surveillance and landscape management.",
        "arxiv_id": "2508.15629",
        "ARXIVID": "2508.15629",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications), as it uses deep learning for multi-perspective monitoring of wildlife and human activities, but the methodological novelty is more in application than in foundational model or spatial intelligence advances.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2508.15476": {
        "authors": [
            "Chengqi Dong",
            "Fenghe Tang",
            "Rongge Mao",
            "Xinpei Gao",
            "S. Kevin Zhou"
        ],
        "title": "LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion",
        "abstract": "arXiv:2508.15476v1 Announce Type: new  Abstract: Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.",
        "arxiv_id": "2508.15476",
        "ARXIVID": "2508.15476",
        "COMMENT": "LGMSNet is a lightweight medical image segmentation model using dual-level multiscale fusion. While it is a vision model, it does not address spatial intelligence for embodied agents, VLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.15367": {
        "authors": [
            "Jacinto Colan",
            "Ana Davila",
            "Yasuhisa Hasegawa"
        ],
        "title": "Transfer learning optimization based on evolutionary selective fine tuning",
        "abstract": "arXiv:2508.15367v1 Announce Type: new  Abstract: Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.",
        "arxiv_id": "2508.15367",
        "ARXIVID": "2508.15367",
        "COMMENT": "BioTune introduces an evolutionary selective fine-tuning method for transfer learning in image classification. While it is a clever statistical trick for model efficiency, it does not directly address any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.15650": {
        "authors": [
            "Shuchao Pang",
            "Zhenghan Chen",
            "Shen Zhang",
            "Liming Lu",
            "Siyuan Liang",
            "Anan Du",
            "Yongbin Zhou"
        ],
        "title": "Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance",
        "abstract": "arXiv:2508.15650v1 Announce Type: new  Abstract: Deep neural networks for 3D point clouds have been demonstrated to be vulnerable to adversarial examples. Previous 3D adversarial attack methods often exploit certain information about the target models, such as model parameters or outputs, to generate adversarial point clouds. However, in realistic scenarios, it is challenging to obtain any information about the target models under conditions of absolute security. Therefore, we focus on transfer-based attacks, where generating adversarial point clouds does not require any information about the target models. Based on our observation that the critical features used for point cloud classification are consistent across different DNN architectures, we propose CFG, a novel transfer-based black-box attack method that improves the transferability of adversarial point clouds via the proposed Critical Feature Guidance. Specifically, our method regularizes the search of adversarial point clouds by computing the importance of the extracted features, prioritizing the corruption of critical features that are likely to be adopted by diverse architectures. Further, we explicitly constrain the maximum deviation extent of the generated adversarial point clouds in the loss function to ensure their imperceptibility. Extensive experiments conducted on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the proposed CFG outperforms the state-of-the-art attack methods by a large margin.",
        "arxiv_id": "2508.15650",
        "ARXIVID": "2508.15650",
        "COMMENT": "This paper proposes a new black-box attack method for 3D point cloud models, focusing on transferability via critical feature guidance. While it deals with 3D spatial data and neural networks, it does not introduce new methods for spatial understanding or embodied agents, nor does it present a new benchmark or vision foundation model. It is more about adversarial robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}