{
    "2507.01006": {
        "authors": [
            "V Team",
            "Wenyi Hong",
            "Wenmeng Yu",
            "Xiaotao Gu",
            "Guo Wang",
            "Guobing Gan",
            "Haomiao Tang",
            "Jiale Cheng",
            "Ji Qi",
            "Junhui Ji",
            "Lihang Pan",
            "Shuaiqi Duan",
            "Weihan Wang",
            "Yan Wang",
            "Yean Cheng",
            "Zehai He",
            "Zhe Su",
            "Zhen Yang",
            "Ziyang Pan",
            "Aohan Zeng",
            "Baoxu Wang",
            "Boyan Shi",
            "Changyu Pang",
            "Chenhui Zhang",
            "Da Yin",
            "Fan Yang",
            "Guoqing Chen",
            "Jiazheng Xu",
            "Jiali Chen",
            "Jing Chen",
            "Jinhao Chen",
            "Jinghao Lin",
            "Jinjiang Wang",
            "Junjie Chen",
            "Leqi Lei",
            "Letian Gong",
            "Leyi Pan",
            "Mingzhi Zhang",
            "Qinkai Zheng",
            "Sheng Yang",
            "Shi Zhong",
            "Shiyu Huang",
            "Shuyuan Zhao",
            "Siyan Xue",
            "Shangqin Tu",
            "Shengbiao Meng",
            "Tianshu Zhang",
            "Tianwei Luo",
            "Tianxiang Hao",
            "Wenkai Li",
            "Wei Jia",
            "Xin Lyu",
            "Xuancheng Huang",
            "Yanling Wang",
            "Yadong Xue",
            "Yanfeng Wang",
            "Yifan An",
            "Yifan Du",
            "Yiming Shi",
            "Yiheng Huang",
            "Yilin Niu",
            "Yuan Wang",
            "Yuanchang Yue",
            "Yuchen Li",
            "Yutao Zhang",
            "Yuxuan Zhang",
            "Zhanxiao Du",
            "Zhenyu Hou",
            "Zhao Xue",
            "Zhengxiao Du",
            "Zihan Wang",
            "Peng Zhang",
            "Debing Liu",
            "Bin Xu",
            "Juanzi Li",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
        "abstract": "arXiv:2507.01006v2 Announce Type: new  Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding. We open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.",
        "arxiv_id": "2507.01006",
        "ARXIVID": "2507.01006",
        "COMMENT": "Matches criteria 2 and 4. The paper introduces a new vision-language model (GLM-4.1V-Thinking) with advancements in multimodal reasoning and applications across diverse tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2507.00603": {
        "authors": [
            "Yupeng Zheng",
            "Pengxuan Yang",
            "Zebin Xing",
            "Qichao Zhang",
            "Yuhang Zheng",
            "Yinfeng Gao",
            "Pengfei Li",
            "Teng Zhang",
            "Zhongpu Xia",
            "Peng Jia",
            "Dongbin Zhao"
        ],
        "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model",
        "abstract": "arXiv:2507.00603v1 Announce Type: new  Abstract: End-to-end autonomous driving directly generates planning trajectories from raw sensor data, yet it typically relies on costly perception supervision to extract scene information. A critical research challenge arises: constructing an informative driving world model to enable perception annotation-free, end-to-end planning via self-supervised learning. In this paper, we present World4Drive, an end-to-end autonomous driving framework that employs vision foundation models to build latent world models for generating and evaluating multi-modal planning trajectories. Specifically, World4Drive first extracts scene features, including driving intention and world latent representations enriched with spatial-semantic priors provided by vision foundation models. It then generates multi-modal planning trajectories based on current scene features and driving intentions and predicts multiple intention-driven future states within the latent space. Finally, it introduces a world model selector module to evaluate and select the best trajectory. We achieve perception annotation-free, end-to-end planning through self-supervised alignment between actual future observations and predicted observations reconstructed from the latent space. World4Drive achieves state-of-the-art performance without manual perception annotations on both the open-loop nuScenes and closed-loop NavSim benchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower collision rate, and 3.75 faster training convergence. Codes will be accessed at https://github.com/ucaszyp/World4Drive.",
        "arxiv_id": "2507.00603",
        "ARXIVID": "2507.00603",
        "COMMENT": "Matches criteria 1 and 4. The paper introduces a novel framework for autonomous driving using vision foundation models to build latent world models, which aligns with spatial intelligence and vision foundation model applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.00886": {
        "authors": [
            "Anna-Maria Halacheva",
            "Jan-Nico Zaech",
            "Xi Wang",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
        "abstract": "arXiv:2507.00886v1 Announce Type: new  Abstract: As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
        "arxiv_id": "2507.00886",
        "ARXIVID": "2507.00886",
        "COMMENT": "Matches criteria 2 and 3 as it introduces a 3D vision-language model for embodied reasoning and proposes a novel scene-centric approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.00334": {
        "authors": [
            "Mengyi Shan",
            "Zecheng He",
            "Haoyu Ma",
            "Felix Juefei-Xu",
            "Peizhao Zhang",
            "Tingbo Hou",
            "Ching-Yao Chuang"
        ],
        "title": "Populate-A-Scene: Affordance-Aware Human Video Generation",
        "abstract": "arXiv:2507.00334v1 Announce Type: new  Abstract: Can a video generation model be repurposed as an interactive world simulator? We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction. Given a scene image and a prompt describing human actions, we fine-tune the model to insert a person into the scene, while ensuring coherent behavior, appearance, harmonization, and scene affordance. Unlike prior work, we infer human affordance for video generation (i.e., where to insert a person and how they should behave) from a single scene image, without explicit conditions like bounding boxes or body poses. An in-depth study of cross-attention heatmaps demonstrates that we can uncover the inherent affordance perception of a pre-trained video model without labeled affordance datasets.",
        "arxiv_id": "2507.00334",
        "ARXIVID": "2507.00334",
        "COMMENT": "Matches criterion 1 as it explores affordance-aware human video generation, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.00525": {
        "authors": [
            "Djamahl Etchegaray",
            "Yuxia Fu",
            "Zi Huang",
            "Yadan Luo"
        ],
        "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving",
        "abstract": "arXiv:2507.00525v1 Announce Type: new  Abstract: Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries. We introduce Box-QAymo, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes. Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames. To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity. Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions. Project page and dataset are available at https://djamahl99.github.io/qaymo-pages/.",
        "arxiv_id": "2507.00525",
        "ARXIVID": "2507.00525",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for vision-language models in autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.00469": {
        "authors": [
            "Yue Tan",
            "Xiaoqian Hu",
            "Hao Xue",
            "Celso De Melo",
            "Flora D. Salim"
        ],
        "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding",
        "abstract": "arXiv:2507.00469v1 Announce Type: new  Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in video understanding tasks. However, real-world videos typically exist as continuously evolving data streams (e.g., dynamic scenes captured by wearable glasses), necessitating models to continually adapt to shifting data distributions and novel scenarios. Considering the prohibitive computational costs of fine-tuning models on new tasks, usually, a small subset of parameters is updated while the bulk of the model remains frozen. This poses new challenges to existing continual learning frameworks in the context of large multimodal foundation models, i.e., catastrophic forgetting and update conflict. While the foundation models struggle with parameter-efficient continual learning, the hippocampus in the human brain has evolved highly efficient mechanisms for memory formation and consolidation. Inspired by the rapid Binding and pattern separation mechanisms in the hippocampus, in this work, we propose Bisecle for video-language continual learning, where a multi-directional supervision module is used to capture more cross-modal relationships and a contrastive prompt learning scheme is designed to isolate task-specific knowledge to facilitate efficient memory storage. Binding and separation processes further strengthen the ability of VLMs to retain complex experiences, enabling robust and efficient continual learning in video understanding tasks. We perform a thorough evaluation of the proposed Bisecle, demonstrating its ability to mitigate forgetting and enhance cross-task generalization on several VideoQA benchmarks.",
        "arxiv_id": "2507.00469",
        "ARXIVID": "2507.00469",
        "COMMENT": "Matches criterion 3 as it proposes a novel continual learning framework for video-language understanding in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.00490": {
        "authors": [
            "Zijian Chen",
            "Yuan Tian",
            "Yuze Sun",
            "Wei Sun",
            "Zicheng Zhang",
            "Weisi Lin",
            "Guangtao Zhai",
            "Wenjun Zhang"
        ],
        "title": "Just Noticeable Difference for Large Multimodal Models",
        "abstract": "arXiv:2507.00490v2 Announce Type: new  Abstract: Just noticeable difference (JND), the minimum change that the human visual system (HVS) can perceive, has been studied for decades. Although recent work has extended this line of research into machine vision, there has been a scarcity of studies systematically exploring its perceptual boundaries across multiple tasks and stimulus types, particularly in the current era of rapidly advancing large multimodal models (LMMs), where studying the multifaceted capabilities of models has become a mainstream focus. Moreover, the perceptual defects of LMMs are not investigated thoroughly, resulting in potential security issues and suboptimal response efficiency. In this paper, we take an initial attempt and demonstrate that there exist significant visual blind spots in current LMMs. To systemically quantify this characteristic, we propose a new concept, {\\bf LMM-JND}, together with its determination pipeline. Targeting uncovering the behavior commonalities in HVS-aligned visual perception tasks, we delve into several LMM families and construct a large-scale dataset, named VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12 distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle with basic comparison queries and fall significantly short of human-level visual performance. We further explore the effects of vision and language backbones and find a notable correlation between their design philosophy that may instruct the future refinement of LMMs for their visual acuity. Together, our research underscores the significance of LMM-JND as a unique perspective for studying LMMs, and predictable LMM-JND is crucial for security concerns. This work will be available at https://github.com/zijianchen98/LMM-JND.",
        "arxiv_id": "2507.00490",
        "ARXIVID": "2507.00490",
        "COMMENT": "Matches criterion 2 as it explores perceptual boundaries and visual blind spots in large multimodal models (LMMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.00033": {
        "authors": [
            "Mustafa Chasmai",
            "Gauri Jagatap",
            "Gouthaman KV",
            "Grant Van Horn",
            "Subhransu Maji",
            "Andrea Fanelli"
        ],
        "title": "Moment Sampling in Video LLMs for Long-Form Video QA",
        "abstract": "arXiv:2507.00033v1 Announce Type: new  Abstract: Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose \"moment sampling\", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.",
        "arxiv_id": "2507.00033",
        "ARXIVID": "2507.00033",
        "COMMENT": "Matches criteria 2. The paper proposes a novel approach for long-form video question answering using moment sampling, which aligns with advancements in video large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.00648": {
        "authors": [
            "Siyuan Yao",
            "Rui Zhu",
            "Ziqi Wang",
            "Wenqi Ren",
            "Yanyang Yan",
            "Xiaochun Cao"
        ],
        "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions",
        "abstract": "arXiv:2507.00648v1 Announce Type: new  Abstract: Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin. Our code is available at https://github.com/Z-Z188/UMDATrack.",
        "arxiv_id": "2507.00648",
        "ARXIVID": "2507.00648",
        "COMMENT": "Matches criteria 3. The paper proposes a novel framework for visual object tracking under adverse weather conditions, focusing on domain adaptation and robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.00980": {
        "authors": [
            "Yuheng Du",
            "Sheng Yang",
            "Lingxuan Wang",
            "Zhenghua Hou",
            "Chengying Cai",
            "Zhitao Tan",
            "Mingxia Chen",
            "Shi-Sheng Huang",
            "Qiang Li"
        ],
        "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization",
        "abstract": "arXiv:2507.00980v1 Announce Type: new  Abstract: While recent online HD mapping methods relieve burdened offline pipelines and solve map freshness, they remain limited by perceptual inaccuracies, occlusion in dense traffic, and an inability to fuse multi-agent observations. We propose RTMap to enhance these single-traversal methods by persistently crowdsourcing a multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap simultaneously addresses three core challenges in an end-to-end fashion: (1) Uncertainty-aware positional modeling for HD map elements, (2) probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3) real-time detection for possible road structural changes. Experiments on several public autonomous driving datasets demonstrate our solid performance on both the prior-aided map quality and the localization accuracy, demonstrating our effectiveness of robustly serving downstream prediction and planning modules while gradually improving the accuracy and freshness of the crowdsourced prior-map asynchronously. Our source-code will be made publicly available at https://github.com/CN-ADLab/RTMap (Camera ready version incorporating reviewer suggestions will be updated soon).",
        "arxiv_id": "2507.00980",
        "ARXIVID": "2507.00980",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for real-time recursive mapping in autonomous driving, focusing on novel angles like multi-agent observations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.00754": {
        "authors": [
            "Selim Kuzucu",
            "Muhammad Ferjad Naeem",
            "Anna Kukleva",
            "Federico Tombari",
            "Bernt Schiele"
        ],
        "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs",
        "abstract": "arXiv:2507.00754v1 Announce Type: new  Abstract: The integration of Large Language Model (LLMs) blocks with Vision Transformers (ViTs) holds immense promise for vision-only tasks by leveraging the rich semantic knowledge and reasoning capabilities of LLMs. However, a fundamental challenge lies in the inherent modality mismatch between text-centric pretraining of LLMs and vision-centric training of ViTs. Direct fusion often fails to fully exploit the LLM's potential and suffers from unstable finetuning. As a result, LLM blocks are kept frozen while only the vision components are learned. As a remedy to these challenges, we introduce Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges this modality mismatch through a synergistic pre-training strategy. LUViT co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and (2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM block using the MAE objective. This joint optimization guides the ViT to produce LLM-aligned features and the LLM to effectively interpret visual information. We demonstrate through extensive experiments that LUViT significantly improves performance on various downstream vision tasks, showcasing a more effective and efficient pathway to harness LLM knowledge for visual understanding.",
        "arxiv_id": "2507.00754",
        "ARXIVID": "2507.00754",
        "COMMENT": "Matches criterion 2 as it introduces a new method for integrating LLMs with Vision Transformers, which is relevant to visual large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.00429": {
        "authors": [
            "Jingyi Pan",
            "Dan Xu",
            "Qiong Luo"
        ],
        "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting",
        "abstract": "arXiv:2507.00429v1 Announce Type: new  Abstract: Developing a unified pipeline that enables users to remove, re-texture, or replace objects in a versatile manner is crucial for text-guided 3D inpainting. However, there are still challenges in performing multiple 3D inpainting tasks within a unified framework: 1) Single reference inpainting methods lack robustness when dealing with views that are far from the reference view. 2) Appearance inconsistency arises when independently inpainting multi-view images with 2D diffusion priors; 3) Geometry inconsistency limits performance when there are significant geometric changes in the inpainting regions. To tackle these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting pipeline that leverages diffusion models to propagate consistent appearance and geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy for selecting multiple reference views to reduce errors during propagation. Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that propagates attention features from the selected reference views to other views via diffusion models to maintain appearance consistency. Furthermore, DiGA3D introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to further improve the geometric consistency of inpainted 3D scenes. Extensive experiments on multiple 3D inpainting tasks demonstrate the effectiveness of our method. The project page is available at https://rorisis.github.io/DiGA3D/.",
        "arxiv_id": "2507.00429",
        "ARXIVID": "2507.00429",
        "COMMENT": "Matches criterion 4 as it focuses on a novel 3D inpainting pipeline leveraging diffusion models, which is related to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.00454": {
        "authors": [
            "Yihao Zhen",
            "Qiang Wang",
            "Yu Qiao",
            "Liangqiong Qu",
            "Huijie Fan"
        ],
        "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales",
        "abstract": "arXiv:2507.00454v1 Announce Type: new  Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment between visual inputs and language descriptions caused by target movement. Previous trackers have explored many effective feature modification methods to preserve more aligned features. However, an important yet unexplored factor ultimately hinders their capability, which is the inherent differences in the temporal and spatial scale of information between visual and language inputs. To address this issue, we propose a novel visual-language tracker that enhances the effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and \\textbf{S}patial scale of different input components, named as \\textbf{ATSTrack}. Specifically, we decompose each language description into phrases with different attributes based on their temporal and spatial correspondence with visual inputs, and modify their features in a fine-grained manner. Moreover, we introduce a Visual-Language token that comprises modified linguistic information from the previous frame to guide the model to extract visual features that are more relevant to language description, thereby reducing the impact caused by the differences in spatial scale. Experimental results show that our proposed ATSTrack achieves performance comparable to existing methods. Our code will be released.",
        "arxiv_id": "2507.00454",
        "ARXIVID": "2507.00454",
        "COMMENT": "Matches criterion 2 as it discusses a novel visual-language tracker (VLT) and addresses alignment issues between visual and language inputs, which is relevant to VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.00898": {
        "authors": [
            "Zifu Wan",
            "Ce Zhang",
            "Silong Yong",
            "Martin Q. Ma",
            "Simon Stepputtis",
            "Louis-Philippe Morency",
            "Deva Ramanan",
            "Katia Sycara",
            "Yaqi Xie"
        ],
        "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models",
        "abstract": "arXiv:2507.00898v1 Announce Type: new  Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm for understanding and reasoning about image input through textual responses. Although they have achieved remarkable performance across a range of multi-modal tasks, they face the persistent challenge of hallucination, which introduces practical weaknesses and raises concerns about their reliable deployment in real-world applications. Existing work has explored contrastive decoding approaches to mitigate this issue, where the output of the original LVLM is compared and contrasted with that of a perturbed version. However, these methods require two or more queries that slow down LVLM response generation, making them less suitable for real-time applications. To overcome this limitation, we propose ONLY, a training-free decoding approach that requires only a single query and a one-layer intervention during decoding, enabling efficient real-time deployment. Specifically, we enhance textual outputs by selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token. Extensive experimental results demonstrate that our proposed ONLY consistently outperforms state-of-the-art methods across various benchmarks while requiring minimal implementation effort and computational cost. Code is available at https://github.com/zifuwan/ONLY.",
        "arxiv_id": "2507.00898",
        "ARXIVID": "2507.00898",
        "COMMENT": "Matches criterion 2 as it proposes a novel decoding approach to mitigate hallucinations in LVLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.00045": {
        "authors": [
            "Ming Li",
            "Chenguang Wang",
            "Yijun Liang",
            "Xiyao Wang",
            "Yuhang Zhou",
            "Xiyang Wu",
            "Yuqing Zhang",
            "Ruiyi Zhang",
            "Tianyi Zhou"
        ],
        "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning",
        "abstract": "arXiv:2507.00045v1 Announce Type: new  Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.",
        "arxiv_id": "2507.00045",
        "ARXIVID": "2507.00045",
        "COMMENT": "Matches criterion 2 as it explores the reasoning capabilities of multimodal large language models in challenging scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.00721": {
        "authors": [
            "Xiao Zhang",
            "Fei Wei",
            "Yong Wang",
            "Wenda Zhao",
            "Feiyi Li",
            "Xiangxiang Chu"
        ],
        "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement",
        "abstract": "arXiv:2507.00721v1 Announce Type: new  Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at https://github.com/AMAP-ML/UPRE.",
        "arxiv_id": "2507.00721",
        "ARXIVID": "2507.00721",
        "COMMENT": "Matches criterion 2 as it proposes a framework for zero-shot domain adaptation using vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.00676": {
        "authors": [
            "Edward Effendy",
            "Kuan-Wei Tseng",
            "Rei Kawakami"
        ],
        "title": "A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation",
        "abstract": "arXiv:2507.00676v1 Announce Type: new  Abstract: Accepted in the ICIP 2025   We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.",
        "arxiv_id": "2507.00676",
        "ARXIVID": "2507.00676",
        "COMMENT": "Matches criterion 3 as it introduces a novel transformer-based framework for whole-body grasping motion generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.00371": {
        "authors": [
            "Xin Yang (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Ruiming Du (Department of Biological and Environmental Engineering",
            "Cornell University)",
            "Hanyang Huang (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Jiayang Xie (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Pengyao Xie (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Leisen Fang (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Ziyue Guo (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)",
            "Nanjun Jiang (Amway)",
            "Yu Jiang (Horticulture Section",
            "School of Integrative Plant Science",
            "Cornell AgriTech)",
            "Haiyan Cen (College of Biosystems Engineering and Food Science",
            "Zhejiang University",
            "Key Laboratory of Spectroscopy Sensing",
            "Ministry of Agriculture and Rural Affairs)"
        ],
        "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching",
        "abstract": "arXiv:2507.00371v1 Announce Type: new  Abstract: Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex datasets. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant datasets, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science.",
        "arxiv_id": "2507.00371",
        "ARXIVID": "2507.00371",
        "COMMENT": "Matches criterion 4 as it applies vision foundation models (NeRF) to plant 3D reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.00659": {
        "authors": [
            "Juelin Zhu",
            "Shuaibang Peng",
            "Long Wang",
            "Hanlin Tan",
            "Yu Liu",
            "Maojun Zhang",
            "Shen Yan"
        ],
        "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment",
        "abstract": "arXiv:2507.00659v1 Announce Type: new  Abstract: We propose a novel method for aerial visual localization over low Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method LoD-Loc has shown promising localization results leveraging LoD models. However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the majority of available models and those many countries plan to construct nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD city models could unlock drones' potential for global urban localization. To address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine strategy using explicit silhouette alignment to achieve accurate localization over low-LoD city models in the air. Specifically, given a query image, LoD-Loc v2 first applies a building segmentation network to shape building silhouettes. Then, in the coarse pose selection stage, we construct a pose cost volume by uniformly sampling pose hypotheses around a prior pose to represent the pose probability distribution. Each cost of the volume measures the degree of alignment between the projected and predicted silhouettes. We select the pose with maximum value as the coarse pose. In the fine pose estimation stage, a particle filtering method incorporating a multi-beam tracking approach is used to efficiently explore the hypothesis space and obtain the final pose estimation. To further facilitate research in this field, we release two datasets with LoD1 city models covering 10.7 km , along with real RGB queries and ground-truth pose annotations. Experimental results show that LoD-Loc v2 improves estimation accuracy with high-LoD models and enables localization with low-LoD models for the first time. Moreover, it outperforms state-of-the-art baselines by large margins, even surpassing texture-model-based methods, and broadens the convergence basin to accommodate larger prior errors.",
        "arxiv_id": "2507.00659",
        "ARXIVID": "2507.00659",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for aerial visual localization and releases new datasets for benchmarking.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.00493": {
        "authors": [
            "Fenil R. Doshi",
            "Thomas Fel",
            "Talia Konkle",
            "George Alvarez"
        ],
        "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models",
        "abstract": "arXiv:2507.00493v1 Announce Type: new  Abstract: Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.",
        "arxiv_id": "2507.00493",
        "ARXIVID": "2507.00493",
        "COMMENT": "Matches criterion 4 as it explores vision foundation models and their shape processing capabilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.00817": {
        "authors": [
            "Jiaming Zhang",
            "Rui Hu",
            "Qing Guo",
            "Wei Yang Bryan Lim"
        ],
        "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs",
        "abstract": "arXiv:2507.00817v1 Announce Type: new  Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.",
        "arxiv_id": "2507.00817",
        "ARXIVID": "2507.00817",
        "COMMENT": "Matches criterion 2 as it focuses on adversarial attacks on video multimodal large language models, which is directly relevant to VLLMs/MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.00861": {
        "authors": [
            "Xiaoshuai Hao",
            "Lingdong Kong",
            "Rong Yin",
            "Pengwei Wang",
            "Jing Zhang",
            "Yunfeng Diao",
            "Shu Zhao"
        ],
        "title": "SafeMap: Robust HD Map Construction from Incomplete Observations",
        "abstract": "arXiv:2507.00861v1 Announce Type: new  Abstract: Robust high-definition (HD) map construction is vital for autonomous driving, yet existing methods often struggle with incomplete multi-view camera data. This paper presents SafeMap, a novel framework specifically designed to secure accuracy even when certain camera views are missing. SafeMap integrates two key components: the Gaussian-based Perspective View Reconstruction (G-PVR) module and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module. G-PVR leverages prior knowledge of view importance to dynamically prioritize the most informative regions based on the relationships among available camera views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV representations derived from incomplete observations. Together, these components facilitate the end-to-end map reconstruction and robust HD map generation. SafeMap is easy to implement and integrates seamlessly into existing systems, offering a plug-and-play solution for enhanced robustness. Experimental results demonstrate that SafeMap significantly outperforms previous methods in both complete and incomplete scenarios, highlighting its superior performance and reliability.",
        "arxiv_id": "2507.00861",
        "ARXIVID": "2507.00861",
        "COMMENT": "Matches criterion 4 as it focuses on robust HD map construction, which is an application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.00792": {
        "authors": [
            "Hendric Voss",
            "Stefan Kopp"
        ],
        "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters",
        "abstract": "arXiv:2507.00792v1 Announce Type: new  Abstract: Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/TF-JAX-IK",
        "arxiv_id": "2507.00792",
        "ARXIVID": "2507.00792",
        "COMMENT": "Matches criterion 3 as it introduces a novel real-time inverse kinematics solver for virtual human motion, which could be relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.01376": {
        "authors": [
            "Yinwang Ren",
            "Yangyang Liu",
            "Tang Ji",
            "Xun Xu"
        ],
        "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing",
        "abstract": "arXiv:2507.01376v1 Announce Type: new  Abstract: AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face.",
        "arxiv_id": "2507.01376",
        "ARXIVID": "2507.01376",
        "COMMENT": "Matches criterion 2 as it discusses MLLMs and their applications in manufacturing.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.00790": {
        "authors": [
            "Huaqiu Li",
            "Yong Wang",
            "Tongwen Huang",
            "Hailang Huang",
            "Haoqian Wang",
            "Xiangxiang Chu"
        ],
        "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling",
        "abstract": "arXiv:2507.00790v1 Announce Type: new  Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS.",
        "arxiv_id": "2507.00790",
        "ARXIVID": "2507.00790",
        "COMMENT": "Matches criterion 4 as it discusses a novel approach using latent diffusion models for unified image restoration, which relates to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.01012": {
        "authors": [
            "Zhe Kong",
            "Le Li",
            "Yong Zhang",
            "Feng Gao",
            "Shaoshu Yang",
            "Tao Wang",
            "Kaihao Zhang",
            "Zhuoliang Kang",
            "Xiaoming Wei",
            "Guanying Chen",
            "Wenhan Luo"
        ],
        "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution",
        "abstract": "arXiv:2507.01012v1 Announce Type: new  Abstract: Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.",
        "arxiv_id": "2507.01012",
        "ARXIVID": "2507.01012",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and video super-resolution.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.00969": {
        "authors": [
            "Alberto Neri",
            "Maximilan Fehrentz",
            "Veronica Penza",
            "Leonardo S. Mattos",
            "Nazim Haouchine"
        ],
        "title": "Surgical Neural Radiance Fields from One Image",
        "abstract": "arXiv:2507.00969v1 Announce Type: new  Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D reconstruction and view synthesis, yet their reliance on extensive multi-view data limits their application in surgical intraoperative settings where only limited data is available. In particular, collecting such extensive data intraoperatively is impractical due to time constraints. This work addresses this challenge by leveraging a single intraoperative image and preoperative data to train NeRF efficiently for surgical scenarios.   Methods: We leverage preoperative MRI data to define the set of camera viewpoints and images needed for robust and unobstructed training. Intraoperatively, the appearance of the surgical image is transferred to the pre-constructed training set through neural style transfer, specifically combining WTC2 and STROTSS to prevent over-stylization. This process enables the creation of a dataset for instant and fast single-image NeRF training.   Results: The method is evaluated with four clinical neurosurgical cases. Quantitative comparisons to NeRF models trained on real surgical microscope images demonstrate strong synthesis agreement, with similarity metrics indicating high reconstruction fidelity and stylistic alignment. When compared with ground truth, our method demonstrates high structural similarity, confirming good reconstruction quality and texture preservation.   Conclusion: Our approach demonstrates the feasibility of single-image NeRF training in surgical settings, overcoming the limitations of traditional multi-view methods.",
        "arxiv_id": "2507.00969",
        "ARXIVID": "2507.00969",
        "COMMENT": "Does not match any specific criteria but is related to the general interest area of 3D reconstruction and neural radiance fields.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.00583": {
        "authors": [
            "Christian Intern\\`o",
            "Robert Geirhos",
            "Markus Olhofer",
            "Sunny Liu",
            "Barbara Hammer",
            "David Klindt"
        ],
        "title": "AI-Generated Video Detection via Perceptual Straightening",
        "abstract": "arXiv:2507.00583v1 Announce Type: new  Abstract: The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the \"perceptual straightening\" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.",
        "arxiv_id": "2507.00583",
        "ARXIVID": "2507.00583",
        "COMMENT": "Does not match any specific criteria but is related to the general interest area of computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.00261": {
        "authors": [
            "Zhiyin Lin",
            "Purvi Goel",
            "Joy Yun",
            "C. Karen Liu",
            "Joao Pedro Araujo"
        ],
        "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos",
        "abstract": "arXiv:2507.00261v1 Announce Type: new  Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical motions. While most motions fall into a few high-level actions (e.g. step, lunge, parry), the execution can vary widely-fast vs. slow, large vs. small, offensive vs. defensive. Moreover, a fencer's actions are informed by a strategy that often comes in response to the opponent's behavior. This combination of motion diversity with underlying two-player strategy motivates the application of data-driven modeling to fencing. We present VirtualFencer, a system capable of extracting 3D fencing motion and strategy from in-the-wild video without supervision, and then using that extracted knowledge to generate realistic fencing behavior. We demonstrate the versatile capabilities of our system by having it (i) fence against itself (self-play), (ii) fence against a real fencer's motion from online video, and (iii) fence interactively against a professional fencer.",
        "arxiv_id": "2507.00261",
        "ARXIVID": "2507.00261",
        "COMMENT": "Does not match any specific criterion but is generally interesting as it involves embodied AI and motion generation, which aligns with your friend's general interest in embodied AI and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00566": {
        "authors": [
            "Kai Zhou",
            "Shuhai Zhang",
            "Zeng You",
            "Jinwu Hu",
            "Mingkui Tan",
            "Fei Liu"
        ],
        "title": "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment",
        "abstract": "arXiv:2507.00566v1 Announce Type: new  Abstract: Zero-shot skeleton-based action recognition aims to classify unseen skeleton-based human actions without prior exposure to such categories during training. This task is extremely challenging due to the difficulty in generalizing from known to unknown actions. Previous studies typically use two-stage training: pre-training skeleton encoders on seen action categories using cross-entropy loss and then aligning pre-extracted skeleton and text features, enabling knowledge transfer to unseen classes through skeleton-text alignment and language models' generalization. However, their efficacy is hindered by 1) insufficient discrimination for skeleton features, as the fixed skeleton encoder fails to capture necessary alignment information for effective skeleton-text alignment; 2) the neglect of alignment bias between skeleton and unseen text features during testing. To this end, we propose a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive training framework to improve skeleton-text alignment, ensuring sufficient discrimination for skeleton features. Additionally, we introduce a prototype-guided text feature alignment strategy to mitigate the adverse impact of the distribution discrepancy during testing. We provide a theoretical analysis to support our prototype-guided text feature alignment strategy and empirically evaluate our overall PGFA on three well-known datasets. Compared with the top competitor SMIE method, our PGFA achieves absolute accuracy improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD datasets, respectively.",
        "arxiv_id": "2507.00566",
        "ARXIVID": "2507.00566",
        "COMMENT": "Does not match any specific criteria but is related to skeleton-based action recognition and cross-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.01009": {
        "authors": [
            "Anna Foix Romero",
            "Craig Russell",
            "Alexander Krull",
            "Virginie Uhlmann"
        ],
        "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification",
        "abstract": "arXiv:2507.01009v1 Announce Type: new  Abstract: The shape of objects is an important source of visual information in a wide range of applications. One of the core challenges of shape quantification is to ensure that the extracted measurements remain invariant to transformations that preserve an object's intrinsic geometry, such as changing its size, orientation, and position in the image. In this work, we introduce ShapeEmbed, a self-supervised representation learning framework designed to encode the contour of objects in 2D images, represented as a Euclidean distance matrix, into a shape descriptor that is invariant to translation, scaling, rotation, reflection, and point indexing. Our approach overcomes the limitations of traditional shape descriptors while improving upon existing state-of-the-art autoencoder-based approaches. We demonstrate that the descriptors learned by our framework outperform their competitors in shape classification tasks on natural and biological images. We envision our approach to be of particular relevance to biological imaging applications.",
        "arxiv_id": "2507.01009",
        "ARXIVID": "2507.01009",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and self-supervised learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00373": {
        "authors": [
            "Jian Jin",
            "Fanxin Xia",
            "Feng Ding",
            "Xinfeng Zhang",
            "Meiqin Liu",
            "Yao Zhao",
            "Weisi Lin",
            "Lili Meng"
        ],
        "title": "Customizable ROI-Based Deep Image Compression",
        "abstract": "arXiv:2507.00373v2 Announce Type: new  Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by prioritizing ROI for higher-quality reconstruction. However, as the users (including human clients and downstream machine tasks) become more diverse, ROI-based image compression needs to be customizable to support various preferences. For example, different users may define distinct ROI or require different quality trade-offs between ROI and non-ROI. Existing ROI-based image compression schemes predefine the ROI, making it unchangeable, and lack effective mechanisms to balance reconstruction quality between ROI and non-ROI. This work proposes a paradigm for customizable ROI-based deep image compression. First, we develop a Text-controlled Mask Acquisition (TMA) module, which allows users to easily customize their ROI for compression by just inputting the corresponding semantic \\emph{text}. It makes the encoder controlled by text. Second, we design a Customizable Value Assign (CVA) mechanism, which masks the non-ROI with a changeable extent decided by users instead of a constant one to manage the reconstruction quality trade-off between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA) module, where the latent spatial prior of the mask and the latent Rate-Distortion Optimization (RDO) prior of the image are extracted and fused in the latent space, and further used to optimize the latent representation of the source image. Experimental results demonstrate that our proposed customizable ROI-based deep image compression paradigm effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.",
        "arxiv_id": "2507.00373",
        "ARXIVID": "2507.00373",
        "COMMENT": "Does not match any specific criteria but discusses customizable ROI-based deep image compression.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00372": {
        "authors": [
            "Xinge Yang",
            "Chuong Nguyen",
            "Wenbin Wang",
            "Kaizhang Kang",
            "Wolfgang Heidrich",
            "Xiaoxing Li"
        ],
        "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur",
        "abstract": "arXiv:2507.00372v1 Announce Type: new  Abstract: Modern cameras with large apertures often suffer from a shallow depth of field, resulting in blurry images of objects outside the focal plane. This limitation is particularly problematic for fixed-focus cameras, such as those used in smart glasses, where adding autofocus mechanisms is challenging due to form factor and power constraints. Due to unmatched optical aberrations and defocus properties unique to each camera system, deep learning models trained on existing open-source datasets often face domain gaps and do not perform well in real-world settings. In this paper, we propose an efficient and scalable dataset synthesis approach that does not rely on fine-tuning with real-world data. Our method simultaneously models depth-dependent defocus and spatially varying optical aberrations, addressing both computational complexity and the scarcity of high-quality RGB-D datasets. Experimental results demonstrate that a network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes.",
        "arxiv_id": "2507.00372",
        "ARXIVID": "2507.00372",
        "COMMENT": "Does not match any specific criteria but discusses a novel dataset synthesis approach for defocus deblurring.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00724": {
        "authors": [
            "Linghui Zhu",
            "Yiming Li",
            "Haiqin Weng",
            "Yan Liu",
            "Tianwei Zhang",
            "Shu-Tao Xia",
            "Zhi Wang"
        ],
        "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
        "abstract": "arXiv:2507.00724v1 Announce Type: new  Abstract: Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.",
        "arxiv_id": "2507.00724",
        "ARXIVID": "2507.00724",
        "COMMENT": "Does not match any specific criteria but discusses model ownership verification for personalized large vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00825": {
        "authors": [
            "Hongxing Peng",
            "Lide Chen",
            "Hui Zhu",
            "Yan Chen"
        ],
        "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery",
        "abstract": "arXiv:2507.00825v1 Announce Type: new  Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial challenges, including small target sizes, high-density distributions, and cluttered backgrounds in UAV imagery. Current algorithms often depend on hand-crafted components like anchor boxes, which demand fine-tuning and exhibit limited generalization, and Non-Maximum Suppression (NMS), which is threshold-sensitive and prone to misclassifying dense objects. These generic architectures thus struggle to adapt to aerial imaging characteristics, resulting in performance limitations. Moreover, emerging end-to-end frameworks have yet to effectively mitigate these aerial-specific challenges.To address these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time Detection Transformer framework tailored for UAVs. First, we introduce the High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone. HFESNet preserves critical high-frequency spatial details to extract robust semantic features, thereby improving discriminative capability for small and occluded targets in complex backgrounds. Second, our Efficient Small Object Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with minimal computational overhead, significantly boosting small object detection. Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware Positional Encoding (GAPE) modules enhance the detector's decoder stability and localization accuracy, effectively optimizing bounding boxes and providing explicit spatial priors for dense scenes. Experiments on the VisDrone dataset demonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase over the baseline, while maintaining real-time speed and reducing parameter count by 4M.",
        "arxiv_id": "2507.00825",
        "ARXIVID": "2507.00825",
        "COMMENT": "Does not match any specific criteria but is related to the general interest area of object detection and UAV imagery.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00916": {
        "authors": [
            "Tianshi Cao",
            "Marie-Julie Rakotosaona",
            "Ben Poole",
            "Federico Tombari",
            "Michael Niemeyer"
        ],
        "title": "Masks make discriminative models great again!",
        "abstract": "arXiv:2507.00916v1 Announce Type: new  Abstract: We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.",
        "arxiv_id": "2507.00916",
        "ARXIVID": "2507.00916",
        "COMMENT": "Does not match any specific criteria but is related to the general interest area of 3D reconstruction and discriminative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00049": {
        "authors": [
            "Feiyang Kang",
            "Nadine Chang",
            "Maying Shen",
            "Marc T. Law",
            "Rafid Mahmood",
            "Ruoxi Jia",
            "Jose M. Alvarez"
        ],
        "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training",
        "abstract": "arXiv:2507.00049v1 Announce Type: new  Abstract: The computational burden and inherent redundancy of large-scale datasets challenge the training of contemporary machine learning models. Data pruning offers a solution by selecting smaller, informative subsets, yet existing methods struggle: density-based approaches can be task-agnostic, while model-based techniques may introduce redundancy or prove computationally prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid framework that synergistically integrates density-based pruning with model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions data and applies an initial density-based pruning. It then employs a proxy model to evaluate the impact of this initial pruning within each cluster by comparing losses on kept versus pruned samples. This task-aware signal adaptively adjusts cluster-specific pruning thresholds, enabling more aggressive pruning in redundant clusters while preserving critical data in informative ones. Extensive experiments on large-scale object detection benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms prominent baselines, substantially reduces performance degradation (e.g., over 54% versus random sampling on Waymo), and achieves near-original model performance while pruning 20% of data, highlighting its efficacy in enhancing data efficiency for large-scale model training. Code is open-sourced.",
        "arxiv_id": "2507.00049",
        "ARXIVID": "2507.00049",
        "COMMENT": "Does not match any specific criteria but is related to the general interest area of efficient training methods in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.01231": {
        "authors": [
            "I\\~naki Dellibarda Varela",
            "Pablo Romero-Sorozabal",
            "Eduardo Rocon",
            "Manuel Cebrian"
        ],
        "title": "Rethinking the Illusion of Thinking",
        "abstract": "arXiv:2507.01231v1 Announce Type: new  Abstract: Earlier this year, Apple ignited controversy by publishing \"The Illusion of Thinking,\" prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate by replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue, we show that previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here.",
        "arxiv_id": "2507.01231",
        "ARXIVID": "2507.01231",
        "COMMENT": "Does not match any specific criterion but discusses reasoning capabilities in large reasoning models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00447": {
        "authors": [
            "Xin Luo",
            "Menglin Zhang",
            "Yunwei Lan",
            "Tianyu Zhang",
            "Rui Li",
            "Chang Liu",
            "Dong Liu"
        ],
        "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration",
        "abstract": "arXiv:2507.00447v1 Announce Type: new  Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face restoration algorithms must balance perceptual quality and fidelity. To achieve minimal distortion while maintaining perfect perceptual quality, Posterior-Mean Rectified Flow (PMRF) proposes a flow based approach where source distribution is minimum distortion estimations. Although PMRF is shown to be effective, its pixel-space modeling approach limits its ability to align with human perception, where human perception is defined as how humans distinguish between two image distributions. In this work, we propose Latent-PMRF, which reformulates PMRF in the latent space of a variational autoencoder (VAE), facilitating better alignment with human perception during optimization. By defining the source distribution on latent representations of minimum distortion estimation, we bound the minimum distortion by the VAE's reconstruction error. Moreover, we reveal the design of VAE is crucial, and our proposed VAE significantly outperforms existing VAEs in both reconstruction and restoration. Extensive experiments on blind face restoration demonstrate the superiority of Latent-PMRF, offering an improved PD-tradeoff compared to existing methods, along with remarkable convergence efficiency, achieving a 5.79X speedup over PMRF in terms of FID. Our code will be available as open-source.",
        "arxiv_id": "2507.00447",
        "ARXIVID": "2507.00447",
        "COMMENT": "Does not match any specific criterion but focuses on face restoration using latent space modeling, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00043": {
        "authors": [
            "Mehmet Yigit Avci",
            "Pedro Borges",
            "Paul Wright",
            "Mehmet Yigitsoy",
            "Sebastien Ourselin",
            "Jorge Cardoso"
        ],
        "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations",
        "abstract": "arXiv:2507.00043v1 Announce Type: new  Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at https://github.com/myigitavci/MR-CLIP.",
        "arxiv_id": "2507.00043",
        "ARXIVID": "2507.00043",
        "COMMENT": "Does not match any specific criterion but is related to multimodal learning in medical imaging, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.00849": {
        "authors": [
            "Wei Li",
            "Jiaman Tang",
            "Yang Li",
            "Beihao Xia",
            "Ligang Tan",
            "Hongmao Qin"
        ],
        "title": "UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection",
        "abstract": "arXiv:2507.00849v1 Announce Type: new  Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in traffic management, agriculture, emergency rescue, etc. However, it faces significant challenges, including occlusions, small object sizes, and irregular shapes. These challenges highlight the necessity for a robust and efficient multimodal UAV object detection method. Mamba has demonstrated considerable potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a multimodal UAV object detection framework based on Mamba architectures. To improve geometric adaptability, we propose the Deformable Token Mamba Block (DTMB) to generate deformable tokens by incorporating adaptive patches from deformable convolutions alongside normal patches from normal convolutions, which serve as the inputs to the Mamba Block. To optimize the multimodal feature complementarity, we design two separate DTMBs for the RGB and infrared (IR) modalities, with the outputs from both DTMBs integrated into the Mamba Block for feature extraction and into the Fusion Mamba Block for feature fusion. Additionally, to improve multiscale object detection, especially for small objects, we stack four DTMBs at different scales to produce multiscale feature representations, which are then sent to the Detection Neck for Mamba (DNM). The DNM module, inspired by the YOLO series, includes modifications to the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In particular, we employ cross-enhanced spatial attention before the DTMB and cross-channel attention after the Fusion Mamba Block to extract more discriminative features. Experimental results on the DroneVehicle dataset show that our method outperforms the baseline OAFA method by 3.6% in the mAP metric. Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.",
        "arxiv_id": "2507.00849",
        "ARXIVID": "2507.00849",
        "COMMENT": "Does not match any specific criterion but focuses on multimodal UAV detection, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.00585": {
        "authors": [
            "Tang Hao",
            "Guo ZhiQing",
            "Wang LieJun",
            "Liu Chao"
        ],
        "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation",
        "abstract": "arXiv:2507.00585v1 Announce Type: new  Abstract: In recent years, it has been found that \"grandmother cells\" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.",
        "arxiv_id": "2507.00585",
        "ARXIVID": "2507.00585",
        "COMMENT": "Does not match any specific criterion but is related to medical image segmentation, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}