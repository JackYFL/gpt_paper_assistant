{
    "2509.03501": {
        "authors": [
            "Honglu Zhou",
            "Xiangyu Peng",
            "Shrikant Kendre",
            "Michael S. Ryoo",
            "Silvio Savarese",
            "Caiming Xiong",
            "Juan Carlos Niebles"
        ],
        "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data",
        "abstract": "arXiv:2509.03501v1 Announce Type: new  Abstract: Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.",
        "arxiv_id": "2509.03501",
        "ARXIVID": "2509.03501",
        "COMMENT": "Matches criteria 1 and 2. Presents a new method for spatiotemporal reasoning in Video LLMs, with a synthetic instruction data generation framework for spatial and temporal references. Directly advances spatial understanding in embodied agents and introduces a new VLLM capability.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.02807": {
        "authors": [
            "Mennatullah Siam"
        ],
        "title": "PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?",
        "abstract": "arXiv:2509.02807v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) have shown impressive generalization across tasks using images and text modalities. While their extension to video has enabled tasks such as video question answering and video captioning, their pixel-level visual grounding abilities are less studied. In this work, we raise the pertinent question of whether motion is used in pixel-level visual grounding and whether video MLLMs can segment objects based on natural language expressions describing their motion patterns. We identify the shortcomings in the current benchmarks, where we show that a single frame can often suffice for capturing the motion referring expression without any temporal reasoning. To address this, we introduce four motion-centric probing techniques, particularly designed for the visual grounding task, to study video MLLMs' ability to identify true motion from a fake one and their ability to grasp the motion order. Consequently, we provide a motion-centric benchmark, MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging the interaction between motion and language rather than being dominated by static appearance cues emphasized in existing visual grounding datasets. We further establish strong single-image baselines that are on par with or outperform prior methods. Finally, we explore simple motion-centric adaptation techniques that provide state-of-the-art performance on our MoCentric-Bench. Our motion-centric benchmark, evaluation and findings challenge future models to improve dense spatiotemporal grounding and pixel-level understanding within videos. Code and datasets will be made publicly available at https://github.com/MSiam/PixFoundation-2.0.git.",
        "arxiv_id": "2509.02807",
        "ARXIVID": "2509.02807",
        "COMMENT": "This paper directly addresses criterion 2 (new MLLMs/VLLMs) by probing video multi-modal LLMs for their use of motion in visual grounding, introduces a new motion-centric benchmark (criterion 3), and provides surprising empirical findings about the limitations of current models. It is highly relevant to your friend's interests in vision-language models, multi-modal learning, and benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.03324": {
        "authors": [
            "Yixiong Jing",
            "Cheng Zhang",
            "Haibing Wu",
            "Guangming Wang",
            "Olaf Wysocki",
            "Brian Sheil"
        ],
        "title": "InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds",
        "abstract": "arXiv:2509.03324v1 Announce Type: new  Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.",
        "arxiv_id": "2509.03324",
        "ARXIVID": "2509.03324",
        "COMMENT": "Matches criteria 4. Presents InfraDiffusion, a zero-shot diffusion-based method for depth map restoration from sparse point clouds, improving segmentation with vision foundation models (SAM). Novel application of diffusion models and foundation models in infrastructure monitoring.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.02966": {
        "authors": [
            "Yujin Wang",
            "Tianyi Wang",
            "Quanfeng Liu",
            "Wenxian Fan",
            "Junfeng Jiao",
            "Christian Claudel",
            "Yunbing Yan",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "title": "KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models",
        "abstract": "arXiv:2509.02966v1 Announce Type: new  Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable autonomous driving, yet existing vision-language models (VLMs) often fail to effectively ground their reasoning in scene dynamics and domain knowledge. To address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM framework that predicts ego trajectories directly from consecutive front-view driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video encoder, trained via self-supervised learning with hard-negative mining, with a scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars. Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit planning constraints, while a triple-stage fine-tuning schedule incrementally aligns the language head to metric spatial cues, physically feasible motion, and temporally conditioned front-view planning. Evaluated on nuScenes dataset, KEPT achieves state-of-the-art performance across open-loop protocols: under NoAvg, it achieves 0.70m average L2 with a 0.21\\% collision rate; under TemAvg with lightweight ego status, it attains 0.31m average L2 and a 0.07\\% collision rate. Ablation studies show that all three fine-tuning stages contribute complementary benefits, and that using Top-2 retrieved exemplars yields the best accuracy-safety trade-off. The k-means-clustered HNSW index delivers sub-millisecond retrieval latency, supporting practical deployment. These results indicate that retrieval-augmented, CoT-guided VLMs offer a promising, data-efficient pathway toward interpretable and trustworthy autonomous driving.",
        "arxiv_id": "2509.02966",
        "ARXIVID": "2509.02966",
        "COMMENT": "Matches criteria 2. Introduces KEPT, a knowledge-enhanced vision-language model for trajectory prediction in autonomous driving, with retrieval-augmented, chain-of-thought-guided VLMs. Strong focus on VLMs for spatial reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.03113": {
        "authors": [
            "Shan Wang",
            "Maying Shen",
            "Nadine Chang",
            "Chuong Nguyen",
            "Hongdong Li",
            "Jose M. Alvarez"
        ],
        "title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection",
        "abstract": "arXiv:2509.03113v1 Announce Type: new  Abstract: Hallucinations in multimodal large language model are caused by the text-visual bias and the co-occurrence bias. The former reflects an over-reliance on text information in the decision-making process, while the latter arises from the statistical object-pairing patterns abstracted from the training data. Existing mitigation methods heuristically address these biases without understanding the fluctuating bias level across the instances. We first propose estimating the influence of respective token types (visual, prompt, and previous outputs) using a gradient-based self-reflection method. The estimated token influence further enables the detection of object-related visual tokens and their integration into an influence-aware contrastive decoding framework to mitigate both types of biases simultaneously. Our method operates without the need for additional resources, such as costly fine-tuning, extra models, or data statistics. Extensive experiments show it effectively reduces hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.",
        "arxiv_id": "2509.03113",
        "ARXIVID": "2509.03113",
        "COMMENT": "Matches criteria 2. Proposes a new method for mitigating hallucinations in multimodal LLMs using gradient-based self-reflection and influence-aware decoding. Directly relevant to VLLM/MLLM reliability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.03170": {
        "authors": [
            "Mattia Litrico",
            "Feng Chen",
            "Michael Pound",
            "Sotirios A Tsaftaris",
            "Sebastiano Battiato",
            "Mario Valerio Giuffrida"
        ],
        "title": "Count2Density: Crowd Density Estimation without Location-level Annotations",
        "abstract": "arXiv:2509.03170v1 Announce Type: new  Abstract: Crowd density estimation is a well-known computer vision task aimed at estimating the density distribution of people in an image. The main challenge in this domain is the reliance on fine-grained location-level annotations, (i.e. points placed on top of each individual) to train deep networks. Collecting such detailed annotations is both tedious, time-consuming, and poses a significant barrier to scalability for real-world applications. To alleviate this burden, we present Count2Density: a novel pipeline designed to predict meaningful density maps containing quantitative spatial information using only count-level annotations (i.e., total number of people) during training. To achieve this, Count2Density generates pseudo-density maps leveraging past predictions stored in a Historical Map Bank, thereby reducing confirmation bias. This bank is initialised using an unsupervised saliency estimator to provide an initial spatial prior and is iteratively updated with an EMA of predicted density maps. These pseudo-density maps are obtained by sampling locations from estimated crowd areas using a hypergeometric distribution, with the number of samplings determined by the count-level annotations. To further enhance the spatial awareness of the model, we add a self-supervised contrastive spatial regulariser to encourage similar feature representations within crowded regions while maximising dissimilarity with background regions. Experimental results demonstrate that our approach significantly outperforms cross-domain adaptation methods and achieves better results than recent state-of-the-art approaches in semi-supervised settings across several datasets. Additional analyses validate the effectiveness of each individual component of our pipeline, confirming the ability of Count2Density to effectively retrieve spatial information from count-level annotations and enabling accurate subregion counting.",
        "arxiv_id": "2509.03170",
        "ARXIVID": "2509.03170",
        "COMMENT": "Matches criteria 1. Proposes a new method for spatial understanding in crowd density estimation using only count-level annotations, with clever statistical tricks (Historical Map Bank, hypergeometric sampling, contrastive spatial regularizer) to recover spatial information.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.03277": {
        "authors": [
            "Qihang Zhou",
            "Shibo He",
            "Jiangtao Yan",
            "Wenchao Meng",
            "Jiming Chen"
        ],
        "title": "PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection",
        "abstract": "arXiv:2509.03277v1 Announce Type: new  Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization capabilities to identify 3D anomalies across unseen objects of highly diverse class semantics. To this end, we propose a unified framework to comprehensively detect and segment 3D anomalies by leveraging both point- and pixel-level information. We first design PointAD, which leverages point-pixel correspondence to represent 3D anomalies through their associated rendering pixel representations. This approach is referred to as implicit 3D representation, as it focuses solely on rendering pixel anomalies but neglects the inherent spatial relationships within point clouds. Then, we propose PointAD+ to further broaden the interpretation of 3D anomalies by introducing explicit 3D representation, emphasizing spatial abnormality to uncover abnormal spatial relationships. Hence, we propose G-aggregation to involve geometry information to enable the aggregated point representations spatially aware. To simultaneously capture rendering and spatial abnormality, PointAD+ proposes hierarchical representation learning, incorporating implicit and explicit anomaly semantics into hierarchical text prompts: rendering prompts for the rendering layer and geometry prompts for the geometry layer. A cross-hierarchy contrastive alignment is further introduced to promote the interaction between the rendering and geometry layers, facilitating mutual anomaly learning. Finally, PointAD+ integrates anomaly semantics from both layers to capture the generalized anomaly semantics. During the test, PointAD+ can integrate RGB information in a plug-and-play manner and further improve its detection performance. Extensive experiments demonstrate the superiority of PointAD+ in ZS 3D anomaly detection across unseen objects with highly diverse class semantics, achieving a holistic understanding of abnormality.",
        "arxiv_id": "2509.03277",
        "ARXIVID": "2509.03277",
        "COMMENT": "This paper proposes PointAD+, a framework for zero-shot 3D anomaly detection by transferring CLIP's 2D generalization to 3D, using hierarchical representations and contrastive alignment. It is highly relevant to vision foundation models and their applications (criterion 4), and also touches on spatial understanding (criterion 1) in 3D anomaly detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.03462": {
        "authors": [
            "Zhuo Cao",
            "Yunxiao Shi",
            "Min Xu"
        ],
        "title": "sam-llm: interpretable lane change trajectoryprediction via parametric finetuning",
        "abstract": "arXiv:2509.03462v1 Announce Type: new  Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the gap between the contextual reasoning of Large Language Models (LLMs) and the physical precision of kinematic lane change models for autonomous driving. The system is designed for interpretable lane change trajectory prediction by finetuning an LLM to output the core physical parameters of a trajectory model instead of raw coordinates. For lane-keeping scenarios, the model predicts discrete coordinates, but for lane change maneuvers, it generates the parameters for an enhanced Sinusoidal Acceleration Model (SAM), including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach yields a complete, continuous, and physically plausible trajectory model that is inherently interpretable and computationally efficient, achieving an 80% reduction in output size compared to coordinate-based methods. The SAM-LLM achieves a state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating performance equivalent to traditional LLM predictors while offering significant advantages in explainability and resource efficiency.",
        "arxiv_id": "2509.03462",
        "ARXIVID": "2509.03462",
        "COMMENT": "Matches criteria 2. Introduces a hybrid LLM-based approach for interpretable lane change trajectory prediction, using LLMs for physical parameter prediction in autonomous driving. This is a novel application of LLMs in vision-language reasoning for spatial tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.03032": {
        "authors": [
            "Kaicong Huang",
            "Talha Azfar",
            "Jack M. Reilly",
            "Thomas Guggisberg",
            "Ruimin Ke"
        ],
        "title": "Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification",
        "abstract": "arXiv:2509.03032v1 Announce Type: new  Abstract: Person re-identification faces two core challenges: precisely locating the foreground target while suppressing background noise and extracting fine-grained features from the target region. Numerous visual-only approaches address these issues by partitioning an image and applying attention modules, yet they rely on costly manual annotations and struggle with complex occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic cues to guide visual understanding. However, they focus solely on foreground information, but overlook the potential value of background cues. Inspired by human perception, we argue that background semantics are as important as the foreground semantics in ReID, as humans tend to eliminate background distractions while focusing on target appearance. Therefore, this paper proposes an end-to-end framework that jointly models foreground and background information within a dual-branch cross-modal feature extraction pipeline. To help the network distinguish between the two domains, we propose an intra-semantic alignment and inter-semantic adversarial learning strategy. Specifically, we align visual and textual features that share the same semantics across domains, while simultaneously penalizing similarity between foreground and background features to enhance the network's discriminative power. This strategy drives the model to actively suppress noisy background regions and enhance attention toward identity-relevant foreground cues. Comprehensive experiments on two holistic and two occluded ReID benchmarks demonstrate the effectiveness and generality of the proposed method, with results that match or surpass those of current state-of-the-art approaches.",
        "arxiv_id": "2509.03032",
        "ARXIVID": "2509.03032",
        "COMMENT": "Matches criteria 2. Proposes a language-enhanced adversarial framework for person re-identification, leveraging cross-modal (vision-language) features and adversarial learning. Focuses on vision-language modeling for fine-grained recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.03041": {
        "authors": [
            "Pengyang Yu",
            "Haoquan Wang",
            "Gerard Marks",
            "Tahar Kechadi",
            "Laurence T. Yang",
            "Sahraoui Dhelim",
            "Nyothiri Aung"
        ],
        "title": "MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model",
        "abstract": "arXiv:2509.03041v1 Announce Type: new  Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours.",
        "arxiv_id": "2509.03041",
        "ARXIVID": "2509.03041",
        "COMMENT": "Matches criteria 4. Presents a lightweight hybrid vision foundation model (CNN + Transformer) for medical image segmentation, with architectural innovations for small datasets.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.03465": {
        "authors": [
            "Kuan-Chuan Peng"
        ],
        "title": "Joint Training of Image Generator and Detector for Road Defect Detection",
        "abstract": "arXiv:2509.03465v1 Announce Type: new  Abstract: Road defect detection is important for road authorities to reduce the vehicle damage caused by road defects. Considering the practical scenarios where the defect detectors are typically deployed on edge devices with limited memory and computational resource, we aim at performing road defect detection without using ensemble-based methods or test-time augmentation (TTA). To this end, we propose to Jointly Train the image Generator and Detector for road defect detection (dubbed as JTGD). We design the dual discriminators for the generative model to enforce both the synthesized defect patches and overall images to look plausible. The synthesized image quality is improved by our proposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in JTGD is trained jointly with the detector to encourage the generative model to synthesize harder examples for the detector. Since harder synthesized images of better quality caused by the aforesaid design are used in the data augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road defect detection benchmark across various countries under the condition of no ensemble and TTA. JTGD only uses less than 20% of the number of parameters compared with the competing baseline, which makes it more suitable for deployment on edge devices in practice.",
        "arxiv_id": "2509.03465",
        "ARXIVID": "2509.03465",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications) to some extent. The paper proposes a joint training method for an image generator and detector for road defect detection, using a CLIP-based loss and focusing on efficient deployment. The use of generative models for hard example synthesis and CLIP-based FID loss is a methodological improvement, and the application of CLIP (a vision foundation model) is relevant.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.02969": {
        "authors": [
            "Dasong Li",
            "Sizhuo Ma",
            "Hang Hua",
            "Wenjie Li",
            "Jian Wang",
            "Chris Wei Zhou",
            "Fengbin Guan",
            "Xin Li",
            "Zihao Yu",
            "Yiting Lu",
            "Ru-Ling Liao",
            "Yan Ye",
            "Zhibo Chen",
            "Wei Sun",
            "Linhan Cao",
            "Yuqin Cao",
            "Weixia Zhang",
            "Wen Wen",
            "Kaiwei Zhang",
            "Zijian Chen",
            "Fangfang Lu",
            "Xiongkuo Min",
            "Guangtao Zhai",
            "Erjia Xiao",
            "Lingfeng Zhang",
            "Zhenjie Su",
            "Hao Cheng",
            "Yu Liu",
            "Renjing Xu",
            "Long Chen",
            "Xiaoshuai Hao",
            "Zhenpeng Zeng",
            "Jianqin Wu",
            "Xuxu Wang",
            "Qian Yu",
            "Bo Hu",
            "Weiwei Wang",
            "Pinxin Liu",
            "Yunlong Tang",
            "Luchuan Song",
            "Jinxi He",
            "Jiaru Wu",
            "Hanjia Lyu"
        ],
        "title": "VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results",
        "abstract": "arXiv:2509.02969v1 Announce Type: new  Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.",
        "arxiv_id": "2509.02969",
        "ARXIVID": "2509.02969",
        "COMMENT": "This paper describes a challenge and dataset for engagement prediction in short videos using multi-modal features. It introduces a new benchmark (criterion 3) for multi-modal learning in video, which is relevant to your friend's interests.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.02962": {
        "authors": [
            "Shuai Jiang",
            "Yunfeng Ma",
            "Jingyu Zhou",
            "Yuan Bian",
            "Yaonan Wang",
            "Min Liu"
        ],
        "title": "Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability",
        "abstract": "arXiv:2509.02962v1 Announce Type: new  Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and locate defect in industrial products by fusing RGB and 3D modalities. This article focuses on modality-missing problems caused by uncertain sensors availability in MISDD. In this context, the fusion of multiple modalities encounters several troubles, including learning mode transformation and information vacancy. To this end, we first propose cross-modal prompt learning, which includes: i) the cross-modal consistency prompt serves the establishment of information consistency of dual visual modalities; ii) the modality-specific prompt is inserted to adapt different input patterns; iii) the missing-aware prompt is attached to compensate for the information vacancy caused by dynamic modalities-missing. In addition, we propose symmetric contrastive learning, which utilizes text modality as a bridge for fusion of dual vision modalities. Specifically, a paired antithetical text prompt is designed to generate binary text semantics, and triple-modal contrastive pre-training is offered to accomplish multimodal learning. Experiment results show that our proposed method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7 for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58% respectively), and outperforms existing approaches to varying degrees under different missing types and rates. The source code will be available at https://github.com/SvyJ/MISDD-MM.",
        "arxiv_id": "2509.02962",
        "ARXIVID": "2509.02962",
        "COMMENT": "This paper proposes a multimodal industrial surface defect detection method that handles missing modalities using cross-modal prompt learning and contrastive learning. It is relevant to multi-modal learning and vision foundation models (criterion 4), especially with its use of prompts and contrastive pre-training, but does not introduce a new VLLM/MLLM (criterion 2).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03219": {
        "authors": [
            "Leonidas Bakopoulos",
            "Georgios Chalkiadakis"
        ],
        "title": "Uncertainty-driven Adaptive Exploration",
        "abstract": "arXiv:2509.03219v1 Announce Type: new  Abstract: Adaptive exploration methods propose ways to learn complex policies via alternating between exploration and exploitation. An important question for such methods is to determine the appropriate moment to switch between exploration and exploitation and vice versa. This is critical in domains that require the learning of long and complex sequences of actions. In this work, we present a generic adaptive exploration framework that employs uncertainty to address this important issue in a principled manner. Our framework includes previous adaptive exploration approaches as special cases. Moreover, we can incorporate in our framework any uncertainty-measuring mechanism of choice, for instance mechanisms used in intrinsic motivation or epistemic uncertainty-based exploration methods. We experimentally demonstrate that our framework gives rise to adaptive exploration strategies that outperform standard ones across several MuJoCo environments.",
        "arxiv_id": "2509.03219",
        "ARXIVID": "2509.03219",
        "COMMENT": "This paper presents a generic adaptive exploration framework using uncertainty, with experiments in MuJoCo environments. It is related to embodied AI and exploration strategies, which is tangentially related to criterion 3 (new methods in embodied AI), but does not introduce a new benchmark or focus on spatial intelligence per se.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03141": {
        "authors": [
            "Mattia Litrico",
            "Francesco Guarnera",
            "Mario Valerio Giuffrida",
            "Daniele Rav\\`i",
            "Sebastiano Battiato"
        ],
        "title": "Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation",
        "abstract": "arXiv:2509.03141v1 Announce Type: new  Abstract: Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.",
        "arxiv_id": "2509.03141",
        "ARXIVID": "2509.03141",
        "COMMENT": "This paper introduces a temporally-aware 3D diffusion model for brain MRI progression modeling. While it is a generative model in vision, it does not match the specific criteria (no embodied AI, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.02751": {
        "authors": [
            "Matthew Russo",
            "Tim Kraska"
        ],
        "title": "Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics",
        "abstract": "arXiv:2509.02751v1 Announce Type: new  Abstract: With advances in large language models (LLMs), researchers are creating new systems that can perform AI-driven analytics over large unstructured datasets. Recent work has explored executing such analytics queries using semantic operators -- a declarative set of AI-powered data transformations with natural language specifications. However, even when optimized, these operators can be expensive to execute on millions of records and their iterator execution semantics make them ill-suited for interactive data analytics tasks. In another line of work, Deep Research systems have demonstrated an ability to answer natural language question(s) over large datasets. These systems use one or more LLM agent(s) to plan their execution, process the dataset(s), and iteratively refine their answer. However, these systems do not explicitly optimize their query plans which can lead to poor plan execution. In order for AI-driven analytics to excel, we need a runtime which combines the optimized execution of semantic operators with the flexibility and more dynamic execution of Deep Research systems. As a first step towards this vision, we build a prototype which enables Deep Research agents to write and execute optimized semantic operator programs. We evaluate our prototype and demonstrate that it can outperform a handcrafted semantic operator program and open Deep Research systems on two basic queries. Compared to a standard open Deep Research agent, our prototype achieves up to 1.95x better F1-score. Furthermore, even if we give the agent access to semantic operators as tools, our prototype still achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its optimized execution.",
        "arxiv_id": "2509.02751",
        "ARXIVID": "2509.02751",
        "COMMENT": "Does not match any specific criteria. Focuses on AI-driven analytics systems and runtime optimization, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03221": {
        "authors": [
            "Jing Zhang",
            "Siying Tao",
            "Jiao Li",
            "Tianhe Wang",
            "Junchen Wu",
            "Ruqian Hao",
            "Xiaohui Du",
            "Ruirong Tan",
            "Rui Li"
        ],
        "title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking",
        "abstract": "arXiv:2509.03221v1 Announce Type: new  Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.",
        "arxiv_id": "2509.03221",
        "ARXIVID": "2509.03221",
        "COMMENT": "This paper proposes a new method for organoid segmentation and tracking using a fusion of CNN and Transformer features. While it is a methodological improvement in vision, it does not match the specific criteria (no embodied AI, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.02993": {
        "authors": [
            "Chao Fan",
            "Xibin Jia",
            "Anqi Xiao",
            "Hongyuan Yu",
            "Zhenghan Yang",
            "Dawei Yang",
            "Hui Xu",
            "Yan Huang",
            "Liang Wang"
        ],
        "title": "SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation",
        "abstract": "arXiv:2509.02993v1 Announce Type: new  Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intra-class variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies between the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing state-of-the-art methods, achieving superior performance.",
        "arxiv_id": "2509.02993",
        "ARXIVID": "2509.02993",
        "COMMENT": "This paper introduces a new prototype enhancement network for few-shot medical image segmentation. While it is a methodological improvement in computer vision, it does not match any of the specific criteria (no embodied AI, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.02918": {
        "authors": [
            "Midhat Urooj",
            "Ayan Banerjee",
            "Farhat Shaikh",
            "Kuntal Thakur",
            "Sandeep Gupta"
        ],
        "title": "Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach",
        "abstract": "arXiv:2509.02918v1 Announce Type: new  Abstract: Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.",
        "arxiv_id": "2509.02918",
        "ARXIVID": "2509.02918",
        "COMMENT": "This paper proposes a neuro-symbolic framework for domain generalization in medical imaging, integrating vision transformers with symbolic reasoning. While it involves spatial reasoning in a medical context, it does not focus on embodied agents or spatial intelligence in the sense of embodied AI (criterion 1), nor does it introduce a new VLLM/MLLM (criterion 2), new embodied AI benchmarks (criterion 3), or vision foundation models in a general sense (criterion 4).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03185": {
        "authors": [
            "Debopom Sutradhar",
            "Ripon Kumar Debnath",
            "Mohaimenul Azam Khan Raiaan",
            "Yan Zhang",
            "Reem E. Mohamed",
            "Sami Azam"
        ],
        "title": "PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising",
        "abstract": "arXiv:2509.03185v1 Announce Type: new  Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation exposure, but it often leads to increased noise and reduced image quality. Traditional denoising methods, such as iterative optimization or supervised learning, often fail to preserve image quality. To address these challenges, we introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in which an advanced posterior policy optimization (PPO) algorithm is used to optimize denoising policies in real time, based on image quality feedback, trained via a custom gym environment. The experimental results on the low dose CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT model outperforms traditional denoising techniques and other DL-based methods, achieving a peak signal-to-noise ratio of 41.87, a structural similarity index measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of 41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality of denoising using a classification task in the COVID-19 LDCT dataset, where the images processed by our method improved the classification accuracy to 94\\%, achieving 4\\% higher accuracy compared to denoising without RL-based denoising. This method offers a promising solution for safer and more accurate LDCT imaging.",
        "arxiv_id": "2509.03185",
        "ARXIVID": "2509.03185",
        "COMMENT": "Does not match any specific criteria. Focuses on RL-based denoising for medical imaging, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}