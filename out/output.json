{
    "2512.09907": {
        "authors": [
            "Daoan Zhang",
            "Pai Liu",
            "Xiaofei Zhou",
            "Yuan Ge",
            "Guangchen Lan",
            "Jing Bi",
            "Christopher Brinton",
            "Ehsan Hoque",
            "Jiebo Luo"
        ],
        "title": "VisualActBench: Can VLMs See and Act like a Human?",
        "abstract": "arXiv:2512.09907v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.",
        "arxiv_id": "2512.09907",
        "ARXIVID": "2512.09907",
        "COMMENT": "Directly matches criterion 3 (embodied AI, new benchmark for vision-language models acting in the world) and criterion 4 (vision foundation models and applications). Introduces a new benchmark (VisualActBench) for evaluating VLMs' ability to reason and act proactively, which is a novel and much-needed contribution.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.09824": {
        "authors": [
            "Xianghao Kong",
            "Zeyu Zhang",
            "Yuwei Guo",
            "Zhuoran Zhao",
            "Songchun Zhang",
            "Anyi Rao"
        ],
        "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
        "abstract": "arXiv:2512.09824v1 Announce Type: new  Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
        "arxiv_id": "2512.09824",
        "ARXIVID": "2512.09824",
        "COMMENT": "Matches criterion 2 and 4: introduces a new method for visual concept composition from images and videos using prompt binding in diffusion transformers, relevant to multi-modal generative modeling and vision foundation models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.09663": {
        "authors": [
            "Tao Zhang",
            "Yuyang Hong",
            "Yang Xia",
            "Kun Ding",
            "Zeyu Zhang",
            "Ying Wang",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
        "abstract": "arXiv:2512.09663v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
        "arxiv_id": "2512.09663",
        "ARXIVID": "2512.09663",
        "COMMENT": "Directly matches criterion 3 (new embodied AI benchmarks) and criterion 2 (MLLMs). Introduces IF-Bench, the first benchmark for MLLMs on infrared images, and proposes a novel generative visual prompting method to bridge domain gaps. Includes systematic evaluation of 40+ MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08980": {
        "authors": [
            "Chengqi Dong",
            "Chuhuai Yue",
            "Hang He",
            "Rongge Mao",
            "Fenghe Tang",
            "S Kevin Zhou",
            "Zekun Xu",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Wei Lin",
            "Guojun Yin"
        ],
        "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning",
        "abstract": "arXiv:2512.08980v1 Announce Type: new  Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.",
        "arxiv_id": "2512.08980",
        "ARXIVID": "2512.08980",
        "COMMENT": "Matches criterion 2: introduces IMAgent, a new VLM-based agent trained via end-to-end RL for multi-image tasks, with a new dataset (MIFG-QA) and specialized tools for visual reasoning. Also relevant to criterion 3 (novel embodied agent method for spatial reasoning).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.09864": {
        "authors": [
            "Hao Lu",
            "Ziyang Liu",
            "Guangfeng Jiang",
            "Yuanfei Luo",
            "Sheng Chen",
            "Yangang Zhang",
            "Ying-Cong Chen"
        ],
        "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
        "abstract": "arXiv:2512.09864v1 Announce Type: new  Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
        "arxiv_id": "2512.09864",
        "ARXIVID": "2512.09864",
        "COMMENT": "Directly matches criterion 2 (VLLMs/MLLMs) and criterion 3 (embodied AI, new methods for autonomous driving with vision-language models and planning). Proposes a unified framework integrating VLMs and video generation for end-to-end autonomous driving, with a novel training strategy and new datasets.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.09307": {
        "authors": [
            "Shivanshu Agnihotri",
            "Snehashis Majhi",
            "Deepak Ranjan Nayak",
            "Debesh Jha"
        ],
        "title": "From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation",
        "abstract": "arXiv:2512.09307v1 Announce Type: new  Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.",
        "arxiv_id": "2512.09307",
        "ARXIVID": "2512.09307",
        "COMMENT": "Matches criterion 4: proposes a distillation framework (Polyp-DiFoM) to transfer vision foundation model knowledge (SAM, DINOv2, etc.) to lightweight segmentation models for medical imaging, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.09282": {
        "authors": [
            "Xiang Chen",
            "Jinshan Pan",
            "Jiangxin Dong",
            "Jian Yang",
            "Jinhui Tang"
        ],
        "title": "FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model",
        "abstract": "arXiv:2512.09282v1 Announce Type: new  Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.",
        "arxiv_id": "2512.09282",
        "ARXIVID": "2512.09282",
        "COMMENT": "Matches criterion 4: proposes a new image restoration foundation model (FoundIR-v2) with a novel data mixture optimization and MoE-driven scheduler for generative pre-training, relevant to vision foundation models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.09407": {
        "authors": [
            "Haobo Jiang",
            "Jin Xie",
            "Jian Yang",
            "Liang Yu",
            "Jianmin Zheng"
        ],
        "title": "Generative Point Cloud Registration",
        "abstract": "arXiv:2512.09407v1 Announce Type: new  Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.",
        "arxiv_id": "2512.09407",
        "ARXIVID": "2512.09407",
        "COMMENT": "Matches criterion 1 (spatial understanding) and criterion 4 (vision foundation models and applications): introduces a generative 3D registration paradigm that bridges 2D generative models with 3D matching, using a controllable 2D generative model for robust 3D registration.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.09373": {
        "authors": [
            "Haobo Jiang",
            "Jin Xie",
            "Jian Yang",
            "Liang Yu",
            "Jianmin Zheng"
        ],
        "title": "FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement",
        "abstract": "arXiv:2512.09373v1 Announce Type: new  Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.",
        "arxiv_id": "2512.09373",
        "ARXIVID": "2512.09373",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications): proposes a transformer for multiview 3D registration, uses 2D attention priors from foundation models to enhance 3D feature interaction, and introduces a diffusion-based refinement.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.09924": {
        "authors": [
            "Xinyu Liu",
            "Hangjie Yuan",
            "Yujie Wei",
            "Jiazheng Xing",
            "Yujin Han",
            "Jiahao Pan",
            "Yanbiao Ma",
            "Chi-Min Chan",
            "Kang Zhao",
            "Shiwei Zhang",
            "Wenhan Luo",
            "Yike Guo"
        ],
        "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "abstract": "arXiv:2512.09924v1 Announce Type: new  Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
        "arxiv_id": "2512.09924",
        "ARXIVID": "2512.09924",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a unified model for reasoning-informed video editing, introduces a new benchmark (RVE-Bench), and a self-reflective learning framework that leverages internal VLMs for feedback.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.09354": {
        "authors": [
            "Xinkui Zhao",
            "Zuxin Wang",
            "Yifan Zhang",
            "Guanjie Cheng",
            "Yueshen Xu",
            "Shuiguang Deng",
            "Chang Liu",
            "Naibo Wang",
            "Jianwei Yin"
        ],
        "title": "Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding",
        "abstract": "arXiv:2512.09354v1 Announce Type: new  Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.",
        "arxiv_id": "2512.09354",
        "ARXIVID": "2512.09354",
        "COMMENT": "Matches criterion 2: proposes Video-QTR, a lightweight framework for video understanding using query-driven temporal reasoning, and demonstrates state-of-the-art results on MLLM benchmarks. Directly relevant to new MLLMs and efficient video reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.09573": {
        "authors": [
            "Yuan Li",
            "Zitang Sun",
            "Yen-Ju Chen",
            "Shin'ya Nishida"
        ],
        "title": "Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment",
        "abstract": "arXiv:2512.09573v1 Announce Type: new  Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.",
        "arxiv_id": "2512.09573",
        "ARXIVID": "2512.09573",
        "COMMENT": "Directly matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Provides an insightful empirical analysis of MLLMs' low-level visual perception and proposes a method to improve it, which is a clever diagnostic and improvement for vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.09016": {
        "authors": [
            "Haiqian Han",
            "Lingdong Kong",
            "Jianing Li",
            "Ao Liang",
            "Chengtao Zhu",
            "Jiacheng Lyu",
            "Lai Xing Ng",
            "Xiangyang Ji",
            "Wei Tsang Ooi",
            "Benoit R. Cottereau"
        ],
        "title": "Learning to Remove Lens Flare in Event Camera",
        "abstract": "arXiv:2512.09016v1 Announce Type: new  Abstract: Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.",
        "arxiv_id": "2512.09016",
        "ARXIVID": "2512.09016",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (E-Deflare Benchmark) and method (E-DeflareNet) for event camera lens flare removal, a previously overlooked problem in spatial-temporal vision for embodied agents. Also relevant to spatial understanding (criterion 1) due to the spatio-temporal modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.09172": {
        "authors": [
            "Sauda Maryam",
            "Sara Nadeem",
            "Faisal Qureshi",
            "Mohsen Ali"
        ],
        "title": "Prompt-Based Continual Compositional Zero-Shot Learning",
        "abstract": "arXiv:2512.09172v1 Announce Type: new  Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.",
        "arxiv_id": "2512.09172",
        "ARXIVID": "2512.09172",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): proposes a prompt-based continual compositional zero-shot learning framework built on a vision-language model backbone, with novel loss functions and evaluation protocol.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.09247": {
        "authors": [
            "Cheng Liu",
            "Yiren Song",
            "Haofan Wang",
            "Mike Zheng Shou"
        ],
        "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
        "abstract": "arXiv:2512.09247v1 Announce Type: new  Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
        "arxiv_id": "2512.09247",
        "ARXIVID": "2512.09247",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and generative modeling in multi-modal learning. Proposes a diffusion transformer for layered PSD file generation and decomposition, with a new RGBA-layered dataset.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.09112": {
        "authors": [
            "Fr\\'ed\\'eric Fortier-Chouinard",
            "Yannick Hold-Geoffroy",
            "Valentin Deschaintre",
            "Matheus Gadelha",
            "Jean-Fran\\c{c}ois Lalonde"
        ],
        "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
        "abstract": "arXiv:2512.09112v1 Announce Type: new  Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
        "arxiv_id": "2512.09112",
        "ARXIVID": "2512.09112",
        "COMMENT": "Matches criterion 4: introduces GimbalDiffusion, a gravity-aware camera control framework for video generation, and establishes a new benchmark for camera-aware video generation. Relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.09417": {
        "authors": [
            "Yanan Wang",
            "Shengcai Liao",
            "Panwen Hu",
            "Xin Li",
            "Fan Yang",
            "Xiaodan Liang"
        ],
        "title": "DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping",
        "abstract": "arXiv:2512.09417v1 Announce Type: new  Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\\TrainNum{} videos) and benchmarking (\\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.",
        "arxiv_id": "2512.09417",
        "ARXIVID": "2512.09417",
        "COMMENT": "Matches criterion 3: introduces HeadSwapBench, a new benchmark dataset for video head swapping, and proposes a novel mask-free method (DirectSwap) with a new loss for expression and motion consistency. Also relevant to vision foundation models (criterion 4) via video diffusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.09435": {
        "authors": [
            "Xufan He",
            "Yushuang Wu",
            "Xiaoyang Guo",
            "Chongjie Ye",
            "Jiaqing Zhou",
            "Tianlei Hu",
            "Xiaoguang Han",
            "Dong Du"
        ],
        "title": "UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents",
        "abstract": "arXiv:2512.09435v1 Announce Type: new  Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.",
        "arxiv_id": "2512.09435",
        "ARXIVID": "2512.09435",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Proposes a new latent representation and a two-stage diffusion framework for part-level 3D generation, which is a novel approach in 3D vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.09867": {
        "authors": [
            "Fengli Wu",
            "Vaidehi Patil",
            "Jaehong Yoon",
            "Yue Zhang",
            "Mohit Bansal"
        ],
        "title": "MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI",
        "abstract": "arXiv:2512.09867v1 Announce Type: new  Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the \"right to be forgotten\". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.",
        "arxiv_id": "2512.09867",
        "ARXIVID": "2512.09867",
        "COMMENT": "Matches criterion 2 (new MLLMs): introduces a hierarchy-aware multimodal unlearning testbed for medical AI, evaluates SOTA MLLMs, and provides a benchmark for privacy-compliant unlearning in multimodal models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.09633": {
        "authors": [
            "Senem Aktas",
            "Charles Markham",
            "John McDonald",
            "Rozenn Dahyot"
        ],
        "title": "Benchmarking SAM2-based Trackers on FMOX",
        "abstract": "arXiv:2512.09633v1 Announce Type: new  Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.",
        "arxiv_id": "2512.09633",
        "ARXIVID": "2512.09633",
        "COMMENT": "Matches criterion 3: This paper benchmarks SAM2-based trackers on a new challenging dataset (FMOX) for fast moving objects, providing new insights into tracker limitations. The focus on benchmarking in a challenging scenario and analyzing tracker behavior is a novel angle in embodied AI and simulator-related evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.09477": {
        "authors": [
            "Guillem Arias",
            "Ariadna Sol\\`a",
            "Mart\\'i Armengod",
            "Maria Vanrell"
        ],
        "title": "Color encoding in Latent Space of Stable Diffusion Models",
        "abstract": "arXiv:2512.09477v1 Announce Type: new  Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",
        "arxiv_id": "2512.09477",
        "ARXIVID": "2512.09477",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications). Provides an insightful analysis of how color is encoded in the latent space of Stable Diffusion, with implications for model understanding and editing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.09363": {
        "authors": [
            "Ke Xing",
            "Longfei Li",
            "Yuyang Yin",
            "Hanwen Liang",
            "Guixun Luo",
            "Chen Fang",
            "Jue Wang",
            "Konstantinos N. Plataniotis",
            "Xiaojie Jin",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
        "abstract": "arXiv:2512.09363v1 Announce Type: new  Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
        "arxiv_id": "2512.09363",
        "ARXIVID": "2512.09363",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and generative modeling in multi-modal learning. Presents a geometry-aware monocular-to-stereo video generation framework, with a new large-scale stereo video dataset.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.09670": {
        "authors": [
            "Gil Weissman",
            "Amir Ivry",
            "Israel Cohen"
        ],
        "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
        "abstract": "arXiv:2512.09670v1 Announce Type: new  Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
        "arxiv_id": "2512.09670",
        "ARXIVID": "2512.09670",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications). The paper presents an automated framework for satellite tasking using AI models, including vision-language models, for spatial reasoning and intelligence in real-world scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.09335": {
        "authors": [
            "Seonghwa Choi",
            "Moonkyeong Choi",
            "Mingyu Jang",
            "Jaekyung Kim",
            "Jianfei Cai",
            "Wen-Huang Cheng",
            "Sanghoon Lee"
        ],
        "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
        "abstract": "arXiv:2512.09335v1 Announce Type: new  Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
        "arxiv_id": "2512.09335",
        "ARXIVID": "2512.09335",
        "COMMENT": "Matches criterion 4: proposes a new 3DGS-based framework for relightable and dynamic human avatars from monocular video, with a new dataset and regularization for geometric detail. Relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.09736": {
        "authors": [
            "Jingtian Yan",
            "Zhifei Li",
            "William Kang",
            "Stephen F. Smith",
            "Jiaoyang Li"
        ],
        "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
        "abstract": "arXiv:2512.09736v1 Announce Type: new  Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
        "arxiv_id": "2512.09736",
        "ARXIVID": "2512.09736",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator-related work). Analyzes planner design trade-offs for multi-agent path finding under realistic simulation, which is a novel angle for embodied AI benchmarking.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.08981": {
        "authors": [
            "Tahar Chettaoui",
            "Naser Damer",
            "Fadi Boutros"
        ],
        "title": "Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding",
        "abstract": "arXiv:2512.08981v1 Announce Type: new  Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.",
        "arxiv_id": "2512.08981",
        "ARXIVID": "2512.08981",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method to mitigate demographic bias in face recognition using vision-language models, which is a clever application of cross-modal alignment.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.09871": {
        "authors": [
            "Yimin Zhu",
            "Lincoln Linlin Xu"
        ],
        "title": "Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling",
        "abstract": "arXiv:2512.09871v1 Announce Type: new  Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.",
        "arxiv_id": "2512.09871",
        "ARXIVID": "2512.09871",
        "COMMENT": "Somewhat related to criterion 4: This paper proposes a diffusion-based posterior sampler for hyperspectral unmixing, leveraging generative modeling (diffusion models) for spatial and spectral understanding. While not directly about vision foundation models, it uses diffusion models in a novel way for remote sensing, which is adjacent to vision foundation model applications.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2512.09792": {
        "authors": [
            "Pierre Ancey",
            "Andrew Price",
            "Saqib Javed",
            "Mathieu Salzmann"
        ],
        "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation",
        "abstract": "arXiv:2512.09792v1 Announce Type: new  Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.",
        "arxiv_id": "2512.09792",
        "ARXIVID": "2512.09792",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications), as it proposes a ViT-based architecture for real-time spacecraft pose estimation, with a novel mathematical formalism for pose regression.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09847": {
        "authors": [
            "Shijia Feng",
            "Michael Wray",
            "Walterio Mayol-Cuevas"
        ],
        "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities",
        "abstract": "arXiv:2512.09847v1 Announce Type: new  Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.",
        "arxiv_id": "2512.09847",
        "ARXIVID": "2512.09847",
        "COMMENT": "Somewhat matches criterion 3 (embodied AI, new methods for real-time struggle detection and anticipation in assistive systems). Focuses on online understanding and anticipation, which is a novel angle for embodied AI and human skill assessment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09580": {
        "authors": [
            "Hancheng Zhu",
            "Xinyu Liu",
            "Rui Yao",
            "Kunyang Sun",
            "Leida Li",
            "Abdulmotaleb El Saddik"
        ],
        "title": "Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation",
        "abstract": "arXiv:2512.09580v1 Announce Type: new  Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.",
        "arxiv_id": "2512.09580",
        "ARXIVID": "2512.09580",
        "COMMENT": "Somewhat matches criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications). Proposes a multimodal model for content-adaptive image retouching guided by attribute-based text, which is a novel application of vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09727": {
        "authors": [
            "Junlin Xiao",
            "Victor-Alexandru Darvariu",
            "Bruno Lacerda",
            "Nick Hawes"
        ],
        "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
        "abstract": "arXiv:2512.09727v1 Announce Type: new  Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
        "arxiv_id": "2512.09727",
        "ARXIVID": "2512.09727",
        "COMMENT": "Partially matches criterion 3: proposes a new statistical aggregation method for Monte Carlo Tree Search in continuous action spaces, which is relevant to planning in embodied AI, but not a new benchmark or simulator.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09311": {
        "authors": [
            "Kuldeep Singh Yadav",
            "Lalan Kumar"
        ],
        "title": "Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance",
        "abstract": "arXiv:2512.09311v1 Announce Type: new  Abstract: Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.",
        "arxiv_id": "2512.09311",
        "ARXIVID": "2512.09311",
        "COMMENT": "Partially matches criterion 4: introduces a new large-scale vision dataset (USE50k) and a transformer-based multimodal fusion framework for visual surveillance, but not a general-purpose vision foundation model. Some relevance to multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09687": {
        "authors": [
            "Er Jin",
            "Yang Zhang",
            "Yongli Mou",
            "Yanfei Dong",
            "Stefan Decker",
            "Kenji Kawaguchi",
            "Johannes Stegmaier"
        ],
        "title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized",
        "abstract": "arXiv:2512.09687v1 Announce Type: new  Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.",
        "arxiv_id": "2512.09687",
        "ARXIVID": "2512.09687",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and their applications): proposes a model pruning approach to mitigate memorization in generative models, which is relevant to vision foundation models and their responsible use.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.09095": {
        "authors": [
            "Xinyue Pan",
            "Yuhao Chen",
            "Jiangpeng He",
            "Fengqing Zhu"
        ],
        "title": "Food Image Generation on Multi-Noun Categories",
        "abstract": "arXiv:2512.09095v1 Announce Type: new  Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt \"egg noodle\" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.",
        "arxiv_id": "2512.09095",
        "ARXIVID": "2512.09095",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) and generative modeling in multi-modal learning, as it addresses food image generation with multi-noun categories and proposes a new method for layout refinement.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2512.09583": {
        "authors": [
            "Alberto Rota",
            "Mert Kiray",
            "Mert Asim Karaoglu",
            "Patrick Ruhkamp",
            "Elena De Momi",
            "Nassir Navabm",
            "Benjamin Busam"
        ],
        "title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision",
        "abstract": "arXiv:2512.09583v1 Announce Type: new  Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/",
        "arxiv_id": "2512.09583",
        "ARXIVID": "2512.09583",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications), as it uses a frozen vision transformer encoder and proposes a new method for highlight removal in images, but not a direct match to the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.09071": {
        "authors": [
            "Nick Trinh",
            "Damian Lyons"
        ],
        "title": "Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics",
        "abstract": "arXiv:2512.09071v1 Announce Type: new  Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.",
        "arxiv_id": "2512.09071",
        "ARXIVID": "2512.09071",
        "COMMENT": "Related to criterion 1 (spatial understanding): proposes an adaptive thresholding method for visual place recognition, which is a spatial intelligence task, but the methodological novelty is moderate.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2512.09897": {
        "authors": [
            "Haoye Lu",
            "Pavan Seshadri",
            "Kaheer Suleman"
        ],
        "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
        "abstract": "arXiv:2512.09897v1 Announce Type: new  Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
        "arxiv_id": "2512.09897",
        "ARXIVID": "2512.09897",
        "COMMENT": "Somewhat related to embodied AI (criterion 3), as it discusses hierarchical planning in text-based environments using LLMs, but not focused on spatial or visual intelligence or new simulators.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.09629": {
        "authors": [
            "Emanuele La Malfa",
            "Ping Zhu",
            "Samuele Marro",
            "Sara Bernardini",
            "Michael Wooldridge"
        ],
        "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
        "abstract": "arXiv:2512.09629v1 Announce Type: new  Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
        "arxiv_id": "2512.09629",
        "ARXIVID": "2512.09629",
        "COMMENT": "Somewhat relevant to criterion 2: uses LLMs for planning, but not specifically visual or multi-modal LLMs. More focused on natural language to PDDL planning.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.09525": {
        "authors": [
            "Hongyou Zhou",
            "Cederic A{\\ss}mann",
            "Alaa Bejaoui",
            "Heiko Tzsch\\\"atzsch",
            "Mark Heyland",
            "Julian Zierke",
            "Niklas Tuttle",
            "Sebastian H\\\"olzl",
            "Timo Auer",
            "David A. Back",
            "Marc Toussaint"
        ],
        "title": "Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction",
        "abstract": "arXiv:2512.09525v1 Announce Type: new  Abstract: Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair",
        "arxiv_id": "2512.09525",
        "ARXIVID": "2512.09525",
        "COMMENT": "Partially matches criterion 1: proposes a spatial transformer network and autoencoder for spatial registration and reconstruction in medical imaging, but not directly about embodied agents or general spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.09402": {
        "authors": [
            "Rui Wang",
            "Yuting Jiang",
            "Xiaoqing Luo",
            "Xiao-Jun Wu",
            "Nicu Sebe",
            "Ziheng Chen"
        ],
        "title": "Wasserstein-Aligned Hyperbolic Multi-View Clustering",
        "abstract": "arXiv:2512.09402v1 Announce Type: new  Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.",
        "arxiv_id": "2512.09402",
        "ARXIVID": "2512.09402",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications): proposes a hyperbolic multi-view clustering method, but not directly about foundation models or multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.09115": {
        "authors": [
            "Sander Riis{\\o}en Jyhne",
            "Christian Igel",
            "Morten Goodwin",
            "Per-Arne Andersen",
            "Serge Belongie",
            "Nico Lang"
        ],
        "title": "SuperF: Neural Implicit Fields for Multi-Image Super-Resolution",
        "abstract": "arXiv:2512.09115v1 Announce Type: new  Abstract: High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to \"hallucinated\" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.   The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.",
        "arxiv_id": "2512.09115",
        "ARXIVID": "2512.09115",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications): proposes a neural implicit field approach for multi-image super-resolution, but does not directly involve foundation models or multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.08996": {
        "authors": [
            "Riqiang Gao",
            "Simon Arberet",
            "Martin Kraus",
            "Han Liu",
            "Wilko FAR Verbakel",
            "Dorin Comaniciu",
            "Florin-Cristian Ghesu",
            "Ali Kamen"
        ],
        "title": "Demo: Generative AI helps Radiotherapy Planning with User Preference",
        "abstract": "arXiv:2512.08996v1 Announce Type: new  Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.",
        "arxiv_id": "2512.08996",
        "ARXIVID": "2512.08996",
        "COMMENT": "Does not match any specific criterion. Presents a generative model for radiotherapy planning, but not related to spatial intelligence, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.09497": {
        "authors": [
            "Jinmiao Zhao",
            "Chuang Yu",
            "Zelin Shi",
            "Yunpeng Liu",
            "Yingdi Zhang"
        ],
        "title": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "abstract": "arXiv:2512.09497v1 Announce Type: new  Abstract: Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net",
        "arxiv_id": "2512.09497",
        "ARXIVID": "2512.09497",
        "COMMENT": "Does not match any specific criterion. Focuses on infrared small target detection with a novel gradient-guided network, but not on spatial intelligence, embodied AI, or foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.09471": {
        "authors": [
            "Yiqun Wang",
            "Lujun Li",
            "Meiru Yue",
            "Radu State"
        ],
        "title": "Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach",
        "abstract": "arXiv:2512.09471v1 Announce Type: new  Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.",
        "arxiv_id": "2512.09471",
        "ARXIVID": "2512.09471",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications): proposes a ViViT-based framework for MSI reconstruction, but not directly about foundation models or multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.09315": {
        "authors": [
            "Yuan Ma",
            "Junlin Hou",
            "Chao Zhang",
            "Yukun Zhou",
            "Zongyuan Ge",
            "Haoran Xie",
            "Lie Ju"
        ],
        "title": "Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook",
        "abstract": "arXiv:2512.09315v1 Announce Type: new  Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \\textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.",
        "arxiv_id": "2512.09315",
        "ARXIVID": "2512.09315",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications): introduces a benchmark for learning with noisy labels in medical imaging, but does not focus on foundation models or multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.09235": {
        "authors": [
            "Md Eimran Hossain Eimon",
            "Hyomin Choi",
            "Fabien Racap\\'e",
            "Mateen Ulhaq",
            "Velibor Adzic",
            "Hari Kalva",
            "Borko Furht"
        ],
        "title": "Efficient Feature Compression for Machines with Global Statistics Preservation",
        "abstract": "arXiv:2512.09235v1 Announce Type: new  Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.",
        "arxiv_id": "2512.09235",
        "ARXIVID": "2512.09235",
        "COMMENT": "Somewhat relevant to vision foundation models (criterion 4) as it discusses feature compression for AI models, but not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}