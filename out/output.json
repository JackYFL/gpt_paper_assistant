{
    "2508.04700": {
        "authors": [
            "Zeyi Sun",
            "Ziyu Liu",
            "Yuhang Zang",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Tong Wu",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
        "abstract": "arXiv:2508.04700v1 Announce Type: new  Abstract: Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.",
        "arxiv_id": "2508.04700",
        "ARXIVID": "2508.04700",
        "COMMENT": "Matches criteria 1 and 3: Proposes a new agentic self-evolving framework for computer use agents (CUAs) that autonomously learn spatial and software environments, with a novel benchmark (OS-World) and new learning methods for embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.04655": {
        "authors": [
            "Hao Wang",
            "Limeng Qiao",
            "Zequn Jie",
            "Zhijian Huang",
            "Chengjian Feng",
            "Qingfang Zheng",
            "Lin Ma",
            "Xiangyuan Lan",
            "Xiaodan Liang"
        ],
        "title": "X-SAM: From Segment Anything to Any Segmentation",
        "abstract": "arXiv:2508.04655v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.",
        "arxiv_id": "2508.04655",
        "ARXIVID": "2508.04655",
        "COMMENT": "Matches criteria 2 (new MLLMs) and 4 (vision foundation models, segmentation). Proposes X-SAM, a unified MLLM framework for any segmentation task, introducing a new segmentation task and unified training strategy. Strong focus on pixel-level visual understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.04416": {
        "authors": [
            "Haoji Zhang",
            "Xin Gu",
            "Jiawen Li",
            "Chixiang Ma",
            "Sule Bai",
            "Chubin Zhang",
            "Bowen Zhang",
            "Zhichao Zhou",
            "Dongliang He",
            "Yansong Tang"
        ],
        "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning",
        "abstract": "arXiv:2508.04416v1 Announce Type: new  Abstract: The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. All code, data and model weight will be made publicly available.",
        "arxiv_id": "2508.04416",
        "ARXIVID": "2508.04416",
        "COMMENT": "Matches criteria 1 (spatial intelligence on embodied agents) and 2 (new MLLMs for video reasoning). Proposes a novel agentic video reasoning framework (VITAL) with tool-augmented learning, new datasets, and a new RL algorithm for long video reasoning. Strong focus on spatial/temporal reasoning and multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.04453": {
        "authors": [
            "Qingguo Hu",
            "Ante Wang",
            "Jia Song",
            "Delai Qiu",
            "Qingsong Liu",
            "Jinsong Su"
        ],
        "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion",
        "abstract": "arXiv:2508.04453v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have experienced significant advancements in recent years. However, their performance still falls short in tasks requiring deep visual perception, such as identifying subtle differences between images. A potential cause is the scarcity of visual knowledge in popular instruction-tuning corpora, resulting in inadequate visual perception and reasoning capabilities. To address this challenge, we introduce a self-improvement framework grounded in a novel visual knowledge-intensive task, \\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion (CVC). This task requires LVLMs to infer the masked object in an image based on its \\textit{causal} relationships with the other visible information. We first obtain rich examples cheaply through our automated instance construction pipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or human assistance. Then, LVLMs effectively self-improve through trial and error learning using these created instances. Our experiments demonstrate substantial gains across four challenging specialized tasks and four widely-used comprehensive benchmarks. Especially on specialized tasks, our method achieves an average improvement of 5.4\\% and 4.0\\% compared to the corresponding baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code is available at https://github.com/XMUDeepLIT/CVC.",
        "arxiv_id": "2508.04453",
        "ARXIVID": "2508.04453",
        "COMMENT": "This paper introduces a causality-driven visual object completion task for self-improvement of LVLMs, with automated instance construction and demonstrated improvements on multiple benchmarks. This is a direct match to criterion 2 (new VLLMs/MLLMs) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.04059": {
        "authors": [
            "Zhaochen Liu",
            "Kaiwen Gao",
            "Shuyi Liang",
            "Bin Xiao",
            "Limeng Qiao",
            "Lin Ma",
            "Tingting Jiang"
        ],
        "title": "Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models",
        "abstract": "arXiv:2508.04059v1 Announce Type: new  Abstract: Occlusion perception, a critical foundation for human-level spatial understanding, embodies the challenge of integrating visual recognition and reasoning. Though multimodal large language models (MLLMs) have demonstrated remarkable capabilities, their performance on occlusion perception remains under-explored. To address this gap, we introduce O-Bench, the first visual question answering (VQA) benchmark specifically designed for occlusion perception. Based on SA-1B, we construct 1,365 images featuring semantically coherent occlusion scenarios through a novel layered synthesis approach. Upon this foundation, we annotate 4,588 question-answer pairs in total across five tailored tasks, employing a reliable, semi-automatic workflow. Our extensive evaluation of 22 representative MLLMs against the human baseline reveals a significant performance gap between current MLLMs and humans, which, we find, cannot be sufficiently bridged by model scaling or thinking process. We further identify three typical failure patterns, including an overly conservative bias, a fragile gestalt prediction, and a struggle with quantitative tasks. We believe O-Bench can not only provide a vital evaluation tool for occlusion perception, but also inspire the development of MLLMs for better visual intelligence. Our benchmark will be made publicly available upon paper publication.",
        "arxiv_id": "2508.04059",
        "ARXIVID": "2508.04059",
        "COMMENT": "Matches criteria 1 and 2: Proposes O-Bench, a new VQA benchmark for occlusion perception in MLLMs, directly addressing spatial understanding and evaluating 22 MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04681": {
        "authors": [
            "Liang Xu",
            "Chengqun Yang",
            "Zili Lin",
            "Fei Xu",
            "Yifan Liu",
            "Congsheng Xu",
            "Yiyi Zhang",
            "Jie Qin",
            "Xingdong Sheng",
            "Yunhui Liu",
            "Xin Jin",
            "Yichao Yan",
            "Wenjun Zeng",
            "Xiaokang Yang"
        ],
        "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions",
        "abstract": "arXiv:2508.04681v1 Announce Type: new  Abstract: Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.",
        "arxiv_id": "2508.04681",
        "ARXIVID": "2508.04681",
        "COMMENT": "Matches criteria 3: Introduces a new large-scale egocentric human-object-human interaction dataset and benchmarks for embodied AI, focusing on first-person perception and action, which is a novel angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04043": {
        "authors": [
            "Yuheng Ji",
            "Yipu Wang",
            "Yuyang Liu",
            "Xiaoshuai Hao",
            "Yue Liu",
            "Yuting Zhao",
            "Huaihai Lyu",
            "Xiaolong Zheng"
        ],
        "title": "VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning",
        "abstract": "arXiv:2508.04043v1 Announce Type: new  Abstract: Visual transformation reasoning (VTR) is a vital cognitive capability that empowers intelligent agents to understand dynamic scenes, model causal relationships, and predict future states, and thereby guiding actions and laying the foundation for advanced intelligent systems. However, existing benchmarks suffer from a sim-to-real gap, limited task complexity, and incomplete reasoning coverage, limiting their practical use in real-world scenarios. To address these limitations, we introduce VisualTrans, the first comprehensive benchmark specifically designed for VTR in real-world human-object interaction scenarios. VisualTrans encompasses 12 semantically diverse manipulation tasks and systematically evaluates three essential reasoning dimensions - spatial, procedural, and quantitative - through 6 well-defined subtask types. The benchmark features 472 high-quality question-answer pairs in various formats, including multiple-choice, open-ended counting, and target enumeration. We introduce a scalable data construction pipeline built upon first-person manipulation videos, which integrates task selection, image pair extraction, automated metadata annotation with large multimodal models, and structured question generation. Human verification ensures the final benchmark is both high-quality and interpretable. Evaluations of various state-of-the-art vision-language models show strong performance in static spatial tasks. However, they reveal notable shortcomings in dynamic, multi-step reasoning scenarios, particularly in areas like intermediate state recognition and transformation sequence planning. These findings highlight fundamental weaknesses in temporal modeling and causal reasoning, providing clear directions for future research aimed at developing more capable and generalizable VTR systems. The dataset and code are available at https://github.com/WangYipu2002/VisualTrans.",
        "arxiv_id": "2508.04043",
        "ARXIVID": "2508.04043",
        "COMMENT": "Matches criteria 3 (new benchmark for embodied AI, spatial reasoning). Introduces VisualTrans, a real-world benchmark for visual transformation reasoning in human-object interaction, with a focus on spatial, procedural, and quantitative reasoning. Highlights weaknesses in current VLMs for dynamic, multi-step reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04028": {
        "authors": [
            "Yifan Wang",
            "Tao Wang",
            "Chenwei Tang",
            "Caiyang Yu",
            "Zhengqing Zang",
            "Mengmi Zhang",
            "Shudong Huang",
            "Jiancheng Lv"
        ],
        "title": "Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval",
        "abstract": "arXiv:2508.04028v1 Announce Type: new  Abstract: Recently, prompt learning has demonstrated remarkable success in adapting pre-trained Vision-Language Models (VLMs) to various downstream tasks such as image classification. However, its application to the downstream Image-Text Retrieval (ITR) task is more challenging. We find that the challenge lies in discriminating both fine-grained attributes and similar subcategories of the downstream data. To address this challenge, we propose Dual prompt Learning with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning framework to achieve precise image-text matching. The framework dynamically adjusts prompt vectors from both semantic and visual dimensions to improve the performance of CLIP on the downstream ITR task. Based on the prompt paradigm, DCAR jointly optimizes attribute and class features to enhance fine-grained representation learning. Specifically, (1) at the attribute level, it dynamically updates the weights of attribute descriptions based on text-image mutual information correlation; (2) at the category level, it introduces negative samples from multiple perspectives with category-matching weighting to learn subcategory distinctions. To validate our method, we construct the Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging benchmark for ITR in downstream data domains. It covers over 1,500 downstream fine categories and 230,000 image-caption pairs with detailed attribute annotations. Extensive experiments on FDRD demonstrate that DCAR achieves state-of-the-art performance over existing baselines.",
        "arxiv_id": "2508.04028",
        "ARXIVID": "2508.04028",
        "COMMENT": "This paper proposes a dual-prompt learning framework for adapting vision-language models (VLMs) to image-text retrieval, with a new dataset and method for fine-grained matching. This directly matches criterion 2 (new VLLMs or MLLMs) and is also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04418": {
        "authors": [
            "Jinxing Zhou",
            "Yanghao Zhou",
            "Mingfei Han",
            "Tong Wang",
            "Xiaojun Chang",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer"
        ],
        "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation",
        "abstract": "arXiv:2508.04418v1 Announce Type: new  Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM/SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\\textsuperscript{2}-AVSBench. Code will be available at https://github.com/jasongief/TGS-Agent.",
        "arxiv_id": "2508.04418",
        "ARXIVID": "2508.04418",
        "COMMENT": "Matches criterion 2 (shows a new multimodal language model, Ref-Thinker, for audio-visual segmentation) and criterion 3 (introduces a new benchmark R^2-AVSBench for reasoning-intensive references). Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04335": {
        "authors": [
            "Yanyan Li",
            "Ze Yang",
            "Keisuke Tateno",
            "Federico Tombari Liang Zhao",
            "Gim Hee Lee"
        ],
        "title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization",
        "abstract": "arXiv:2508.04335v1 Announce Type: new  Abstract: Minimal parametrization of 3D lines plays a critical role in camera localization and structural mapping. Existing representations in robotics and computer vision predominantly handle independent lines, overlooking structural regularities such as sets of parallel lines that are pervasive in man-made environments. This paper introduces \\textbf{RiemanLine}, a unified minimal representation for 3D lines formulated on Riemannian manifolds that jointly accommodates both individual lines and parallel-line groups. Our key idea is to decouple each line landmark into global and local components: a shared vanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled normal vectors constrained on orthogonal subspaces, enabling compact encoding of structural regularities. For $n$ parallel lines, the proposed representation reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally embedding parallelism without explicit constraints. We further integrate this parameterization into a factor graph framework, allowing global direction alignment and local reprojection optimization within a unified manifold-based bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic benchmarks demonstrate that our method achieves significantly more accurate pose estimation and line reconstruction, while reducing parameter dimensionality and improving convergence stability.",
        "arxiv_id": "2508.04335",
        "ARXIVID": "2508.04335",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding, specifically 3D line representation and structural regularities for mapping and localization).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.04122": {
        "authors": [
            "Maximilian Ulmer",
            "Wout Boerdijk",
            "Rudolph Triebel",
            "Maximilian Durner"
        ],
        "title": "Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation",
        "abstract": "arXiv:2508.04122v1 Announce Type: new  Abstract: This paper presents OC-DiT, a novel class of diffusion models designed for object-centric prediction, and applies it to zero-shot instance segmentation. We propose a conditional latent diffusion framework that generates instance masks by conditioning the generative process on object templates and image features within the diffusion model's latent space. This allows our model to effectively disentangle object instances through the diffusion process, which is guided by visual object descriptors and localized image cues. Specifically, we introduce two model variants: a coarse model for generating initial object instance proposals, and a refinement model that refines all proposals in parallel. We train these models on a newly created, large-scale synthetic dataset comprising thousands of high-quality object meshes. Remarkably, our model achieves state-of-the-art performance on multiple challenging real-world benchmarks, without requiring any retraining on target data. Through comprehensive ablation studies, we demonstrate the potential of diffusion models for instance segmentation tasks.",
        "arxiv_id": "2508.04122",
        "ARXIVID": "2508.04122",
        "COMMENT": "This paper presents a conditional latent diffusion model for zero-shot instance segmentation, with a novel object-centric approach and strong empirical results. It is highly relevant to criterion 4 (vision foundation models and applications) and also generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.04566": {
        "authors": [
            "Jinxing Zhou",
            "Ziheng Zhou",
            "Yanghao Zhou",
            "Yuxin Mao",
            "Zhangling Duan",
            "Dan Guo"
        ],
        "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization",
        "abstract": "arXiv:2508.04566v1 Announce Type: new  Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \\textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \\textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \\textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.",
        "arxiv_id": "2508.04566",
        "ARXIVID": "2508.04566",
        "COMMENT": "This paper proposes a new method for weakly-supervised dense audio-visual event localization, introducing cross-modal salient anchors and new benchmarks. It matches criterion 3 (embodied AI papers on new benchmarks or methods with novel angles), as it establishes new benchmarks and methods for multi-modal event localization.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.04467": {
        "authors": [
            "Shuzhou Yang",
            "Xiaodong Cun",
            "Xiaoyu Li",
            "Yaowei Li",
            "Jian Zhang"
        ],
        "title": "4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation",
        "abstract": "arXiv:2508.04467v1 Announce Type: new  Abstract: Given the high complexity of directly generating high-dimensional data such as 4D, we present 4DVD, a cascaded video diffusion model that generates 4D content in a decoupled manner. Unlike previous multi-view video methods that directly model 3D space and temporal features simultaneously with stacked cross view/temporal attention modules, 4DVD decouples this into two subtasks: coarse multi-view layout generation and structure-aware conditional generation, and effectively unifies them. Specifically, given a monocular video, 4DVD first predicts the dense view content of its layout with superior cross-view and temporal consistency. Based on the produced layout priors, a structure-aware spatio-temporal generation branch is developed, combining these coarse structural priors with the exquisite appearance content of input monocular video to generate final high-quality dense-view videos. Benefit from this, explicit 4D representation~(such as 4D Gaussian) can be optimized accurately, enabling wider practical application. To train 4DVD, we collect a dynamic 3D object dataset, called D-Objaverse, from the Objaverse benchmark and render 16 videos with 21 frames for each object. Extensive experiments demonstrate our state-of-the-art performance on both novel view synthesis and 4D generation. Our project page is https://4dvd.github.io/",
        "arxiv_id": "2508.04467",
        "ARXIVID": "2508.04467",
        "COMMENT": "This paper introduces 4DVD, a cascaded video diffusion model for high-quality 4D content generation, with a novel decoupling of multi-view layout and structure-aware conditional generation. It also introduces a new dynamic 3D object dataset (D-Objaverse) for training and evaluation. This matches criterion 3 (new benchmark and novel method for embodied AI/simulators) and is highly relevant to vision foundation models and generative modeling (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04663": {
        "authors": [
            "Young D. Kwon",
            "Rui Li",
            "Sijia Li",
            "Da Li",
            "Sourav Bhattacharya",
            "Stylianos I. Venieris"
        ],
        "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models",
        "abstract": "arXiv:2508.04663v1 Announce Type: new  Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, when combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Last but not least, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.",
        "arxiv_id": "2508.04663",
        "ARXIVID": "2508.04663",
        "COMMENT": "Matches criterion 4: Proposes a novel compression framework (HierarchicalPrune) for large-scale diffusion models, enabling on-device inference with minimal quality loss.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04424": {
        "authors": [
            "Tong Wang",
            "Guanyu Yang",
            "Nian Liu",
            "Zongyan Han",
            "Jinxing Zhou",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "title": "Composed Object Retrieval: Object-level Retrieval via Composed Expressions",
        "abstract": "arXiv:2508.04424v1 Announce Type: new  Abstract: Retrieving fine-grained visual content based on user intent remains a challenge in multi-modal systems. Although current Composed Image Retrieval (CIR) methods combine reference images with retrieval texts, they are constrained to image-level matching and cannot localize specific objects. To this end, we propose Composed Object Retrieval (COR), a brand-new task that goes beyond image-level retrieval to achieve object-level precision, allowing the retrieval and segmentation of target objects based on composed expressions combining reference objects and retrieval texts. COR presents significant challenges in retrieval flexibility, which requires systems to identify arbitrary objects satisfying composed expressions while avoiding semantically similar but irrelevant negative objects within the same scene. We construct COR127K, the first large-scale COR benchmark that contains 127,166 retrieval triplets with various semantic transformations in 408 categories. We also present CORE, a unified end-to-end model that integrates reference region encoding, adaptive visual-textual interaction, and region-level contrastive learning. Extensive experiments demonstrate that CORE significantly outperforms existing models in both base and novel categories, establishing a simple and effective baseline for this challenging task while opening new directions for fine-grained multi-modal retrieval research.",
        "arxiv_id": "2508.04424",
        "ARXIVID": "2508.04424",
        "COMMENT": "Matches criterion 4: Proposes a new object-level retrieval task and benchmark (COR127K) for fine-grained multi-modal retrieval, with a new model (CORE) for region-level contrastive learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03725": {
        "authors": [
            "Yida Wang",
            "Taiting Lu",
            "Runze Liu",
            "Lanqing Yang",
            "Yifan Yang",
            "Zhe Chen",
            "Yuehai Wang",
            "Yixin Liu",
            "Kaiyuan Lin",
            "Xiaomeng Chen",
            "Dian Ding",
            "Yijie Li",
            "Yi-Chao Chen",
            "Yincheng Jin",
            "Mahanth Gowda"
        ],
        "title": "A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding",
        "abstract": "arXiv:2508.03725v1 Announce Type: new  Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated circuits (IC) is essential in defining the physical interface between components and the PCB layout, requiring exceptional visual perception proficiency. However, due to the unstructured footprint drawing and abstract diagram annotations, automated parsing and accurate footprint geometry modeling remain highly challenging. Despite its importance, no methods currently exist for automated package geometry labeling directly from IC mechanical drawings. In this paper, we first investigate the visual perception performance of Large Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our findings reveal that current LMMs severely suffer from inaccurate geometric perception, which hinders their performance in solving the footprint geometry labeling problem. To address these limitations, we propose LLM4-IC8K, a novel framework that treats IC mechanical drawings as images and leverages LLMs for structured geometric interpretation. To mimic the step-by-step reasoning approach used by human engineers, LLM4-IC8K addresses three sub-tasks: perceiving the number of pins, computing the center coordinates of each pin, and estimating the dimensions of individual pins. We present a two-stage framework that first trains LMMs on synthetically generated IC footprint diagrams to learn fundamental geometric reasoning and then fine-tunes them on real-world datasheet drawings to enhance robustness and accuracy in practical scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with 8,608 labeled samples, including 4138 hand-crafted IC footprint samples and 4470 synthetically generated samples. Extensive experiments demonstrate that our model outperforms state-of-the-art LMMs on the proposed benchmark.",
        "arxiv_id": "2508.03725",
        "ARXIVID": "2508.03725",
        "COMMENT": "Matches criteria 2 and 4: Proposes a new framework using LMMs for geometric understanding of IC footprints, introduces a new multi-modal dataset (ICGeo8K), and demonstrates improved performance over SOTA LMMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04037": {
        "authors": [
            "Liang Tang",
            "Shuxian Li",
            "Yuhao Cheng",
            "Yukang Huo",
            "Zhepeng Wang",
            "Yiqiang Yan",
            "Kaer Huang",
            "Yanzhe Jing",
            "Tiaonan Duan"
        ],
        "title": "SEA: Self-Evolution Agent with Step-wise Reward for Computer Use",
        "abstract": "arXiv:2508.04037v1 Announce Type: new  Abstract: Computer use agent is an emerging area in artificial intelligence that aims to operate the computers to achieve the user's tasks, which attracts a lot of attention from both industry and academia. However, the present agents' performance is far from being used. In this paper, we propose the Self-Evolution Agent (SEA) for computer use, and to develop this agent, we propose creative methods in data generation, reinforcement learning, and model enhancement. Specifically, we first propose an automatic pipeline to generate the verifiable trajectory for training. And then, we propose efficient step-wise reinforcement learning to alleviate the significant computational requirements for long-horizon training. In the end, we propose the enhancement method to merge the grounding and planning ability into one model without any extra training. Accordingly, based on our proposed innovation of data generation, training strategy, and enhancement, we get the Selfevolution Agent (SEA) for computer use with only 7B parameters, which outperforms models with the same number of parameters and has comparable performance to larger ones. We will make the models' weight and related codes open-source in the future.",
        "arxiv_id": "2508.04037",
        "ARXIVID": "2508.04037",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial intelligence on embodied agents) and criterion 3 (embodied AI, new methods for computer use agents, novel data generation and RL strategies).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04389": {
        "authors": [
            "Weitai Kang",
            "Bin Lei",
            "Gaowen Liu",
            "Caiwen Ding",
            "Yan Yan"
        ],
        "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning",
        "abstract": "arXiv:2508.04389v1 Announce Type: new  Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI agents, has primarily relied on supervised fine-tuning (SFT) of multimodal large language models (MLLMs), which demands extensive data curation and significant training costs. However, as MLLMs continue to advance and even cover GUI domains during pretraining, the necessity of exhaustive SFT post-training becomes increasingly questionable. Meanwhile, recent successes of rule-based reinforcement fine-tuning (RFT) suggest a more efficient alternative. Despite this promise, the optimal manner of applying RFT for GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a reinforcement learning-based GUI-VG method built on a systematic empirical study and a novel stabilization technique. We find that naive application of RFT underperforms the SFT baseline, motivating a deeper exploration. First, we decompose RFT into its core components and analyze the optimal formulation of each. Second, we propose a novel Adversarial KL Factor that dynamically stabilizes training to mitigate reward over-optimization. Third, we further explore the training configurations of RFT to enhance effectiveness. Extensive experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT methods trained on over 10M samples, achieving a 7.7% improvement on ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on ScreenSpotV2.",
        "arxiv_id": "2508.04389",
        "ARXIVID": "2508.04389",
        "COMMENT": "Matches criteria 2 (new MLLMs) and 3 (embodied AI, new RL-based method for GUI visual grounding). Proposes a reinforcement learning-based approach for GUI visual grounding, with a novel stabilization technique and empirical study, outperforming SFT with much less data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04472": {
        "authors": [
            "Hongxu Chen",
            "Zhen Wang",
            "Taoran Mei",
            "Lin Li",
            "Bowei Zhu",
            "Runshi Li",
            "Long Chen"
        ],
        "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model",
        "abstract": "arXiv:2508.04472v1 Announce Type: new  Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from generating content associated with semantic-harmful concepts (i.e., target concepts), is getting increased attention. State-of-the-art methods formulate this task as an optimization problem: they align all target concepts with semantic-harmless anchor concepts, and apply closed-form solutions to update the model accordingly. While these closed-form methods are efficient, we argue that existing methods have two overlooked limitations: 1) They often result in incomplete erasure due to \"non-zero alignment residual\", especially when text prompts are relatively complex. 2) They may suffer from generation quality degradation as they always concentrate parameter updates in a few deep layers. To address these issues, we propose a novel closed-form method ErasePro: it is designed for more complete concept erasure and better preserving overall generative quality. Specifically, ErasePro first introduces a strict zero-residual constraint into the optimization objective, ensuring perfect alignment between target and anchor concept features and enabling more complete erasure. Secondly, it employs a progressive, layer-wise update strategy that gradually transfers target concept features to those of the anchor concept from shallow to deep layers. As the depth increases, the required parameter changes diminish, thereby reducing deviations in sensitive deep layers and preserving generative quality. Empirical results across different concept erasure tasks (including instance, art style, and nudity erasure) have demonstrated the effectiveness of our ErasePro.",
        "arxiv_id": "2508.04472",
        "ARXIVID": "2508.04472",
        "COMMENT": "Matches criterion 4. Proposes ErasePro, a new closed-form method for concept erasure in text-to-image models, with a zero-residual constraint and progressive layer-wise updates. This is a novel method for controlling vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03967": {
        "authors": [
            "Mamadou Keita",
            "Wassim Hamidouche",
            "Hessen Bougueffa Eutamene",
            "Abdelmalik Taleb-Ahmed",
            "Abdenour Hadid"
        ],
        "title": "RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification",
        "abstract": "arXiv:2508.03967v1 Announce Type: new  Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image detection that leverages visual retrieval-augmented generation (RAG). While RAG methods have shown promise in mitigating factual inaccuracies in foundation models, they have primarily focused on text, leaving visual knowledge underexplored. Meanwhile, existing detection methods, which struggle with generalization and robustness, often rely on low-level artifacts and model-specific features, limiting their adaptability. To address this, RAVID dynamically retrieves relevant images to enhance detection. Our approach utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with category-related prompts to improve representation learning. We further integrate a vision-language model (VLM) to fuse retrieved images with the query, enriching the input and improving accuracy. Given a query image, RAVID generates an embedding using RAVID CLIP, retrieves the most relevant images from a database, and combines these with the query image to form an enriched input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the UniversalFakeDetect benchmark, which covers 19 generative models, show that RAVID achieves state-of-the-art performance with an average accuracy of 93.85%. RAVID also outperforms traditional methods in terms of robustness, maintaining high accuracy even under image degradations such as Gaussian blur and JPEG compression. Specifically, RAVID achieves an average accuracy of 80.27% under degradation conditions, compared to 63.44% for the state-of-the-art model C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG compression scenarios. The code will be publicly available upon acceptance.",
        "arxiv_id": "2508.03967",
        "ARXIVID": "2508.03967",
        "COMMENT": "Matches criterion 4. Introduces RAVID, a retrieval-augmented visual detection framework for AI-generated image identification, leveraging vision foundation models (CLIP, VLMs) and retrieval-augmented generation. The use of retrieval-augmented generation in vision is a novel application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04175": {
        "authors": [
            "Jingyi Liao",
            "Yongyi Su",
            "Rong-Cheng Tu",
            "Zhao Jin",
            "Wenhao Sun",
            "Yiting Li",
            "Dacheng Tao",
            "Xun Xu",
            "Xulei Yang"
        ],
        "title": "AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization",
        "abstract": "arXiv:2508.04175v1 Announce Type: new  Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities across diverse domains, their application to specialized anomaly detection (AD) remains constrained by domain adaptation challenges. Existing Group Relative Policy Optimization (GRPO) based approaches suffer from two critical limitations: inadequate training data utilization when models produce uniform responses, and insufficient supervision over reasoning processes that encourage immediate binary decisions without deliberative analysis. We propose a comprehensive framework addressing these limitations through two synergistic innovations. First, we introduce a multi-stage deliberative reasoning process that guides models from region identification to focused examination, generating diverse response patterns essential for GRPO optimization while enabling structured supervision over analytical workflows. Second, we develop a fine-grained reward mechanism incorporating classification accuracy and localization supervision, transforming binary feedback into continuous signals that distinguish genuine analytical insight from spurious correctness. Comprehensive evaluation across multiple industrial datasets demonstrates substantial performance improvements in adapting general vision-language models to specialized anomaly detection. Our method achieves superior accuracy with efficient adaptation of existing annotations, effectively bridging the gap between general-purpose MLLM capabilities and the fine-grained visual discrimination required for detecting subtle manufacturing defects and structural irregularities.",
        "arxiv_id": "2508.04175",
        "ARXIVID": "2508.04175",
        "COMMENT": "Matches criterion 2. Proposes AD-FM, a new framework for adapting MLLMs to anomaly detection with multi-stage reasoning and fine-grained reward optimization. The focus on structured supervision and reward mechanisms for MLLMs is a novel contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04101": {
        "authors": [
            "Zelin Peng",
            "Yichen Zhao",
            "Yu Huang",
            "Piao Yang",
            "Feilong Tang",
            "Zhengqin Xu",
            "Xiaokang Yang",
            "Wei Shen"
        ],
        "title": "NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding",
        "abstract": "arXiv:2508.04101v1 Announce Type: new  Abstract: Computer-aided medical image analysis is crucial for disease diagnosis and treatment planning, yet limited annotated datasets restrict medical-specific model development. While vision-language models (VLMs) like CLIP offer strong generalization capabilities, their direct application to medical imaging analysis is impeded by a significant domain gap. Existing approaches to bridge this gap, including prompt learning and one-way modality interaction techniques, typically focus on introducing domain knowledge to a single modality. Although this may offer performance gains, it often causes modality misalignment, thereby failing to unlock the full potential of VLMs. In this paper, we propose \\textbf{NEARL-CLIP} (i\\underline{N}teracted qu\\underline{E}ry \\underline{A}daptation with o\\underline{R}thogona\\underline{L} Regularization), a novel cross-modality interaction VLM-based framework that contains two contributions: (1) Unified Synergy Embedding Transformer (USEformer), which dynamically generates cross-modality queries to promote interaction between modalities, thus fostering the mutual enrichment and enhancement of multi-modal medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA introduces an orthogonality technique to decouple the new knowledge from USEformer into two distinct components: the truly novel information and the incremental knowledge. By isolating the learning process from the interference of incremental knowledge, OCA enables a more focused acquisition of new information, thereby further facilitating modality interaction and unleashing the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in a parameter-efficient style, which only introduces \\textbf{1.46M} learnable parameters.",
        "arxiv_id": "2508.04101",
        "ARXIVID": "2508.04101",
        "COMMENT": "Matches criterion 2. Proposes NEARL-CLIP, a novel VLM-based framework for medical vision-language understanding, with new cross-modality interaction and orthogonal regularization techniques. This is a new method for adapting VLMs to the medical domain.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04682": {
        "authors": [
            "Zewei Zhou",
            "Seth Z. Zhao",
            "Tianhui Cai",
            "Zhiyu Huang",
            "Bolei Zhou",
            "Jiaqi Ma"
        ],
        "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction",
        "abstract": "arXiv:2508.04682v1 Announce Type: new  Abstract: End-to-end training of multi-agent systems offers significant advantages in improving multi-task performance. However, training such models remains challenging and requires extensive manual design and monitoring. In this work, we introduce TurboTrain, a novel and efficient training framework for multi-agent perception and prediction. TurboTrain comprises two key components: a multi-agent spatiotemporal pretraining scheme based on masked reconstruction learning and a balanced multi-task learning strategy based on gradient conflict suppression. By streamlining the training process, our framework eliminates the need for manually designing and tuning complex multi-stage training pipelines, substantially reducing training time and improving performance. We evaluate TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and demonstrate that it further improves the performance of state-of-the-art multi-agent perception and prediction models. Our results highlight that pretraining effectively captures spatiotemporal multi-agent features and significantly benefits downstream tasks. Moreover, the proposed balanced multi-task learning strategy enhances detection and prediction.",
        "arxiv_id": "2508.04682",
        "ARXIVID": "2508.04682",
        "COMMENT": "Matches criterion 1 and 3. TurboTrain introduces a new multi-agent spatiotemporal pretraining scheme and a balanced multi-task learning strategy for multi-agent perception and prediction, which is highly relevant to spatial intelligence and embodied AI methods. The focus on multi-agent spatiotemporal features and efficient multi-task learning is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04324": {
        "authors": [
            "Xiaoxuan He",
            "Siming Fu",
            "Yuke Zhao",
            "Wanli Li",
            "Jian Yang",
            "Dacheng Yin",
            "Fengyun Rao",
            "Bo Zhang"
        ],
        "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
        "abstract": "arXiv:2508.04324v1 Announce Type: new  Abstract: Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce \\textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks.",
        "arxiv_id": "2508.04324",
        "ARXIVID": "2508.04324",
        "COMMENT": "This paper introduces a temporally-aware reinforcement learning framework for flow-based text-to-image models, focusing on human preference alignment. It is relevant to criterion 4 (vision foundation models and applications), and also presents a novel angle for generative modeling with RL, which is of interest for surprising empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2508.04090": {
        "authors": [
            "Yi-Ting Chen",
            "Ting-Hsuan Liao",
            "Pengsheng Guo",
            "Alexander Schwing",
            "Jia-Bin Huang"
        ],
        "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework",
        "abstract": "arXiv:2508.04090v1 Announce Type: new  Abstract: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions. Code will be released.",
        "arxiv_id": "2508.04090",
        "ARXIVID": "2508.04090",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to criterion 1 (spatial understanding via 3D consistency in super-resolution). Proposes a 3D Gaussian-splatting-based super-resolution framework leveraging diffusion models for 3D-consistent scene reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04201": {
        "authors": [
            "Ben Zhang",
            "LuLu Yu",
            "Lei Gao",
            "Jing Liu",
            "QuanJiang Guo",
            "Hui Gao"
        ],
        "title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs",
        "abstract": "arXiv:2508.04201v1 Announce Type: new  Abstract: In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs when a model generates a correct answer but follows an incorrect reasoning path. Existing methods based on specific multi-step reasoning datasets and reinforcement learning strategies, leading to high training costs and limited generalization. In this work, we propose ViFP, a general framework for enhancing visual reasoning reliability. It improves both answer accuracy and reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset dependency and poor generalization by constructing sub-question templates grounded in the core dimensions of visual reasoning, such as object localization, characteristic description, and object discovery. ViFP then builds effective reasoning paths via multi-turn QA to improve reasoning accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning path to identify potential FPs, and introduces a targeted chain-of-thought (CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby reducing logical errors in the reasoning path while preserving accuracy. Finally, we introduce a reliability evaluation metric-VoC, which integrates answer accuracy and the FP rate, providing a quantitative tool to assess whether a VLM not only answers correctly, but also reasons reliably. Our experiments on closed-source VLMs show that ViFP consistently improves performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by 4.3%, and significantly reduces the number of FPs, validating its benefits in enhancing reasoning reliability.",
        "arxiv_id": "2508.04201",
        "ARXIVID": "2508.04201",
        "COMMENT": "Matches criterion 2 (improves VLM reasoning reliability with a new framework, ViFP, for visual false positive detection and reasoning path analysis). Also introduces a new evaluation metric (VoC).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04147": {
        "authors": [
            "Lijuan Liu",
            "Wenfa Li",
            "Dongbo Zhang",
            "Shuo Wang",
            "Shaohui Jiao"
        ],
        "title": "IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control",
        "abstract": "arXiv:2508.04147v1 Announce Type: new  Abstract: We present IDC-Net (Image-Depth Consistency Network), a novel framework designed to generate RGB-D video sequences under explicit camera trajectory control. Unlike approaches that treat RGB and depth generation separately, IDC-Net jointly synthesizes both RGB images and corresponding depth maps within a unified geometry-aware diffusion model. The joint learning framework strengthens spatial and geometric alignment across frames, enabling more precise camera control in the generated sequences. To support the training of this camera-conditioned model and ensure high geometric fidelity, we construct a camera-image-depth consistent dataset with metric-aligned RGB videos, depth maps, and accurate camera poses, which provides precise geometric supervision with notably improved inter-frame geometric consistency. Moreover, we introduce a geometry-aware transformer block that enables fine-grained camera control, enhancing control over the generated sequences. Extensive experiments show that IDC-Net achieves improvements over state-of-the-art approaches in both visual quality and geometric consistency of generated scene sequences. Notably, the generated RGB-D sequences can be directly feed for downstream 3D Scene reconstruction tasks without extra post-processing steps, showcasing the practical benefits of our joint learning framework. See more at https://idcnet-scene.github.io.",
        "arxiv_id": "2508.04147",
        "ARXIVID": "2508.04147",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to criterion 1 (spatial understanding via joint RGB-D generation with camera control). IDC-Net is a geometry-aware diffusion model for RGB-D video generation with precise camera control, supporting downstream 3D scene reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04025": {
        "authors": [
            "Chao Hao",
            "Shuai Wang",
            "Kaiwen Zhou"
        ],
        "title": "Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement",
        "abstract": "arXiv:2508.04025v1 Announce Type: new  Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile tasks but still struggle with input redundancy and decision ambiguity. In this paper, we present \\textbf{RecAgent}, an uncertainty-aware agent that addresses these issues through adaptive perception. We distinguish two types of uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input redundancy and noise from comprehensive screen information, and (2) decision uncertainty, arising from ambiguous tasks and complex reasoning. To reduce perceptual uncertainty, RecAgent employs a component recommendation mechanism that identifies and focuses on the most relevant UI elements. For decision uncertainty, it uses an interactive module to request user feedback in ambiguous situations, enabling intent-aware decisions. These components are integrated into a unified framework that proactively reduces input complexity and reacts to high-uncertainty cases via human-in-the-loop refinement. Additionally, we propose a dataset called \\textbf{ComplexAction} to evaluate the success rate of GUI agents in executing specified single-step actions within complex scenarios. Extensive experiments validate the effectiveness of our approach. The dataset and code will be available at https://github.com/Fanye12/RecAgent.",
        "arxiv_id": "2508.04025",
        "ARXIVID": "2508.04025",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (new benchmark for GUI agents, ComplexAction). The paper introduces RecAgent, an uncertainty-aware GUI agent with adaptive perception and human-in-the-loop refinement, and a new dataset for evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04267": {
        "authors": [
            "Duzhen Zhang",
            "Yong Ren",
            "Wei Cong",
            "Junhao Zheng",
            "Qiaoyi Su",
            "Shuncheng Jia",
            "Zhong-Zhi Li",
            "Xuanle Zhao",
            "Ye Bai",
            "Feilong Chen",
            "Qi Tian",
            "Tielin Zhang"
        ],
        "title": "Revisiting Continual Semantic Segmentation with Pre-trained Vision Models",
        "abstract": "arXiv:2508.04267v1 Announce Type: new  Abstract: Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment novel classes while preserving knowledge of previously encountered ones. Recent advancements in CSS have been largely driven by the adoption of Pre-trained Vision Models (PVMs) as backbones. Among existing strategies, Direct Fine-Tuning (DFT), which sequentially fine-tunes the model across classes, remains the most straightforward approach. Prior work often regards DFT as a performance lower bound due to its presumed vulnerability to severe catastrophic forgetting, leading to the development of numerous complex mitigation techniques. However, we contend that this prevailing assumption is flawed. In this paper, we systematically revisit forgetting in DFT across two standard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using two representative PVM backbones: ResNet101 and Swin-B. Through a detailed probing analysis, our findings reveal that existing methods significantly underestimate the inherent anti-forgetting capabilities of PVMs. Even under DFT, PVMs retain previously learned knowledge with minimal forgetting. Further investigation of the feature space indicates that the observed forgetting primarily arises from the classifier's drift away from the PVM, rather than from degradation of the backbone representations. Based on this insight, we propose DFT*, a simple yet effective enhancement to DFT that incorporates strategies such as freezing the PVM backbone and previously learned classifiers, as well as pre-allocating future classifiers. Extensive experiments show that DFT* consistently achieves competitive or superior performance compared to sixteen state-of-the-art CSS methods, while requiring substantially fewer trainable parameters and less training time.",
        "arxiv_id": "2508.04267",
        "ARXIVID": "2508.04267",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications, specifically pre-trained vision models in continual semantic segmentation).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.04050": {
        "authors": [
            "Jitong Liao",
            "Yulu Gao",
            "Shaofei Huang",
            "Jialin Gao",
            "Jie Lei",
            "Ronghua Liang",
            "Si Liu"
        ],
        "title": "DOMR: Establishing Cross-View Segmentation via Dense Object Matching",
        "abstract": "arXiv:2508.04050v1 Announce Type: new  Abstract: Cross-view object correspondence involves matching objects between egocentric (first-person) and exocentric (third-person) views. It is a critical yet challenging task for visual understanding. In this work, we propose the Dense Object Matching and Refinement (DOMR) framework to establish dense object correspondences across views. The framework centers around the Dense Object Matcher (DOM) module, which jointly models multiple objects. Unlike methods that directly match individual object masks to image features, DOM leverages both positional and semantic relationships among objects to find correspondences. DOM integrates a proposal generation module with a dense matching module that jointly encodes visual, spatial, and semantic cues, explicitly constructing inter-object relationships to achieve dense matching among objects. Furthermore, we combine DOM with a mask refinement head designed to improve the completeness and accuracy of the predicted masks, forming the complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark demonstrate that our approach achieves state-of-the-art performance with a mean IoU of 49.7% on Ego$\\to$Exo and 55.2% on Exo$\\to$Ego. These results outperform those of previous methods by 5.8% and 4.3%, respectively, validating the effectiveness of our integrated approach for cross-view understanding.",
        "arxiv_id": "2508.04050",
        "ARXIVID": "2508.04050",
        "COMMENT": "Matches criterion 1 (spatial understanding, cross-view object correspondence, spatial/semantic relationship modeling).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03789": {
        "authors": [
            "Yuhang Ma",
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Hongsheng Li"
        ],
        "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
        "abstract": "arXiv:2508.03789v1 Announce Type: new  Abstract: Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.",
        "arxiv_id": "2508.03789",
        "ARXIVID": "2508.03789",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications, VLM-based preference model for image evaluation).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.04549": {
        "authors": [
            "Quang-Trung Truong",
            "Yuk-Kwan Wong",
            "Vo Hoang Kim Tuyen Dang",
            "Rinaldi Gotama",
            "Duc Thanh Nguyen",
            "Sai-Kit Yeung"
        ],
        "title": "MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning",
        "abstract": "arXiv:2508.04549v1 Announce Type: new  Abstract: Marine videos present significant challenges for video understanding due to the dynamics of marine objects and the surrounding environment, camera motion, and the complexity of underwater scenes. Existing video captioning datasets, typically focused on generic or human-centric domains, often fail to generalize to the complexities of the marine environment and gain insights about marine life. To address these limitations, we propose a two-stage marine object-oriented video captioning pipeline. We introduce a comprehensive video understanding benchmark that leverages the triplets of video, text, and segmentation masks to facilitate visual grounding and captioning, leading to improved marine video understanding and analysis, and marine video generation. Additionally, we highlight the effectiveness of video splitting in order to detect salient object transitions in scene changes, which significantly enrich the semantics of captioning content. Our dataset and code have been released at https://msc.hkustvgd.com.",
        "arxiv_id": "2508.04549",
        "ARXIVID": "2508.04549",
        "COMMENT": "Matches criterion 3. This paper introduces a new marine wildlife video dataset with grounded segmentation and clip-level captioning, which is a new benchmark for video understanding in a challenging domain. The focus on triplets of video, text, and segmentation masks for visual grounding and captioning is a novel angle for embodied AI and multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03864": {
        "authors": [
            "Zhenyu Pan",
            "Yiting Zhang",
            "Yutong Zhang",
            "Jianshu Zhang",
            "Haozheng Luo",
            "Yuwei Han",
            "Dennis Wu",
            "Hong-Yu Chen",
            "Philip S. Yu",
            "Manling Li",
            "Han Liu"
        ],
        "title": "Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety",
        "abstract": "arXiv:2508.03864v1 Announce Type: new  Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.",
        "arxiv_id": "2508.03864",
        "ARXIVID": "2508.03864",
        "COMMENT": "Matches criterion 3 (new method for embodied AI: Evo-MARL, a co-evolutionary multi-agent RL framework for internalized safety in multi-agent systems with MLLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.04228": {
        "authors": [
            "Kangrui Cen",
            "Baixuan Zhao",
            "Yi Xin",
            "Siqi Luo",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation",
        "abstract": "arXiv:2508.04228v1 Announce Type: new  Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a challenging and relatively under-explored area, particularly in scenarios involving multiple moving objects. Most community models and datasets in the T2V domain are designed for single-object motion, limiting the performance of current generative models in multi-object tasks. Additionally, existing motion control methods in T2V either lack support for multi-object motion scenes or experience severe performance degradation when object trajectories intersect, primarily due to the semantic conflicts in colliding regions. To address these limitations, we introduce LayerT2V, the first approach for generating video by compositing background and foreground objects layer by layer. This layered generation enables flexible integration of multiple independent elements within a video, positioning each element on a distinct \"layer\" and thus facilitating coherent multi-object synthesis while enhancing control over the generation process. Extensive experiments demonstrate the superiority of LayerT2V in generating complex multi-object scenarios, showcasing 1.4x and 4.5x improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods. Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .",
        "arxiv_id": "2508.04228",
        "ARXIVID": "2508.04228",
        "COMMENT": "This paper introduces LayerT2V, a layered approach for multi-object trajectory control in text-to-video generation, addressing multi-object motion and compositionality. This is a novel generative modeling method in multi-modal learning, relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2508.04572": {
        "authors": [
            "Jun Li",
            "Che Liu",
            "Wenjia Bai",
            "Mingxuan Liu",
            "Rossella Arcucci",
            "Cosmin I. Bercea",
            "Julia A. Schnabel"
        ],
        "title": "Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding",
        "abstract": "arXiv:2508.04572v1 Announce Type: new  Abstract: In this work, we address the problem of grounding abnormalities in medical images, where the goal is to localize clinical findings based on textual descriptions. While generalist Vision-Language Models (VLMs) excel in natural grounding tasks, they often struggle in the medical domain due to rare, compositional, and domain-specific terms that are poorly aligned with visual patterns. Specialized medical VLMs address this challenge via large-scale domain pretraining, but at the cost of substantial annotation and computational resources. To overcome these limitations, we propose \\textbf{Knowledge to Sight (K2Sight)}, a framework that introduces structured semantic supervision by decomposing clinical concepts into interpretable visual attributes, such as shape, density, and anatomical location. These attributes are distilled from domain ontologies and encoded into concise instruction-style prompts, which guide region-text alignment during training. Unlike conventional report-level supervision, our approach explicitly bridges domain knowledge and spatial structure, enabling data-efficient training of compact models. We train compact models with 0.23B and 2B parameters using only 1.5\\% of the data required by state-of-the-art medical VLMs. Despite their small size and limited training data, these models achieve performance on par with or better than 7B+ medical VLMs, with up to 9.82\\% improvement in $mAP_{50}$. Code and models: \\href{https://lijunrio.github.io/K2Sight/}{\\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.",
        "arxiv_id": "2508.04572",
        "ARXIVID": "2508.04572",
        "COMMENT": "This paper introduces a framework for grounding abnormalities in medical images by decomposing clinical concepts into visual attributes, enabling efficient training of compact models. It is a novel application of vision-language models in the medical domain, matching criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.04033": {
        "authors": [
            "Hee-Yeun Kim",
            "Byeonggyu Park",
            "Byonghyok Choi",
            "Hansang Cho",
            "Byungkwan Kim",
            "Soomok Lee",
            "Mingu Jeon",
            "Seung-Woo Seo",
            "Seong-Woo Kim"
        ],
        "title": "Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation",
        "abstract": "arXiv:2508.04033v1 Announce Type: new  Abstract: The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside parking in urban environments poses a significant challenge to road safety, particularly due to the sudden emergence of pedestrians. mmWave technology leverages diffraction and reflection to observe NLoS regions, and recent studies have demonstrated its potential for detecting obscured objects. However, existing approaches predominantly rely on predefined spatial information or assume simple wall reflections, thereby limiting their generalizability and practical applicability. A particular challenge arises in scenarios where pedestrians suddenly appear from between parked vehicles, as these parked vehicles act as temporary spatial obstructions. Furthermore, since parked vehicles are dynamic and may relocate over time, spatial information obtained from satellite maps or other predefined sources may not accurately reflect real-time road conditions, leading to erroneous sensor interpretations. To address this limitation, we propose an NLoS pedestrian localization framework that integrates monocular camera image with 2D radar point cloud (PCD) data. The proposed method initially detects parked vehicles through image segmentation, estimates depth to infer approximate spatial characteristics, and subsequently refines this information using 2D radar PCD to achieve precise spatial inference. Experimental evaluations conducted in real-world urban road environments demonstrate that the proposed approach enhances early pedestrian detection and contributes to improved road safety. Supplementary materials are available at https://hiyeun.github.io/NLoS/.",
        "arxiv_id": "2508.04033",
        "ARXIVID": "2508.04033",
        "COMMENT": "This paper proposes a new method for spatial understanding in embodied agents by integrating monocular camera images with 2D radar point cloud data for NLoS pedestrian localization. This matches criterion 1 (new methodological improvements to spatial understanding on embodied agents).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.04625": {
        "authors": [
            "Zichen Tang",
            "Haihong E",
            "Jiacheng Liu",
            "Zhongjun Yang",
            "Rongjin Li",
            "Zihua Rong",
            "Haoyang He",
            "Zhuodi Hao",
            "Xinyang Hu",
            "Kun Ji",
            "Ziyan Ma",
            "Mengyuan Ji",
            "Jun Zhang",
            "Chenghao Ma",
            "Qianhe Zheng",
            "Yang Liu",
            "Yiling Huang",
            "Xinyi Hu",
            "Qing Huang",
            "Zijian Xie",
            "Shiyao Peng"
        ],
        "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging",
        "abstract": "arXiv:2508.04625v1 Announce Type: new  Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.",
        "arxiv_id": "2508.04625",
        "ARXIVID": "2508.04625",
        "COMMENT": "Matches criterion 2: Introduces a new multimodal benchmark (FinMMR) for MLLMs in financial numerical reasoning, with challenging tasks and comprehensive coverage.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04559": {
        "authors": [
            "Jinxi Liu",
            "Zijian He",
            "Guangrun Wang",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose",
        "abstract": "arXiv:2508.04559v1 Announce Type: new  Abstract: Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios-for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce \\textbf{OMFA} (\\emph{One Model For All}), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. For example, OMFA enables removing garments from a source person (try-off) and transferring them onto a target person (try-on), while also allowing the generated target to appear in novel poses-even without access to multi-pose images of that person. OMFA is built upon a novel \\emph{partial diffusion} strategy that selectively applies noise and denoising to individual components of the joint input-such as the garment, the person image, or the face-enabling dynamic subtask control and efficient bidirectional garment-person transformation. The framework is entirely mask-free and requires only a single portrait and a target pose as input, making it well-suited for real-world applications. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical and generalizable solution for virtual garment synthesis. The project page is here: https://onemodelforall.github.io/.",
        "arxiv_id": "2508.04559",
        "ARXIVID": "2508.04559",
        "COMMENT": "Related to criteria 4 (vision foundation models, generative modeling). Proposes a unified diffusion model for virtual try-on/try-off in arbitrary poses, with a novel partial diffusion strategy. Strong generative modeling angle.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.04022": {
        "authors": [
            "Junyi Wang",
            "Jinjiang Li",
            "Guodong Fan",
            "Yakun Ju",
            "Xiang Fang",
            "Alex C. Kot"
        ],
        "title": "Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation",
        "abstract": "arXiv:2508.04022v1 Announce Type: new  Abstract: In the semantic segmentation of remote sensing images, acquiring complete ground objects is critical for achieving precise analysis. However, this task is severely hindered by two major challenges: high intra-class variance and high inter-class similarity. Traditional methods often yield incomplete segmentation results due to their inability to effectively unify class representations and distinguish between similar features. Even emerging class-guided approaches are limited by coarse class prototype representations and a neglect of target structural information.   Therefore, this paper proposes a Prototype-Driven Structure Synergy Network (PDSSNet). The design of this network is based on a core concept, a complete ground object is jointly defined by its invariant class semantics and its variant spatial structure. To implement this, we have designed three key modules. First, the Adaptive Prototype Extraction Module (APEM) ensures semantic accuracy from the source by encoding the ground truth to extract unbiased class prototypes. Subsequently, the designed Semantic-Structure Coordination Module (SSCM) follows a hierarchical semantics-first, structure-second principle. This involves first establishing a global semantic cognition, then leveraging structural information to constrain and refine the semantic representation, thereby ensuring the integrity of class information. Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic step-size adjustment mechanism to focus on discriminative features between classes.   Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art methods. The source code is available at https://github.com/wangjunyi-1/PDSSNet.",
        "arxiv_id": "2508.04022",
        "ARXIVID": "2508.04022",
        "COMMENT": "Matches criteria 1 (spatial understanding in segmentation) and 4 (vision foundation models, segmentation). Proposes a new network for remote sensing image segmentation with explicit spatial structure modeling and prototype-driven synergy.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04422": {
        "authors": [
            "Christian Bohn",
            "Thomas Kurbiel",
            "Klaus Friedrichs",
            "Hasan Tercan",
            "Tobias Meisen"
        ],
        "title": "Efficient Inter-Task Attention for Multitask Transformer Models",
        "abstract": "arXiv:2508.04422v1 Announce Type: new  Abstract: In both Computer Vision and the wider Deep Learning field, the Transformer architecture is well-established as state-of-the-art for many applications. For Multitask Learning, however, where there may be many more queries necessary compared to single-task models, its Multi-Head-Attention often approaches the limits of what is computationally feasible considering practical hardware limitations. This is due to the fact that the size of the attention matrix scales quadratically with the number of tasks (assuming roughly equal numbers of queries for all tasks). As a solution, we propose our novel Deformable Inter-Task Self-Attention for Multitask models that enables the much more efficient aggregation of information across the feature maps from different tasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we demonstrate an order-of-magnitude reduction in both FLOPs count and inference latency. At the same time, we also achieve substantial improvements by up to 7.4% in the individual tasks' prediction quality metrics.",
        "arxiv_id": "2508.04422",
        "ARXIVID": "2508.04422",
        "COMMENT": "Matches criterion 1. Proposes a novel deformable inter-task self-attention for multitask transformer models, improving efficiency and prediction quality in computer vision tasks. This is a methodological improvement relevant to spatial understanding and multi-task learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04485": {
        "authors": [
            "Bowen Chai",
            "Zheng Chen",
            "Libo Zhu",
            "Wenbo Li",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution",
        "abstract": "arXiv:2508.04485v1 Announce Type: new  Abstract: Diffusion models have shown superior performance in real-world video super-resolution (VSR). However, the slow processing speeds and heavy resource consumption of diffusion models hinder their practical application and deployment. Quantization offers a potential solution for compressing the VSR model. Nevertheless, quantizing VSR models is challenging due to their temporal characteristics and high fidelity requirements. To address these issues, we propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a spatio-temporal complexity aware (STCA) mechanism, where we first utilize the calibration dataset to measure both spatial and temporal complexities for each layer. Based on these statistics, we allocate layer-specific ranks to the low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine the FP and low-bit branches to achieve simultaneous optimization. In addition, we propose a learnable bias alignment (LBA) module to reduce the biased quantization errors. Extensive experiments on synthetic and real-world datasets demonstrate that our method obtains comparable performance with the FP model and significantly outperforms recent leading low-bit quantization methods. Code is available at: https://github.com/bowenchai/QuantVSR.",
        "arxiv_id": "2508.04485",
        "ARXIVID": "2508.04485",
        "COMMENT": "This paper proposes a low-bit quantization method for video super-resolution diffusion models, with innovations in spatio-temporal complexity and bias alignment. It is relevant to criterion 4 (vision foundation models and applications), especially for efficient deployment of generative models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.04016": {
        "authors": [
            "Weilun Feng",
            "Haotong Qin",
            "Chuanguang Yang",
            "Xiangqi Li",
            "Han Yang",
            "Yuqi Li",
            "Zhulin An",
            "Libo Huang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "title": "$\\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation",
        "abstract": "arXiv:2508.04016v1 Announce Type: new  Abstract: Diffusion transformers have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose \\textbf{$\\text{S}^2$Q-VDiT}, a post-training quantization framework for V-DMs that leverages \\textbf{S}alient data and \\textbf{S}parse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the sparse attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, $\\text{S}^2$Q-VDiT achieves lossless performance while delivering $3.9\\times$ model compression and $1.3\\times$ inference acceleration. Code will be available at \\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.",
        "arxiv_id": "2508.04016",
        "ARXIVID": "2508.04016",
        "COMMENT": "This paper introduces a quantization framework for video diffusion transformers, with innovations in calibration and token distillation. It is relevant to criterion 4 (vision foundation models and applications), as it deals with efficient deployment of large video generative models, and also touches on generative modeling in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.04229": {
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Xiao Ren",
            "You-Fu Li",
            "He Kong"
        ],
        "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction",
        "abstract": "arXiv:2508.04229v1 Announce Type: new  Abstract: Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.",
        "arxiv_id": "2508.04229",
        "ARXIVID": "2508.04229",
        "COMMENT": "Matches criterion 1 (methodological improvement to spatial understanding: intention-aware diffusion model for multimodal pedestrian trajectory prediction).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04120": {
        "authors": [
            "Likai Wang",
            "Ruize Han",
            "Xiangqun Zhang",
            "Wei Feng"
        ],
        "title": "CLIPVehicle: A Unified Framework for Vision-based Vehicle Search",
        "abstract": "arXiv:2508.04120v1 Announce Type: new  Abstract: Vehicles, as one of the most common and significant objects in the real world, the researches on which using computer vision technologies have made remarkable progress, such as vehicle detection, vehicle re-identification, etc. To search an interested vehicle from the surveillance videos, existing methods first pre-detect and store all vehicle patches, and then apply vehicle re-identification models, which is resource-intensive and not very practical. In this work, we aim to achieve the joint detection and re-identification for vehicle search. However, the conflicting objectives between detection that focuses on shared vehicle commonness and re-identification that focuses on individual vehicle uniqueness make it challenging for a model to learn in an end-to-end system. For this problem, we propose a new unified framework, namely CLIPVehicle, which contains a dual-granularity semantic-region alignment module to leverage the VLMs (Vision-Language Models) for vehicle discrimination modeling, and a multi-level vehicle identification learning strategy to learn the identity representation from global, instance and feature levels. We also construct a new benchmark, including a real-world dataset CityFlowVS, and two synthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods of both vehicle Re-ID and person search tasks.",
        "arxiv_id": "2508.04120",
        "ARXIVID": "2508.04120",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) by leveraging VLMs for unified vehicle search. Also introduces a new benchmark for vehicle search.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04316": {
        "authors": [
            "Kun Gui",
            "Hongliang Ren",
            "Shang Shi",
            "Jin Lu",
            "Changqiu Yu",
            "Quanjun Cao",
            "Guomin Gu",
            "Qi Xuan"
        ],
        "title": "A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks",
        "abstract": "arXiv:2508.04316v1 Announce Type: new  Abstract: Distributed Acoustic Sensing (DAS) technology finds growing applications across various domains. However, data distribution disparities due to heterogeneous sensing environments pose challenges for data-driven artificial intelligence (AI) models, limiting cross-domain generalization and facing a shortage of labeled training data. To address these issues, this study proposes a foundational model for DAS signal recognition based on a Masked Autoencoder, named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples, encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter security, 2D time-frequency images for pipeline leakage, and open-dataset signals including whale vocalizations and seismic activities, using a self-supervised mask reconstruction task to capture deep semantic features of DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition tasks. This method freezes the pretrained backbone parameters and fine-tunes only a small set of learnable visual prompt vectors inserted into the Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super platform validate MAEPD using indoor gait recognition as a downstream task. The VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322% of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT) method by 0.61% and reducing training time by 45%. The model also exhibits robust performance in pipeline leakage detection, confirming the generality, efficiency, and scalability of MAEPD as a foundational model. This approach offers a novel paradigm for addressing the limited generalization of signal recognition models in the DAS domain.",
        "arxiv_id": "2508.04316",
        "ARXIVID": "2508.04316",
        "COMMENT": "This paper proposes a foundation model for DAS signal recognition using a masked autoencoder and visual prompt tuning. It is a vision foundation model applied to a new domain (DAS signals), matching criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04297": {
        "authors": [
            "Yaopeng Lou",
            "Liao Shen",
            "Tianqi Liu",
            "Jiaqi Li",
            "Zihao Huang",
            "Huiqiang Sun",
            "Zhiguo Cao"
        ],
        "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction",
        "abstract": "arXiv:2508.04297v1 Announce Type: new  Abstract: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuRF achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets.",
        "arxiv_id": "2508.04297",
        "ARXIVID": "2508.04297",
        "COMMENT": "Related to criteria 4 (vision foundation models, 3D Gaussian splatting for novel view synthesis). Presents a new method for generalizable 3D reconstruction and view synthesis, integrating MVS and monocular depth, with strong empirical results.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.04286": {
        "authors": [
            "Chenlei Lv",
            "Hui Huang"
        ],
        "title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space",
        "abstract": "arXiv:2508.04286v1 Announce Type: new  Abstract: Point cloud registration is a classical topic in the field of 3D Vision and Computer Graphics. Generally, the implementation of registration is typically sensitive to similarity transformations (translation, scaling, and rotation), noisy points, and incomplete geometric structures. Especially, the non-uniform scales and defective parts of point clouds increase probability of struck local optima in registration task. In this paper, we propose a robust point cloud registration PKSS-Align that can handle various influences, including similarity transformations, non-uniform densities, random noisy points, and defective parts. The proposed method measures shape feature-based similarity between point clouds on the Pre-Kendall shape space (PKSS), \\textcolor{black}{which is a shape measurement-based scheme and doesn't require point-to-point or point-to-plane metric.} The employed measurement can be regarded as the manifold metric that is robust to various representations in the Euclidean coordinate system. Benefited from the measurement, the transformation matrix can be directly generated for point clouds with mentioned influences at the same time. The proposed method does not require data training and complex feature encoding. Based on a simple parallel acceleration, it can achieve significant improvement for efficiency and feasibility in practice. Experiments demonstrate that our method outperforms the relevant state-of-the-art methods.",
        "arxiv_id": "2508.04286",
        "ARXIVID": "2508.04286",
        "COMMENT": "This paper proposes a new method for robust point cloud registration using Pre-Kendall Shape Space, which is a methodological improvement in spatial understanding for 3D data. This matches criterion 1 (new methodological improvements to spatial understanding on embodied agents), though it is not directly about embodied agents, but the method could be relevant for spatial intelligence in such systems.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.03955": {
        "authors": [
            "Lin Zhang",
            "Zefan Cai",
            "Yufan Zhou",
            "Shentong Mo",
            "Jinhong Lin",
            "Cheng-En Wu",
            "Yibing Wei",
            "Yijing Zhang",
            "Ruiyi Zhang",
            "Wen Xiao",
            "Tong Sun",
            "Junjie Hu",
            "Pedro Morgado"
        ],
        "title": "Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm",
        "abstract": "arXiv:2508.03955v1 Announce Type: new  Abstract: Recent advances in audio-synchronized visual animation enable control of video content using audios from specific classes. However, existing methods rely heavily on expensive manual curation of high-quality, class-specific training videos, posing challenges to scaling up to diverse audio-video classes in the open world. In this work, we propose an efficient two-stage training paradigm to scale up audio-synchronized visual animation using abundant but noisy videos. In stage one, we automatically curate large-scale videos for pretraining, allowing the model to learn diverse but imperfect audio-video alignments. In stage two, we finetune the model on manually curated high-quality examples, but only at a small scale, significantly reducing the required human effort. We further enhance synchronization by allowing each frame to access rich audio context via multi-feature conditioning and window attention. To efficiently train the model, we leverage pretrained text-to-video generator and audio encoders, introducing only 1.9\\% additional trainable parameters to learn audio-conditioning capability without compromising the generator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark with videos from 48 classes, which is 3$\\times$ more diverse than previous benchmarks. Extensive experiments show that our method significantly reduces reliance on manual curation by over 10$\\times$, while generalizing to many open classes.",
        "arxiv_id": "2508.03955",
        "ARXIVID": "2508.03955",
        "COMMENT": "This paper introduces a scalable training paradigm for audio-synchronized visual animation, leveraging large-scale noisy data and a new benchmark (AVSync48). While it is relevant to multi-modal learning and introduces a new benchmark, it does not directly address embodied AI or spatial intelligence. Closest to criterion 4 (vision foundation models and applications) and criterion 3 (new benchmark for multi-modal generative modeling).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.04227": {
        "authors": [
            "Yuyang Liu",
            "Qiuhe Hong",
            "Linlan Huang",
            "Alexandra Gomez-Villa",
            "Dipam Goswami",
            "Xialei Liu",
            "Joost van de Weijer",
            "Yonghong Tian"
        ],
        "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting",
        "abstract": "arXiv:2508.04227v1 Announce Type: new  Abstract: Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However, enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs (VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose a challenge-driven taxonomy that maps solutions to their target problems: (1) \\textit{Multi-Modal Replay Strategies} address cross-modal drift through explicit or implicit memory mechanisms; (2) \\textit{Cross-Modal Regularization} preserves modality alignment during updates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter interference with modular or low-rank updates. We further analyze current evaluation protocols, datasets, and metrics, highlighting the need for better benchmarks that capture VLM-specific forgetting and compositional generalization. Finally, we outline open problems and future directions, including continual pre-training and compositional zero-shot learning. This survey aims to serve as a comprehensive and diagnostic reference for researchers developing lifelong vision-language systems. All resources are available at: https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.",
        "arxiv_id": "2508.04227",
        "ARXIVID": "2508.04227",
        "COMMENT": "Matches criterion 2: A survey on continual learning for vision-language models (VLMs), providing taxonomy and open problems, but as a survey, less novel.",
        "RELEVANCE": 7,
        "NOVELTY": 4
    },
    "2508.04273": {
        "authors": [
            "Junan Lin",
            "Daizong Liu",
            "Xianke Chen",
            "Xiaoye Qu",
            "Xun Yang",
            "Jixiang Zhu",
            "Sanyuan Zhang",
            "Jianfeng Dong"
        ],
        "title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval",
        "abstract": "arXiv:2508.04273v1 Announce Type: new  Abstract: Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically related to the given query. To tackle this task, most existing VMR methods solely focus on the visual and textual modalities while neglecting the complementary but important audio modality. Although a few recent works try to tackle the joint audio-vision-text reasoning, they treat all modalities equally and simply embed them without fine-grained interaction for moment retrieval. These designs are counter-practical as: Not all audios are helpful for video moment retrieval, and the audio of some videos may be complete noise or background sound that is meaningless to the moment determination. To this end, we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which learns to dynamically and selectively aggregate the audio-vision-text contexts for VMR. Specifically, after integrating the textual guidance with vision and audio separately, we first design a pseudo-label-supervised audio importance predictor that predicts the importance score of the audio, and accordingly assigns weights to mitigate the interference caused by noisy audio. Then, we design a multi-granularity audio fusion module that adaptively fuses audio and visual modalities at local-, event-, and global-level, fully capturing their complementary contexts. We further propose a cross-modal knowledge distillation strategy to address the challenge of missing audio modality during inference. To evaluate our method, we further construct a new VMR dataset, i.e., Charades-AudioMatter, where audio-related samples are manually selected and re-organized from the original Charades-STA to validate the model's capability in utilizing audio modality. Extensive experiments validate the effectiveness of our method, achieving state-of-the-art with audio-video fusion in VMR methods. Our code is available at https://github.com/HuiGuanLab/IMG.",
        "arxiv_id": "2508.04273",
        "ARXIVID": "2508.04273",
        "COMMENT": "Somewhat related to criteria 2 (multi-modal learning, audio-vision-text fusion), but not a new MLLM. Proposes a new method and dataset for audio-aware video moment retrieval, with clever statistical tricks for audio importance prediction and fusion.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04055": {
        "authors": [
            "Fangmin Zhao",
            "Weichao Zeng",
            "Zhenhang Li",
            "Dongbao Yang",
            "Binbin Li",
            "Xiaojun Bi",
            "Yu Zhou"
        ],
        "title": "Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion",
        "abstract": "arXiv:2508.04055v1 Announce Type: new  Abstract: Removing various degradations from damaged documents greatly benefits digitization, downstream document analysis, and readability. Previous methods often treat each restoration task independently with dedicated models, leading to a cumbersome and highly complex document processing system. Although recent studies attempt to unify multiple tasks, they often suffer from limited scalability due to handcrafted prompts and heavy preprocessing, and fail to fully exploit inter-task synergy within a shared architecture. To address the aforementioned challenges, we propose Uni-DocDiff, a Unified and highly scalable Document restoration model based on Diffusion. Uni-DocDiff develops a learnable task prompt design, ensuring exceptional scalability across diverse tasks. To further enhance its multi-task capabilities and address potential task interference, we devise a novel \\textbf{Prior \\textbf{P}ool}, a simple yet comprehensive mechanism that combines both local high-frequency features and global low-frequency features. Additionally, we design the \\textbf{Prior \\textbf{F}usion \\textbf{M}odule (PFM)}, which enables the model to adaptively select the most relevant prior information for each specific task. Extensive experiments show that the versatile Uni-DocDiff achieves performance comparable or even superior performance compared with task-specific expert models, and simultaneously holds the task scalability for seamless adaptation to new tasks.",
        "arxiv_id": "2508.04055",
        "ARXIVID": "2508.04055",
        "COMMENT": "Related to criteria 4 (vision foundation models, generative modeling). Proposes a unified diffusion-based model for document restoration, with learnable task prompts and a novel prior fusion module. Multi-task, scalable approach.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04051": {
        "authors": [
            "Chen Luo",
            "Qiyu Jin",
            "Taofeng Xie",
            "Xuemei Wang",
            "Huayu Wang",
            "Congcong Liu",
            "Liming Tang",
            "Guoqing Chen",
            "Zhuo-Xu Cui",
            "Dong Liang"
        ],
        "title": "Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach",
        "abstract": "arXiv:2508.04051v1 Announce Type: new  Abstract: Interpolating missing data in k-space is essential for accelerating imaging. However, existing methods, including convolutional neural network-based deep learning, primarily exploit local predictability while overlooking the inherent global dependencies in k-space. Recently, Transformers have demonstrated remarkable success in natural language processing and image analysis due to their ability to capture long-range dependencies. This inspires the use of Transformers for k-space interpolation to better exploit its global structure. However, their lack of interpretability raises concerns regarding the reliability of interpolated data. To address this limitation, we propose GPI-WT, a white-box Transformer framework based on Globally Predictable Interpolation (GPI) for k-space. Specifically, we formulate GPI from the perspective of annihilation as a novel k-space structured low-rank (SLR) model. The global annihilation filters in the SLR model are treated as learnable parameters, and the subgradients of the SLR model naturally induce a learnable attention mechanism. By unfolding the subgradient-based optimization algorithm of SLR into a cascaded network, we construct the first white-box Transformer specifically designed for accelerated MRI. Experimental results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in k-space interpolation accuracy while providing superior interpretability.",
        "arxiv_id": "2508.04051",
        "ARXIVID": "2508.04051",
        "COMMENT": "This paper introduces a white-box transformer for k-space interpolation in MRI, focusing on interpretability and global predictability. While it is a vision foundation model application (criterion 4), it is in the medical imaging domain and not directly related to embodied AI or VLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.04551": {
        "authors": [
            "Angang Zhang",
            "Fang Deng",
            "Hao Chen",
            "Zhongjian Chen",
            "Junyan Li"
        ],
        "title": "Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis",
        "abstract": "arXiv:2508.04551v1 Announce Type: new  Abstract: While recent advances in virtual try-on (VTON) have achieved realistic garment transfer to human subjects, its inverse task, virtual try-off (VTOFF), which aims to reconstruct canonical garment templates from dressed humans, remains critically underexplored and lacks systematic investigation. Existing works predominantly treat them as isolated tasks: VTON focuses on garment dressing while VTOFF addresses garment extraction, thereby neglecting their complementary symmetry. To bridge this fundamental gap, we propose the Two-Way Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified framework for joint clothing-centric image synthesis that simultaneously resolves both mask-guided VTON and mask-free VTOFF through bidirectional feature disentanglement. Specifically, our framework employs dual-conditioned guidance from both latent and pixel spaces of reference images to seamlessly bridge the dual tasks. On the other hand, to resolve the inherent mask dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a phased training paradigm that progressively bridges this modality gap. Extensive qualitative and quantitative experiments conducted across the DressCode and VITON-HD datasets validate the efficacy and competitive edge of our proposed approach.",
        "arxiv_id": "2508.04551",
        "ARXIVID": "2508.04551",
        "COMMENT": "Relevant to criterion 4 (vision foundation models and applications) as it proposes a unified diffusion framework for garment transfer, but not a direct match to spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04406": {
        "authors": [
            "Yinan Yu",
            "Alex Gonzalez-Caceres",
            "Samuel Scheidegger",
            "Sanjay Somanath",
            "Alexander Hollberg"
        ],
        "title": "Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models",
        "abstract": "arXiv:2508.04406v1 Announce Type: new  Abstract: Renovating existing buildings is essential for climate impact. Early-phase renovation planning requires simulations based on thermal 3D models at Level of Detail (LoD) 3, which include features like windows. However, scalable and accurate identification of such features remains a challenge. This paper presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that generates LoD3 thermal models by extracting geometries from images using both computer vision and deep learning. Unlike existing methods relying on segmentation and projection, SI3FP directly models geometric primitives in the orthographic image plane, providing a unified interface while reducing perspective distortions. SI3FP supports both sparse (e.g., Google Street View) and dense (e.g., hand-held camera) data sources. Tested on typical Swedish residential buildings, SI3FP achieved approximately 5% error in window-to-wall ratio estimates, demonstrating sufficient accuracy for early-stage renovation analysis. The pipeline facilitates large-scale energy renovation planning and has broader applications in urban development and planning.",
        "arxiv_id": "2508.04406",
        "ARXIVID": "2508.04406",
        "COMMENT": "This paper presents a deep learning pipeline for scalable image-to-3D facade parsing, which is a methodological improvement in spatial understanding and could be relevant for embodied agents in urban environments (criterion 1). However, it is more of an application paper.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.04513": {
        "authors": [
            "Uzay G\\\"okay",
            "Federico Spurio",
            "Dominik R. Bach",
            "Juergen Gall"
        ],
        "title": "Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation",
        "abstract": "arXiv:2508.04513v1 Announce Type: new  Abstract: Current state-of-the-art methods for skeleton-based temporal action segmentation are predominantly supervised and require annotated data, which is expensive to collect. In contrast, existing unsupervised temporal action segmentation methods have focused primarily on video data, while skeleton sequences remain underexplored, despite their relevance to real-world applications, robustness, and privacy-preserving nature. In this paper, we propose a novel approach for unsupervised skeleton-based temporal action segmentation. Our method utilizes a sequence-to-sequence temporal autoencoder that keeps the information of the different joints disentangled in the embedding space. Latent skeleton sequences are then divided into non-overlapping patches and quantized to obtain distinctive skeleton motion words, driving the discovery of semantically meaningful action clusters. We thoroughly evaluate the proposed approach on three widely used skeleton-based datasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our model outperforms the current state-of-the-art unsupervised temporal action segmentation methods. Code is available at https://github.com/bachlab/SMQ .",
        "arxiv_id": "2508.04513",
        "ARXIVID": "2508.04513",
        "COMMENT": "This paper proposes an unsupervised method for skeleton-based temporal action segmentation using a novel quantization approach. While it is relevant to spatial understanding and action segmentation, it does not directly address embodied agents or introduce a new benchmark. Closest to criterion 1 (spatial understanding), but not a perfect match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.04224": {
        "authors": [
            "Jiahui Li",
            "Shengeng Tang",
            "Jingxuan He",
            "Gang Huang",
            "Zhangye Wang",
            "Yantao Pan",
            "Lechao Cheng"
        ],
        "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition",
        "abstract": "arXiv:2508.04224v1 Announce Type: new  Abstract: Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \\textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.",
        "arxiv_id": "2508.04224",
        "ARXIVID": "2508.04224",
        "COMMENT": "Somewhat related to vision foundation models (criterion 4) via dynamic scene reconstruction, but not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03925": {
        "authors": [
            "Shen Zhu",
            "Yinzhu Jin",
            "Ifrah Zawar",
            "P. Thomas Fletcher"
        ],
        "title": "Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model",
        "abstract": "arXiv:2508.03925v1 Announce Type: new  Abstract: We propose a diffusion model designed to generate point-based shape representations with correspondences. Traditional statistical shape models have considered point correspondences extensively, but current deep learning methods do not take them into account, focusing on unordered point clouds instead. Current deep generative models for point clouds do not address generating shapes with point correspondences between generated shapes. This work aims to formulate a diffusion model that is capable of generating realistic point-based shape representations, which preserve point correspondences that are present in the training data. Using shape representation data with correspondences derived from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our correspondence-preserving model effectively generates point-based hippocampal shape representations that are highly realistic compared to existing methods. We further demonstrate the applications of our generative model by downstream tasks, such as conditional generation of healthy and AD subjects and predicting morphological changes of disease progression by counterfactual generation.",
        "arxiv_id": "2508.03925",
        "ARXIVID": "2508.03925",
        "COMMENT": "Relevant to generative modeling and spatial understanding, but does not directly match any specific criterion (focuses on diffusion models for point-based shape generation with correspondences).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04568": {
        "authors": [
            "Yijie Li",
            "Wei Zhang",
            "Xi Zhu",
            "Ye Wu",
            "Yogesh Rathi",
            "Lauren J. O'Donnell",
            "Fan Zhang"
        ],
        "title": "DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling",
        "abstract": "arXiv:2508.04568v1 Announce Type: new  Abstract: This paper presents DDTracking, a novel deep generative framework for diffusion MRI tractography that formulates streamline propagation as a conditional denoising diffusion process. In DDTracking, we introduce a dual-pathway encoding network that jointly models local spatial encoding (capturing fine-scale structural details at each streamline point) and global temporal dependencies (ensuring long-range consistency across the entire streamline). Furthermore, we design a conditional diffusion model module, which leverages the learned local and global embeddings to predict streamline propagation orientations for tractography in an end-to-end trainable manner. We conduct a comprehensive evaluation across diverse, independently acquired dMRI datasets, including both synthetic and clinical data. Experiments on two well-established benchmarks with ground truth (ISMRM Challenge and TractoInferno) demonstrate that DDTracking largely outperforms current state-of-the-art tractography methods. Furthermore, our results highlight DDTracking's strong generalizability across heterogeneous datasets, spanning varying health conditions, age groups, imaging protocols, and scanner types. Collectively, DDTracking offers anatomically plausible and robust tractography, presenting a scalable, adaptable, and end-to-end learnable solution for broad dMRI applications. Code is available at: https://github.com/yishengpoxiao/DDtracking.git",
        "arxiv_id": "2508.04568",
        "ARXIVID": "2508.04568",
        "COMMENT": "Relevant to generative modeling and spatial understanding in medical imaging, but does not directly match any specific criterion (focuses on diffusion models for tractography, not embodied agents or VLLMs).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03735": {
        "authors": [
            "Gopalji Gaur",
            "Mohammadreza Zolfaghari",
            "Thomas Brox"
        ],
        "title": "StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization",
        "abstract": "arXiv:2508.03735v1 Announce Type: new  Abstract: Generating a coherent sequence of images that tells a visual story, using text-to-image diffusion models, often faces the critical challenge of maintaining subject consistency across all story scenes. Existing approaches, which typically rely on fine-tuning or retraining models, are computationally expensive, time-consuming, and often interfere with the model's pre-existing capabilities. In this paper, we follow a training-free approach and propose an efficient consistent-subject-generation method. This approach works seamlessly with pre-trained diffusion models by introducing masked cross-image attention sharing to dynamically align subject features across a batch of images, and Regional Feature Harmonization to refine visually similar details for improved subject consistency. Experimental results demonstrate that our approach successfully generates visually consistent subjects across a variety of scenarios while maintaining the creative abilities of the diffusion model.",
        "arxiv_id": "2508.03735",
        "ARXIVID": "2508.03735",
        "COMMENT": "Relevant to generative modeling and vision, but does not directly match any specific criterion (focuses on subject consistency in text-to-image diffusion, not VLLMs or embodied AI).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04366": {
        "authors": [
            "Songyun Yang",
            "Yufei Han",
            "Jilong Zhang",
            "Kongming Liang",
            "Peng Yu",
            "Zhaowei Qu",
            "Heng Guo"
        ],
        "title": "RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light",
        "abstract": "arXiv:2508.04366v1 Announce Type: new  Abstract: Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface shapes and reflectances from images captured under varying views and illuminations. However, existing MVPS methods often require controlled darkroom settings for varying illuminations or overlook the recovery of reflectances and illuminations properties, limiting their applicability in natural illumination scenarios and downstream inverse rendering tasks. In this paper, we propose RotatedMVPS to solve shape and reflectance recovery under rotated natural light, achievable with a practical rotation stage. By ensuring light consistency across different camera and object poses, our method reduces the unknowns associated with complex environment light. Furthermore, we integrate data priors from off-the-shelf learning-based single-view photometric stereo methods into our MVPS framework, significantly enhancing the accuracy of shape and reflectance recovery. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach.",
        "arxiv_id": "2508.04366",
        "ARXIVID": "2508.04366",
        "COMMENT": "Relevant to computer vision and spatial understanding, but does not directly match any specific criterion (focuses on photometric stereo under natural light, not embodied agents or VLLMs).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03963": {
        "authors": [
            "Zewen Liu",
            "Juntong Ni",
            "Xianfeng Tang",
            "Max S. Y. Lau",
            "Wei Jin"
        ],
        "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?",
        "abstract": "arXiv:2508.03963v1 Announce Type: new  Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration dating back to Kepler's discovery of planetary motion, remains a core challenge in scientific discovery and artificial intelligence. While Large Language Models show promise in structured reasoning tasks, their ability to infer interpretable, context-aligned symbolic structures from time series data is still underexplored. To systematically evaluate this capability, we introduce SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning over real-world time series across three tasks: multivariate symbolic regression, Boolean network inference, and causal discovery. Unlike prior efforts limited to simple algebraic equations, SymbolBench spans a diverse set of symbolic forms with varying complexity. We further propose a unified framework that integrates LLMs with genetic programming to form a closed-loop symbolic reasoning system, where LLMs act both as predictors and evaluators. Our empirical results reveal key strengths and limitations of current models, highlighting the importance of combining domain knowledge, context alignment, and reasoning structure to improve LLMs in automated scientific discovery.",
        "arxiv_id": "2508.03963",
        "ARXIVID": "2508.03963",
        "COMMENT": "This paper introduces SymbolBench, a benchmark for symbolic reasoning over time series, and a framework combining LLMs with genetic programming. While it is interesting for reasoning and LLMs, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04200": {
        "authors": [
            "Wengang Guo",
            "Wei Ye",
            "Chunchun Chen",
            "Xin Sun",
            "Christian B\\\"ohm",
            "Claudia Plant",
            "Susanto Rahardja"
        ],
        "title": "Bootstrap Deep Spectral Clustering with Optimal Transport",
        "abstract": "arXiv:2508.04200v1 Announce Type: new  Abstract: Spectral clustering is a leading clustering method. Two of its major shortcomings are the disjoint optimization process and the limited representation capacity. To address these issues, we propose a deep spectral clustering model (named BootSC), which jointly learns all stages of spectral clustering -- affinity matrix construction, spectral embedding, and $k$-means clustering -- using a single network in an end-to-end manner. BootSC leverages effective and efficient optimal-transport-derived supervision to bootstrap the affinity matrix and the cluster assignment matrix. Moreover, a semantically-consistent orthogonal re-parameterization technique is introduced to orthogonalize spectral embeddings, significantly enhancing the discrimination capability. Experimental results indicate that BootSC achieves state-of-the-art clustering performance. For example, it accomplishes a notable 16\\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs dataset. Our code is available at https://github.com/spdj2271/BootSC.",
        "arxiv_id": "2508.04200",
        "ARXIVID": "2508.04200",
        "COMMENT": "This paper proposes a new deep spectral clustering model with optimal transport and orthogonal re-parameterization. While it is a methodological advance in clustering, it does not directly relate to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04482": {
        "authors": [
            "Xueyu Hu",
            "Tao Xiong",
            "Biao Yi",
            "Zishu Wei",
            "Ruixuan Xiao",
            "Yurun Chen",
            "Jiasheng Ye",
            "Meiling Tao",
            "Xiangxin Zhou",
            "Ziyu Zhao",
            "Yuhuai Li",
            "Shengze Xu",
            "Shenzhi Wang",
            "Xinchen Xu",
            "Shuofei Qiao",
            "Zhaokai Wang",
            "Kun Kuang",
            "Tieyong Zeng",
            "Liang Wang",
            "Jiwei Li",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou",
            "Guoyin Wang",
            "Keting Yin",
            "Zhou Zhao",
            "Hongxia Yang",
            "Fan Wu",
            "Shengyu Zhang",
            "Fei Wu"
        ],
        "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
        "abstract": "arXiv:2508.04482v1 Announce Type: new  Abstract: The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.",
        "arxiv_id": "2508.04482",
        "ARXIVID": "2508.04482",
        "COMMENT": "This is a survey paper on MLLM-based agents for general computing devices, covering the landscape of OS Agents. It matches criterion 2 (new VLLMs/MLLMs) in a survey sense, and is also relevant to embodied AI (criterion 3) as it discusses agents operating in real-world environments, but does not introduce new methods or benchmarks.",
        "RELEVANCE": 6,
        "NOVELTY": 3
    },
    "2508.04233": {
        "authors": [
            "Saifullah Saifullah",
            "Stefan Agne",
            "Andreas Dengel",
            "Sheraz Ahmed"
        ],
        "title": "DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification",
        "abstract": "arXiv:2508.04233v1 Announce Type: new  Abstract: As black-box AI-driven decision-making systems become increasingly widespread in modern document processing workflows, improving their transparency and reliability has become critical, especially in high-stakes applications where biases or spurious correlations in decision-making could lead to serious consequences. One vital component often found in such document processing workflows is document image classification, which, despite its widespread use, remains difficult to explain. While some recent works have attempted to explain the decisions of document image classification models through feature-importance maps, these maps are often difficult to interpret and fail to provide insights into the global features learned by the model. In this paper, we aim to bridge this research gap by introducing generative document counterfactuals that provide meaningful insights into the model's decision-making through actionable explanations. In particular, we propose DocVCE, a novel approach that leverages latent diffusion models in combination with classifier guidance to first generate plausible in-distribution visual counterfactual explanations, and then performs hierarchical patch-wise refinement to search for a refined counterfactual that is closest to the target factual image. We demonstrate the effectiveness of our approach through a rigorous qualitative and quantitative assessment on 3 different document classification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3 different models -- ResNet, ConvNeXt, and DiT -- using well-established evaluation criteria such as validity, closeness, and realism. To the best of the authors' knowledge, this is the first work to explore generative counterfactual explanations in document image analysis.",
        "arxiv_id": "2508.04233",
        "ARXIVID": "2508.04233",
        "COMMENT": "This paper proposes a diffusion-based method for generating visual counterfactual explanations in document image classification. It is relevant to generative modeling in vision, but does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04161": {
        "authors": [
            "Yuqin Cao",
            "Yixuan Gao",
            "Wei Sun",
            "Xiaohong Liu",
            "Yulun Zhang",
            "Xiongkuo Min"
        ],
        "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning",
        "abstract": "arXiv:2508.04161v1 Announce Type: new  Abstract: Face videos accompanied by audio have become integral to our daily lives, while they often suffer from complex degradations. Most face video restoration methods neglect the intrinsic correlations between the visual and audio features, especially in mouth regions. A few audio-aided face video restoration methods have been proposed, but they only focus on compression artifact removal. In this paper, we propose a General Audio-assisted face Video restoration Network (GAVN) to address various types of streaming video distortions via identity and temporal complementary learning. Specifically, GAVN first captures inter-frame temporal features in the low-resolution space to restore frames coarsely and save computational cost. Then, GAVN extracts intra-frame identity features in the high-resolution space with the assistance of audio signals and face landmarks to restore more facial details. Finally, the reconstruction module integrates temporal features and identity features to generate high-quality face videos. Experimental results demonstrate that GAVN outperforms the existing state-of-the-art methods on face video compression artifact removal, deblurring, and super-resolution. Codes will be released upon publication.",
        "arxiv_id": "2508.04161",
        "ARXIVID": "2508.04161",
        "COMMENT": "This paper proposes a new method for audio-assisted face video restoration, focusing on temporal and identity complementary learning. While it involves multi-modal learning (audio and video), it does not introduce a new VLLM/MLLM (criterion 2), nor is it about spatial intelligence in embodied agents (criterion 1), nor does it present a new benchmark or simulator for embodied AI (criterion 3), nor is it about vision foundation models (criterion 4). It is relevant to multi-modal generative modeling, but not a direct match to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04190": {
        "authors": [
            "Fengyi Wu",
            "Yimian Dai",
            "Tianfang Zhang",
            "Yixuan Ding",
            "Jian Yang",
            "Ming-Ming Cheng",
            "Zhenming Peng"
        ],
        "title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation",
        "abstract": "arXiv:2508.04190v1 Announce Type: new  Abstract: Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.",
        "arxiv_id": "2508.04190",
        "ARXIVID": "2508.04190",
        "COMMENT": "Somewhat related to vision foundation models (criterion 4) via interpretable deep networks for object segmentation, but not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03754": {
        "authors": [
            "Bevin V",
            "Ananthakrishnan P V",
            "Ragesh KR",
            "Sanjay M",
            "Vineeth S",
            "Bibin Wilson"
        ],
        "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement",
        "abstract": "arXiv:2508.03754v1 Announce Type: new  Abstract: The performance of machine learning models for automated invoice processing is critically dependent on large-scale, diverse datasets. However, the acquisition of such datasets is often constrained by privacy regulations and the high cost of manual annotation. To address this, we present a novel pipeline for generating high-fidelity, synthetic invoice documents and their corresponding structured data. Our method first utilizes Optical Character Recognition (OCR) to extract the text content and precise spatial layout from a source invoice. Select data fields are then replaced with contextually realistic, synthetic content generated by a large language model (LLM). Finally, we employ an inpainting technique to erase the original text from the image and render the new, synthetic text in its place, preserving the exact layout and font characteristics. This process yields a pair of outputs: a visually realistic new invoice image and a perfectly aligned structured data file (JSON) reflecting the synthetic content. Our approach provides a scalable and automated solution to amplify small, private datasets, enabling the creation of large, varied corpora for training more robust and accurate document intelligence models.",
        "arxiv_id": "2508.03754",
        "ARXIVID": "2508.03754",
        "COMMENT": "Relevant to vision and generative modeling, but does not directly match any specific criterion (focuses on synthetic document generation, not spatial intelligence, VLLMs, or embodied AI).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04428": {
        "authors": [
            "Si Chen",
            "Izzy Molnar",
            "Ting Hua",
            "Peiyu Li",
            "Le Huy Khiem",
            "G. Alex Ambrose",
            "Jim Lang",
            "Ronald Metoyer",
            "Nitesh V. Chawla"
        ],
        "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices",
        "abstract": "arXiv:2508.04428v1 Announce Type: new  Abstract: High-quality, multi-turn instructional dialogues between novices and experts are essential for developing AI systems that support teaching, learning, and decision-making. These dialogues often involve scaffolding -- the process by which an expert supports a novice's thinking through questions, feedback, and step-by-step guidance. However, such data are scarce due to privacy concerns in recording and the vulnerability inherent in help-seeking. We present SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding dialogues. Using teaching development coaching as an example domain, SimInstruct simulates novice instructors via LLMs, varying their teaching challenges and LLM's persona traits, while human experts provide multi-turn feedback, reasoning, and instructional support. This design enables the creation of realistic, pedagogically rich dialogues without requiring real novice participants. Our results reveal that persona traits, such as extroversion and introversion, meaningfully influence how experts engage. Compared to real mentoring recordings, SimInstruct dialogues demonstrate comparable pedagogical relevance and cognitive depth. Experts also reported the process as engaging and reflective, improving both data quality and their own professional insight. We further fine-tuned a LLaMA model to be an expert model using the augmented dataset, which outperformed GPT-4o in instructional quality. Our analysis highlights GPT-4o's limitations in weak reflective questioning, overuse of generic praise, a condescending tone, and a tendency to overwhelm novices with excessive suggestions.",
        "arxiv_id": "2508.04428",
        "ARXIVID": "2508.04428",
        "COMMENT": "Somewhat related to criteria 2 (uses LLMs for simulating dialogues, fine-tunes LLaMA for expert instructional quality), but not focused on vision or spatial intelligence. More about dialogue data collection and LLMs in education.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03740": {
        "authors": [
            "Jianqiao Chen",
            "Tingting Zhu",
            "Huishi Song",
            "Nan Ma",
            "Xiaodong Xu"
        ],
        "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission",
        "abstract": "arXiv:2508.03740v1 Announce Type: new  Abstract: Discretization of semantic features enables interoperability between semantic and digital communication systems, showing significant potential for practical applications. The fundamental difficulty in digitizing semantic features stems from the need to preserve continuity and context in inherently analog representations during their compression into discrete symbols while ensuring robustness to channel degradation. In this paper, we propose a vector quantized (VQ)-enabled digital semantic communication system with channel adaptive image transmission, named VQ-DeepISC. Guided by deep joint source-channel coding (DJSCC), we first design a Swin Transformer backbone for hierarchical semantic feature extraction, followed by VQ modules projecting features into discrete latent spaces. Consequently, it enables efficient index-based transmission instead of raw feature transmission. To further optimize this process, we develop an attention mechanism-driven channel adaptation module to dynamically optimize index transmission. Secondly, to counteract codebook collapse during training process, we impose a distributional regularization by minimizing the Kullback-Leibler divergence (KLD) between codeword usage frequencies and a uniform prior. Meanwhile, exponential moving average (EMA) is employed to stabilize training and ensure balanced feature coverage during codebook updates. Finally, digital communication is implemented using quadrature phase shift keying (QPSK) modulation alongside orthogonal frequency division multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental results demonstrate superior reconstruction fidelity of the proposed system over benchmark methods.",
        "arxiv_id": "2508.03740",
        "ARXIVID": "2508.03740",
        "COMMENT": "This paper presents a vector quantized-enabled digital semantic communication system for channel adaptive image transmission. While it uses vision transformers and quantization, it is not directly about spatial intelligence, VLLMs, or embodied AI. It is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04124": {
        "authors": [
            "Matthias Bartolo",
            "Konstantinos Makantasis",
            "Dylan Seychell"
        ],
        "title": "Learning Using Privileged Information for Litter Detection",
        "abstract": "arXiv:2508.04124v1 Announce Type: new  Abstract: As litter pollution continues to rise globally, developing automated tools capable of detecting litter effectively remains a significant challenge. This study presents a novel approach that combines, for the first time, privileged information with deep learning object detection to improve litter detection while maintaining model efficiency. We evaluate our method across five widely used object detection models, addressing challenges such as detecting small litter and objects partially obscured by grass or stones. In addition to this, a key contribution of our work can also be attributed to formulating a means of encoding bounding box information as a binary mask, which can be fed to the detection model to refine detection guidance. Through experiments on both within-dataset evaluation on the renowned SODA dataset and cross-dataset evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate consistent performance improvements across all models. Our approach not only bolsters detection accuracy within the training sets but also generalises well to other litter detection contexts. Crucially, these improvements are achieved without increasing model complexity or adding extra layers, ensuring computational efficiency and scalability. Our results suggest that this methodology offers a practical solution for litter detection, balancing accuracy and efficiency in real-world applications.",
        "arxiv_id": "2508.04124",
        "ARXIVID": "2508.04124",
        "COMMENT": "This paper proposes a new method for litter detection using privileged information and a novel way to encode bounding box information as a binary mask. While it is a methodological improvement in object detection, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more of an application paper.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.04552": {
        "authors": [
            "Franz Thaler",
            "Darko Stern",
            "Gernot Plank",
            "Martin Urschler"
        ],
        "title": "Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation",
        "abstract": "arXiv:2508.04552v1 Announce Type: new  Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the development of more sophisticated methods to analyze the heart and its substructures from medical images like Computed Tomography (CT) and Magnetic Resonance (MR). Semantic segmentations of important cardiac structures that represent the whole heart are useful to assess patient-specific cardiac morphology and pathology. Furthermore, accurate semantic segmentations can be used to generate cardiac digital twin models which allows e.g. electrophysiological simulation and personalized therapy planning. Even though deep learning-based methods for medical image segmentation achieved great advancements over the last decade, retaining good performance under domain shift -- i.e. when training and test data are sampled from different data distributions -- remains challenging. In order to perform well on domains known at training-time, we employ a (1) balanced joint training approach that utilizes CT and MR data in equal amounts from different source domains. Further, aiming to alleviate domain shift towards domains only encountered at test-time, we rely on (2) strong intensity and spatial augmentation techniques to greatly diversify the available training data. Our proposed whole heart segmentation method, a 5-fold ensemble with our contributions, achieves the best performance for MR data overall and a performance similar to the best performance for CT data when compared to a model trained solely on CT. With 93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR data, our method demonstrates great potential to efficiently obtain accurate semantic segmentations from which patient-specific cardiac twin models can be generated.",
        "arxiv_id": "2508.04552",
        "ARXIVID": "2508.04552",
        "COMMENT": "This paper focuses on domain generalization and joint training for whole heart segmentation in medical images. While it uses spatial augmentation, it is not directly about embodied agents, VLLMs, or vision foundation models. It is relevant to computer vision and machine learning, but not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}