{
    "2508.19320": {
        "authors": [
            "Ming Chen",
            "Liyuan Cui",
            "Wenyuan Zhang",
            "Haoxian Zhang",
            "Yan Zhou",
            "Xiaohan Li",
            "Xiaoqiang Liu",
            "Pengfei Wan"
        ],
        "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation",
        "abstract": "arXiv:2508.19320v1 Announce Type: new  Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
        "arxiv_id": "2508.19320",
        "ARXIVID": "2508.19320",
        "COMMENT": "MIDAS introduces a real-time, multimodal, autoregressive video generation framework for interactive digital humans, using a large language model with multimodal conditioning (audio, pose, text) and a diffusion head. This is a direct match to criterion 2 (new MLLMs/VLLMs), and also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.19652": {
        "authors": [
            "Zongxia Li",
            "Wenhao Yu",
            "Chengsong Huang",
            "Rui Liu",
            "Zhenwen Liang",
            "Fuxiao Liu",
            "Jingxi Che",
            "Dian Yu",
            "Jordan Boyd-Graber",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
        "abstract": "arXiv:2508.19652v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
        "arxiv_id": "2508.19652",
        "ARXIVID": "2508.19652",
        "COMMENT": "This paper introduces Vision-SR1, a new self-rewarding vision-language model that decomposes reasoning into visual perception and language reasoning, and uses a novel self-rewarding RL approach to improve visual reasoning and reduce hallucinations. This is a direct match to criterion 2 (new VLLMs/MLLMs) and also provides surprising empirical results on mitigating hallucinations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.20072": {
        "authors": [
            "Zhixuan Liang",
            "Yizhuo Li",
            "Tianshuo Yang",
            "Chengyue Wu",
            "Sitong Mao",
            "Liuao Pei",
            "Xiaokang Yang",
            "Jiangmiao Pang",
            "Yao Mu",
            "Ping Luo"
        ],
        "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
        "abstract": "arXiv:2508.20072v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
        "arxiv_id": "2508.20072",
        "ARXIVID": "2508.20072",
        "COMMENT": "Matches criterion 2 and 3. This paper introduces Discrete Diffusion VLA, a new action decoder for vision-language-action policies using discrete diffusion, enabling adaptive and parallel decoding for embodied agents. It is a novel method for VLA models, directly relevant to new VLLM/MLLM architectures and embodied AI methods.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.19638": {
        "authors": [
            "Yang Li",
            "Quan Yuan",
            "Guiyang Luo",
            "Xiaoyuan Fu",
            "Rui Pan",
            "Yujia Yang",
            "Congzhang Shao",
            "Yuewen Liu",
            "Jinglin Li"
        ],
        "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception",
        "abstract": "arXiv:2508.19638v1 Announce Type: new  Abstract: Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at https://github.com/CheeryLeeyy/CoPLOT.",
        "arxiv_id": "2508.19638",
        "ARXIVID": "2508.19638",
        "COMMENT": "Matches criterion 1 and 3. This paper introduces CoPLOT, a collaborative perception framework using point-level tokens instead of BEV for multi-agent perception, with novel modules for token reordering, sequence modeling, and spatial alignment. It addresses spatial understanding and proposes a new method for embodied agents, focusing on aspects (point-level tokens, alignment) that previous BEV-based work ignored.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.19679": {
        "authors": [
            "Qihang Ai",
            "Pi Bu",
            "Yue Cao",
            "Yingyao Wang",
            "Jihao Gu",
            "Jingxuan Xing",
            "Zekun Zhu",
            "Wei Jiang",
            "Zhicheng Zheng",
            "Jun Song",
            "Yuning Jiang",
            "Bo Zheng"
        ],
        "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2508.19679v1 Announce Type: new  Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents to perceive and interact with real-world mobile environments based on human instructions. However, the current fully autonomous paradigm poses potential safety risks when model understanding or reasoning capabilities are insufficient. To address this challenge, we first introduce \\textbf{InquireBench}, a comprehensive benchmark specifically designed to evaluate mobile agents' capabilities in safe interaction and proactive inquiry with users, encompassing 5 categories and 22 sub-categories, where most existing VLM-based agents demonstrate near-zero performance. In this paper, we aim to develop an interactive system that actively seeks human confirmation at critical decision points. To achieve this, we propose \\textbf{InquireMobile}, a novel model inspired by reinforcement learning, featuring a two-stage training strategy and an interactive pre-action reasoning mechanism. Finally, our model achieves an 46.8% improvement in inquiry success rate and the best overall success rate among existing baselines on InquireBench. We will open-source all datasets, models, and evaluation codes to facilitate development in both academia and industry.",
        "arxiv_id": "2508.19679",
        "ARXIVID": "2508.19679",
        "COMMENT": "Matches criterion 3: Embodied AI paper with a new benchmark (InquireBench) and method (InquireMobile) for VLM-based mobile agents to request human assistance. The paper addresses a novel angle\u2014safe interaction and proactive inquiry\u2014largely ignored in previous work, and provides empirical results on a new benchmark.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.20018": {
        "authors": [
            "Quanfeng Lu",
            "Zhantao Ma",
            "Shuai Zhong",
            "Jin Wang",
            "Dahai Yu",
            "Michael K. Ng",
            "Ping Luo"
        ],
        "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control",
        "abstract": "arXiv:2508.20018v1 Announce Type: new  Abstract: The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.",
        "arxiv_id": "2508.20018",
        "ARXIVID": "2508.20018",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for agent systems) and partially criterion 2 (multi-modal LLMs) as it introduces SWIRL, a new staged workflow for multi-agent RL, applied to GUI control with LVLMs, and provides theoretical guarantees and empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.20096": {
        "authors": [
            "Zeyi Sun",
            "Yuhang Cao",
            "Jianze Liang",
            "Qiushi Sun",
            "Ziyu Liu",
            "Zhixiong Zhang",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Kai Chen",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning",
        "abstract": "arXiv:2508.20096v1 Announce Type: new  Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
        "arxiv_id": "2508.20096",
        "ARXIVID": "2508.20096",
        "COMMENT": "Matches criterion 3. This paper introduces CODA, a dual-brain agent for GUI-based scientific computing, with a novel compositional framework and a two-stage decoupled RL training pipeline. It proposes a new method for embodied AI agents in a domain (scientific GUIs) that is less explored, and establishes a new benchmark (ScienceBoard).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.19967": {
        "authors": [
            "Oliver Grainge",
            "Sania Waheed",
            "Jack Stilgoe",
            "Michael Milford",
            "Shoaib Ehsan"
        ],
        "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models",
        "abstract": "arXiv:2508.19967v1 Announce Type: new  Abstract: Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is little work on systematically evaluating the geolocation precision of Generative VLMs, their limits and potential for unintended inferences. To bridge this gap, we conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments. Our results offer insight into the internal reasoning of VLMs and highlight their strengths, limitations, and potential societal risks. Our findings indicate that current VLMs perform poorly on generic street-level images yet achieve notably high accuracy (61\\%) on images resembling social media content, raising significant and urgent privacy concerns.",
        "arxiv_id": "2508.19967",
        "ARXIVID": "2508.19967",
        "COMMENT": "Matches criterion 2: Shows new VLLMs (vision-language large models) and evaluates their geolocation capabilities, limitations, and societal risks. The paper provides a systematic empirical assessment of 25 SOTA VLMs on geolocation tasks, revealing surprising results about their strengths and privacy risks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.19527": {
        "authors": [
            "Zhiting Gao",
            "Dan Song",
            "Diqiong Jiang",
            "Chao Xue",
            "An-An Liu"
        ],
        "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment",
        "abstract": "arXiv:2508.19527v1 Announce Type: new  Abstract: Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.",
        "arxiv_id": "2508.19527",
        "ARXIVID": "2508.19527",
        "COMMENT": "Matches criterion 1: New methodological improvements to spatial understanding for embodied agents. The paper introduces MotionFLUX, an efficient text-guided motion generation framework for animating virtual characters and embodied agents, with a novel deterministic flow matching approach and preference alignment for semantic consistency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19699": {
        "authors": [
            "Yupeng Zhang",
            "Dezhi Zheng",
            "Ping Lu",
            "Han Zhang",
            "Lei Wang",
            "Liping xiang",
            "Cheng Luo",
            "Kaijun Deng",
            "Xiaowen Fu",
            "Linlin Shen",
            "Jinbao Wang"
        ],
        "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
        "abstract": "arXiv:2508.19699v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.",
        "arxiv_id": "2508.19699",
        "ARXIVID": "2508.19699",
        "COMMENT": "Matches criterion 4: Vision foundation models and applications. This paper proposes LabelGS, a method for 3D scene segmentation by augmenting 3D Gaussian Splatting with semantic labels, cross-view consistency, and efficient optimization. It advances 3D scene understanding using vision foundation model techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19909": {
        "authors": [
            "Lechun You",
            "Zhonghua Wu",
            "Weide Liu",
            "Xulei Yang",
            "Jun Cheng",
            "Wei Zhou",
            "Bharadwaj Veeravalli",
            "Guosheng Lin"
        ],
        "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation",
        "abstract": "arXiv:2508.19909v1 Announce Type: new  Abstract: Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.",
        "arxiv_id": "2508.19909",
        "ARXIVID": "2508.19909",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. This paper leverages 2D foundation models (like SAM) to improve 3D point cloud segmentation, bridging 2D and 3D data. The approach is novel in propagating 2D segmentation masks into 3D space and using consistency regularization for pseudo-label selection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19864": {
        "authors": [
            "Oussama Hadjerci",
            "Antoine Letienne",
            "Mohamed Abbas Hedjazi",
            "Adel Hafiane"
        ],
        "title": "Self-supervised structured object representation learning",
        "abstract": "arXiv:2508.19864v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for learning visual representations. While recent SSL approaches achieve strong results in global image understanding, they are limited in capturing the structured representation in scenes. In this work, we propose a self-supervised approach that progressively builds structured visual representations by combining semantic grouping, instance level separation, and hierarchical structuring. Our approach, based on a novel ProtoScale module, captures visual elements across multiple spatial scales. Unlike common strategies like DINO that rely on random cropping and global embeddings, we preserve full scene context across augmented views to improve performance in dense prediction tasks. We validate our method on downstream object detection tasks using a combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results show that our method learns object centric representations that enhance supervised object detection and outperform the state-of-the-art methods, even when trained with limited annotated data and fewer fine-tuning epochs.",
        "arxiv_id": "2508.19864",
        "ARXIVID": "2508.19864",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding) as it proposes a self-supervised method for structured object representation learning, focusing on spatial structure and hierarchical scene understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.19498": {
        "authors": [
            "Yimu Wang",
            "Weiming Zhuang",
            "Chen Chen",
            "Jiabo Huang",
            "Jingtao Li",
            "Lingjuan Lyu"
        ],
        "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models",
        "abstract": "arXiv:2508.19498v1 Announce Type: new  Abstract: In the era of deep learning, the increasing number of pre-trained models available online presents a wealth of knowledge. These models, developed with diverse architectures and trained on varied datasets for different tasks, provide unique interpretations of the real world. Their collective consensus is likely universal and generalizable to unseen data. However, effectively harnessing this collective knowledge poses a fundamental challenge due to the heterogeneity of pre-trained models. Existing knowledge integration solutions typically rely on strong assumptions about training data distributions and network architectures, limiting them to learning only from specific types of models and resulting in data and/or inductive biases. In this work, we introduce a novel framework, namely UNIFORM, for knowledge transfer from a diverse set of off-the-shelf models into one student model without such constraints. Specifically, we propose a dedicated voting mechanism to capture the consensus of knowledge both at the logit level -- incorporating teacher models that are capable of predicting target classes of interest -- and at the feature level, utilizing visual representations learned on arbitrary label spaces. Extensive experiments demonstrate that UNIFORM effectively enhances unsupervised object recognition performance compared to strong knowledge transfer baselines. Notably, it exhibits remarkable scalability by benefiting from over one hundred teachers, while existing methods saturate at a much smaller scale.",
        "arxiv_id": "2508.19498",
        "ARXIVID": "2508.19498",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a new framework (UNIFORM) for unifying knowledge from diverse pre-trained models, including visual models, and demonstrates improved unsupervised object recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.19499": {
        "authors": [
            "Xiangxu Wang",
            "Tianhong Zhao",
            "Wei Tu",
            "Bowen Zhang",
            "Guanzhou Chen",
            "Jinzhou Cao"
        ],
        "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery",
        "abstract": "arXiv:2508.19499v1 Announce Type: new  Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility analysis, underpinning applications in traffic forecasting, infrastructure planning, and policy design. However, existing methods suffer from two critical limitations: (1) reliance on auxiliary features (e.g., Points of Interest, socioeconomic statistics) that are costly to collect and have limited spatial coverage; and (2) sensitivity to spatial topology, where minor index reordering of urban regions (e.g., census tract relabeling) disrupts structural coherence in generated flows. To address these challenges, we propose Sat2Flow, a latent structure-aware diffusion-based framework that generates structurally coherent OD flows using solely satellite imagery as input. Our approach introduces a multi-kernel encoder to capture diverse regional interactions and employs a permutation-aware diffusion process that aligns latent representations across different regional orderings. Through a joint contrastive training objective that bridges satellite-derived features with OD patterns, combined with equivariant diffusion training that enforces structural consistency, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experimental results on real-world urban datasets demonstrate that Sat2Flow outperforms both physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce urban environments, eliminating region-specific auxiliary data dependencies while maintaining structural invariance for robust mobility modeling.",
        "arxiv_id": "2508.19499",
        "ARXIVID": "2508.19499",
        "COMMENT": "Sat2Flow introduces a structure-aware diffusion framework for generating human flow (OD matrices) from satellite imagery, using permutation-aware diffusion and contrastive training. This is a novel application of generative modeling in multi-modal learning (satellite images to flow matrices), and involves clever statistical tricks (permutation invariance). Closely matches criterion 4 (vision foundation models and applications), and is of interest for generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.19254": {
        "authors": [
            "Jookyung Song",
            "Mookyoung Kang",
            "Nojun Kwak"
        ],
        "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration",
        "abstract": "arXiv:2508.19254v1 Announce Type: new  Abstract: This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.",
        "arxiv_id": "2508.19254",
        "ARXIVID": "2508.19254",
        "COMMENT": "This paper presents a real-time AI drawing system that integrates formal and contextual intent using vision-language models, enabling collaborative, multi-user co-creation. It leverages VLMs for semantic understanding and is a novel application of vision-language models (criterion 4), with some relevance to multi-modal generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.19786": {
        "authors": [
            "Han Jiao",
            "Jiakai Sun",
            "Yexing Xu",
            "Lei Zhao",
            "Wei Xing",
            "Huaizhong Lin"
        ],
        "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction",
        "abstract": "arXiv:2508.19786v1 Announce Type: new  Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.",
        "arxiv_id": "2508.19786",
        "ARXIVID": "2508.19786",
        "COMMENT": "This paper proposes MAPo, a new method for dynamic scene reconstruction using motion-aware partitioning of 3D Gaussian splatting. It introduces a novel partitioning strategy and cross-frame consistency loss for high-fidelity dynamic scene rendering. This is a methodological improvement in spatial understanding (criterion 1), and also relevant to vision foundation models (criterion 4) as it advances 3D scene representation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.19705": {
        "authors": [
            "Qiang Hu",
            "Ying Zhou",
            "Gepeng Ji",
            "Nick Barnes",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation",
        "abstract": "arXiv:2508.19705v1 Announce Type: new  Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance between spatiotemporal modeling and domain generalization, limiting their applicability in real clinical scenarios. To embrace this challenge, we recast the VPS task as a track-by-detect paradigm that leverages the spatial contexts captured by the image polyp segmentation (IPS) model while integrating the temporal modeling capabilities of segment anything model 2 (SAM2). However, during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error accumulation, resulting in a snowball effect that compromises segmentation stability. We mitigate this issue by repurposing SAM2 as a video polyp segmenter with two training-free modules. In particular, the intra-association filtering module eliminates spatial inaccuracies originating from the detecting stage, reducing false positives. The inter-association refinement module adaptively updates the memory bank to prevent error propagation over time, enhancing temporal coherence. Both modules work synergistically to stabilize SAM2, achieving cutting-edge performance in both in-domain and out-of-domain scenarios. Furthermore, we demonstrate the robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential reliable clinical analysis.",
        "arxiv_id": "2508.19705",
        "ARXIVID": "2508.19705",
        "COMMENT": "Matches criterion 4: Vision foundation models and applications. This paper repurposes SAM2 (a vision foundation model) for video polyp segmentation, introducing training-free modules for improved stability and generalization. The approach is novel in adapting a foundation model for a challenging medical video task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.19565": {
        "authors": [
            "Yuhang Zhao",
            "Zixing Wang"
        ],
        "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection",
        "abstract": "arXiv:2508.19565v1 Announce Type: new  Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time applications, yet their high computational cost remains a significant barrier, particularly for complex scenarios like intersection traffic monitoring. To address this challenge, we propose FlowDet, a high-speed detector featuring a decoupled encoder optimization strategy applied to the DETR architecture. Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to maintain high representational power across extreme scale variations. To rigorously evaluate the model's performance in environments with severe occlusion and high object density, we collected the Intersection-Flow-5k dataset, a new challenging scene for this task. Evaluated on Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by 1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference speed by 16.2%. Our work demonstrates a new path towards building highly efficient and accurate detectors for demanding, real-world perception systems. The Intersection-Flow-5k dataset is available at https://github.com/AstronZh/Intersection-Flow-5K.",
        "arxiv_id": "2508.19565",
        "ARXIVID": "2508.19565",
        "COMMENT": "FlowDet is a new real-time traffic detector with geometric and scale-aware modules, and introduces a new challenging dataset. This matches criterion 3 (new benchmark/dataset for embodied AI or perception), as it provides a new dataset and method for real-world traffic detection.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.19798": {
        "authors": [
            "Muhammad Ali",
            "Omar Ali AlSuwaidi"
        ],
        "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization",
        "abstract": "arXiv:2508.19798v1 Announce Type: new  Abstract: In the realm of waste management, automating the sorting process for non-biodegradable materials presents considerable challenges due to the complexity and variability of waste streams. To address these challenges, we introduce an enhanced neural architecture that builds upon an existing Encoder-Decoder structure to improve the accuracy and efficiency of waste sorting systems. Our model integrates several key innovations: a Comprehensive Attention Block within the decoder, which refines feature representations by combining convolutional and upsampling operations. In parallel, we utilize attention through the Mamba architecture, providing an additional performance boost. We also introduce a Data Fusion Block that fuses images with more than three channels. To achieve this, we apply PCA transformation to reduce the dimensionality while retaining the maximum variance and essential information across three dimensions, which are then used for further processing. We evaluated the model on RGB, hyperspectral, multispectral, and a combination of RGB and hyperspectral data. The results demonstrate that our approach outperforms existing methods by a significant margin.",
        "arxiv_id": "2508.19798",
        "ARXIVID": "2508.19798",
        "COMMENT": "This paper introduces a new neural architecture for multi-modal (RGB, hyperspectral, multispectral) waste segmentation, with advanced decoding and modality fusion. It is a methodological improvement in spatial understanding, especially with the Data Fusion Block and attention mechanisms. Closely matches criterion 1 (spatial intelligence/methodological improvements for spatial understanding on embodied agents), though the application is waste sorting rather than embodied agents per se.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19773": {
        "authors": [
            "Jakob Seitz",
            "Tobias Lengfeld",
            "Radu Timofte"
        ],
        "title": "The Return of Structural Handwritten Mathematical Expression Recognition",
        "abstract": "arXiv:2508.19773v1 Announce Type: new  Abstract: Handwritten Mathematical Expression Recognition is foundational for educational technologies, enabling applications like digital note-taking and automated grading. While modern encoder-decoder architectures with large language models excel at LaTeX generation, they lack explicit symbol-to-trace alignment, a critical limitation for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates. This paper introduces a structural recognition approach with two innovations: 1 an automatic annotation system that uses a neural network to map LaTeX equations to raw traces, automatically generating annotations for symbol segmentation, classification, and spatial relations, and 2 a modular structural recognition system that independently optimizes segmentation, classification, and relation prediction. By leveraging a dataset enriched with structural annotations from our auto-labeling system, the proposed recognition system combines graph-based trace sorting, a hybrid convolutional-recurrent network, and transformer-based correction to achieve competitive performance on the CROHME-2023 benchmark. Crucially, our structural recognition system generates a complete graph structure that directly links handwritten traces to predicted symbols, enabling transparent error analysis and interpretable outputs.",
        "arxiv_id": "2508.19773",
        "ARXIVID": "2508.19773",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding) as it introduces a structural recognition system for handwritten mathematical expressions, focusing on spatial relations and interpretability, but not in embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.19927": {
        "authors": [
            "Fayaz Ali",
            "Muhammad Zawish",
            "Steven Davy",
            "Radu Timofte"
        ],
        "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution",
        "abstract": "arXiv:2508.19927v1 Announce Type: new  Abstract: Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.",
        "arxiv_id": "2508.19927",
        "ARXIVID": "2508.19927",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) as it proposes a new transformer-based architecture for image super-resolution, but not directly about foundation models or multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.19575": {
        "authors": [
            "Zhu Xu",
            "Zhaowen Wang",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "title": "Interact-Custom: Customized Human Object Interaction Image Generation",
        "abstract": "arXiv:2508.19575v1 Announce Type: new  Abstract: Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application.Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities.To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them.Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics.To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses.Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features.Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.",
        "arxiv_id": "2508.19575",
        "ARXIVID": "2508.19575",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it proposes a new compositional image generation method for human-object interaction, but does not focus on foundation models per se.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.19579": {
        "authors": [
            "Haomiao Zhang",
            "Miao Cao",
            "Xuan Yu",
            "Hui Luo",
            "Yanling Piao",
            "Mengjie Qin",
            "Zhangyuan Li",
            "Ping Wang",
            "Xin Yuan"
        ],
        "title": "High-Speed FHD Full-Color Video Computer-Generated Holography",
        "abstract": "arXiv:2508.19579v1 Announce Type: new  Abstract: Computer-generated holography (CGH) is a promising technology for next-generation displays. However, generating high-speed, high-quality holographic video requires both high frame rate display and efficient computation, but is constrained by two key limitations: ($i$) Learning-based models often produce over-smoothed phases with narrow angular spectra, causing severe color crosstalk in high frame rate full-color displays such as depth-division multiplexing and thus resulting in a trade-off between frame rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods typically optimize frames independently, neglecting spatial-temporal correlations between consecutive frames and leading to computationally inefficient solutions. To overcome these challenges, in this paper, we propose a novel high-speed full-color video CGH generation scheme. First, we introduce Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase distributions via frequency modulation, enabling high-fidelity full-color display at high frame rates. Second, we present HoloMamba, a lightweight asymmetric Mamba-Unet architecture that explicitly models spatial-temporal correlations across video sequences to enhance reconstruction quality and computational efficiency. Extensive simulated and real-world experiments demonstrate that SGDDM achieves high-fidelity full-color display without compromise in frame rate, while HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior state-of-the-art Divide-Conquer-and-Merge Strategy.",
        "arxiv_id": "2508.19579",
        "ARXIVID": "2508.19579",
        "COMMENT": "This paper presents a new high-speed, full-color video computer-generated holography system using a novel spectrum-guided multiplexing and a Mamba-Unet architecture. While it is a vision application with a new architecture, it does not directly match any of the four criteria, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19895": {
        "authors": [
            "Ziyun Qian",
            "Runyu Xiao",
            "Shuyuan Tu",
            "Wei Xue",
            "Dingkang Yang",
            "Mingcheng Li",
            "Dongliang Kou",
            "Minghao Han",
            "Zizhi Chen",
            "Lihua Zhang"
        ],
        "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos",
        "abstract": "arXiv:2508.19895v1 Announce Type: new  Abstract: Recent advances in motion generation show remarkable progress. However, several limitations remain: (1) Existing pose-guided character motion transfer methods merely replicate motion without learning its style characteristics, resulting in inexpressive characters. (2) Motion style transfer methods rely heavily on motion capture data, which is difficult to obtain. (3) Generated motions sometimes violate physical laws. To address these challenges, this paper pioneers a new task: Video-to-Video Motion Personalization. We propose a novel framework, PersonaAnimator, which learns personalized motion patterns directly from unconstrained videos. This enables personalized motion transfer. To support this task, we introduce PersonaVid, the first video-based personalized motion dataset. It contains 20 motion content categories and 120 motion style categories. We further propose a Physics-aware Motion Style Regularization mechanism to enforce physical plausibility in the generated motions. Extensive experiments show that PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for the Video-to-Video Motion Personalization task.",
        "arxiv_id": "2508.19895",
        "ARXIVID": "2508.19895",
        "COMMENT": "This paper introduces PersonaAnimator, a framework for personalized motion transfer from unconstrained videos, and a new dataset. It is relevant to motion generation and transfer, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.20088": {
        "authors": [
            "Yuxin Guo",
            "Teng Wang",
            "Yuying Ge",
            "Shijie Ma",
            "Yixiao Ge",
            "Wei Zou",
            "Ying Shan"
        ],
        "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models",
        "abstract": "arXiv:2508.20088v1 Announce Type: new  Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory",
        "arxiv_id": "2508.20088",
        "ARXIVID": "2508.20088",
        "COMMENT": "This paper introduces AudioStory, a framework for long-form narrative audio generation using LLMs and TTA systems. While it is a multi-modal generative model, it is focused on audio rather than vision or spatial intelligence, and does not directly address the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19316": {
        "authors": [
            "Shreyans Jain",
            "Alexandra Yost",
            "Amirali Abdullah"
        ],
        "title": "Sycophancy as compositions of Atomic Psychometric Traits",
        "abstract": "arXiv:2508.19316v1 Announce Type: new  Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an isolated failure mode that occurs via a single causal mechanism. We instead propose modeling it as geometric and causal compositions of psychometric traits such as emotionality, openness, and agreeableness - similar to factor decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we map activation directions to these factors and study how different combinations may give rise to sycophancy (e.g., high extraversion combined with low conscientiousness). This perspective allows for interpretable and compositional vector-based interventions like addition, subtraction and projection; that may be used to mitigate safety-critical behaviors in LLMs.",
        "arxiv_id": "2508.19316",
        "ARXIVID": "2508.19316",
        "COMMENT": "This paper analyzes sycophancy in LLMs using psychometric trait decomposition. While interesting for LLM interpretability and safety, it does not match the criteria focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19461": {
        "authors": [
            "Neil Kale",
            "Chen Bo Calvin Zhang",
            "Kevin Zhu",
            "Ankit Aich",
            "Paula Rodriguez",
            "Scale Red Team",
            "Christina Q. Knight",
            "Zifan Wang"
        ],
        "title": "Reliable Weak-to-Strong Monitoring of LLM Agents",
        "abstract": "arXiv:2508.19461v1 Announce Type: new  Abstract: We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.",
        "arxiv_id": "2508.19461",
        "ARXIVID": "2508.19461",
        "COMMENT": "This paper focuses on monitoring LLM agents for misbehavior, proposing a new red teaming workflow and hybrid scaffolding. While it is relevant to LLM safety and agent monitoring, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19742": {
        "authors": [
            "Chenguang Liu",
            "Chisheng Wang",
            "Yuhua Cai",
            "Chuanhua Zhu",
            "Qingquan Li"
        ],
        "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection",
        "abstract": "arXiv:2508.19742v1 Announce Type: new  Abstract: Line segment detection in images has been studied for several decades. Existing line segment detectors can be roughly divided into two categories: generic line segment detectors and wireframe line segment detectors. Generic line segment detectors aim to detect all meaningful line segments in images and traditional approaches usually fall into this category. Recent deep learning based approaches are mostly wireframe line segment detectors. They detect only line segments that are geometrically meaningful and have large spatial support. Due to the difference in the aim of design, the performance of generic line segment detectors for the task of wireframe line segment detection won't be satisfactory, and vice versa. In this work, we propose a robust framework that can be used for both generic line segment detection and wireframe line segment detection. The proposed method is an improved version of the Pixel Orientation Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments from edge strength maps, and can be combined with any edge detector. We show in our experiments that by combining the proposed POEv2 with an efficient edge detector, it achieves state-of-the-art performance on three publicly available datasets.",
        "arxiv_id": "2508.19742",
        "ARXIVID": "2508.19742",
        "COMMENT": "Does not directly match any specific criterion. It is a new method for line segment detection, which is a classic computer vision problem, but not directly about spatial intelligence in embodied agents, VLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19573": {
        "authors": [
            "Luhu Li",
            "Bowen Lin",
            "Mukhtiar Khan",
            "Shujun Fu"
        ],
        "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection",
        "abstract": "arXiv:2508.19573v1 Announce Type: new  Abstract: Anomaly detection in medical images is challenging due to limited annotations and a domain gap compared to natural images. Existing reconstruction methods often rely on frozen pre-trained encoders, which limits adaptation to domain-specific features and reduces localization accuracy. Prototype-based learning offers interpretability and clustering benefits but suffers from prototype collapse, where few prototypes dominate training, harming diversity and generalization. To address this, we propose a unified framework combining a trainable encoder with prototype-guided reconstruction and a novel Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum branch, enables stable domain-adaptive feature learning. A lightweight Prototype Extractor mines informative normal prototypes to guide the decoder via attention for precise reconstruction. Our loss enforces balanced prototype use through diversity constraints and per-prototype normalization, effectively preventing collapse. Experiments on multiple medical imaging benchmarks show significant improvements in representation quality and anomaly localization, outperforming prior methods. Visualizations and prototype assignment analyses further validate the effectiveness of our anti-collapse mechanism and enhanced interpretability.",
        "arxiv_id": "2508.19573",
        "ARXIVID": "2508.19573",
        "COMMENT": "This paper proposes a prototype-guided contrastive reconstruction method for medical anomaly detection, with a novel diversity-aware alignment loss. While it is a methodological improvement in representation learning, it does not directly match the four criteria, as it is not about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19555": {
        "authors": [
            "Yu-Wei Zhang",
            "Tongju Han",
            "Lipeng Gao",
            "Mingqiang Wei",
            "Hui Liu",
            "Changbao Li",
            "Caiming Zhang"
        ],
        "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery",
        "abstract": "arXiv:2508.19555v1 Announce Type: new  Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly recovering 2.5D reliefs from single images under complex material and illumination variations. In contrast to its predecessor, MonoRelief V1 [1], which was solely trained on synthetic data, MonoRelief V2 incorporates real data to achieve improved robustness, accuracy and efficiency. To overcome the challenge of acquiring large-scale real-world dataset, we generate approximately 15,000 pseudo real images using a text-to-image generative model, and derive corresponding depth pseudo-labels through fusion of depth and normal predictions. Furthermore, we construct a small-scale real-world dataset (800 samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is then progressively trained on the pseudo-real and real-world datasets. Comprehensive experiments demonstrate its state-of-the-art performance both in depth and normal predictions, highlighting its strong potential for a range of downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.",
        "arxiv_id": "2508.19555",
        "ARXIVID": "2508.19555",
        "COMMENT": "MonoRelief V2 is an improved monocular relief recovery model that leverages real and pseudo-real data for better 2.5D reconstruction. While it is a vision foundation model application, it does not introduce a new foundation model or a novel multi-modal method. It is a solid application paper.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19664": {
        "authors": [
            "Weicheng Liao",
            "Zan Chen",
            "Jianyang Xie",
            "Yalin Zheng",
            "Yuhui Ma",
            "Yitian Zhao"
        ],
        "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement",
        "abstract": "arXiv:2508.19664v1 Announce Type: new  Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware self-supervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high- and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.",
        "arxiv_id": "2508.19664",
        "ARXIVID": "2508.19664",
        "COMMENT": "This paper proposes a frequency-aware self-supervised learning method for ultra-wide-field retinal image enhancement. It is a novel method for medical image enhancement, but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19626": {
        "authors": [
            "Jiajun Sun",
            "Zhen Yu",
            "Siyuan Yan",
            "Jason J. Ong",
            "Zongyuan Ge",
            "Lei Zhang"
        ],
        "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model",
        "abstract": "arXiv:2508.19626v1 Announce Type: new  Abstract: Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and types as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.",
        "arxiv_id": "2508.19626",
        "ARXIVID": "2508.19626",
        "COMMENT": "This paper presents a controllable skin image synthesis model using a lesion-focused VQ-VAE and VAR Transformer. It is a generative model for medical images, but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19290": {
        "authors": [
            "Alexandros Gkillas",
            "Ioulia Kapsali",
            "Nikos Piperigkos",
            "Aris S. Lalos"
        ],
        "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation",
        "abstract": "arXiv:2508.19290v1 Announce Type: new  Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous vehicles, yet modern segmentation networks are highly susceptible to adversarial attacks that can compromise safety. Most existing defenses are designed for networks operating directly on raw 3D point clouds and rely on large, computationally intensive generative models. However, many state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D range view representations. Despite their widespread adoption, dedicated lightweight adversarial defenses for this domain remain largely unexplored. We introduce an efficient model-based purification framework tailored for adversarial defense in 2D range-view LiDAR segmentation. We propose a direct attack formulation in the range-view domain and develop an explainable purification network based on a mathematical justified optimization problem, achieving strong adversarial resilience with minimal computational overhead. Our method achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. More importantly, real-world deployment on a demo vehicle demonstrates the framework's ability to deliver accurate operation in practical autonomous driving scenarios.",
        "arxiv_id": "2508.19290",
        "ARXIVID": "2508.19290",
        "COMMENT": "This paper introduces a lightweight adversarial defense for LiDAR segmentation in autonomous vehicles. While it is relevant to computer vision and robustness, it does not address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19544": {
        "authors": [
            "Eduardo Davalos",
            "Yike Zhang",
            "Namrata Srivastava",
            "Yashvitha Thatigotla",
            "Jorge A. Salas",
            "Sara McFadden",
            "Sun-Joo Cho",
            "Amanda Goodwin",
            "Ashwin TS",
            "Gautam Biswas"
        ],
        "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization",
        "abstract": "arXiv:2508.19544v1 Announce Type: new  Abstract: With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at https://github.com/RedForestAi/WebEyeTrack.",
        "arxiv_id": "2508.19544",
        "ARXIVID": "2508.19544",
        "COMMENT": "This paper introduces a scalable, browser-based eye-tracking framework using on-device few-shot learning and lightweight gaze estimation models. While it is a computer vision application, it does not directly address spatial understanding for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more of an applied system for gaze estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19314": {
        "authors": [
            "Mahdis Tourian (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Sareh Rowlands (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Remy Vandaele (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Max Fancourt (Natural England",
            "York",
            "UK)",
            "Rebecca Mein (Natural England",
            "York",
            "UK)",
            "Hywel T. P. Williams (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)"
        ],
        "title": "Automated classification of natural habitats using ground-level imagery",
        "abstract": "arXiv:2508.19314v1 Announce Type: new  Abstract: Accurate classification of terrestrial habitats is critical for biodiversity conservation, ecological monitoring, and land-use planning. Several habitat classification schemes are in use, typically based on analysis of satellite imagery with validation by field ecologists. Here we present a methodology for classification of habitats based solely on ground-level imagery (photographs), offering improved validation and the ability to classify habitats at scale (for example using citizen-science imagery). In collaboration with Natural England, a public sector organisation responsible for nature conservation in England, this study develops a classification system that applies deep learning to ground-level habitat photographs, categorising each image into one of 18 classes defined by the 'Living England' framework. Images were pre-processed using resizing, normalisation, and augmentation; re-sampling was used to balance classes in the training data and enhance model robustness. We developed and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label to each photograph. Using five-fold cross-validation, the model demonstrated strong overall performance across 18 habitat classes, with accuracy and F1-scores varying between classes. Across all folds, the model achieved a mean F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or ambiguous classes scoring lower. These findings demonstrate the potential of this approach for ecological monitoring. Ground-level imagery is readily obtained, and accurate computational methods for habitat classification based on such data have many potential applications. To support use by practitioners, we also provide a simple web application that classifies uploaded images using our model.",
        "arxiv_id": "2508.19314",
        "ARXIVID": "2508.19314",
        "COMMENT": "This paper presents a deep learning method for classifying natural habitats from ground-level imagery. While it uses computer vision, it does not introduce new methods for spatial intelligence in embodied agents, VLLMs/MLLMs, new embodied AI benchmarks, or vision foundation models. It is an application paper.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}