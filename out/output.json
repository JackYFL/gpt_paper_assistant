{
    "2510.14861": {
        "authors": [
            "Le Cong",
            "Zaixi Zhang",
            "Xiaotong Wang",
            "Yin Di",
            "Ruofan Jin",
            "Michal Gerasimiuk",
            "Yinkai Wang",
            "Ravi K. Dinesh",
            "David Smerkous",
            "Alex Smerkous",
            "Xuekun Wu",
            "Shilong Liu",
            "Peishan Li",
            "Yi Zhu",
            "Simran Serrao",
            "Ning Zhao",
            "Imran A. Mohammad",
            "John B. Sunwoo",
            "Joseph C. Wu",
            "Mengdi Wang"
        ],
        "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
        "abstract": "arXiv:2510.14861v1 Announce Type: new  Abstract: Modern science advances fastest when thought meets action. LabOS represents the first AI co-scientist that unites computational reasoning with physical experimentation through multimodal perception, self-evolving agents, and Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. Across applications--from cancer immunotherapy target discovery to stem-cell engineering -- LabOS shows that AI can move beyond computational design to participation, turning the laboratory into an intelligent, collaborative environment where human and machine discovery evolve together.",
        "arxiv_id": "2510.14861",
        "ARXIVID": "2510.14861",
        "COMMENT": "Matches criterion 1 and 3. LabOS is an embodied AI system that unites multimodal perception, self-evolving agents, and XR-enabled human-AI collaboration for real-world scientific experimentation. The focus on spatial understanding, embodied agents, and novel human-AI collaboration in physical environments is highly relevant.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14560": {
        "authors": [
            "Yulin Zhang",
            "Cheng Shi",
            "Yang Wang",
            "Sibei Yang"
        ],
        "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
        "abstract": "arXiv:2510.14560v1 Announce Type: new  Abstract: Envision an AI capable of functioning in human-like settings, moving beyond mere observation to actively understand, anticipate, and proactively respond to unfolding events. Towards this vision, we focus on the innovative task where, given ego-streaming video input, an assistant proactively answers diverse, evolving questions at the opportune moment, while maintaining synchronized perception and reasoning. This task embodies three key properties: (1) Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized Efficiency. To evaluate and address these properties, we first introduce ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a novel framework designed for their rigorous assessment. Secondly, we propose a comprehensive technical pipeline to enable models to tackle this challenging task. This pipeline comprises: (1) a data engine, (2) a multi-stage training strategy, and (3) a proactive dynamic compression technique. Our proposed model effectively addresses these critical properties while outperforming multiple baselines across diverse online and offline benchmarks. Project Page:https://zhangyl4.github.io/publications/eyes-wide-open/",
        "arxiv_id": "2510.14560",
        "ARXIVID": "2510.14560",
        "COMMENT": "Matches criterion 2 and 3. This paper introduces a new benchmark (ESTP-Bench) and metric (ESTP-F1) for proactive video-LLMs in ego-streaming video, and proposes a technical pipeline for proactive, just-in-time, synchronized perception and reasoning. The focus on streaming video, proactive reasoning, and benchmarking for embodied agents is a novel angle.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14965": {
        "authors": [
            "Miao Hu",
            "Zhiwei Huang",
            "Tai Wang",
            "Jiangmiao Pang",
            "Dahua Lin",
            "Nanning Zheng",
            "Runsen Xu"
        ],
        "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
        "abstract": "arXiv:2510.14965v1 Announce Type: new  Abstract: Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
        "arxiv_id": "2510.14965",
        "ARXIVID": "2510.14965",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark): Introduces ChangingGrounding, a new benchmark for 3D visual grounding in changing scenes, and proposes a novel memory-driven method. Focuses on practical, memory-centric 3DVG for real-world embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14979": {
        "authors": [
            "Haiwen Diao",
            "Mingxuan Li",
            "Silei Wu",
            "Linjun Dai",
            "Xiaohua Wang",
            "Hanming Deng",
            "Lewei Lu",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
        "abstract": "arXiv:2510.14979v1 Announce Type: new  Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "arxiv_id": "2510.14979",
        "ARXIVID": "2510.14979",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Introduces NEO, a new family of native vision-language models, with a focus on first-principles design and efficient pixel-word alignment. Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14374": {
        "authors": [
            "Han Qiu",
            "Peng Gao",
            "Lewei Lu",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ],
        "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
        "abstract": "arXiv:2510.14374v1 Announce Type: new  Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR",
        "arxiv_id": "2510.14374",
        "ARXIVID": "2510.14374",
        "COMMENT": "This paper directly addresses criterion 1 and 2: it proposes a new method (Spatial Preference Rewarding) to improve spatial understanding in MLLMs, enhancing fine-grained spatial perception and localization. It is a methodological improvement for spatial intelligence in embodied agents and MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14621": {
        "authors": [
            "Yuanyi Song",
            "Heyuan Huang",
            "Qiqiang Lin",
            "Yin Zhao",
            "Xiangmou Qu",
            "Jun Wang",
            "Xingyu Lou",
            "Weiwen Liu",
            "Zhuosheng Zhang",
            "Jun Wang",
            "Yong Yu",
            "Weinan Zhang",
            "Zhaoxiang Wang"
        ],
        "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks",
        "abstract": "arXiv:2510.14621v1 Announce Type: new  Abstract: The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined \"golden path\", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.",
        "arxiv_id": "2510.14621",
        "ARXIVID": "2510.14621",
        "COMMENT": "ColorBench is a new benchmark for evaluating mobile agents on complex, long-horizon tasks using a graph-structured framework. This directly matches criterion 3 (embodied AI, new benchmark/simulator, novel evaluation angles).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14388": {
        "authors": [
            "Zhe Wu",
            "Hongjin Lu",
            "Junliang Xing",
            "Changhao Zhang",
            "Yin Zhu",
            "Yuhao Yang",
            "Yuheng Jing",
            "Kai Li",
            "Kun Shao",
            "Jianye Hao",
            "Jun Wang",
            "Yuanchun Shi"
        ],
        "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
        "abstract": "arXiv:2510.14388v1 Announce Type: new  Abstract: Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.",
        "arxiv_id": "2510.14388",
        "ARXIVID": "2510.14388",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (embodied AI, new methods for mobile device control with hierarchical vision-language agent). Introduces a hierarchical VLM agent for mobile device control with new training and planning methods.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.14978": {
        "authors": [
            "Nupur Kumari",
            "Sheng-Yu Wang",
            "Nanxuan Zhao",
            "Yotam Nitzan",
            "Yuheng Li",
            "Krishna Kumar Singh",
            "Richard Zhang",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Xun Huang"
        ],
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "abstract": "arXiv:2510.14978v1 Announce Type: new  Abstract: Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "arxiv_id": "2510.14978",
        "ARXIVID": "2510.14978",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). The paper proposes a new training paradigm for image editing using feedback from vision-language models (VLMs) and achieves strong results without paired data, which is a clever methodological improvement in generative modeling and vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.14958": {
        "authors": [
            "Weikang Shi",
            "Aldrich Yu",
            "Rongyao Fang",
            "Houxing Ren",
            "Ke Wang",
            "Aojun Zhou",
            "Changyao Tian",
            "Xinyu Fu",
            "Yuxuan Hu",
            "Zimu Lu",
            "Linjiang Huang",
            "Si Liu",
            "Rui Liu",
            "Hongsheng Li"
        ],
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
        "abstract": "arXiv:2510.14958v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "arxiv_id": "2510.14958",
        "ARXIVID": "2510.14958",
        "COMMENT": "Matches criterion 2. MathCanvas introduces a framework for intrinsic visual chain-of-thought in multimodal mathematical reasoning, including a new LMM (BAGEL-Canvas), large-scale datasets, and a benchmark. The focus on visual chain-of-thought and LMMs for math is a novel and significant contribution.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.14624": {
        "authors": [
            "Natan Bagrov",
            "Eugene Khvedchenia",
            "Borys Tymchenko",
            "Shay Aharon",
            "Lior Kadoch",
            "Tomer Keren",
            "Ofri Masad",
            "Yonatan Geifman",
            "Ran Zilberstein",
            "Tuomas Rintamaki",
            "Matthieu Le",
            "Andrew Tao"
        ],
        "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
        "abstract": "arXiv:2510.14624v1 Announce Type: new  Abstract: Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.",
        "arxiv_id": "2510.14624",
        "ARXIVID": "2510.14624",
        "COMMENT": "Matches criteria 2: Proposes Efficient Video Sampling (EVS), a plug-and-play method for pruning redundant tokens in video for faster VLM (Vision-Language Model) inference, directly improving VLM scalability and efficiency.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.14945": {
        "authors": [
            "JoungBin Lee",
            "Jaewoo Jung",
            "Jisang Han",
            "Takuya Narihira",
            "Kazumi Fukuda",
            "Junyoung Seo",
            "Sunghwan Hong",
            "Yuki Mitsufuji",
            "Seungryong Kim"
        ],
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
        "abstract": "arXiv:2510.14945v1 Announce Type: new  Abstract: We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
        "arxiv_id": "2510.14945",
        "ARXIVID": "2510.14945",
        "COMMENT": "Matches criteria 1 and 3: Presents a new method for spatially consistent, camera-controllable video generation using 3D scene memory and dynamic SLAM, which is highly relevant to spatial understanding and embodied agents. Also introduces a novel approach to scene consistency in generative video models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.14885": {
        "authors": [
            "Logan Lawrence",
            "Oindrila Saha",
            "Megan Wei",
            "Chen Sun",
            "Subhransu Maji",
            "Grant Van Horn"
        ],
        "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
        "abstract": "arXiv:2510.14885v1 Announce Type: new  Abstract: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
        "arxiv_id": "2510.14885",
        "ARXIVID": "2510.14885",
        "COMMENT": "Matches criterion 2 (new MLLMs): Proposes a new method (nlg2choice) to improve fine-grained visual recognition in MLLMs, with empirical results on multiple datasets. Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.14112": {
        "authors": [
            "Huiliang Zhang",
            "Di Wu",
            "Arnaud Zinflou",
            "Benoit Boulet"
        ],
        "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management",
        "abstract": "arXiv:2510.14112v1 Announce Type: new  Abstract: Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.",
        "arxiv_id": "2510.14112",
        "ARXIVID": "2510.14112",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents). The paper proposes a spatial-temporal graph learning framework (GCN-Transformer fusion) for multi-agent coordination in building energy management, with a focus on spatial-temporal dependencies and safety constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14661": {
        "authors": [
            "Weikang Yu",
            "Vincent Nwazelibe",
            "Xianping Ma",
            "Xiaokang Zhang",
            "Richard Gloaguen",
            "Xiao Xiang Zhu",
            "Pedram Ghamisi"
        ],
        "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
        "abstract": "arXiv:2510.14661v1 Announce Type: new  Abstract: Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at https://github.com/EricYu97/EuroMineNet.",
        "arxiv_id": "2510.14661",
        "ARXIVID": "2510.14661",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmark/simulator related). The paper introduces a new large-scale, multitemporal benchmark (EuroMineNet) for spatiotemporal mining footprint analysis, with novel metrics and tasks for GeoAI models. This is a new benchmark with a novel angle (temporal consistency and explainability in environmental monitoring).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14553": {
        "authors": [
            "Song Tang",
            "Peihao Gong",
            "Kunyu Li",
            "Kai Guo",
            "Boyu Wang",
            "Mao Ye",
            "Jianwei Zhang",
            "Xiatian Zhu"
        ],
        "title": "Consistent text-to-image generation via scene de-contextualization",
        "abstract": "arXiv:2510.14553v1 Announce Type: new  Abstract: Consistent text-to-image (T2I) generation seeks to produce identity-preserving images of the same subject across diverse scenes, yet it often fails due to a phenomenon called identity (ID) shift. Previous methods have tackled this issue, but typically rely on the unrealistic assumption of knowing all target scenes in advance. This paper reveals that a key source of ID shift is the native correlation between subject and scene context, called scene contextualization, which arises naturally as T2I models fit the training distribution of vast natural images. We formally prove the near-universality of this scene-ID correlation and derive theoretical bounds on its strength. On this basis, we propose a novel, efficient, training-free prompt embedding editing approach, called Scene De-Contextualization (SDeC), that imposes an inversion process of T2I's built-in scene contextualization. Specifically, it identifies and suppresses the latent scene-ID correlation within the ID prompt's embedding by quantifying the SVD directional stability to adaptively re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene use (one scene per prompt) without requiring prior access to all target scenes. This makes it a highly flexible and general solution well-suited to real-world applications where such prior knowledge is often unavailable or varies over time. Experiments demonstrate that SDeC significantly enhances identity preservation while maintaining scene diversity.",
        "arxiv_id": "2510.14553",
        "ARXIVID": "2510.14553",
        "COMMENT": "Matches criterion 4. This paper proposes Scene De-Contextualization (SDeC), a training-free prompt embedding editing approach for consistent text-to-image generation, addressing identity shift by suppressing scene-ID correlation. This is a novel method for improving vision foundation models' consistency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14179": {
        "authors": [
            "Yuancheng Xu",
            "Wenqi Xian",
            "Li Ma",
            "Julien Philip",
            "Ahmet Levent Ta\\c{s}el",
            "Yiwei Zhao",
            "Ryan Burgert",
            "Mingming He",
            "Oliver Hermann",
            "Oliver Pilarski",
            "Rahul Garg",
            "Paul Debevec",
            "Ning Yu"
        ],
        "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
        "abstract": "arXiv:2510.14179v1 Announce Type: new  Abstract: We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
        "arxiv_id": "2510.14179",
        "ARXIVID": "2510.14179",
        "COMMENT": "Matches criterion 4. This paper presents a framework for customizing camera-controllable video diffusion models with multi-view performance captures, enabling multi-view character consistency and 3D camera control. The focus on video diffusion models and virtual production is a novel application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14882": {
        "authors": [
            "Keli Liu",
            "Zhendong Wang",
            "Wengang Zhou",
            "Shaodong Xu",
            "Ruixiao Dong",
            "Houqiang Li"
        ],
        "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
        "abstract": "arXiv:2510.14882v1 Announce Type: new  Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.",
        "arxiv_id": "2510.14882",
        "ARXIVID": "2510.14882",
        "COMMENT": "Matches criterion 4. ScaleWeaver is a new framework for controllable text-to-image generation using visual autoregressive models, introducing a novel reference attention module for efficient and precise control. This is a methodological improvement in vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14702": {
        "authors": [
            "Penglong Zhai",
            "Jie Li",
            "Fanyi Di",
            "Yue Liu",
            "Yifang Yuan",
            "Jie Huang",
            "Peng Wu",
            "Sicong Wang",
            "Mingyang Yin",
            "Tingting Hu",
            "Yao Xu",
            "Xin Li"
        ],
        "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction",
        "abstract": "arXiv:2510.14702v1 Announce Type: new  Abstract: The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in \"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of CoAST.",
        "arxiv_id": "2510.14702",
        "ARXIVID": "2510.14702",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents): Proposes CoAST, a cognitive-aligned spatio-temporal LLM for next point-of-interest prediction, integrating spatial, temporal, and world knowledge for improved spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14270": {
        "authors": [
            "Alexander Valverde",
            "Brian Xu",
            "Yuyin Zhou",
            "Meng Xu",
            "Hongyun Wang"
        ],
        "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
        "abstract": "arXiv:2510.14270v1 Announce Type: new  Abstract: Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
        "arxiv_id": "2510.14270",
        "ARXIVID": "2510.14270",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Uses 2D foundation models (like DINO) to improve 3D scene reconstruction, showing a hybrid approach that leverages foundation model features for 3D tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14605": {
        "authors": [
            "Yuyang Hong",
            "Jiaqi Gu",
            "Qi Yang",
            "Lubin Fan",
            "Yue Wu",
            "Ying Wang",
            "Kun Ding",
            "Shiming Xiang",
            "Jieping Ye"
        ],
        "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
        "abstract": "arXiv:2510.14605v1 Announce Type: new  Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
        "arxiv_id": "2510.14605",
        "ARXIVID": "2510.14605",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Proposes a new VLM for knowledge-based visual question answering with a novel three-stage method and reinforcement learning. Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14359": {
        "authors": [
            "Zichen Wen",
            "Yiyu Wang",
            "Chenfei Liao",
            "Boxue Yang",
            "Junxian Li",
            "Weifeng Liu",
            "Haocong He",
            "Bolong Feng",
            "Xuyang Liu",
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Xuming Hu",
            "Linfeng Zhang"
        ],
        "title": "AI for Service: Proactive Assistance with AI Glasses",
        "abstract": "arXiv:2510.14359v1 Announce Type: new  Abstract: In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
        "arxiv_id": "2510.14359",
        "ARXIVID": "2510.14359",
        "COMMENT": "Matches criteria 3: Introduces a new paradigm and framework (Alpha-Service) for proactive, real-time assistance using AI glasses, with a multi-agent system for embodied AI in egocentric video. Focuses on novel agentic interaction and real-world deployment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14847": {
        "authors": [
            "Meiqi Wu",
            "Jiashu Zhu",
            "Xiaokun Feng",
            "Chubin Chen",
            "Chen Zhu",
            "Bingze Song",
            "Fangyuan Mao",
            "Jiahong Wu",
            "Xiangxiang Chu",
            "Kaiqi Huang"
        ],
        "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "abstract": "arXiv:2510.14847v1 Announce Type: new  Abstract: Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
        "arxiv_id": "2510.14847",
        "ARXIVID": "2510.14847",
        "COMMENT": "ImagerySearch introduces a new adaptive test-time search strategy for video generation, and also presents a new benchmark (LDT-Bench) for imaginative video generation. This matches criterion 3 (new benchmark for generative models in vision/embodied AI) and is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14819": {
        "authors": [
            "Ji Cao",
            "Yu Wang",
            "Tongya Zheng",
            "Zujie Ren",
            "Canghong Jin",
            "Gang Chen",
            "Mingli Song"
        ],
        "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
        "abstract": "arXiv:2510.14819v1 Announce Type: new  Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \\textbf{P}erception and explicit \\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation learning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: https://anonymous.4open.science/r/PRTraj.",
        "arxiv_id": "2510.14819",
        "ARXIVID": "2510.14819",
        "COMMENT": "Matches criterion 1 (spatial understanding in trajectory representation learning) and introduces a novel framework unifying environment perception and route choice modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14543": {
        "authors": [
            "Ziqi Jiang",
            "Yanghao Wang",
            "Long Chen"
        ],
        "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
        "abstract": "arXiv:2510.14543v1 Announce Type: new  Abstract: Aligning features from different modalities, is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment. It is insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have demonstrated that FMA can consistently yield significant performance gains across various benchmarks and backbones, particularly on challenging datasets.",
        "arxiv_id": "2510.14543",
        "ARXIVID": "2510.14543",
        "COMMENT": "Matches criterion 2 (new methodological improvements for cross-modal alignment in VLLMs/MLLMs) and is relevant to multi-modal learning and vision-language models. Proposes a novel multi-step PEFT method for better cross-modal alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.14954": {
        "authors": [
            "Zhe Li",
            "Weihao Yuan",
            "Weichao Shen",
            "Siyu Zhu",
            "Zilong Dong",
            "Chang Xu"
        ],
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
        "abstract": "arXiv:2510.14954v1 Announce Type: new  Abstract: Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
        "arxiv_id": "2510.14954",
        "ARXIVID": "2510.14954",
        "COMMENT": "Relevant to criterion 2 (multi-modal large language models): Presents a new multi-modal motion generation transformer integrating text, speech, and music. Focuses on generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.14975": {
        "authors": [
            "Hengyuan Xu",
            "Wei Cheng",
            "Peng Xing",
            "Yixiao Fang",
            "Shuhan Wu",
            "Rui Wang",
            "Xianfang Zeng",
            "Daxin Jiang",
            "Gang Yu",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "abstract": "arXiv:2510.14975v1 Announce Type: new  Abstract: Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "arxiv_id": "2510.14975",
        "ARXIVID": "2510.14975",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Introduces a new dataset and a contrastive identity loss for controllable, identity-consistent image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.14874": {
        "authors": [
            "Guangyi Han",
            "Wei Zhai",
            "Yuhang Yang",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
        "abstract": "arXiv:2510.14874v1 Announce Type: new  Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent. Existing HOI generation research is predominantly confined to fixed grasping patterns, where control is tied to physical priors such as force closure or generic intent instructions, even when expressed through elaborate language. Such an overly general conditioning imposes a strong inductive bias for stable grasps, thus failing to capture the diversity of daily HOI. To address these limitations, we introduce Free-Form HOI Generation, which aims to generate controllable, diverse, and physically plausible HOI conditioned on fine-grained intent, extending HOI from grasping to free-form interactions, like pushing, poking, and rotating. To support this task, we construct WildO2, an in-the-wild diverse 3D HOI dataset, which includes diverse HOI derived from internet videos. Specifically, it contains 4.4k unique interactions across 92 intents and 610 object categories, each with detailed semantic annotations. Building on this dataset, we propose TOUCH, a three-stage framework centered on a multi-level diffusion model that facilitates fine-grained semantic control to generate versatile hand poses beyond grasping priors. This process leverages explicit contact modeling for conditioning and is subsequently refined with contact consistency and physical constraints to ensure realism. Comprehensive experiments demonstrate our method's ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities. The project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
        "arxiv_id": "2510.14874",
        "ARXIVID": "2510.14874",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Proposes a new dataset and a multi-level diffusion model for controllable hand-object interaction generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.14862": {
        "authors": [
            "Mihai-Cristian P\\^irvu",
            "Marius Leordeanu"
        ],
        "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
        "abstract": "arXiv:2510.14862v1 Announce Type: new  Abstract: The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.",
        "arxiv_id": "2510.14862",
        "ARXIVID": "2510.14862",
        "COMMENT": "Matches criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications). The paper presents a multi-modal data pipeline using pre-trained experts and a model (PHG-MAE) for efficient multi-modal learning, with applications to real-time semantic segmentation and depth estimation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.14255": {
        "authors": [
            "Liao Shen",
            "Wentao Jiang",
            "Yiran Zhu",
            "Tiezheng Ge",
            "Zhiguo Cao",
            "Bo Zheng"
        ],
        "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
        "abstract": "arXiv:2510.14255v1 Announce Type: new  Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
        "arxiv_id": "2510.14255",
        "ARXIVID": "2510.14255",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Proposes a reward-guided optimization for identity-preserving image-to-video generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.14741": {
        "authors": [
            "Simone Carnemolla",
            "Matteo Pennisi",
            "Sarinda Samarasinghe",
            "Giovanni Bellitto",
            "Simone Palazzo",
            "Daniela Giordano",
            "Mubarak Shah",
            "Concetto Spampinato"
        ],
        "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
        "abstract": "arXiv:2510.14741v1 Announce Type: new  Abstract: Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
        "arxiv_id": "2510.14741",
        "ARXIVID": "2510.14741",
        "COMMENT": "This paper introduces DEXTER, a framework that uses diffusion models and large language models to generate textual explanations for vision classifiers. It is a novel application of vision foundation models (criterion 4), and leverages generative modeling for model interpretability.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.14512": {
        "authors": [
            "Haoyuan Li",
            "Mathias Funk",
            "Aaqib Saeed"
        ],
        "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration",
        "abstract": "arXiv:2510.14512v1 Announce Type: new  Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.",
        "arxiv_id": "2510.14512",
        "ARXIVID": "2510.14512",
        "COMMENT": "Partially matches criterion 3 (embodied AI, new benchmark for federated learning agentic systems), but is more focused on federated learning automation than embodied AI or spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.14657": {
        "authors": [
            "Kieran Carrigg",
            "Rob van Gastel",
            "Melda Yeghaian",
            "Sander Dalm",
            "Faysal Boughorbel",
            "Marcel van Gerven"
        ],
        "title": "Decorrelation Speeds Up Vision Transformers",
        "abstract": "arXiv:2510.14657v1 Announce Type: new  Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.",
        "arxiv_id": "2510.14657",
        "ARXIVID": "2510.14657",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models): Proposes a training optimization for Vision Transformers (ViTs), which are often used as vision foundation models. Focus is on training efficiency rather than new model architectures or applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.14803": {
        "authors": [
            "Pedro R. A. S. Bassi",
            "Xinze Zhou",
            "Wenxuan Li",
            "Szymon P{\\l}otka",
            "Jieneng Chen",
            "Qi Chen",
            "Zheren Zhu",
            "Jakub Prz\\k{a}do",
            "Ibrahim E. Hamac{\\i}",
            "Sezgin Er",
            "Yuhan Wang",
            "Ashwin Kumar",
            "Bjoern Menze",
            "Jaros{\\l}aw B. \\'Cwik{\\l}a",
            "Yuyin Zhou",
            "Akshay S. Chaudhari",
            "Curtis P. Langlotz",
            "Sergio Decherchi",
            "Andrea Cavalli",
            "Kang Wang",
            "Yang Yang",
            "Alan L. Yuille",
            "Zongwei Zhou"
        ],
        "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
        "abstract": "arXiv:2510.14803v1 Announce Type: new  Abstract: Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types.   We plan to release our trained models, code, and dataset at https://github.com/MrGiovanni/R-Super",
        "arxiv_id": "2510.14803",
        "ARXIVID": "2510.14803",
        "COMMENT": "This paper introduces R-Super, a method for training AI to segment tumors using medical reports instead of masks. While it is a novel approach to scaling medical vision models, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models in a general sense. It is relevant to multi-modal learning and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2510.14942": {
        "authors": [
            "Yao Zhang",
            "Yu Wu",
            "Haowei Zhang",
            "Weiguo Li",
            "Haokun Chen",
            "Jingpei Wu",
            "Guohao Li",
            "Zhen Han",
            "Volker Tresp"
        ],
        "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
        "abstract": "arXiv:2510.14942v1 Announce Type: new  Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
        "arxiv_id": "2510.14942",
        "ARXIVID": "2510.14942",
        "COMMENT": "Somewhat relevant to criterion 2 (MLLMs): Proposes a new process reward modeling framework for step-level reasoning in LLMs, but not specifically vision-language or multi-modal.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14831": {
        "authors": [
            "Qi Chen",
            "Xinze Zhou",
            "Chen Liu",
            "Hao Chen",
            "Wenxuan Li",
            "Zekun Jiang",
            "Ziyan Huang",
            "Yuxuan Zhao",
            "Dexin Yu",
            "Junjun He",
            "Yefeng Zheng",
            "Ling Shao",
            "Alan Yuille",
            "Zongwei Zhou"
        ],
        "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
        "abstract": "arXiv:2510.14831v1 Announce Type: new  Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation--based on lessons from the JHH dataset--for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests.",
        "arxiv_id": "2510.14831",
        "ARXIVID": "2510.14831",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications): Focuses on large-scale tumor segmentation datasets and the use of synthetic data, but not directly about new foundation models or methods.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14406": {
        "authors": [
            "Xikai Zhang",
            "Bo Wang",
            "Likang Xiao",
            "Yongzhi Li",
            "Quan Chen",
            "Wenju Wu",
            "Liu Liu"
        ],
        "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
        "abstract": "arXiv:2510.14406v1 Announce Type: new  Abstract: Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
        "arxiv_id": "2510.14406",
        "ARXIVID": "2510.14406",
        "COMMENT": "Somewhat related to multi-agent systems and reasoning, but not directly to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Focuses on integrating MAS into a single model for reasoning and planning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14256": {
        "authors": [
            "Xiangyu Meng",
            "Zixian Zhang",
            "Zhenghao Zhang",
            "Junchao Liao",
            "Long Qin",
            "Weizhi Wang"
        ],
        "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
        "abstract": "arXiv:2510.14256v1 Announce Type: new  Abstract: While advanced methods like VACE and Phantom have advanced video generation for specific subjects in diverse scenarios, they struggle with multi-human identity preservation in dynamic interactions, where consistent identities across multiple characters are critical. To address this, we propose Identity-GRPO, a human feedback-driven optimization pipeline for refining multi-human identity-preserving video generation. First, we construct a video reward model trained on a large-scale preference dataset containing human-annotated and synthetic distortion data, with pairwise annotations focused on maintaining human consistency throughout the video. We then employ a GRPO variant tailored for multi-human consistency, which greatly enhances both VACE and Phantom. Through extensive ablation studies, we evaluate the impact of annotation quality and design choices on policy optimization. Experiments show that Identity-GRPO achieves up to 18.9% improvement in human consistency metrics over baseline methods, offering actionable insights for aligning reinforcement learning with personalized video generation.",
        "arxiv_id": "2510.14256",
        "ARXIVID": "2510.14256",
        "COMMENT": "Related to generative modeling and video generation, but not directly to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. Focuses on multi-human identity preservation in video generation.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14253": {
        "authors": [
            "Wangtao Sun",
            "Xiang Cheng",
            "Jialin Fan",
            "Yao Xu",
            "Xing Yu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "title": "Towards Agentic Self-Learning LLMs in Search Environment",
        "abstract": "arXiv:2510.14253v1 Announce Type: new  Abstract: We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \\textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning",
        "arxiv_id": "2510.14253",
        "ARXIVID": "2510.14253",
        "COMMENT": "Somewhat related to agentic learning and LLMs, but not directly to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Focuses on self-learning in LLM-based agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14241": {
        "authors": [
            "Soumyya Kanti Datta",
            "Tanvi Ranga",
            "Chengzhe Sun",
            "Siwei Lyu"
        ],
        "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis",
        "abstract": "arXiv:2510.14241v1 Announce Type: new  Abstract: The rise of manipulated media has made deepfakes a particularly insidious threat, involving various generative manipulations such as lip-sync modifications, face-swaps, and avatar-driven facial synthesis. Conventional detection methods, which predominantly depend on manually designed phoneme-viseme alignment thresholds, fundamental frame-level consistency checks, or a unimodal detection strategy, inadequately identify modern-day deepfakes generated by advanced generative models such as GANs, diffusion models, and neural rendering techniques. These advanced techniques generate nearly perfect individual frames yet inadvertently create minor temporal discrepancies frequently overlooked by traditional detectors. We present a novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic Analysis(PIA), incorporating language, dynamic face motion, and facial identification cues to address these limitations. We utilize phoneme sequences, lip geometry data, and advanced facial identity embeddings. This integrated method significantly improves the detection of subtle deepfake alterations by identifying inconsistencies across multiple complementary modalities. Code is available at https://github.com/skrantidatta/PIA",
        "arxiv_id": "2510.14241",
        "ARXIVID": "2510.14241",
        "COMMENT": "Somewhat related to multi-modal learning and generative model detection, but not directly to the listed criteria. Focuses on deepfake detection using multi-modal cues.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14265": {
        "authors": [
            "Xukai Wang",
            "Xuanbo Liu",
            "Mingrui Chen",
            "Haitian Zhong",
            "Xuanlin Yang",
            "Bohan Zeng",
            "Jinbo Hu",
            "Hao Liang",
            "Junbo Niu",
            "Xuchen Li",
            "Ruitao Wu",
            "Ruichuan An",
            "Yang Shi",
            "Liu Liu",
            "Xu-Yao Zhang",
            "Qiang Liu",
            "Zhouchen Lin",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
        "abstract": "arXiv:2510.14265v1 Announce Type: new  Abstract: With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.",
        "arxiv_id": "2510.14265",
        "ARXIVID": "2510.14265",
        "COMMENT": "Partially matches criterion 3 (new benchmark for reasoning in large models), but is not focused on embodied AI or spatial intelligence. More about reasoning evaluation.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.14564": {
        "authors": [
            "Junyi Wu",
            "Jiaming Xu",
            "Jinhao Li",
            "Yongkang Zhou",
            "Jiayi Pan",
            "Xingyang Li",
            "Guohao Dai"
        ],
        "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
        "abstract": "arXiv:2510.14564v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.   To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.   Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
        "arxiv_id": "2510.14564",
        "ARXIVID": "2510.14564",
        "COMMENT": "This paper proposes BalanceGS, an algorithm-system co-design for efficient 3D Gaussian Splatting training on GPU. While it is a technical improvement in 3D reconstruction, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. It is relevant to computer vision and graphics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14976": {
        "authors": [
            "Shaowei Liu",
            "Chuan Guo",
            "Bing Zhou",
            "Jian Wang"
        ],
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
        "abstract": "arXiv:2510.14976v1 Announce Type: new  Abstract: Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
        "arxiv_id": "2510.14976",
        "ARXIVID": "2510.14976",
        "COMMENT": "This paper introduces Ponimator, a framework for human-human interaction animation using pose priors and conditional diffusion models. While it is relevant to generative modeling and computer vision, it does not directly address the specific criteria of spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14668": {
        "authors": [
            "Md. Abdur Rahman",
            "Mohaimenul Azam Khan Raiaan",
            "Sami Azam",
            "Asif Karim",
            "Jemima Beissbarth",
            "Amanda Leach"
        ],
        "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging",
        "abstract": "arXiv:2510.14668v1 Announce Type: new  Abstract: Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.",
        "arxiv_id": "2510.14668",
        "ARXIVID": "2510.14668",
        "COMMENT": "This paper proposes a new weakly-supervised chained distillation network for multimodal medical imaging, which is a methodological improvement in knowledge distillation for multimodal models. However, it is focused on medical imaging and not directly on spatial intelligence, embodied agents, or VLLMs/MLLMs. It is relevant to general computer vision and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14648": {
        "authors": [
            "Xinyao Liao",
            "Xianfang Zeng",
            "Ziye Song",
            "Zhoujie Fu",
            "Gang Yu",
            "Guosheng Lin"
        ],
        "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "abstract": "arXiv:2510.14648v1 Announce Type: new  Abstract: Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
        "arxiv_id": "2510.14648",
        "ARXIVID": "2510.14648",
        "COMMENT": "This paper proposes a new in-context learning strategy for instruction-based video editing using unpaired clips. While it is a novel generative modeling approach for video, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models as per the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14962": {
        "authors": [
            "Thao Nguyen",
            "Jiaqi Ma",
            "Fahad Shahbaz Khan",
            "Souhaib Ben Taieb",
            "Salman Khan"
        ],
        "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
        "abstract": "arXiv:2510.14962v1 Announce Type: new  Abstract: Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
        "arxiv_id": "2510.14962",
        "ARXIVID": "2510.14962",
        "COMMENT": "RainDiff introduces a token-wise attention diffusion model for precipitation nowcasting. While it is a novel generative model for spatio-temporal prediction, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, or vision foundation models as per the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14588": {
        "authors": [
            "Zhifei Chen",
            "Tianshuo Xu",
            "Leyi Wu",
            "Luozhou Wang",
            "Dongyu Yan",
            "Zihan You",
            "Wenting Luo",
            "Guo Zhang",
            "Yingcong Chen"
        ],
        "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
        "abstract": "arXiv:2510.14588v1 Announce Type: new  Abstract: Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.",
        "arxiv_id": "2510.14588",
        "ARXIVID": "2510.14588",
        "COMMENT": "STANCE proposes a new method for motion-coherent video generation using sparse-to-dense anchored encoding. While it is a generative model for video, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, or benchmarks for embodied AI. It is relevant to vision foundation models and generative modeling, but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.14576": {
        "authors": [
            "Dongwook Lee",
            "Sol Han",
            "Jinwhan Kim"
        ],
        "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification",
        "abstract": "arXiv:2510.14576v1 Announce Type: new  Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based multi-branch neural network for vehicle re-identification. The proposed model addresses the challenge of learning discriminative and complementary features from three-dimensional point clouds to distinguish between vehicles. CALM-Net employs a multi-branch architecture that integrates edge convolution, point attention, and a curvature embedding that characterizes local surface variation in point clouds. By combining these mechanisms, the model learns richer geometric and contextual features that are well suited for the re-identification task. Experimental evaluation on the large-scale nuScenes dataset demonstrates that CALM-Net achieves a mean re-identification accuracy improvement of approximately 1.97\\% points compared with the strongest baseline in our study. The results confirms the effectiveness of incorporating curvature information into deep learning architectures and highlight the benefit of multi-branch feature learning for LiDAR point cloud-based vehicle re-identification.",
        "arxiv_id": "2510.14576",
        "ARXIVID": "2510.14576",
        "COMMENT": "Somewhat relevant to general computer vision and machine learning interests, but does not directly match any specific criterion. The focus is on LiDAR point cloud-based vehicle re-identification using curvature-aware features, which is a technical improvement in 3D vision but not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.14726": {
        "authors": [
            "Dingzhou Xie",
            "Rushi Lan",
            "Cheng Pang",
            "Enhao Ning",
            "Jiahao Zeng",
            "Wei Zheng"
        ],
        "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
        "abstract": "arXiv:2510.14726v1 Announce Type: new  Abstract: Recent object detection methods have made remarkable progress by leveraging attention mechanisms to improve feature discriminability. However, most existing approaches are confined to refining single-layer or fusing dual-layer features, overlooking the rich inter-layer dependencies across multi-scale representations. This limits their ability to capture comprehensive contextual information essential for detecting objects with large scale variations. In this paper, we propose a novel Cross-Layer Feature Self-Attention Module (CFSAM), which holistically models both local and global dependencies within multi-scale feature maps. CFSAM consists of three key components: a convolutional local feature extractor, a Transformer-based global modeling unit that efficiently captures cross-layer interactions, and a feature fusion mechanism to restore and enhance the original representations. When integrated into the SSD300 framework, CFSAM significantly boosts detection performance, achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the module accelerates convergence during training without introducing substantial computational overhead. Our work highlights the importance of explicit cross-layer attention modeling in advancing multi-scale object detection.",
        "arxiv_id": "2510.14726",
        "ARXIVID": "2510.14726",
        "COMMENT": "Related to computer vision and attention mechanisms, but does not directly match any of the four criteria (not about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.14240": {
        "authors": [
            "Jiayu Wang",
            "Yifei Ming",
            "Riya Dulepet",
            "Qinglin Chen",
            "Austin Xu",
            "Zixuan Ke",
            "Frederic Sala",
            "Aws Albarghouthi",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild",
        "abstract": "arXiv:2510.14240v1 Announce Type: new  Abstract: Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.",
        "arxiv_id": "2510.14240",
        "ARXIVID": "2510.14240",
        "COMMENT": "Does not directly match any criteria. Focuses on benchmarking agentic systems for deep research, but not specifically on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.14403": {
        "authors": [
            "Chao Tu",
            "Kun Huang",
            "Jie Zhang",
            "Qianjin Feng",
            "Yu Zhang",
            "Zhenyuan Ning"
        ],
        "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis",
        "abstract": "arXiv:2510.14403v1 Announce Type: new  Abstract: The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning model, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at https://github.com/tuuuc/DCMIL.",
        "arxiv_id": "2510.14403",
        "ARXIVID": "2510.14403",
        "COMMENT": "DCMIL is a progressive representation learning model for whole slide images in cancer prognosis. It is a vision model with some novel learning strategies, but does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.14314": {
        "authors": [
            "Shivangi Yadav",
            "Arun Ross"
        ],
        "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
        "abstract": "arXiv:2510.14314v1 Announce Type: new  Abstract: An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method.",
        "arxiv_id": "2510.14314",
        "ARXIVID": "2510.14314",
        "COMMENT": "This paper presents a new generative framework (MID-StyleGAN) for creating synthetic ocular images, combining diffusion models and GANs. While it is a generative model and uses vision techniques, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models as per the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.13985": {
        "authors": [
            "Mar\\'ia Victoria Carro",
            "Denise Alejandra Mester",
            "Francisca Gauna Selasco",
            "Giovanni Franco Gabriel Marraffini",
            "Mario Alejandro Leiva",
            "Gerardo I. Simari",
            "Mar\\'ia Vanina Martinez"
        ],
        "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment",
        "abstract": "arXiv:2510.13985v1 Announce Type: new  Abstract: Causal learning is the cognitive process of developing the capability of making causal inferences based on available information, often guided by normative principles. This process is prone to errors and biases, such as the illusion of causality, in which people perceive a causal relationship between two variables despite lacking supporting evidence. This cognitive bias has been proposed to underlie many societal problems, including social prejudice, stereotype formation, misinformation, and superstitious thinking. In this work, we examine whether large language models are prone to developing causal illusions when faced with a classic cognitive science paradigm: the contingency judgment task. To investigate this, we constructed a dataset of 1,000 null contingency scenarios (in which the available information is not sufficient to establish a causal relationship between variables) within medical contexts and prompted LLMs to evaluate the effectiveness of potential causes. Our findings show that all evaluated models systematically inferred unwarranted causal relationships, revealing a strong susceptibility to the illusion of causality. While there is ongoing debate about whether LLMs genuinely understand causality or merely reproduce causal language without true comprehension, our findings support the latter hypothesis and raise concerns about the use of language models in domains where accurate causal reasoning is essential for informed decision-making.",
        "arxiv_id": "2510.13985",
        "ARXIVID": "2510.13985",
        "COMMENT": "Does not match any specific criterion. Focuses on biases in causal learning in LLMs, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.14460": {
        "authors": [
            "Sven Jacob",
            "Weijia Shao",
            "Gjergji Kasneci"
        ],
        "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
        "abstract": "arXiv:2510.14460v1 Announce Type: new  Abstract: Video-based object detection plays a vital role in safety-critical applications. While deep learning-based object detectors have achieved impressive performance, they remain vulnerable to adversarial attacks, particularly those involving universal perturbations. In this work, we propose a minimally distorted universal adversarial attack tailored for video object detection, which leverages nuclear norm regularization to promote structured perturbations concentrated in the background. To optimize this formulation efficiently, we employ an adaptive, optimistic exponentiated gradient method that enhances both scalability and convergence. Our results demonstrate that the proposed attack outperforms both low-rank projected gradient descent and Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness. All code and data are publicly available at https://github.com/jsve96/AO-Exp-Attack.",
        "arxiv_id": "2510.14460",
        "ARXIVID": "2510.14460",
        "COMMENT": "The paper proposes a new universal adversarial attack for video object detection. While it is relevant to computer vision and machine learning, it does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.14143": {
        "authors": [
            "Alexandr A. Kalinin",
            "Anne E. Carpenter",
            "Shantanu Singh",
            "Matthew J. O'Meara"
        ],
        "title": "cubic: CUDA-accelerated 3D Bioimage Computing",
        "abstract": "arXiv:2510.14143v1 Announce Type: new  Abstract: Quantitative analysis of multidimensional biological images is useful for understanding complex cellular phenotypes and accelerating advances in biomedical research. As modern microscopy generates ever-larger 2D and 3D datasets, existing computational approaches are increasingly limited by their scalability, efficiency, and integration with modern scientific computing workflows. Existing bioimage analysis tools often lack application programmable interfaces (APIs), do not support graphics processing unit (GPU) acceleration, lack broad 3D image processing capabilities, and/or have poor interoperability for compute-heavy workflows. Here, we introduce cubic, an open-source Python library that addresses these challenges by augmenting widely used SciPy and scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM. cubic's API is device-agnostic and dispatches operations to GPU when data reside on the device and otherwise executes on CPU, seamlessly accelerating a broad range of image processing routines. This approach enables GPU acceleration of existing bioimage analysis workflows, from preprocessing to segmentation and feature extraction for 2D and 3D data. We evaluate cubic both by benchmarking individual operations and by reproducing existing deconvolution and segmentation pipelines, achieving substantial speedups while maintaining algorithmic fidelity. These advances establish a robust foundation for scalable, reproducible bioimage analysis that integrates with the broader Python scientific computing ecosystem, including other GPU-accelerated methods, enabling both interactive exploration and automated high-throughput analysis workflows. cubic is openly available at https://github$.$com/alxndrkalinin/cubic",
        "arxiv_id": "2510.14143",
        "ARXIVID": "2510.14143",
        "COMMENT": "Does not match any specific criterion. Focuses on GPU-accelerated bioimage analysis, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}