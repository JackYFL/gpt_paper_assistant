{
    "2512.23365": {
        "authors": [
            "Kanghee Lee",
            "Injae Lee",
            "Minseok Kwak",
            "Kwonyoung Ryu",
            "Jungi Hong",
            "Jaesik Park"
        ],
        "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility",
        "abstract": "arXiv:2512.23365v1 Announce Type: new  Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.",
        "arxiv_id": "2512.23365",
        "ARXIVID": "2512.23365",
        "COMMENT": "Matches criteria 1 and 3: New methodological improvements to spatial understanding/spatial intelligence on embodied agents, and a new benchmark for spatial reasoning in VLMs/MLLMs. Proposes a new dataset (SpatialMosaic) and benchmark (SpatialMosaic-Bench) for multi-view spatial reasoning under partial visibility, and a hybrid VLM framework for robust spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2512.23568": {
        "authors": [
            "Siyu Jiao",
            "Yiheng Lin",
            "Yujie Zhong",
            "Qi She",
            "Wei Zhou",
            "Xiaohan Lan",
            "Zilong Huang",
            "Fei Yu",
            "Yingchen Yu",
            "Yunqing Zhao",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "ThinkGen: Generalized Thinking for Visual Generation",
        "abstract": "arXiv:2512.23568v1 Announce Type: new  Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen",
        "arxiv_id": "2512.23568",
        "ARXIVID": "2512.23568",
        "COMMENT": "Matches criterion 2: Presents ThinkGen, a new MLLM-based framework for visual generation that leverages chain-of-thought reasoning, with a novel training paradigm and strong results across benchmarks. The decoupled MLLM + DiT architecture is a notable methodological contribution.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.23646": {
        "authors": [
            "Keda Tao",
            "Wenjie Du",
            "Bohan Yu",
            "Weiqiang Wang",
            "Jian Liu",
            "Huan Wang"
        ],
        "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
        "abstract": "arXiv:2512.23646v1 Announce Type: new  Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
        "arxiv_id": "2512.23646",
        "ARXIVID": "2512.23646",
        "COMMENT": "Matches criteria 2 and 3: Introduces OmniAgent, an omnimodal (audio-visual) large language model agent for active perception, with a new paradigm for audio-guided reasoning and dynamic tool orchestration. Shows strong empirical results and advances agentic multimodal understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.23044": {
        "authors": [
            "Zhengyang Liang",
            "Yan Shu",
            "Xiangrui Liu",
            "Minghao Qin",
            "Kaixin Liang",
            "Paolo Rota",
            "Nicu Sebe",
            "Zheng Liu",
            "Lizi Liao"
        ],
        "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
        "abstract": "arXiv:2512.23044v1 Announce Type: new  Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \\textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.",
        "arxiv_id": "2512.23044",
        "ARXIVID": "2512.23044",
        "COMMENT": "Matches criterion 3: This paper introduces a new benchmark (Video-BrowseComp) for agentic video reasoning on the open web, focusing on proactive video reasoning and visual grounding, which previous benchmarks ignored. It also provides surprising empirical results showing current models' limitations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.22626": {
        "authors": [
            "Yuming Gu",
            "Yizhi Wang",
            "Yining Hong",
            "Yipeng Gao",
            "Hao Jiang",
            "Angtian Wang",
            "Bo Liu",
            "Nathaniel S. Dennler",
            "Zhengfei Kuang",
            "Hao Li",
            "Gordon Wetzstein",
            "Chongyang Ma"
        ],
        "title": "Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion",
        "abstract": "arXiv:2512.22626v1 Announce Type: new  Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.",
        "arxiv_id": "2512.22626",
        "ARXIVID": "2512.22626",
        "COMMENT": "Matches criterion 1 and 3: Proposes Envision, a diffusion-based visual planning framework for embodied agents, introducing goal-conditioned video generation for spatially consistent planning, which is a novel method for embodied AI and spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.23374": {
        "authors": [
            "Yifei Li",
            "Haoyuan He",
            "Yu Zheng",
            "Bingyao Yu",
            "Wenzhao Zheng",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization",
        "abstract": "arXiv:2512.23374v1 Announce Type: new  Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.",
        "arxiv_id": "2512.23374",
        "ARXIVID": "2512.23374",
        "COMMENT": "Directly matches criterion 3 (embodied AI, new benchmark/simulator) by introducing NeXT-IMDL, a large-scale diagnostic benchmark for next-generation image manipulation detection and localization, with rigorous cross-dimension evaluation protocols revealing generalization failures.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.22799": {
        "authors": [
            "Jingchao Wang",
            "Kaiwen Zhou",
            "Zhijian Wu",
            "Kunhua Ji",
            "Dingjiang Huang",
            "Yefeng Zheng"
        ],
        "title": "VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM",
        "abstract": "arXiv:2512.22799v1 Announce Type: new  Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.",
        "arxiv_id": "2512.22799",
        "ARXIVID": "2512.22799",
        "COMMENT": "Directly matches criterion 2 (new MLLMs) and criterion 1 (spatial intelligence on embodied agents) by introducing a global vision-language tracking framework using MLLMs and a novel location-aware visual prompting mechanism for robust object tracking.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.22615": {
        "authors": [
            "Jiacheng Ye",
            "Shansan Gong",
            "Jiahui Gao",
            "Junming Fan",
            "Shuang Wu",
            "Wei Bi",
            "Haoli Bai",
            "Lifeng Shang",
            "Lingpeng Kong"
        ],
        "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
        "abstract": "arXiv:2512.22615v1 Announce Type: new  Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $\\pi_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
        "arxiv_id": "2512.22615",
        "ARXIVID": "2512.22615",
        "COMMENT": "Matches criteria 2 and 4: Shows new VLLMs/MLLMs (Dream-VL, Dream-VLA) and applies diffusion-based LLMs to vision-language and vision-language-action tasks, including embodied AI and visual planning. Also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.22351": {
        "authors": [
            "Zhengfei Kuang",
            "Rui Lin",
            "Long Zhao",
            "Gordon Wetzstein",
            "Saining Xie",
            "Sanghyun Woo"
        ],
        "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
        "abstract": "arXiv:2512.22351v1 Announce Type: new  Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io",
        "arxiv_id": "2512.22351",
        "ARXIVID": "2512.22351",
        "COMMENT": "Strongly matches criterion 2 (new MLLMs) and criterion 1 (spatial intelligence on embodied agents). Proposes a tool-augmented multi-agent MLLM system for 3D object arrangement, with perceptual feedback and spatial reasoning, and a collaborative agentic framework. Also relevant to criterion 3 (novel embodied AI method for 3D manipulation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.22771": {
        "authors": [
            "Yiqian Li",
            "Wen Jiang",
            "Kostas Daniilidis"
        ],
        "title": "Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting",
        "abstract": "arXiv:2512.22771v1 Announce Type: new  Abstract: Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.",
        "arxiv_id": "2512.22771",
        "ARXIVID": "2512.22771",
        "COMMENT": "Matches criterion 1 and 3: Proposes a new active learning algorithm for view selection in semantic and dynamic 3D Gaussian splatting, directly addressing spatial understanding and providing a principled alternative to previous heuristic methods for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.23180": {
        "authors": [
            "Tianchen Deng",
            "Xuefeng Chen",
            "Yi Chen",
            "Qu Chen",
            "Yuyao Xu",
            "Lijin Yang",
            "Le Xu",
            "Yu Zhang",
            "Bo Zhang",
            "Wuxiong Huang",
            "Hesheng Wang"
        ],
        "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
        "abstract": "arXiv:2512.23180v1 Announce Type: new  Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.",
        "arxiv_id": "2512.23180",
        "ARXIVID": "2512.23180",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and criterion 2 (new VLLMs/MLLMs) by introducing a 3D Gaussian-based driving world model for unified scene understanding and multi-modal generation, with early modality alignment and language-guided sampling.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.22830": {
        "authors": [
            "Zirui Zhou",
            "Junfeng Ni",
            "Shujie Zhang",
            "Yixin Chen",
            "Siyuan Huang"
        ],
        "title": "3D Scene Change Modeling With Consistent Multi-View Aggregation",
        "abstract": "arXiv:2512.22830v1 Announce Type: new  Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.",
        "arxiv_id": "2512.22830",
        "ARXIVID": "2512.22830",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/methods for spatial change detection) by proposing a novel 3D scene change detection framework (SCaR-3D) and introducing a new synthetic dataset (CCS3D) for controlled evaluation of 3D change types.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.22808": {
        "authors": [
            "Libo Zhang",
            "Zekun Li",
            "Tianyu Li",
            "Zeyu Cao",
            "Rui Xu",
            "Xiaoxiao Long",
            "Wenjia Wang",
            "Jingbo Wang",
            "Yuan Liu",
            "Wenping Wang",
            "Daquan Zhou",
            "Taku Komura",
            "Zhiyang Dou"
        ],
        "title": "EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation",
        "abstract": "arXiv:2512.22808v1 Announce Type: new  Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.",
        "arxiv_id": "2512.22808",
        "ARXIVID": "2512.22808",
        "COMMENT": "Matches criterion 3: Embodied AI with a new benchmark and method. Introduces a new dataset (HRD) and a novel autoregressive framework (EgoReAct) for egocentric video-driven 3D human reaction generation, focusing on spatial alignment and causality, which are under-explored angles.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.23705": {
        "authors": [
            "Shaocong Xu",
            "Songlin Wei",
            "Qizhe Wei",
            "Zheng Geng",
            "Hong Li",
            "Licheng Shen",
            "Qianpu Sun",
            "Shu Han",
            "Bin Ma",
            "Bohan Li",
            "Chongjie Ye",
            "Yuhang Zheng",
            "Nan Wang",
            "Saining Zhang",
            "Hao Zhao"
        ],
        "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
        "abstract": "arXiv:2512.23705v1 Announce Type: new  Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
        "arxiv_id": "2512.23705",
        "ARXIVID": "2512.23705",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and generative modeling in multi-modal learning. Repurposes video diffusion models for transparent object depth/normal estimation, showing strong empirical results and a new synthetic dataset. Also relevant to spatial understanding (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.22877": {
        "authors": [
            "Ju-Hsuan Weng",
            "Jia-Wei Liao",
            "Cheng-Fu Chou",
            "Jun-Cheng Chen"
        ],
        "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
        "abstract": "arXiv:2512.22877v1 Announce Type: new  Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
        "arxiv_id": "2512.22877",
        "ARXIVID": "2512.22877",
        "COMMENT": "Matches criterion 4: Introduces M-ErasureBench, a comprehensive multimodal benchmark for concept erasure in diffusion models, and proposes a new method (IRECE) for robust concept erasure, directly relevant to vision foundation models and their safety.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.21482": {
        "authors": [
            "Fanwei Zeng",
            "Changtao Miao",
            "Jing Huang",
            "Zhiya Tan",
            "Shutao Gong",
            "Xiaoming Yu",
            "Yang Wang",
            "Huazhe Tan",
            "Weibin Yao",
            "Jianshu Li"
        ],
        "title": "LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis",
        "abstract": "arXiv:2512.21482v1 Announce Type: new  Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.",
        "arxiv_id": "2512.21482",
        "ARXIVID": "2512.21482",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications) by introducing LogicLens, a unified visual-logical co-reasoning framework for text-centric forgery analysis, with a novel cross-cues-aware chain of thought mechanism and a new dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.22374": {
        "authors": [
            "Xin Yu",
            "Xiaojuan Qi",
            "Zhengqi Li",
            "Kai Zhang",
            "Richard Zhang",
            "Zhe Lin",
            "Eli Shechtman",
            "Tianyu Wang",
            "Yotam Nitzan"
        ],
        "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
        "abstract": "arXiv:2512.22374v1 Announce Type: new  Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.",
        "arxiv_id": "2512.22374",
        "ARXIVID": "2512.22374",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. Introduces a new text-to-image generation model (Self-E) with a novel self-evaluation mechanism, relevant for generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.22315": {
        "authors": [
            "Yang Ding",
            "Yizhen Zhang",
            "Xin Lai",
            "Ruihang Chu",
            "Yujiu Yang"
        ],
        "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning",
        "abstract": "arXiv:2512.22315v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.",
        "arxiv_id": "2512.22315",
        "ARXIVID": "2512.22315",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 1 (spatial/temporal intelligence on embodied agents). Proposes VideoZoomer, an agentic MLLM framework for long video reasoning with reinforcement-learned temporal focusing, enabling dynamic evidence gathering and surpassing existing models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.22624": {
        "authors": [
            "Mohamad Alansari",
            "Muzammal Naseer",
            "Hasan Al Marzouqi",
            "Naoufel Werghi",
            "Sajid Javed"
        ],
        "title": "Rethinking Memory Design in SAM-Based Visual Object Tracking",
        "abstract": "arXiv:2512.22624v1 Announce Type: new  Abstract: \\noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \\textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}",
        "arxiv_id": "2512.22624",
        "ARXIVID": "2512.22624",
        "COMMENT": "Matches criterion 1: The paper presents a systematic study and new unified memory framework for SAM-based visual object tracking, focusing on spatial memory design for robust tracking, which is a core aspect of spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.23176": {
        "authors": [
            "Yi Zhang",
            "Yi Wang",
            "Lei Yao",
            "Lap-Pui Chau"
        ],
        "title": "GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection",
        "abstract": "arXiv:2512.23176v1 Announce Type: new  Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).",
        "arxiv_id": "2512.23176",
        "ARXIVID": "2512.23176",
        "COMMENT": "Matches criterion 1 (spatial understanding) and criterion 4 (vision foundation models and applications) by proposing a synergistic Gaussian-Voxel representation for 3D object detection from images, improving spatial context modeling without depth supervision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.23486": {
        "authors": [
            "Mingyuan Jiu",
            "Hailong Zhu",
            "Wenchuan Wei",
            "Hichem Sahbi",
            "Rongrong Ji",
            "Mingliang Xu"
        ],
        "title": "Multi-label Classification with Panoptic Context Aggregation Networks",
        "abstract": "arXiv:2512.23486v1 Announce Type: new  Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.",
        "arxiv_id": "2512.23486",
        "ARXIVID": "2512.23486",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding) by introducing a new network (PanCAN) for multi-label classification that hierarchically integrates multi-order geometric contexts and cross-scale feature aggregation, improving complex scene understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.23437": {
        "authors": [
            "Shuhong Liu",
            "Chenyu Bao",
            "Ziteng Cui",
            "Yun Liu",
            "Xuangeng Chu",
            "Lin Gu",
            "Marcos V. Conde",
            "Ryo Umagami",
            "Tomohiro Hashimoto",
            "Zijian Hu",
            "Tianhan Xu",
            "Yuan Gan",
            "Yusuke Kurose",
            "Tatsuya Harada"
        ],
        "title": "RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction",
        "abstract": "arXiv:2512.23437v1 Announce Type: new  Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.",
        "arxiv_id": "2512.23437",
        "ARXIVID": "2512.23437",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator for multi-view visual restoration and 3D reconstruction under real-world degradations). RealX3D is a new, physically-degraded 3D benchmark with comprehensive data and analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.23545": {
        "authors": [
            "Shengyi Hua",
            "Jianfeng Wu",
            "Tianle Shen",
            "Kangzhe Hu",
            "Zhongzhen Huang",
            "Shujuan Ni",
            "Zhihong Zhang",
            "Yuan Li",
            "Zhe Wang",
            "Xiaofan Zhang"
        ],
        "title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis",
        "abstract": "arXiv:2512.23545v1 Announce Type: new  Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.",
        "arxiv_id": "2512.23545",
        "ARXIVID": "2512.23545",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes PathFound, an agentic multimodal model for evidence-seeking in pathological diagnosis, integrating visual foundation models, VLMs, and RL-based reasoning for improved diagnostic workflows.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.22310": {
        "authors": [
            "Run Ling",
            "Ke Cao",
            "Jian Lu",
            "Ao Ma",
            "Haowei Liu",
            "Runze He",
            "Changwei Wang",
            "Rongtao Xu",
            "Yihua Shao",
            "Zhanjie Zhang",
            "Peng Wu",
            "Guibing Guo",
            "Wei Feng",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law",
            "Xingwei Wang"
        ],
        "title": "MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation",
        "abstract": "arXiv:2512.22310v1 Announce Type: new  Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.",
        "arxiv_id": "2512.22310",
        "ARXIVID": "2512.22310",
        "COMMENT": "Related to criterion 4: Proposes MoFu, a new framework for multi-subject video generation with novel scale-aware and permutation-invariant mechanisms, and introduces a new benchmark for evaluating these challenges. The use of LLM-guided modules and Fourier Fusion is a clever methodological improvement.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.22969": {
        "authors": [
            "Behnam Raoufi",
            "Hossein Sharify",
            "Mohamad Mahdee Ramezanee",
            "Khosrow Hajsadeghi",
            "Saeed Bagheri Shouraki"
        ],
        "title": "CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision",
        "abstract": "arXiv:2512.22969v1 Announce Type: new  Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.",
        "arxiv_id": "2512.22969",
        "ARXIVID": "2512.22969",
        "COMMENT": "Matches criterion 4: CLIP-Joint-Detect integrates CLIP-style contrastive vision-language supervision into object detectors, which is a direct application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2512.23537": {
        "authors": [
            "Binhe Yu",
            "Zhen Wang",
            "Kexin Li",
            "Yuqian Yuan",
            "Wenqiao Zhang",
            "Long Chen",
            "Juncheng Li",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization",
        "abstract": "arXiv:2512.23537v1 Announce Type: new  Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.",
        "arxiv_id": "2512.23537",
        "ARXIVID": "2512.23537",
        "COMMENT": "Matches criterion 4: AnyMS is a training-free, layout-guided multi-subject customization framework leveraging diffusion models and attention decoupling, which is a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.23273": {
        "authors": [
            "Xu Lin",
            "Jinlong Peng",
            "Zhenye Gan",
            "Jiawen Zhu",
            "Jun Liu"
        ],
        "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
        "abstract": "arXiv:2512.23273v1 Announce Type: new  Abstract: Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.",
        "arxiv_id": "2512.23273",
        "ARXIVID": "2512.23273",
        "COMMENT": "Matches criterion 4: Proposes YOLO-Master, a new real-time object detection framework using Mixture-of-Experts and specialized transformers, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.21699": {
        "authors": [
            "Eranga Bandara",
            "Tharaka Hewa",
            "Ross Gore",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Peter Foytik",
            "Abdul Rahman",
            "Safdar H. Bouk",
            "Xueping Liang",
            "Amin Hass",
            "Sachini Rajapakse",
            "Ng Wee Keong",
            "Kasun De Zoysa",
            "Aruna Withanage",
            "Nilaan Loganathan"
        ],
        "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning",
        "abstract": "arXiv:2512.21699v1 Announce Type: new  Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.",
        "arxiv_id": "2512.21699",
        "ARXIVID": "2512.21699",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and touches on criterion 1 (spatial intelligence in agentic AI) via consensus-driven reasoning with LLMs and VLMs in agentic workflows. The focus is on responsible and explainable agentic AI, with a novel architecture for multi-model consensus and reasoning-layer governance.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.22796": {
        "authors": [
            "Ruoyu Wang",
            "Ziyu Li",
            "Beier Zhu",
            "Liangyu Yuan",
            "Hanwang Zhang",
            "Xun Yang",
            "Xiaojun Chang",
            "Chi Zhang"
        ],
        "title": "Parallel Diffusion Solver via Residual Dirichlet Policy Optimization",
        "abstract": "arXiv:2512.22796v1 Announce Type: new  Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.",
        "arxiv_id": "2512.22796",
        "ARXIVID": "2512.22796",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. Proposes a new ODE solver for diffusion models, improving generative modeling efficiency, which is relevant for vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.22878": {
        "authors": [
            "Hasan Faraz Khan",
            "Noor Fatima",
            "Muzammil Behzad"
        ],
        "title": "SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation",
        "abstract": "arXiv:2512.22878v1 Announce Type: new  Abstract: The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.",
        "arxiv_id": "2512.22878",
        "ARXIVID": "2512.22878",
        "COMMENT": "Matches criterion 2: Shows a new MLLM (SwinTF3D) for text-guided 3D medical image segmentation, integrating vision and language for spatial understanding. Also relevant to criterion 4 (application of vision foundation models).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.21583": {
        "authors": [
            "Zelin Zang",
            "Wenyi Gu",
            "Siqi Ma",
            "Dan Yang",
            "Yue Shen",
            "Zhu Zhang",
            "Guohui Fan",
            "Wing-Kuen Ling",
            "Fuji Yang"
        ],
        "title": "A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning",
        "abstract": "arXiv:2512.21583v1 Announce Type: new  Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.",
        "arxiv_id": "2512.21583",
        "ARXIVID": "2512.21583",
        "COMMENT": "Matches criterion 2: Shows a new VLLM/MLLM (medical multimodal diagnostic framework integrating vision-language models and logic tree reasoning). Also relevant to criterion 4 (application of VLMs in medicine).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22503": {
        "authors": [
            "Xin Chen",
            "Kang Luo",
            "Yangyi Xiao",
            "Hesheng Wang"
        ],
        "title": "SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration",
        "abstract": "arXiv:2512.22503v1 Announce Type: new  Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.",
        "arxiv_id": "2512.22503",
        "ARXIVID": "2512.22503",
        "COMMENT": "Matches criterion 3 (embodied AI, new method for 3D detection in lunar exploration, with novel attention mechanism for small object detection). Also uses multimodal fusion, relevant to spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22525": {
        "authors": [
            "Bin Xia",
            "Bohao Peng",
            "Jiyang Liu",
            "Sitong Wu",
            "Jingyao Li",
            "Junjia Huang",
            "Xu Zhao",
            "Yitong Wang",
            "Ruihang Chu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "title": "DreamOmni3: Scribble-based Editing and Generation",
        "abstract": "arXiv:2512.22525v1 Announce Type: new  Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.",
        "arxiv_id": "2512.22525",
        "ARXIVID": "2512.22525",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces DreamOmni3, a unified model for scribble-based editing and generation, with a novel data synthesis pipeline and joint input scheme for complex multimodal editing tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22217": {
        "authors": [
            "Abdellah Zakaria Sellam",
            "Salah Eddine Bekhouche",
            "Fadi Dornaika",
            "Cosimo Distante",
            "Abdenour Hadid"
        ],
        "title": "VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition",
        "abstract": "arXiv:2512.22217v1 Announce Type: new  Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.",
        "arxiv_id": "2512.22217",
        "ARXIVID": "2512.22217",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs). Presents a modular vision-language model (VLM-PAR) for pedestrian attribute recognition, leveraging SigLIP 2 encoders and cross-attention fusion for improved performance on imbalanced datasets.",
        "RELEVANCE": 8,
        "NOVELTY": 5
    },
    "2512.23035": {
        "authors": [
            "Yi Zhou",
            "Xuechao Zou",
            "Shun Zhang",
            "Kai Li",
            "Shiying Wang",
            "Jingming Chen",
            "Congyan Lang",
            "Tengfei Cao",
            "Pin Tao",
            "Yuanchun Shi"
        ],
        "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
        "abstract": "arXiv:2512.23035v1 Announce Type: new  Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
        "arxiv_id": "2512.23035",
        "ARXIVID": "2512.23035",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 1 (spatial understanding via fusion of global/local features for segmentation). Uses CLIP and DINOv3 as dual vision foundation models for remote sensing segmentation, with explicit-implicit semantic co-guidance and collaborative fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.23464": {
        "authors": [
            "Yuxin Wen",
            "Qing Shuai",
            "Di Kang",
            "Jing Li",
            "Cheng Wen",
            "Yue Qian",
            "Ningxin Jiao",
            "Changhai Chen",
            "Weijie Chen",
            "Yiran Wang",
            "Jinkun Guo",
            "Dongyue An",
            "Han Liu",
            "Yanyu Tong",
            "Chao Zhang",
            "Qing Guo",
            "Juan Chen",
            "Qiao Zhang",
            "Youyi Zhang",
            "Zihao Yao",
            "Cheng Zhang",
            "Hong Duan",
            "Xiaoping Wu",
            "Qi Chen",
            "Fei Cheng",
            "Liang Dong",
            "Peng He",
            "Hao Zhang",
            "Jiaxin Lin",
            "Chao Zhang",
            "Zhongyi Fan",
            "Yifan Li",
            "Zhichao Hu",
            "Yuhong Liu",
            "Linus",
            "Jie Jiang",
            "Xiaolong Li",
            "Linchao Bao"
        ],
        "title": "HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation",
        "abstract": "arXiv:2512.23464v1 Announce Type: new  Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.",
        "arxiv_id": "2512.23464",
        "ARXIVID": "2512.23464",
        "COMMENT": "Somewhat related to criterion 2: Presents a large-scale text-to-motion generation model (HY-Motion 1.0) with instruction-following capabilities, but not a VLLM/MLLM in the traditional sense. Also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.22653": {
        "authors": [
            "Amir El-Ghoussani",
            "Andr\\'e Kaup",
            "Nassir Navab",
            "Gustavo Carneiro",
            "Vasileios Belagiannis"
        ],
        "title": "Visual Autoregressive Modelling for Monocular Depth Estimation",
        "abstract": "arXiv:2512.22653v1 Announce Type: new  Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at \"https://github.com/AmirMaEl/VAR-Depth\".",
        "arxiv_id": "2512.22653",
        "ARXIVID": "2512.22653",
        "COMMENT": "Somewhat related to criterion 4: Proposes a new autoregressive generative model for monocular depth estimation, showing autoregressive priors as a complementary family to diffusion models for 3D vision tasks. Not a foundation model per se, but relevant to generative modeling in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.22274": {
        "authors": [
            "Leslie Gu",
            "Junhwa Hur",
            "Charles Herrmann",
            "Fangneng Zhan",
            "Todd Zickler",
            "Deqing Sun",
            "Hanspeter Pfister"
        ],
        "title": "GeCo: A Differentiable Geometric Consistency Metric for Video Generation",
        "abstract": "arXiv:2512.22274v1 Announce Type: new  Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.",
        "arxiv_id": "2512.22274",
        "ARXIVID": "2512.22274",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications). Proposes GeCo, a differentiable geometric consistency metric for video generation, used for benchmarking and as a training-free guidance loss. Focus is more on evaluation than new models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.23333": {
        "authors": [
            "Ke Niu",
            "Haiyang Yu",
            "Zhuofan Chen",
            "Zhengtao Yao",
            "Weitao Jia",
            "Xiaodong Ge",
            "Jingqun Tang",
            "Benlei Cui",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation",
        "abstract": "arXiv:2512.23333v1 Announce Type: new  Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.",
        "arxiv_id": "2512.23333",
        "ARXIVID": "2512.23333",
        "COMMENT": "Relevant to multi-modal learning and generative modeling, but does not directly match any of the four criteria. Focuses on reinforcement learning for CAD code generation with a multi-expert paradigm.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.22324": {
        "authors": [
            "Jianrong Zhang",
            "Hehe Fan",
            "Yi Yang"
        ],
        "title": "DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models",
        "abstract": "arXiv:2512.22324v1 Announce Type: new  Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.",
        "arxiv_id": "2512.22324",
        "ARXIVID": "2512.22324",
        "COMMENT": "Relevant to generative modeling and compositionality in motion generation, but does not directly match any of the four criteria. Focuses on decompositional learning for human motion generation using energy-based diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.22689": {
        "authors": [
            "Salvador Rodriguez-Sanz",
            "Monica Hernandez"
        ],
        "title": "Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors",
        "abstract": "arXiv:2512.22689v1 Announce Type: new  Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.",
        "arxiv_id": "2512.22689",
        "ARXIVID": "2512.22689",
        "COMMENT": "Does not match any specific criterion. Focuses on multimodal diffeomorphic registration with Neural ODEs, which is more about medical image registration than spatial intelligence for embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22303": {
        "authors": [
            "Noor Fatima",
            "Hasan Faraz Khan",
            "Muzammil Behzad"
        ],
        "title": "Attack-Aware Deepfake Detection under Counter-Forensic Manipulations",
        "abstract": "arXiv:2512.22303v1 Announce Type: new  Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.",
        "arxiv_id": "2512.22303",
        "ARXIVID": "2512.22303",
        "COMMENT": "Does not match any specific criterion. Focuses on deepfake detection and image forensics, which is tangential to vision foundation models but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation model applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.21540": {
        "authors": [
            "Yanhao Li",
            "Lu Ma",
            "Jiaran Zhang",
            "Lexiang Tang",
            "Wentao Zhang",
            "Guibo Luo"
        ],
        "title": "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model",
        "abstract": "arXiv:2512.21540v1 Announce Type: new  Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.",
        "arxiv_id": "2512.21540",
        "ARXIVID": "2512.21540",
        "COMMENT": "Does not match any specific criteria. Focuses on adaptive length penalty and reward shaping for LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.23436": {
        "authors": [
            "Mustafa Demetgul",
            "Sanja Lazarova Molnar"
        ],
        "title": "Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification",
        "abstract": "arXiv:2512.23436v1 Announce Type: new  Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.",
        "arxiv_id": "2512.23436",
        "ARXIVID": "2512.23436",
        "COMMENT": "Does not match any specific criteria. Focuses on road surface classification using deep learning and fuzzy logic, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}