{
    "2601.10551": {
        "authors": [
            "Luxuan Fu",
            "Chong Liu",
            "Bisheng Yang",
            "Zhen Dong"
        ],
        "title": "Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure",
        "abstract": "arXiv:2601.10551v1 Announce Type: new  Abstract: Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.",
        "arxiv_id": "2601.10551",
        "ARXIVID": "2601.10551",
        "COMMENT": "This paper proposes a domain-adapted framework for large vision-language models (VLMs) for intelligent perception of roadside infrastructure, including open-vocabulary fine-tuning, LoRA adaptation, and a dual-modality RAG module. This directly matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.10707": {
        "authors": [
            "Amir Mallak",
            "Erfan Aasi",
            "Shiva Sreeram",
            "Tsun-Hsuan Wang",
            "Daniela Rus",
            "Alaa Maalouf"
        ],
        "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
        "abstract": "arXiv:2601.10707v1 Announce Type: new  Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.",
        "arxiv_id": "2601.10707",
        "ARXIVID": "2601.10707",
        "COMMENT": "This paper proposes Stochastic-Patch-Selection for end-to-end autonomous driving using vision foundation models (BLIP2). It directly matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications), with a novel approach to improve generalization and robustness.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.09954": {
        "authors": [
            "Nahid Alam",
            "Leema Krishna Murali",
            "Siddhant Bharadwaj",
            "Patrick Liu",
            "Timothy Chung",
            "Drishti Sharma",
            "Akshata A",
            "Kranthi Kiran",
            "Wesley Tam",
            "Bala Krishna S Vegesna"
        ],
        "title": "The Spatial Blindspot of Vision-Language Models",
        "abstract": "arXiv:2601.09954v1 Announce Type: new  Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.",
        "arxiv_id": "2601.09954",
        "ARXIVID": "2601.09954",
        "COMMENT": "Matches criterion 1 (spatial understanding in VLMs/embodied agents): Investigates the spatial reasoning blindspot in vision-language models and proposes architectural changes (alternative objectives, 2D positional encodings) to improve spatial awareness, directly addressing spatial intelligence in VLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.10710": {
        "authors": [
            "Cheng Chen",
            "Yuyu Guo",
            "Pengpeng Zeng",
            "Jingkuan Song",
            "Peng Di",
            "Hang Yu",
            "Lianli Gao"
        ],
        "title": "From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion",
        "abstract": "arXiv:2601.10710v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.",
        "arxiv_id": "2601.10710",
        "ARXIVID": "2601.10710",
        "COMMENT": "This paper introduces Cross-Layer Injection (CLI), a new framework for deep vision-language fusion that enables dynamic, many-to-many connections between vision and language model layers. This is a direct match for criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), with a novel approach to multi-modal fusion.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.09879": {
        "authors": [
            "Yang Xing",
            "Jiong Wu",
            "Savas Ozdemir",
            "Ying Zhang",
            "Yang Yang",
            "Wei Shao",
            "Kuang Gong"
        ],
        "title": "MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation",
        "abstract": "arXiv:2601.09879v1 Announce Type: new  Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.",
        "arxiv_id": "2601.09879",
        "ARXIVID": "2601.09879",
        "COMMENT": "This paper introduces MedVL-SAM2, a unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation. It is a strong match for criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), and also addresses spatial reasoning (criterion 1) in 3D medical images.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.10332": {
        "authors": [
            "Siqi Kou",
            "Jiachun Jin",
            "Zetong Zhou",
            "Ye Ma",
            "Yugang Wang",
            "Quan Chen",
            "Peng Jiang",
            "Xiao Yang",
            "Jun Zhu",
            "Kai Yu",
            "Zhijie Deng"
        ],
        "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
        "abstract": "arXiv:2601.10332v1 Announce Type: new  Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
        "arxiv_id": "2601.10332",
        "ARXIVID": "2601.10332",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes a new reasoning-aware text-to-image diffusion model that leverages LLM encoders for improved semantic alignment and factual consistency in image generation. Also relevant to criterion 4 (vision foundation models and applications) due to the integration of LLMs with diffusion models for vision tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.09770": {
        "authors": [
            "Chen Chen",
            "Jiawei Shao",
            "Dakuan Lu",
            "Haoyi Hu",
            "Xiangcheng Liu",
            "Hantao Yao",
            "Wu Liu"
        ],
        "title": "GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents",
        "abstract": "arXiv:2601.09770v1 Announce Type: new  Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.",
        "arxiv_id": "2601.09770",
        "ARXIVID": "2601.09770",
        "COMMENT": "This paper presents GUI-Eyes, a tool-augmented perception framework for visual grounding in GUI agents, using RL and active perception. It is a strong match for criterion 1 (spatial understanding in embodied agents) and criterion 3 (novel methods for embodied AI agents in GUI environments).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.10168": {
        "authors": [
            "Yue Chang",
            "Rufeng Chen",
            "Zhaofan Zhang",
            "Yi Chen",
            "Sihong Xie"
        ],
        "title": "RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation",
        "abstract": "arXiv:2601.10168v1 Announce Type: new  Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.",
        "arxiv_id": "2601.10168",
        "ARXIVID": "2601.10168",
        "COMMENT": "This paper proposes RAG-3DSG, a new method for open-vocabulary 3D Scene Graph generation, with improvements in object-level recognition and aggregation for 3D scenes. This directly matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (embodied AI, new methods for 3D scene understanding relevant to robotics).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.10369": {
        "authors": [
            "Ningyu Sun",
            "Zhaolin Cai",
            "Zitong Xu",
            "Peihang Chen",
            "Huiyu Duan",
            "Yichao Yan",
            "Xiongkuo Min",
            "Xiaokang Yang"
        ],
        "title": "Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs",
        "abstract": "arXiv:2601.10369v1 Announce Type: new  Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.",
        "arxiv_id": "2601.10369",
        "ARXIVID": "2601.10369",
        "COMMENT": "This paper introduces HPE-Bench, a benchmark for human pose editing assessment, and proposes a unified framework based on layer-selective MLLMs. It matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for vision-language/embodied AI from a novel angle).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.10165": {
        "authors": [
            "Chao Huang",
            "Benfeng Wang",
            "Wei Wang",
            "Jie Wen",
            "Li Shen",
            "Wenqi Ren",
            "Yong Xu",
            "Xiaochun Cao"
        ],
        "title": "Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method",
        "abstract": "arXiv:2601.10165v1 Announce Type: new  Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.",
        "arxiv_id": "2601.10165",
        "ARXIVID": "2601.10165",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmarks/methods): Proposes a new benchmark and method for multi-stage video anomaly reasoning using MLLMs, with a focus on structured, adaptive reasoning and risk-aware decision making, which is a novel angle in video anomaly understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2601.10535": {
        "authors": [
            "Chong Liu",
            "Luxuan Fu",
            "Yang Jia",
            "Zhen Dong",
            "Bisheng Yang"
        ],
        "title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery",
        "abstract": "arXiv:2601.10535v1 Announce Type: new  Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.",
        "arxiv_id": "2601.10535",
        "ARXIVID": "2601.10535",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications): Proposes SVII-3D, a unified framework for 3D localization and comprehension from sparse street imagery, integrating spatial-attention, geometry-guided refinement, and a VLM agent for fine-grained state diagnosis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10649": {
        "authors": [
            "Darshan Singh",
            "Arsha Nagrani",
            "Kawshik Manikantan",
            "Harman Singh",
            "Dinesh Tewari",
            "Tobias Weyand",
            "Cordelia Schmid",
            "Anelia Angelova",
            "Shachi Dave"
        ],
        "title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning",
        "abstract": "arXiv:2601.10649v1 Announce Type: new  Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural",
        "arxiv_id": "2601.10649",
        "ARXIVID": "2601.10649",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmarks): Introduces CURVE, a new benchmark for multicultural and multilingual long video reasoning, with complex, culturally-situated questions and reasoning traces. This addresses a previously ignored angle (cultural/multilingual bias) in video reasoning benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2601.10497": {
        "authors": [
            "Wenqing Wang",
            "Da Li",
            "Xiatian Zhu",
            "Josef Kittler"
        ],
        "title": "mergetune: Continued fine-tuning of vision-language models",
        "abstract": "arXiv:2601.10497v1 Announce Type: new  Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \\emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\\% on base-novel generalisation without adding parameters. % We show \\emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.",
        "arxiv_id": "2601.10497",
        "ARXIVID": "2601.10497",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Introduces a new fine-tuning paradigm (MERGETUNE) for vision-language models like CLIP, addressing catastrophic forgetting and improving generalization, with a clever statistical trick (linear mode connectivity).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09851": {
        "authors": [
            "Po-han Li",
            "Shenghui Chen",
            "Ufuk Topcu",
            "Sandeep Chinchali"
        ],
        "title": "ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning",
        "abstract": "arXiv:2601.09851v1 Announce Type: new  Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\\%$ in VQA accuracy without increasing processing load.",
        "arxiv_id": "2601.09851",
        "ARXIVID": "2601.09851",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes ViSIL, a new information-theoretic metric for evaluating information loss in multimodal video captioning using VLM inference, with empirical results showing improved correlation with human and VLM performance.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.10632": {
        "authors": [
            "Chengfeng Zhao",
            "Jiazhi Shu",
            "Yubo Zhao",
            "Tianyu Huang",
            "Jiahao Lu",
            "Zekai Gu",
            "Chengwei Ren",
            "Zhiyang Dou",
            "Qing Shuai",
            "Yuan Liu"
        ],
        "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
        "abstract": "arXiv:2601.10632v1 Announce Type: new  Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
        "arxiv_id": "2601.10632",
        "ARXIVID": "2601.10632",
        "COMMENT": "This paper presents CoMoVi, a co-generative framework for 3D human motion and video generation, including a new dataset. It is relevant to vision foundation models and generative modeling (criterion 4), and also introduces a novel multi-modal approach.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.10148": {
        "authors": [
            "Xiaowei Lv",
            "Zhilin Zhang",
            "Yijun Li",
            "Yusen Huo",
            "Siyuan Ju",
            "Xuyan Li",
            "Chunxiang Hong",
            "Tianyu Wang",
            "Yongcai Wang",
            "Peng Sun",
            "Chuan Yu",
            "Jian Xu",
            "Bo Zheng"
        ],
        "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration",
        "abstract": "arXiv:2601.10148v1 Announce Type: new  Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",
        "arxiv_id": "2601.10148",
        "ARXIVID": "2601.10148",
        "COMMENT": "This paper explores using LLMs for long-sequence decision making by treating trajectories as a distinct modality and aligning them with language, which is a novel multi-modal approach. It is relevant to criterion 2 (new VLLMs/MLLMs) and also touches on criterion 3 (novel methods for decision making in embodied/agent-like settings).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.10169": {
        "authors": [
            "Boaz Carmeli",
            "Ron Meir",
            "Yonatan Belinkov"
        ],
        "title": "CtD: Composition through Decomposition in Emergent Communication",
        "abstract": "arXiv:2601.10169v1 Announce Type: new  Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed \"Composition through Decomposition\", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.",
        "arxiv_id": "2601.10169",
        "ARXIVID": "2601.10169",
        "COMMENT": "This paper proposes a new method (Composition through Decomposition) for compositional generalization in neural agents describing images, with zero-shot generalization. This is a methodological improvement in multi-modal learning and compositionality, relevant to spatial understanding and embodied agents (criterion 1), though not directly about embodied AI or VLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2601.09812": {
        "authors": [
            "Carlo Sgaravatti",
            "Riccardo Pieroni",
            "Matteo Corno",
            "Sergio M. Savaresi",
            "Luca Magri",
            "Giacomo Boracchi"
        ],
        "title": "LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving",
        "abstract": "arXiv:2601.09812v1 Announce Type: new  Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.",
        "arxiv_id": "2601.09812",
        "ARXIVID": "2601.09812",
        "COMMENT": "LCF3D proposes a novel late-cascade fusion framework for 3D object detection in autonomous driving, combining LiDAR and RGB. This is a new method for spatial understanding in embodied agents (criterion 1), and also relates to vision foundation models applications (criterion 4) via sensor fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.09923": {
        "authors": [
            "Hanna Foerster",
            "Robert Mullins",
            "Tom Blanchard",
            "Nicolas Papernot",
            "Kristina Nikoli\\'c",
            "Florian Tram\\`er",
            "Ilia Shumailov",
            "Cheng Zhang",
            "Yiren Zhao"
        ],
        "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
        "abstract": "arXiv:2601.09923v1 Announce Type: new  Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.",
        "arxiv_id": "2601.09923",
        "ARXIVID": "2601.09923",
        "COMMENT": "This paper is about system-level security for computer use agents, with some evaluation in a simulator (OSWorld). It touches on embodied AI and benchmarking (criterion 3), but the focus is on security rather than spatial intelligence or vision-language modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.10386": {
        "authors": [
            "Filippo Ruffini",
            "Camillo Maria Caruso",
            "Claudia Tacconi",
            "Lorenzo Nibid",
            "Francesca Miccolis",
            "Marta Lovino",
            "Carlo Greco",
            "Edy Ippolito",
            "Michele Fiore",
            "Alessio Cortellini",
            "Bruno Beomonte Zobel",
            "Giuseppe Perrone",
            "Bruno Vincenzi",
            "Claudio Marrocco",
            "Alessandro Bria",
            "Elisa Ficarra",
            "Sara Ramella",
            "Valerio Guarrasi",
            "Paolo Soda"
        ],
        "title": "Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer",
        "abstract": "arXiv:2601.10386v1 Announce Type: new  Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.",
        "arxiv_id": "2601.10386",
        "ARXIVID": "2601.10386",
        "COMMENT": "The paper presents a missing-aware multimodal survival framework using foundation models for feature extraction. It is relevant to vision foundation models and their applications (criterion 4), especially in handling missing modalities in medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2601.09881": {
        "authors": [
            "Weili Nie",
            "Julius Berner",
            "Nanye Ma",
            "Chao Liu",
            "Saining Xie",
            "Arash Vahdat"
        ],
        "title": "Transition Matching Distillation for Fast Video Generation",
        "abstract": "arXiv:2601.09881v1 Announce Type: new  Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
        "arxiv_id": "2601.09881",
        "ARXIVID": "2601.09881",
        "COMMENT": "This paper introduces a new distillation framework for fast video generation using diffusion models. While it is relevant to generative modeling in vision, it does not directly address spatial intelligence, VLLMs/MLLMs, or embodied AI benchmarks. It is most relevant to your friend's general interest in generative modeling for vision.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2601.10090": {
        "authors": [
            "Mingzhuo Li",
            "Guang Li",
            "Linfeng Ye",
            "Jiafeng Mao",
            "Takahiro Ogawa",
            "Konstantinos N. Plataniotis",
            "Miki Haseyama"
        ],
        "title": "Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks",
        "abstract": "arXiv:2601.10090v1 Announce Type: new  Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.",
        "arxiv_id": "2601.10090",
        "ARXIVID": "2601.10090",
        "COMMENT": "Difficulty-guided sampling for dataset distillation is a clever statistical trick for improving downstream task performance, but it does not directly match the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.09855": {
        "authors": [
            "Michael R. Metel",
            "Yufei Cui",
            "Boxing Chen",
            "Prasanna Parthasarathi"
        ],
        "title": "Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models",
        "abstract": "arXiv:2601.09855v1 Announce Type: new  Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.",
        "arxiv_id": "2601.09855",
        "ARXIVID": "2601.09855",
        "COMMENT": "This paper introduces a new sequential test-time scaling method (Min-Seek) for large reasoning models, improving accuracy and stability. While it is a methodological improvement for large models, it is not specific to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10029": {
        "authors": [
            "Tingyue Pan",
            "Jie Ouyang",
            "Mingyue Cheng",
            "Qingchuan Li",
            "Zirui Liu",
            "Mingfan Pan",
            "Shuo Yu",
            "Qi Liu"
        ],
        "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
        "abstract": "arXiv:2601.10029v1 Announce Type: new  Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.",
        "arxiv_id": "2601.10029",
        "ARXIVID": "2601.10029",
        "COMMENT": "This paper introduces a new agentic framework and a novel sequence-level policy optimization method (PSPO) for academic paper search, which is a new methodological improvement in sequential decision-making for agents. This is closest to criterion 1 (methodological improvements for spatial/spatial intelligence on embodied agents), but the domain is academic search, not spatial/embodied AI. It is not a VLLM/MLLM or vision foundation model paper.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10073": {
        "authors": [
            "Hyun Do Jung",
            "Jungwon Choi",
            "Hwiyoung Kim"
        ],
        "title": "ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology",
        "abstract": "arXiv:2601.10073v1 Announce Type: new  Abstract: We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\\geq \\tau$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\\approx 8.2$ tiles at $\\tau = 0.90$ and AUKC $\\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.",
        "arxiv_id": "2601.10073",
        "ARXIVID": "2601.10073",
        "COMMENT": "ReaMIL is a new method for evidence-aware multiple instance learning in histopathology. While it is a novel method in computer vision, it does not directly match the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.10200": {
        "authors": [
            "Kim Youwang",
            "Lee Hyoseok",
            "Subin Park",
            "Gerard Pons-Moll",
            "Tae-Hyun Oh"
        ],
        "title": "ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation",
        "abstract": "arXiv:2601.10200v1 Announce Type: new  Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.",
        "arxiv_id": "2601.10200",
        "ARXIVID": "2601.10200",
        "COMMENT": "Does not match any specific criterion. Focuses on efficient head avatar synthesis from monocular video, not directly on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10245": {
        "authors": [
            "Vansh Kapoor",
            "Aman Gupta",
            "Hao Chen",
            "Anurag Beniwal",
            "Jing Huang",
            "Aviral Kumar"
        ],
        "title": "TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks",
        "abstract": "arXiv:2601.10245v1 Announce Type: new  Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.",
        "arxiv_id": "2601.10245",
        "ARXIVID": "2601.10245",
        "COMMENT": "This paper proposes a new hybrid inference method for multi-step reasoning in LLMs, but it is not focused on vision, spatial intelligence, or multi-modal models. It is more about LLM routing and efficiency in reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10193": {
        "authors": [
            "Jiujiu Chen",
            "Weijun Zeng",
            "Shaofeng Hu",
            "Sihong Xie",
            "Hui Xiong"
        ],
        "title": "GFM4GA: Graph Foundation Model for Group Anomaly Detection",
        "abstract": "arXiv:2601.10193v1 Announce Type: new  Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.",
        "arxiv_id": "2601.10193",
        "ARXIVID": "2601.10193",
        "COMMENT": "This paper introduces GFM4GA, a graph foundation model for group anomaly detection. While it is about foundation models, it is not about vision foundation models or multi-modal learning, nor does it address spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10154": {
        "authors": [
            "Leonard N\\\"urnberg",
            "Dennis Bontempi",
            "Suraj Pai",
            "Curtis Lisle",
            "Steve Pieper",
            "Ron Kikinis",
            "Sil van de Leemput",
            "Rahul Soni",
            "Gowtham Murugesan",
            "Cosmin Ciausu",
            "Miriam Groeneveld",
            "Felix J. Dorfner",
            "Jue Jiang",
            "Aneesh Rangnekar",
            "Harini Veeraraghavan",
            "Joeran S. Bosma",
            "Keno Bressem",
            "Raymond Mak",
            "Andrey Fedorov",
            "Hugo JWL Aerts"
        ],
        "title": "MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging",
        "abstract": "arXiv:2601.10154v1 Announce Type: new  Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.",
        "arxiv_id": "2601.10154",
        "ARXIVID": "2601.10154",
        "COMMENT": "This paper introduces MHub.ai, a standardized platform for AI models in medical imaging. While it is relevant to computer vision and reproducibility, it does not match any of the four specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.10696": {
        "authors": [
            "Han Jiang",
            "Yao Xiao",
            "Rachel Hurley",
            "Shichao Liu"
        ],
        "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
        "abstract": "arXiv:2601.10696v1 Announce Type: new  Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
        "arxiv_id": "2601.10696",
        "ARXIVID": "2601.10696",
        "COMMENT": "This paper studies the impact of generative AI on architectural design, focusing on user performance and cognitive load. While it involves generative models, it is an application study and not a methodological advance in vision, spatial intelligence, or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.09814": {
        "authors": [
            "Adil O. Khadidos",
            "Aziida Nanyonga",
            "Alaa O. Khadidos",
            "Olfat M. Mirza",
            "Mustafa Tahsin Yilmaz"
        ],
        "title": "Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images",
        "abstract": "arXiv:2601.09814v1 Announce Type: new  Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.",
        "arxiv_id": "2601.09814",
        "ARXIVID": "2601.09814",
        "COMMENT": "This is an application paper on explainable deep learning for pediatric pneumonia detection. It does not introduce new methods for spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}