{
    "2509.12718": {
        "authors": [
            "Pukun Zhao",
            "Longxiang Wang",
            "Miaowei Wang",
            "Chen Chen",
            "Fanqing Zhou",
            "Haojian Huang"
        ],
        "title": "EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer",
        "abstract": "arXiv:2509.12718v1 Announce Type: new  Abstract: Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate models' abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at https://anonymous.4open.science/r/EvoEmpirBench-143C/.",
        "arxiv_id": "2509.12718",
        "ARXIVID": "2509.12718",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (new embodied AI benchmark with novel angles). Introduces dynamic spatial reasoning benchmarks and a new memory mechanism for embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.13317": {
        "authors": [
            "An-Chieh Cheng",
            "Yang Fu",
            "Yukang Chen",
            "Zhijian Liu",
            "Xiaolong Li",
            "Subhashree Radhakrishnan",
            "Song Han",
            "Yao Lu",
            "Jan Kautz",
            "Pavlo Molchanov",
            "Hongxu Yin",
            "Xiaolong Wang",
            "Sifei Liu"
        ],
        "title": "3D Aware Region Prompted Vision Language Model",
        "abstract": "arXiv:2509.13317v1 Announce Type: new  Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.",
        "arxiv_id": "2509.13317",
        "ARXIVID": "2509.13317",
        "COMMENT": "Matches criteria 1 and 2. Presents a new 3D-aware vision-language model (VLLM) with spatial region prompting and 3D positional embeddings for spatial reasoning, unifying 2D and 3D representations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.13067": {
        "authors": [
            "Xu Li",
            "Yuxuan Liang",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models",
        "abstract": "arXiv:2509.13067v1 Announce Type: new  Abstract: By cropping high-resolution images into local tiles and encoding them independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have demonstrated remarkable fine-grained visual understanding capabilities. However, this divide-and-conquer paradigm significantly increases the number of visual tokens, resulting in substantial computational and memory overhead. To better understand and address this challenge, we empirically investigate visual token utilization in HR-LVLMs and uncover three key findings: (1) the local tiles have varying importance, jointly determined by visual saliency and task relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage attention pattern across layers, with each stage attending to different types of visual tokens; (3) the visual tokens emphasized at different stages encode information at varying levels of granularity, playing complementary roles within LVLMs. Building on these insights, we propose HERO, a High-resolution visual token early dropping framework that integrates content-adaptive token budget allocation with function-aware token selection. By accurately estimating tile-level importance and selectively retaining visual tokens with complementary roles, HERO achieves superior efficiency-accuracy trade-offs across diverse benchmarks and model scales, all in a training-free manner. This study provides both empirical insights and practical solutions toward efficient inference in HR-LVLMs.",
        "arxiv_id": "2509.13067",
        "ARXIVID": "2509.13067",
        "COMMENT": "This paper introduces HERO, a framework for efficient visual token early dropping in high-resolution large vision-language models (HR-LVLMs). It provides empirical insights and a practical solution for efficient inference in VLLMs, directly matching criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.12883": {
        "authors": [
            "Qifei Jia",
            "Yu Liu",
            "Yajie Chai",
            "Xintong Yao",
            "Qiming Lu",
            "Yasen Zhang",
            "Runyu Shi",
            "Ying Huang",
            "Guoquan Zhang"
        ],
        "title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder",
        "abstract": "arXiv:2509.12883v1 Announce Type: new  Abstract: Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.   Code is available: https://github.com/xiaomi-research/lego-edit.",
        "arxiv_id": "2509.12883",
        "ARXIVID": "2509.12883",
        "COMMENT": "Matches criterion 2 (new MLLM) and criterion 4 (vision foundation models and applications). Introduces Lego-Edit, a general image editing framework using MLLMs to compose model-level editing tools, with reinforcement learning for open-domain instructions.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.13161": {
        "authors": [
            "Zhihao He",
            "Tianyao He",
            "Tieyuan Chen",
            "Yun Xu",
            "Huabin Liu",
            "Chaofan Gan",
            "Gui Zou",
            "Weiyao Lin"
        ],
        "title": "Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)",
        "abstract": "arXiv:2509.13161v1 Announce Type: new  Abstract: Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models.",
        "arxiv_id": "2509.13161",
        "ARXIVID": "2509.13161",
        "COMMENT": "This paper proposes a structured multi-video collaborative reasoning framework for video large language models, introducing a spatio-temporal graph representation and graph fusion for multi-video reasoning. It is directly relevant to new VLLMs/MLLMs (criterion 2) and advances in vision-language modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.12715": {
        "authors": [
            "Heng Zhang",
            "Haichuan Hu",
            "Yaomin Shen",
            "Weihao Yu",
            "Yilei Yuan",
            "Haochen You",
            "Guo Cheng",
            "Zijian Zhang",
            "Lubin Gan",
            "Huihui Wei",
            "Hao Zhang",
            "Jin Huang"
        ],
        "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models",
        "abstract": "arXiv:2509.12715v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.",
        "arxiv_id": "2509.12715",
        "ARXIVID": "2509.12715",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs). Proposes AsyMoE, a novel MoE architecture for large vision-language models, addressing modality asymmetry and expert specialization, with significant empirical improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.12482": {
        "authors": [
            "Tianshu Huang",
            "Akarsh Prabhakara",
            "Chuhan Chen",
            "Jay Karhade",
            "Deva Ramanan",
            "Matthew O'Toole",
            "Anthony Rowe"
        ],
        "title": "Towards Foundational Models for Single-Chip Radar",
        "abstract": "arXiv:2509.12482v1 Announce Type: new  Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.   In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20\\% per $10\\times$ data. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a $10\\times$ increase in training data. Finally, we roughly estimate that $\\approx$100M samples (3000 hours) of data are required to fully exploit the potential of GRT.",
        "arxiv_id": "2509.12482",
        "ARXIVID": "2509.12482",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a foundational model for single-chip radar, collects a large dataset, and demonstrates generalization and scaling properties. Also relevant for embodied AI and spatial intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.12430": {
        "authors": [
            "Mayank Patel",
            "Rahul Jain",
            "Asim Unmesh",
            "Karthik Ramani"
        ],
        "title": "DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction",
        "abstract": "arXiv:2509.12430v1 Announce Type: new  Abstract: Understanding the motion of articulated mechanical assemblies from static geometry remains a core challenge in 3D perception and design automation. Prior work on everyday articulated objects such as doors and laptops typically assumes simplified kinematic structures or relies on joint annotations. However, in mechanical assemblies like gears, motion arises from geometric coupling, through meshing teeth or aligned axes, making it difficult for existing methods to reason about relational motion from geometry alone. To address this gap, we introduce MechBench, a benchmark dataset of 693 diverse synthetic gear assemblies with part-wise ground-truth motion trajectories. MechBench provides a structured setting to study coupled motion, where part dynamics are induced by contact and transmission rather than predefined joints. Building on this, we propose DYNAMO, a dependency-aware neural model that predicts per-part SE(3) motion trajectories directly from segmented CAD point clouds. Experiments show that DYNAMO outperforms strong baselines, achieving accurate and temporally consistent predictions across varied gear configurations. Together, MechBench and DYNAMO establish a novel systematic framework for data-driven learning of coupled mechanical motion in CAD assemblies.",
        "arxiv_id": "2509.12430",
        "ARXIVID": "2509.12430",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and method for spatial reasoning in mechanical assemblies). Introduces MechBench and DYNAMO for articulated assembly motion prediction, focusing on coupled motion from geometry.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.12897": {
        "authors": [
            "Jianfei Zhao",
            "Feng Zhang",
            "Xin Sun",
            "Lingxing Kong",
            "Zhixing Tan",
            "Chong Feng"
        ],
        "title": "Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models",
        "abstract": "arXiv:2509.12897v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in images, yet their attention to these objects tends to be very brief. Motivated by the hypothesis that sustained focus on key objects can improve LVLMs' visual capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of CLVS is to incorporate a vision memory that smooths the attention distribution across layers. Specifically, we initialize this vision memory with position-unbiased visual attention in the first layer. In subsequent layers, the model's visual attention jointly considers the vision memory from previous layers, while the memory is updated iteratively, thereby maintaining smooth attention on key objects. Given that visual understanding primarily occurs in the early and middle layers of the model, we use uncertainty as an indicator of completed visual understanding and terminate the smoothing process accordingly. Experiments on four benchmarks across three LVLMs confirm the effectiveness and generalizability of our method. CLVS achieves state-of-the-art performance on a variety of visual understanding tasks, with particularly significant improvements in relation and attribute understanding.",
        "arxiv_id": "2509.12897",
        "ARXIVID": "2509.12897",
        "COMMENT": "Matches criterion 2 (new methodological improvement for LVLMs) and criterion 4 (vision foundation models and applications). Proposes Cross-Layer Vision Smoothing for LVLMs, improving sustained attention and visual understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.12683": {
        "authors": [
            "Xianda Guo",
            "Chenming Zhang",
            "Ruilin Wang",
            "Youmin Zhang",
            "Wenzhao Zheng",
            "Matteo Poggi",
            "Hao Zhao",
            "Qin Zou",
            "Long Chen"
        ],
        "title": "StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo",
        "abstract": "arXiv:2509.12683v1 Announce Type: new  Abstract: Stereo matching plays a crucial role in enabling depth perception for autonomous driving and robotics. While recent years have witnessed remarkable progress in stereo matching algorithms, largely driven by learning-based methods and synthetic datasets, the generalization performance of these models remains constrained by the limited diversity of existing training data. To address these challenges, we present StereoCarla, a high-fidelity synthetic stereo dataset specifically designed for autonomous driving scenarios. Built on the CARLA simulator, StereoCarla incorporates a wide range of camera configurations, including diverse baselines, viewpoints, and sensor placements as well as varied environmental conditions such as lighting changes, weather effects, and road geometries. We conduct comprehensive cross-domain experiments across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury, ETH3D) and demonstrate that models trained on StereoCarla outperform those trained on 11 existing stereo datasets in terms of generalization accuracy across multiple benchmarks. Furthermore, when integrated into multi-dataset training, StereoCarla contributes substantial improvements to generalization accuracy, highlighting its compatibility and scalability. This dataset provides a valuable benchmark for developing and evaluating stereo algorithms under realistic, diverse, and controllable settings, facilitating more robust depth perception systems for autonomous vehicles. Code can be available at https://github.com/XiandaGuo/OpenStereo, and data can be available at https://xiandaguo.net/StereoCarla.",
        "arxiv_id": "2509.12683",
        "ARXIVID": "2509.12683",
        "COMMENT": "Matches criterion 3. Introduces a new high-fidelity, diverse stereo driving dataset (StereoCarla) for benchmarking and improving generalization in stereo matching, with a novel focus on diversity and realism in simulation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.13301": {
        "authors": [
            "Zefan Qu",
            "Zhenwei Wang",
            "Haoyuan Wang",
            "Ke Xu",
            "Gerhard Hancke",
            "Rynson W. H. Lau"
        ],
        "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance",
        "abstract": "arXiv:2509.13301v1 Announce Type: new  Abstract: Creating 3D assets that follow the texture and geometry style of existing ones is often desirable or even inevitable in practical applications like video gaming and virtual reality. While impressive progress has been made in generating 3D objects from text or images, creating style-controllable 3D assets remains a complex and challenging problem. In this work, we propose StyleSculptor, a novel training-free approach for generating style-guided 3D assets from a content image and one or more style images. Unlike previous works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner, enabling fine-grained 3D style control that captures the texture, geometry, or both styles of user-provided style images. At the core of StyleSculptor is a novel Style Disentangled Attention (SD-Attn) module, which establishes a dynamic interaction between the input content image and style image for style-guided 3D asset generation via a cross-3D attention mechanism, enabling stable feature fusion and effective style-guided generation. To alleviate semantic content leakage, we also introduce a style-disentangled feature selection strategy within the SD-Attn module, which leverages the variance of 3D feature patches to disentangle style- and content-significant channels, allowing selective feature injection within the attention framework. With SD-Attn, the network can dynamically compute texture-, geometry-, or both-guided features to steer the 3D generation process. Built upon this, we further propose the Style Guided Control (SGC) mechanism, which enables exclusive geometry- or texture-only stylization, as well as adjustable style intensity control. Extensive experiments demonstrate that StyleSculptor outperforms existing baseline methods in producing high-fidelity 3D assets.",
        "arxiv_id": "2509.13301",
        "ARXIVID": "2509.13301",
        "COMMENT": "Matches criterion 4. Introduces a novel zero-shot, style-controllable 3D asset generation method with dual texture-geometry guidance, relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.12938": {
        "authors": [
            "Abdalla Arafa",
            "Didier Stricker"
        ],
        "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings",
        "abstract": "arXiv:2509.12938v1 Announce Type: new  Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.",
        "arxiv_id": "2509.12938",
        "ARXIVID": "2509.12938",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method for open-vocabulary 3D scene understanding using 3D Gaussian Splatting and CLIP embeddings, with a novel approach to object-level semantic aggregation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.12995": {
        "authors": [
            "Yue Zhou",
            "Xinan He",
            "Kaiqing Lin",
            "Bing Fan",
            "Feng Ding",
            "Jinhua Zeng",
            "Bin Li"
        ],
        "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection",
        "abstract": "arXiv:2509.12995v1 Announce Type: new  Abstract: While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.   Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.",
        "arxiv_id": "2509.12995",
        "ARXIVID": "2509.12995",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Shows that modern vision foundation models outperform specialized detectors for AI image detection, with insightful empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12721": {
        "authors": [
            "Jingdong Zhang",
            "Weikai Chen",
            "Yuan Liu",
            "Jionghao Wang",
            "Zhengming Yu",
            "Zhuowen Shen",
            "Bo Yang",
            "Wenping Wang",
            "Xin Li"
        ],
        "title": "SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation",
        "abstract": "arXiv:2509.12721v1 Announce Type: new  Abstract: Existing single-view 3D generative models typically adopt multiview diffusion priors to reconstruct object surfaces, yet they remain prone to inter-view inconsistencies and are unable to faithfully represent complex internal structure or nontrivial topologies. In particular, we encode geometry information by projecting it onto a bounding sphere and unwrapping it into a compact and structural multi-layer 2D Spherical Projection (SP) representation. Operating solely in the image domain, SPGen offers three key advantages simultaneously: (1) Consistency. The injective SP mapping encodes surface geometry with a single viewpoint which naturally eliminates view inconsistency and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal structures and support direct lifting to watertight or open 3D surfaces; (3) Efficiency. The image-domain formulation allows the direct inheritance of powerful 2D diffusion priors and enables efficient finetuning with limited computational resources. Extensive experiments demonstrate that SPGen significantly outperforms existing baselines in geometric quality and computational efficiency.",
        "arxiv_id": "2509.12721",
        "ARXIVID": "2509.12721",
        "COMMENT": "This paper introduces SPGen, a new spherical projection representation for single-image 3D shape generation, addressing consistency and flexibility in 3D generative models. This is a methodological improvement for spatial understanding and generative modeling (criterion 1), and offers a novel representation for 3D vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12878": {
        "authors": [
            "Qianguang Zhao",
            "Dongli Wang",
            "Yan Zhou",
            "Jianxun Li",
            "Richard Irampa"
        ],
        "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation",
        "abstract": "arXiv:2509.12878v1 Announce Type: new  Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.",
        "arxiv_id": "2509.12878",
        "ARXIVID": "2509.12878",
        "COMMENT": "This paper proposes a new method for few-shot 3D point cloud semantic segmentation using a diffusion learner to expand prototype capacity. This is a methodological improvement for spatial understanding in 3D data and generative modeling (criterion 1), and leverages diffusion models in a novel way.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12437": {
        "authors": [
            "Dingrui Wang",
            "Zhexiao Sun",
            "Zhouheng Li",
            "Cheng Wang",
            "Youlun Peng",
            "Hongyuan Ye",
            "Baha Zarrouki",
            "Wei Li",
            "Mattia Piccinini",
            "Lei Xie",
            "Johannes Betz"
        ],
        "title": "Enhancing Physical Consistency in Lightweight World Models",
        "abstract": "arXiv:2509.12437v1 Announce Type: new  Abstract: A major challenge in deploying world models is the trade-off between size and performance. Large world models can capture rich physical dynamics but require massive computing resources, making them impractical for edge devices. Small world models are easier to deploy but often struggle to learn accurate physics, leading to poor predictions. We propose the Physics-Informed BEV World Model (PIWM), a compact model designed to efficiently capture physical interactions in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training to improve dynamic object modeling and future prediction. We also introduce a simple yet effective technique, Warm Start, for inference to enhance prediction quality with a zero-shot model. Experiments show that at the same parameter scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score. Moreover, even when compared with the largest baseline model (400M), the smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score with a 28% faster inference speed.",
        "arxiv_id": "2509.12437",
        "ARXIVID": "2509.12437",
        "COMMENT": "This paper introduces a lightweight, physics-informed world model (PIWM) for efficient physical consistency in BEV representations, with techniques like Soft Mask and Warm Start. It is relevant to embodied AI and world modeling, and presents a new method for spatial understanding in embodied agents (criteria 1 and 3).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12474": {
        "authors": [
            "Kai Qiu",
            "Xiang Li",
            "Hao Chen",
            "Jason Kuen",
            "Xiaohao Xu",
            "Jiuxiang Gu",
            "Yinyi Luo",
            "Bhiksha Raj",
            "Zhe Lin",
            "Marios Savvides"
        ],
        "title": "Image Tokenizer Needs Post-Training",
        "abstract": "arXiv:2509.12474v1 Announce Type: new  Abstract: Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \\ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.",
        "arxiv_id": "2509.12474",
        "ARXIVID": "2509.12474",
        "COMMENT": "This paper proposes a novel tokenizer training scheme for image generative models, including a post-training phase to improve generation quality. It is directly relevant to vision foundation models and generative modeling (criterion 4), and introduces a new evaluation metric (pFID).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12815": {
        "authors": [
            "Biwen Lei",
            "Yang Li",
            "Xinhai Liu",
            "Shuhui Yang",
            "Lixin Xu",
            "Jingwei Huang",
            "Ruining Tang",
            "Haohan Weng",
            "Jian Liu",
            "Jing Xu",
            "Zhen Zhou",
            "Yiling Zhu",
            "Jiankai Xing",
            "Jiachen Xu",
            "Changfeng Ma",
            "Xinhao Yan",
            "Yunhan Yang",
            "Chunshi Wang",
            "Duoteng Xu",
            "Xueqi Ma",
            "Yuguang Chen",
            "Jing Li",
            "Mingxin Yang",
            "Sheng Zhang",
            "Yifei Feng",
            "Xin Huang",
            "Di Luo",
            "Zebin He",
            "Puhua Jiang",
            "Changrong Hu",
            "Zihan Qin",
            "Shiwei Miao",
            "Haolin Liu",
            "Yunfei Zhao",
            "Zeqiang Lai",
            "Qingxiang Lin",
            "Zibo Zhao",
            "Kunhong Li",
            "Xianghui Yang",
            "Huiwen Shi",
            "Xin Yang",
            "Yuxuan Wang",
            "Zebin Yao",
            "Yihang Lian",
            "Sicong Liu",
            "Xintong Han",
            "Wangchen Qin",
            "Caisheng Ouyang",
            "Jianyin Liu",
            "Tianwen Yuan",
            "Shuai Jiang",
            "Hong Duan",
            "Yanqi Niu",
            "Wencong Lin",
            "Yifu Sun",
            "Shirui Huang",
            "Lin Niu",
            "Gu Gong",
            "Guojian Xiao",
            "Bojian Zheng",
            "Xiang Yuan",
            "Qi Chen",
            "Jie Xiao",
            "Dongyang Zheng",
            "Xiaofeng Yang",
            "Kai Liu",
            "Jianchen Zhu",
            "Lifu Wang",
            "Qinglin Lu",
            "Jie Liu",
            "Liang Dong",
            "Fan Jiang",
            "Ruibin Chen",
            "Lei Wang",
            "Chao Zhang",
            "Jiaxin Lin",
            "Hao Zhang",
            "Zheng Ye",
            "Peng He",
            "Runzhou Wu",
            "Yinhe Wu",
            "Jiayao Du",
            "Jupeng Chen",
            "Xinyue Mao",
            "Dongyuan Guo",
            "Yixuan Tang",
            "Yulin Tsai",
            "Yonghao Tan",
            "Jiaao Yu",
            "Junlin Yu",
            "Keren Zhang",
            "Yifan Li",
            "Peng Chen",
            "Tian Liu",
            "Di Wang",
            "Yuhong Liu",
            "Linus",
            "Jie Jiang",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation",
        "abstract": "arXiv:2509.12815v1 Announce Type: new  Abstract: The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.",
        "arxiv_id": "2509.12815",
        "ARXIVID": "2509.12815",
        "COMMENT": "This paper presents Hunyuan3D Studio, an end-to-end AI pipeline for generating game-ready 3D assets from images or text. It integrates advanced neural modules for 3D generation and is highly relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12963": {
        "authors": [
            "Robin Sch\\\"on",
            "Julian Lorenz",
            "Katja Ludwig",
            "Daniel Kienzle",
            "Rainer Lienhart"
        ],
        "title": "MMMS: Multi-Modal Multi-Surface Interactive Segmentation",
        "abstract": "arXiv:2509.12963v1 Announce Type: new  Abstract: In this paper, we present a method to interactively create segmentation masks on the basis of user clicks. We pay particular attention to the segmentation of multiple surfaces that are simultaneously present in the same image. Since these surfaces may be heavily entangled and adjacent, we also present a novel extended evaluation metric that accounts for the challenges of this scenario. Additionally, the presented method is able to use multi-modal inputs to facilitate the segmentation task. At the center of this method is a network architecture which takes as input an RGB image, a number of non-RGB modalities, an erroneous mask, and encoded clicks. Based on this input, the network predicts an improved segmentation mask. We design our architecture such that it adheres to two conditions: (1) The RGB backbone is only available as a black-box. (2) To reduce the response time, we want our model to integrate the interaction-specific information after the image feature extraction and the multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface interactive segmentation (MMMS). We are able to show the effectiveness of our multi-modal fusion strategy. Using additional modalities, our system reduces the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to 1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline achieves competitive, and in some cases even superior performance when tested in a classical, single-mask interactive segmentation scenario.",
        "arxiv_id": "2509.12963",
        "ARXIVID": "2509.12963",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 2 (multi-modal input for segmentation). The paper proposes a novel multi-modal, multi-surface interactive segmentation method and a new evaluation metric, with a focus on spatially entangled surfaces and multi-modal fusion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12546": {
        "authors": [
            "Yingxin Lai",
            "Zitong Yu",
            "Jun Wang",
            "Linlin Shen",
            "Yong Xu",
            "Xiaochun Cao"
        ],
        "title": "Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection",
        "abstract": "arXiv:2509.12546v1 Announce Type: new  Abstract: Face forgery detection faces a critical challenge: a persistent gap between offline benchmarks and real-world efficacy,which we attribute to the ecological invalidity of training data.This work introduces Agent4FaceForgery to address two fundamental problems: (1) how to capture the diverse intents and iterative processes of human forgery creation, and (2) how to model the complex, often adversarial, text-image interactions that accompany forgeries in social media. To solve this,we propose a multi-agent framework where LLM-poweredagents, equipped with profile and memory modules, simulate the forgery creation process. Crucially, these agents interact in a simulated social environment to generate samples labeled for nuanced text-image consistency, moving beyond simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism ensures data quality and diversity. Extensive experiments validate that the data generated by our simulationdriven approach brings significant performance gains to detectors of multiple architectures, fully demonstrating the effectiveness and value of our framework.",
        "arxiv_id": "2509.12546",
        "ARXIVID": "2509.12546",
        "COMMENT": "Matches criterion 2 (multi-agent LLM framework for vision-language forgery detection) and criterion 4 (vision foundation models and applications). Proposes a simulation-driven data generation approach for face forgery detection.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.12724": {
        "authors": [
            "Yunhan Zhao",
            "Xiang Zheng",
            "Xingjun Ma"
        ],
        "title": "Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models",
        "abstract": "arXiv:2509.12724v1 Announce Type: new  Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks. While recent jailbreaks have achieved notable progress, their effectiveness and efficiency can still be improved. In this work, we reveal an interesting phenomenon: incorporating weak defense into the attack pipeline can significantly enhance both the effectiveness and the efficiency of jailbreaks on VLMs. Building on this insight, we propose Defense2Attack, a novel jailbreak method that bypasses the safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak prompt design. Specifically, Defense2Attack consists of three key components: (1) a visual optimizer that embeds universal adversarial perturbations with affirmative and encouraging semantics; (2) a textual optimizer that refines the input using a defense-styled prompt; and (3) a red-team suffix generator that enhances the jailbreak through reinforcement fine-tuning. We empirically evaluate our method on four VLMs and four safety benchmarks. The results demonstrate that Defense2Attack achieves superior jailbreak performance in a single attempt, outperforming state-of-the-art attack methods that often require multiple tries. Our work offers a new perspective on jailbreaking VLMs.",
        "arxiv_id": "2509.12724",
        "ARXIVID": "2509.12724",
        "COMMENT": "Matches criterion 2 (VLLMs) and provides a novel jailbreak method for vision-language models, with clever statistical tricks and empirical insights.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.12710": {
        "authors": [
            "Siju Ma",
            "Changsiyu Gong",
            "Xiaofeng Fan",
            "Yong Ma",
            "Chengjie Jiang"
        ],
        "title": "RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation",
        "abstract": "arXiv:2509.12710v1 Announce Type: new  Abstract: Text-driven infrared and visible image fusion has gained attention for enabling natural language to guide the fusion process. However, existing methods lack a goal-aligned task to supervise and evaluate how effectively the input text contributes to the fusion outcome. We observe that referring image segmentation (RIS) and text-driven fusion share a common objective: highlighting the object referred to by the text. Motivated by this, we propose RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint optimization. At its core is the LangGatedFusion module, which injects textual features into the fusion backbone to enhance semantic alignment. To support multimodal referring image segmentation task, we introduce MM-RIS, a large-scale benchmark with 12.5k training and 3.5k testing triplets, each consisting of an infrared-visible image pair, a segmentation mask, and a referring expression. Extensive experiments show that RIS-FUSION achieves state-of-the-art performance, outperforming existing methods by over 11% in mIoU. Code and dataset will be released at https://github.com/SijuMa2003/RIS-FUSION.",
        "arxiv_id": "2509.12710",
        "ARXIVID": "2509.12710",
        "COMMENT": "Matches criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications). Proposes a text-driven fusion framework for infrared and visible images, introduces a new benchmark, and leverages language-vision alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.12931": {
        "authors": [
            "Xiao Tang",
            "Guirong Zhuo",
            "Cong Wang",
            "Boyuan Zheng",
            "Minqing Huang",
            "Lianqing Zheng",
            "Long Chen",
            "Shouyi Lu"
        ],
        "title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar",
        "abstract": "arXiv:2509.12931v1 Announce Type: new  Abstract: 3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction.",
        "arxiv_id": "2509.12931",
        "ARXIVID": "2509.12931",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for dynamic scene reconstruction in driving simulators). Proposes a 4D radar-augmented self-supervised 3D reconstruction framework for dynamic driving scenes, with a novel velocity-guided point tracking model.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.12818": {
        "authors": [
            "Maximilian Ilse",
            "Harshita Sharma",
            "Anton Schwaighofer",
            "Sam Bond-Taylor",
            "Fernando P\\'erez-Garc\\'ia",
            "Olesya Melnichenko",
            "Anne-Marie G. Sykes",
            "Kelly K. Horst",
            "Ashish Khandelwal",
            "Maxwell Reynolds",
            "Maria T. Wetscherek",
            "Noel C. F. Codella",
            "Javier Alvarez-Valle",
            "Korfiatis Panagiotis",
            "Valentina Salvatelli"
        ],
        "title": "Data Scaling Laws for Radiology Foundation Models",
        "abstract": "arXiv:2509.12818v1 Announce Type: new  Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M chest x-rays from a single institution, holding compute and evaluation protocols constant. We evaluate on classification (radiology findings, lines and tubes), segmentation (lines and tubes), and radiology report generation. While prior work has primarily focused on tasks related to radiology findings, we include lines and tubes tasks to counterbalance this bias and evaluate a model's ability to extract features that preserve continuity along elongated structures. Our experiments show that MI2 scales more effectively for finding-related tasks, while RAD-DINO is stronger on tube-related tasks. Surprisingly, continually pretraining MI2 with both reports and structured labels using UniCL improves performance, underscoring the value of structured supervision at scale. We further show that for some tasks, as few as 30k in-domain samples are sufficient to surpass open-weights foundation models. These results highlight the utility of center-specific continual pretraining, enabling medical institutions to derive significant performance gains by utilizing in-domain data.",
        "arxiv_id": "2509.12818",
        "ARXIVID": "2509.12818",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Studies scaling laws for radiology foundation models, with surprising empirical findings about data requirements and continual pretraining.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12787": {
        "authors": [
            "Linchun Wu",
            "Qin Zou",
            "Xianbiao Qi",
            "Bo Du",
            "Zhongyuan Wang",
            "Qingquan Li"
        ],
        "title": "Double Helix Diffusion for Cross-Domain Anomaly Image Generation",
        "abstract": "arXiv:2509.12787v1 Announce Type: new  Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.",
        "arxiv_id": "2509.12787",
        "ARXIVID": "2509.12787",
        "COMMENT": "Partially matches criterion 4. Proposes a new diffusion-based generative model for anomaly image generation with cross-domain and multi-modal aspects, relevant to generative modeling in vision.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.12277": {
        "authors": [
            "Mehdi Yousefzadeh",
            "Parsa Esfahanian",
            "Sara Rashidifar",
            "Hossein Salahshoor Gavalan",
            "Negar Sadat Rafiee Tabatabaee",
            "Saeid Gorgin",
            "Dara Rahmati",
            "Maryam Daneshpazhooh"
        ],
        "title": "GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions",
        "abstract": "arXiv:2509.12277v1 Announce Type: new  Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often ignores patient metadata (age, sex, site) and the physical scale needed for geometric analysis. We present GraphDerm, a population-graph framework that fuses imaging, millimeter-scale calibration, and metadata for multiclass dermoscopic classification, to the best of our knowledge the first ISIC-scale application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019, synthesize ruler-embedded images with exact masks, and train U-Nets (SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN. From lesion masks we compute real-scale descriptors (area, perimeter, radius of gyration). Node features use EfficientNet-B3; edges encode metadata/geometry similarity (fully weighted or thresholded). A spectral GNN performs semi-supervised node classification; an image-only ANN is the baseline. Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440 for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99 range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in a population graph yields substantial gains over image-only pipelines on ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient deployment. Scale-aware, graph-based AI is a promising direction for dermoscopic decision support; future work will refine learned edge semantics and evaluate on broader curated benchmarks.",
        "arxiv_id": "2509.12277",
        "ARXIVID": "2509.12277",
        "COMMENT": "This paper introduces GraphDerm, a population-graph classifier that fuses imaging, physical scale, and metadata for dermoscopic lesion classification. The use of graph neural networks and multi-modal fusion is relevant to spatial understanding and multi-modal learning (criterion 1), and the approach is novel in the medical imaging context.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.13107": {
        "authors": [
            "Kohou Wang",
            "Huan Hu",
            "Xiang Liu",
            "Zezhou Chen",
            "Ping Chen",
            "Zhaoxiang Liu",
            "Shiguo Lian"
        ],
        "title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge",
        "abstract": "arXiv:2509.13107v1 Announce Type: new  Abstract: The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.",
        "arxiv_id": "2509.13107",
        "ARXIVID": "2509.13107",
        "COMMENT": "This paper presents a hierarchical deep fusion framework for facial forgery detection, using an ensemble of vision foundation models (criterion 4). The use of multiple pre-trained models and fusion is directly relevant to vision foundation model applications.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2509.12959": {
        "authors": [
            "Yuqi Xie",
            "Shuhan Ye",
            "Chong Wang",
            "Jiazhen Xu",
            "Le Shen",
            "Yuanbin Qian",
            "Jiangbo Qian"
        ],
        "title": "Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain",
        "abstract": "arXiv:2509.12959v1 Announce Type: new  Abstract: The integration of event cameras and spiking neural networks holds great promise for energy-efficient visual processing. However, the limited availability of event data and the sparse nature of DVS outputs pose challenges for effective training. Although some prior work has attempted to transfer semantic knowledge from RGB datasets to DVS, they often overlook the significant distribution gap between the two modalities. In this paper, we propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing strategy that exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time-steps. To enable label mixing in cross-modal scenarios, we further introduce modality-aware auxiliary learning objectives. These objectives support the time-step mixup process and enhance the model's ability to discriminate effectively across different modalities. Our approach enables smoother knowledge transfer, alleviates modality shift during training, and achieves superior performance in spiking image classification tasks. Extensive experiments demonstrate the effectiveness of our method across multiple datasets. The code will be released after the double-blind review process.",
        "arxiv_id": "2509.12959",
        "ARXIVID": "2509.12959",
        "COMMENT": "Partially matches criterion 4. Proposes a new method for knowledge transfer between appearance and event domains using spiking neural networks, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.13116": {
        "authors": [
            "Ruibo Li",
            "Hanyu Shi",
            "Zhe Wang",
            "Guosheng Lin"
        ],
        "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving",
        "abstract": "arXiv:2509.13116v1 Announce Type: new  Abstract: Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.",
        "arxiv_id": "2509.13116",
        "ARXIVID": "2509.13116",
        "COMMENT": "Partially matches criterion 1. Proposes new weakly/self-supervised methods for class-agnostic motion prediction from LiDAR, focusing on spatial understanding in dynamic environments.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.13149": {
        "authors": [
            "Minqing Huang",
            "Shouyi Lu",
            "Boyuan Zheng",
            "Ziyao Li",
            "Xiao Tang",
            "Guirong Zhuo"
        ],
        "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
        "abstract": "arXiv:2509.13149v1 Announce Type: new  Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.",
        "arxiv_id": "2509.13149",
        "ARXIVID": "2509.13149",
        "COMMENT": "This paper introduces a new multi-stage distillation framework for 4D radar super-resolution, which is foundational for spatial perception in autonomous agents. The method is relevant to spatial intelligence and could be of interest for embodied AI (criterion 1), but does not directly address embodied agents or benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12627": {
        "authors": [
            "Pengbo Guo",
            "Chengxu Liu",
            "Guoshuai Zhao",
            "Xingsong Hou",
            "Jialie Shen",
            "Xueming Qian"
        ],
        "title": "Exploring Spectral Characteristics for Single Image Reflection Removal",
        "abstract": "arXiv:2509.12627v1 Announce Type: new  Abstract: Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models.",
        "arxiv_id": "2509.12627",
        "ARXIVID": "2509.12627",
        "COMMENT": "This paper introduces a spectral learning approach for single image reflection removal, including a Spectral Codebook and Spectrum-Aware Transformer. While it is a novel vision method, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.12423": {
        "authors": [
            "Danielle Cohen",
            "Yoni Halpern",
            "Noam Kahlon",
            "Joel Oren",
            "Omri Berkovitch",
            "Sapir Caduri",
            "Ido Dagan",
            "Anatoly Efros"
        ],
        "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition",
        "abstract": "arXiv:2509.12423v1 Announce Type: new  Abstract: Understanding user intents from UI interaction trajectories remains a challenging, yet crucial, frontier in intelligent agent development. While massive, datacenter-based, multi-modal large language models (MLLMs) possess greater capacity to handle the complexities of such sequences, smaller models which can run on-device to provide a privacy-preserving, low-cost, and low-latency user experience, struggle with accurate intent inference. We address these limitations by introducing a novel decomposed approach: first, we perform structured interaction summarization, capturing key information from each user action. Second, we perform intent extraction using a fine-tuned model operating on the aggregated summaries. This method improves intent understanding in resource-constrained models, even surpassing the base performance of large MLLMs.",
        "arxiv_id": "2509.12423",
        "ARXIVID": "2509.12423",
        "COMMENT": "Somewhat relevant to criterion 2 (MLLMs), as it discusses intent extraction from UI interaction using small models and compares to large MLLMs, but not a direct match to vision or spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.13214": {
        "authors": [
            "Fei Wang",
            "Xuecheng Wu",
            "Zheng Zhang",
            "Danlei Huang",
            "Yuheng Huang",
            "BoWang"
        ],
        "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection",
        "abstract": "arXiv:2509.13214v1 Announce Type: new  Abstract: The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.",
        "arxiv_id": "2509.13214",
        "ARXIVID": "2509.13214",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it deals with diffusion models and inpainting detection, but not a direct match to the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.13083": {
        "authors": [
            "Yan Xingyang",
            "Huang Xiaohong",
            "Zhang Zhao",
            "You Tian",
            "Xu Ziheng"
        ],
        "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement",
        "abstract": "arXiv:2509.13083v1 Announce Type: new  Abstract: In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc",
        "arxiv_id": "2509.13083",
        "ARXIVID": "2509.13083",
        "COMMENT": "This paper introduces a novel KL-divergence-based loss for frequency-aware low-light image enhancement, which is a clever statistical trick in computer vision, but not directly related to spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12464": {
        "authors": [
            "Ryan Lucas",
            "Kayhan Behdin",
            "Zhipeng Wang",
            "Qingquan Song",
            "Shao Tang",
            "Rahul Mazumder"
        ],
        "title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction",
        "abstract": "arXiv:2509.12464v1 Announce Type: new  Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought traces during inference time which make them costly to deploy at scale. We show that using compression techniques such as neural network pruning produces greater performance loss than in typical language modeling tasks, and in some cases can make the model slower since they cause the model to produce more thinking tokens but with worse performance. We show that this is partly due to the fact that standard LLM pruning methods often focus on input reconstruction, whereas reasoning is a decode-dominated task. We introduce a simple, drop-in fix: during pruning we jointly reconstruct activations from the input and the model's on-policy chain-of-thought traces. This \"Reasoning-Aware Compression\" (RAC) integrates seamlessly into existing pruning workflows such as SparseGPT, and boosts their performance significantly. Code reproducing the results in the paper can be found at: https://github.com/RyanLucas3/RAC",
        "arxiv_id": "2509.12464",
        "ARXIVID": "2509.12464",
        "COMMENT": "This paper introduces a new pruning method for reasoning language models using chain-of-thought reconstruction. While it is a clever statistical trick for LLMs, it does not focus on vision, spatial intelligence, VLLMs/MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12746": {
        "authors": [
            "Tony Lindeberg",
            "Zahra Babaiee",
            "Peyman M. Kiasari"
        ],
        "title": "Modelling and analysis of the 8 filters from the \"master key filters hypothesis\" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory",
        "abstract": "arXiv:2509.12746v1 Announce Type: new  Abstract: This paper presents the results of analysing and modelling a set of 8 ``master key filters'', which have been extracted by applying a clustering approach to the receptive fields learned in depthwise-separable deep networks based on the ConvNeXt architecture.   For this purpose, we first compute spatial spread measures in terms of weighted mean values and weighted variances of the absolute values of the learned filters, which support the working hypotheses that: (i) the learned filters can be modelled by separable filtering operations over the spatial domain, and that (ii) the spatial offsets of the those learned filters that are non-centered are rather close to half a grid unit. Then, we model the clustered ``master key filters'' in terms of difference operators applied to a spatial smoothing operation in terms of the discrete analogue of the Gaussian kernel, and demonstrate that the resulting idealized models of the receptive fields show good qualitative similarity to the learned filters.   This modelling is performed in two different ways: (i) using possibly different values of the scale parameters in the coordinate directions for each filter, and (ii) using the same value of the scale parameter in both coordinate directions. Then, we perform the actual model fitting by either (i) requiring spatial spread measures in terms of spatial variances of the absolute values of the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or $l_2$-norms between the idealized receptive field models and the learned filters.   Complementary experimental results then demonstrate the idealized models of receptive fields have good predictive properties for replacing the learned filters by idealized filters in depthwise-separable deep networks, thus showing that the learned filters in depthwise-separable deep networks can be well approximated by discrete scale-space filters.",
        "arxiv_id": "2509.12746",
        "ARXIVID": "2509.12746",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models), as it analyzes learned filters in deep networks and models them with scale-space theory, but not a direct match to the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.12784": {
        "authors": [
            "Zhehao Li",
            "Yucheng Qian",
            "Chong Wang",
            "Yinghao Lu",
            "Zhihao Yang",
            "Jiafei Wu"
        ],
        "title": "Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection",
        "abstract": "arXiv:2509.12784v1 Announce Type: new  Abstract: Human-Object Interaction (HOI) detection aims to simultaneously localize human-object pairs and recognize their interactions. While recent two-stage approaches have made significant progress, they still face challenges due to incomplete context modeling. In this work, we introduce a Contextualized Representation Learning Network that integrates both affordance-guided reasoning and contextual prompts with visual cues to better capture complex interactions. We enhance the conventional HOI detection framework by expanding it beyond simple human-object pairs to include multivariate relationships involving auxiliary entities like tools. Specifically, we explicitly model the functional role (affordance) of these auxiliary objects through triplet structures . This enables our model to identify tool-dependent interactions such as 'filling'. Furthermore, the learnable prompt is enriched with instance categories and subsequently integrated with contextual visual features using an attention mechanism. This process aligns language with image content at both global and regional levels. These contextualized representations equip the model with enriched relational cues for more reliable reasoning over complex, context-dependent interactions. Our proposed method demonstrates superior performance on both the HICO-Det and V-COCO datasets in most scenarios. Codes will be released upon acceptance.",
        "arxiv_id": "2509.12784",
        "ARXIVID": "2509.12784",
        "COMMENT": "Relevant to computer vision and multi-modal learning, but does not directly match any specific criterion. Focuses on HOI detection with contextualized representations, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12742": {
        "authors": [
            "Jiateng Liu",
            "Hao Gao",
            "Jiu-Cheng Xie",
            "Chi-Man Pun",
            "Jian Xiong",
            "Haolun Li",
            "Feng Xu"
        ],
        "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
        "abstract": "arXiv:2509.12742v1 Announce Type: new  Abstract: This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters.",
        "arxiv_id": "2509.12742",
        "ARXIVID": "2509.12742",
        "COMMENT": "Relevant to computer vision and generative modeling, but does not directly match any specific criterion. Focuses on Gaussian management for object reconstruction, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12434": {
        "authors": [
            "Jiahao Yu",
            "Zelei Cheng",
            "Xian Wu",
            "Xinyu Xing"
        ],
        "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization",
        "abstract": "arXiv:2509.12434v1 Announce Type: new  Abstract: Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues.   A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs.   While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS.   Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents.   To bridge this gap, we introduce \\sys, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting.   \\sys augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses.   We validate \\sys by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).   To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches.   On the \\swebench leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with \\sys ranks 1st on \\lite and 4th on \\verified on the open-weight leaderboard, surpassed only by models with over 10x more parameters(\\eg$>$350B).",
        "arxiv_id": "2509.12434",
        "ARXIVID": "2509.12434",
        "COMMENT": "This paper proposes a new entropy-enhanced multi-turn preference optimization for coding agents, focusing on LLMs and multi-turn reasoning. While it is a methodological improvement for LLMs, it does not address spatial intelligence, VLLMs, or embodied AI benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13151": {
        "authors": [
            "Rohan Kumar",
            "Jyothi Swaroopa Jinka",
            "Ravi Kiran Sarvadevabhatla"
        ],
        "title": "TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images",
        "abstract": "arXiv:2509.13151v1 Announce Type: new  Abstract: Recognizing textual attributes such as bold, italic, underline and strikeout is essential for understanding text semantics, structure, and visual presentation. These attributes highlight key information, making them crucial for document analysis. Existing methods struggle with computational efficiency or adaptability in noisy, multilingual settings. To address this, we introduce TexTAR, a multi-task, context-aware Transformer for Textual Attribute Recognition (TAR). Our novel data selection pipeline enhances context awareness, and our architecture employs a 2D RoPE (Rotary Positional Embedding)-style mechanism to incorporate input context for more accurate attribute predictions. We also introduce MMTAD, a diverse, multilingual, multi-domain dataset annotated with text attributes across real-world documents such as legal records, notices, and textbooks. Extensive evaluations show TexTAR outperforms existing methods, demonstrating that contextual awareness contributes to state-of-the-art TAR performance.",
        "arxiv_id": "2509.13151",
        "ARXIVID": "2509.13151",
        "COMMENT": "Does not match any specific criteria. Focuses on textual attribute recognition in document images, not spatial intelligence or VLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12875": {
        "authors": [
            "Jiaqi Wang",
            "Binquan Ji",
            "Haibo Luo",
            "Yiyang Qi",
            "Ruiting Li",
            "Huiyan Wang",
            "Yuantao Han",
            "Cangyi Yang",
            "jiaxu Zhang",
            "Feiliang Ren"
        ],
        "title": "LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning",
        "abstract": "arXiv:2509.12875v1 Announce Type: new  Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.",
        "arxiv_id": "2509.12875",
        "ARXIVID": "2509.12875",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning in LLMs, not spatial or multi-modal aspects.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12279": {
        "authors": [
            "He Gao",
            "Baoxiang Huang",
            "Milena Radenkovic",
            "Borui Li",
            "Ge Chen"
        ],
        "title": "Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance",
        "abstract": "arXiv:2509.12279v1 Announce Type: new  Abstract: Synthetic Aperture Radar (SAR), with its all- weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adap- tation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like dis- tributions, minimizing negative transfer. Meanwhile, a Feature- Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated tar- get pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method.",
        "arxiv_id": "2509.12279",
        "ARXIVID": "2509.12279",
        "COMMENT": "This paper proposes a domain adaptation framework for SAR wake detection using similarity filtering and memory guidance. While it involves computer vision and domain adaptation, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12836": {
        "authors": [
            "Shreyas Shivakumara",
            "Gabriel Eilertsen",
            "Karljohan Lundin Palmerius"
        ],
        "title": "Exploring Metric Fusion for Evaluation of NeRFs",
        "abstract": "arXiv:2509.12836v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics.",
        "arxiv_id": "2509.12836",
        "ARXIVID": "2509.12836",
        "COMMENT": "This paper proposes a new metric fusion approach for evaluating NeRFs, which is related to spatial understanding in generative models, but does not directly address embodied agents or spatial intelligence in agents. It is more about evaluation metrics for generative 3D models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.13089": {
        "authors": [
            "Jonas Werheid",
            "Shengjie He",
            "Aymen Gannouni",
            "Anas Abdelrazeq",
            "Robert H. Schmitt"
        ],
        "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control",
        "abstract": "arXiv:2509.13089v1 Announce Type: new  Abstract: Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions.",
        "arxiv_id": "2509.13089",
        "ARXIVID": "2509.13089",
        "COMMENT": "This paper presents a synthetic data pipeline for visual assembly control in manufacturing, using simulated scene generation and object detection. While it is an application of computer vision, it does not match the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12592": {
        "authors": [
            "Aaron Baughman",
            "Gozde Akay",
            "Eduardo Morales",
            "Rahul Agarwal",
            "Preetika Srivastava"
        ],
        "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis",
        "abstract": "arXiv:2509.12592v1 Announce Type: new  Abstract: We present Match Chat, a real-time, agent-driven assistant designed to enhance the tennis fan experience by delivering instant, accurate responses to match-related queries. Match Chat integrates Generative Artificial Intelligence (GenAI) with Generative Computing (GenComp) techniques to synthesize key insights during live tennis singles matches. The system debuted at the 2025 Wimbledon Championships and the 2025 US Open, where it provided about 1 million users with seamless access to streaming and static data through natural language queries. The architecture is grounded in an Agent-Oriented Architecture (AOA) combining rule engines, predictive models, and agents to pre-process and optimize user queries before passing them to GenAI components. The Match Chat system had an answer accuracy of 92.83% with an average response time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over 96.08% of all queries were guided using interactive prompt design, contributing to a user experience that prioritized clarity, responsiveness, and minimal effort. The system was designed to mask architectural complexity, offering a frictionless and intuitive interface that required no onboarding or technical familiarity. Across both Grand Slam deployments, Match Chat maintained 100% uptime and supported nearly 1 million unique users, underscoring the scalability and reliability of the platform. This work introduces key design patterns for real-time, consumer-facing AI systems that emphasize speed, precision, and usability that highlights a practical path for deploying performant agentic systems in dynamic environments.",
        "arxiv_id": "2509.12592",
        "ARXIVID": "2509.12592",
        "COMMENT": "This paper describes a real-time agent-driven assistant for tennis, integrating GenAI and generative computing for live match queries. While it involves agentic systems and generative AI, it is an application paper focused on sports analytics, not on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}