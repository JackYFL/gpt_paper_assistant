{
    "2511.11007": {
        "authors": [
            "Xinlei Yu",
            "Chengming Xu",
            "Guibin Zhang",
            "Zhangquan Chen",
            "Yudong Zhang",
            "Yongbo He",
            "Peng-Tao Jiang",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
        "abstract": "arXiv:2511.11007v1 Announce Type: new  Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
        "arxiv_id": "2511.11007",
        "ARXIVID": "2511.11007",
        "COMMENT": "VisMem proposes a cognitively-aligned latent vision memory framework for vision-language models (VLMs), improving their performance on complex visual tasks. This is a direct match for criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), with a novel memory mechanism inspired by cognitive theory.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.11266": {
        "authors": [
            "Fabian Schmidt",
            "Markus Enzweiler",
            "Abhinav Valada"
        ],
        "title": "GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving",
        "abstract": "arXiv:2511.11266v1 Announce Type: new  Abstract: Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\\% increase in driving score for LMDrive and 17.5\\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.",
        "arxiv_id": "2511.11266",
        "ARXIVID": "2511.11266",
        "COMMENT": "GraphPilot proposes a novel method for conditioning language-based autonomous driving models on structured relational context via scene graphs, improving spatial reasoning and grounding. This is a strong match for criterion 1 (spatial understanding in embodied agents) and criterion 3 (embodied AI with new methods for spatial reasoning in simulators).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.10946": {
        "authors": [
            "Yifan Liu",
            "Fangneng Zhan",
            "Kaichen Zhou",
            "Yilun Du",
            "Paul Pu Liang",
            "Hanspeter Pfister"
        ],
        "title": "Abstract 3D Perception for Spatial Intelligence in Vision-Language Models",
        "abstract": "arXiv:2511.10946v1 Announce Type: new  Abstract: Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.",
        "arxiv_id": "2511.10946",
        "ARXIVID": "2511.10946",
        "COMMENT": "This paper directly addresses new methodological improvements to spatial understanding and spatial intelligence in vision-language models for embodied agents (criterion 1). It introduces a framework (SandboxVLM) for 3D perception in VLMs, showing significant empirical gains in spatial intelligence tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.11113": {
        "authors": [
            "Yifan Jiang",
            "Yueying Wang",
            "Rui Zhao",
            "Toufiq Parag",
            "Zhimin Chen",
            "Zhenyu Liao",
            "Jayakrishnan Unnikrishnan"
        ],
        "title": "VIDEOP2R: Video Understanding from Perception to Reasoning",
        "abstract": "arXiv:2511.11113v1 Announce Type: new  Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.",
        "arxiv_id": "2511.11113",
        "ARXIVID": "2511.11113",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs). Proposes VideoP2R, a process-aware video reinforcement fine-tuning framework for large video language models, with a new chain-of-thought dataset and a novel RL algorithm, achieving SOTA on video reasoning benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.11434": {
        "authors": [
            "Wei Chow",
            "Jiachun Pan",
            "Yongyuan Liang",
            "Mingze Zhou",
            "Xue Song",
            "Liyu Jia",
            "Saining Zhang",
            "Siliang Tang",
            "Juncheng Li",
            "Fengda Zhang",
            "Weijia Wu",
            "Hanwang Zhang",
            "Tat-Seng Chua"
        ],
        "title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
        "abstract": "arXiv:2511.11434v1 Announce Type: new  Abstract: Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.",
        "arxiv_id": "2511.11434",
        "ARXIVID": "2511.11434",
        "COMMENT": "Matches criterion 2 (new unified multimodal models and benchmarks) and criterion 4 (vision foundation models and their applications to in-context, interleaved comprehension and generation).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.11025": {
        "authors": [
            "Jirong Zha",
            "Yuxuan Fan",
            "Tianyu Zhang",
            "Geng Chen",
            "Yingfeng Chen",
            "Chen Gao",
            "Xinlei Chen"
        ],
        "title": "AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning",
        "abstract": "arXiv:2511.11025v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.",
        "arxiv_id": "2511.11025",
        "ARXIVID": "2511.11025",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI, specifically multi-drone collaborative perception and reasoning, with both simulator and real-world data). Also relevant to criterion 2 (MLLMs in collaborative settings).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.11252": {
        "authors": [
            "Mohamed Amine Ferrag",
            "Abderrahmane Lakas",
            "Merouane Debbah"
        ],
        "title": "UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios",
        "abstract": "arXiv:2511.11252v1 Announce Type: new  Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench",
        "arxiv_id": "2511.11252",
        "ARXIVID": "2511.11252",
        "COMMENT": "This paper introduces UAVBench, a new physically grounded benchmark for agentic AI in autonomous UAV systems, generated via LLMs. It matches criterion 3 (new embodied AI benchmark/simulator) and also touches on multi-modal reasoning evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.10974": {
        "authors": [
            "Haoran Chen",
            "Houze Xu",
            "Micah Goldblum",
            "Daoguo Dong",
            "Zuxuan Wu"
        ],
        "title": "Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning",
        "abstract": "arXiv:2511.10974v1 Announce Type: new  Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.",
        "arxiv_id": "2511.10974",
        "ARXIVID": "2511.10974",
        "COMMENT": "Matches criterion 2 (shows new methods for CLIP-based class-incremental learning, which is a VLLM) and criterion 4 (vision foundation models and their applications). The paper proposes a new two-stage framework for continual learning with CLIP, and introduces optimal-transport calibration for memory statistics, which is a clever statistical trick. The focus on cross-modal consistency and continual learning is a novel angle for VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.11168": {
        "authors": [
            "Hangyu Li",
            "Bofeng Cao",
            "Zhaohui Liang",
            "Wuzhen Li",
            "Juyoung Oh",
            "Yuxuan Chen",
            "Shixiao Liang",
            "Hang Zhou",
            "Chengyuan Ma",
            "Jiaxi Liu",
            "Zheng Li",
            "Peng Zhang",
            "KeKe Long",
            "Maolin Liu",
            "Jackson Jiang",
            "Chunlei Yu",
            "Shengxiang Liu",
            "Hongkai Yu",
            "Xiaopeng Li"
        ],
        "title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
        "abstract": "arXiv:2511.11168v1 Announce Type: new  Abstract: Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.",
        "arxiv_id": "2511.11168",
        "ARXIVID": "2511.11168",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks/simulators). Introduces a new real-world dataset for vehicle-to-vehicle cooperative perception in complex adverse traffic scenarios, with multi-modal data and a novel temporal alignment method. This is a new benchmark for embodied AI in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.11232": {
        "authors": [
            "Mingwei Xing",
            "Xinliang Wang",
            "Yifeng Shi"
        ],
        "title": "DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding",
        "abstract": "arXiv:2511.11232v1 Announce Type: new  Abstract: The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.",
        "arxiv_id": "2511.11232",
        "ARXIVID": "2511.11232",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a Mixture-of-Experts framework for generalizable 3D understanding, with a novel domain-guided spatial routing and entropy-controlled allocation, evaluated on major 3D benchmarks. Strong focus on spatial intelligence and generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.11407": {
        "authors": [
            "Manyu Li",
            "Ruian He",
            "Chenxi Ma",
            "Weimin Tan",
            "Bo Yan"
        ],
        "title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model",
        "abstract": "arXiv:2511.11407v1 Announce Type: new  Abstract: Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.",
        "arxiv_id": "2511.11407",
        "ARXIVID": "2511.11407",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 3 (new benchmark for microscopy VQA with graph-based filtering and human refinement, a novel approach for biomedical vision-language reasoning).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.11077": {
        "authors": [
            "Ke Ma",
            "Yizhou Fang",
            "Jean-Baptiste Weibel",
            "Shuai Tan",
            "Xinggang Wang",
            "Yang Xiao",
            "Yi Fang",
            "Tian Xia"
        ],
        "title": "Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids",
        "abstract": "arXiv:2511.11077v1 Announce Type: new  Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.",
        "arxiv_id": "2511.11077",
        "ARXIVID": "2511.11077",
        "COMMENT": "Matches criterion 3 (new benchmark/dataset for embodied AI, with a focus on physics-informed simulation for liquid perception, which is a novel and underexplored angle).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.10948": {
        "authors": [
            "Ren Zhang",
            "Huilai Li",
            "Chao qi",
            "Guoliang Xu",
            "Tianyu Zhou",
            "Wei wei",
            "Jianqin Yin"
        ],
        "title": "DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition",
        "abstract": "arXiv:2511.10948v1 Announce Type: new  Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.",
        "arxiv_id": "2511.10948",
        "ARXIVID": "2511.10948",
        "COMMENT": "DEFT-LLM introduces a multimodal large language model (MLLM) for micro-expression recognition, with a novel disentanglement approach and a new motion-driven instruction dataset. This is a direct match for criterion 2 (new MLLMs) and also relevant to multi-modal generative modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.11034": {
        "authors": [
            "Pooja Singh",
            "Siddhant Ujjain",
            "Tapan Kumar Gandhi",
            "Sandeep Kumar"
        ],
        "title": "CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging",
        "abstract": "arXiv:2511.11034v1 Announce Type: new  Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.",
        "arxiv_id": "2511.11034",
        "ARXIVID": "2511.11034",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for multi-modal LLMs in medical imaging). Introduces a new benchmark for compositional generalization in medical vision-language models, and evaluates open-source MLLMs, showing unique strengths in compositional generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11043": {
        "authors": [
            "Asen Nachkov",
            "Jan-Nico Zaech",
            "Danda Pani Paudel",
            "Xi Wang",
            "Luc Van Gool"
        ],
        "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation",
        "abstract": "arXiv:2511.11043v1 Announce Type: new  Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.",
        "arxiv_id": "2511.11043",
        "ARXIVID": "2511.11043",
        "COMMENT": "Matches criterion 3 (new method for embodied AI path planning using differentiable simulation, a novel approach for autonomous vehicles).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10997": {
        "authors": [
            "Jiajun Chen",
            "Sai Cheng",
            "Yutao Yuan",
            "Yirui Zhang",
            "Haitao Yuan",
            "Peng Peng",
            "Yi Zhong"
        ],
        "title": "PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities",
        "abstract": "arXiv:2511.10997v1 Announce Type: new  Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.",
        "arxiv_id": "2511.10997",
        "ARXIVID": "2511.10997",
        "COMMENT": "Matches criterion 2 (new multimodal large language model methods) and is relevant to robust cross-modal representation learning, especially under missing modalities, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11132": {
        "authors": [
            "Yu Zhao",
            "Ying Zhang",
            "Xuhui Sui",
            "Baohang Zhou",
            "Li Shen",
            "Dacheng Tao"
        ],
        "title": "Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering",
        "abstract": "arXiv:2511.11132v1 Announce Type: new  Abstract: Knowledge-based Visual Question Answering (KBVQA) necessitates external knowledge incorporation beyond cross-modal understanding. Existing KBVQA methods either utilize implicit knowledge in multimodal large language models (MLLMs) via in-context learning or explicit knowledge via retrieval augmented generation. However, their reasoning processes remain implicit, without explicit multi-step trajectories from MLLMs. To address this gap, we provide a Hindsight Distilled Reasoning (HinD) framework with Knowledge Encouragement Preference Optimization (KEPO), designed to elicit and harness internal knowledge reasoning ability in MLLMs. First, to tackle the reasoning supervision problem, we propose to emphasize the hindsight wisdom of MLLM by prompting a frozen 7B-size MLLM to complete the reasoning process between the question and its ground truth answer, constructing Hindsight-Zero training data. Then we self-distill Hindsight-Zero into Chain-of-Thought (CoT) Generator and Knowledge Generator, enabling the generation of sequential steps and discrete facts. Secondly, to tackle the misalignment between knowledge correctness and confidence, we optimize the Knowledge Generator with KEPO, preferring under-confident but helpful knowledge over the over-confident but unhelpful one. The generated CoT and sampled knowledge are then exploited for answer prediction. Experiments on OK-VQA and A-OKVQA validate the effectiveness of HinD, showing that HinD with elicited reasoning from 7B-size MLLM achieves superior performance without commercial model APIs or outside knowledge.",
        "arxiv_id": "2511.11132",
        "ARXIVID": "2511.11132",
        "COMMENT": "HinD introduces a new framework for knowledge-based visual question answering (KBVQA) using MLLMs, with a focus on explicit multi-step reasoning and knowledge preference optimization. This is a direct match for criterion 2 (new MLLMs) and also relevant to vision-language reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11470": {
        "authors": [
            "Yijie Kang",
            "Xinliang Wang",
            "Zhenyu Wu",
            "Yifeng Shi",
            "Hailong Zhu"
        ],
        "title": "Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery",
        "abstract": "arXiv:2511.11470v1 Announce Type: new  Abstract: Recent advances in generative modeling have substantially enhanced 3D urban generation, enabling applications in digital twins, virtual cities, and large-scale simulations. However, existing methods face two key challenges: (1) the need for large-scale 3D city assets for supervised training, which are difficult and costly to obtain, and (2) reliance on semantic or height maps, which are used exclusively for generating buildings in virtual worlds and lack connection to real-world appearance, limiting the realism and generalizability of generated cities. To address these limitations, we propose Sat2RealCity, a geometry-aware and appearance-controllable framework for 3D urban generation from real-world satellite imagery. Unlike previous city-level generation methods, Sat2RealCity builds generation upon individual building entities, enabling the use of rich priors and pretrained knowledge from 3D object generation while substantially reducing dependence on large-scale 3D city assets. Specifically, (1) we introduce the OSM-based spatial priors strategy to achieve interpretable geometric generation from spatial topology to building instances; (2) we design an appearance-guided controllable modeling mechanism for fine-grained appearance realism and style control; and (3) we construct an MLLM-powered semantic-guided generation pipeline, bridging semantic interpretation and geometric reconstruction. Extensive quantitative and qualitative experiments demonstrate that Sat2RealCity significantly surpasses existing baselines in structural consistency and appearance realism, establishing a strong foundation for real-world aligned 3D urban content creation. The code will be released soon.",
        "arxiv_id": "2511.11470",
        "ARXIVID": "2511.11470",
        "COMMENT": "Sat2RealCity proposes a geometry-aware and appearance-controllable 3D urban generation framework from satellite imagery, with an MLLM-powered semantic-guided generation pipeline. This matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications), and also touches on spatial intelligence (criterion 1) in the context of urban modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11410": {
        "authors": [
            "Jiaxi Huang",
            "Dongxu Wu",
            "Hanwei Zhu",
            "Lingyu Zhu",
            "Jun Xing",
            "Xu Wang",
            "Baoliang Chen"
        ],
        "title": "Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models",
        "abstract": "arXiv:2511.11410v1 Announce Type: new  Abstract: The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:   https://github.com/cydxf/Q-Doc.",
        "arxiv_id": "2511.11410",
        "ARXIVID": "2511.11410",
        "COMMENT": "This paper introduces a new benchmark for evaluating document image quality assessment in MLLMs (criterion 2 and 3). It systematically probes the DIQA capabilities of MLLMs and provides a new benchmark and analysis, which is a direct match for new MLLM evaluation and benchmarking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11519": {
        "authors": [
            "Adam Stein",
            "Matthew Trager",
            "Benjamin Bowman",
            "Michael Kleinman",
            "Aditya Chattopadhyay",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
        "abstract": "arXiv:2511.11519v1 Announce Type: new  Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
        "arxiv_id": "2511.11519",
        "ARXIVID": "2511.11519",
        "COMMENT": "Matches criterion 1 (methodological improvements to agentic AI reasoning strategies) and is relevant to spatial intelligence in embodied agents, as it proposes a meta-strategy for inference-time adaptation in agentic systems. Also of interest for clever statistical/meta-learning tricks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.11243": {
        "authors": [
            "Jitesh Chavan",
            "Rohit Lal",
            "Anand Kamat",
            "Mengjia Xu"
        ],
        "title": "Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs",
        "abstract": "arXiv:2511.11243v1 Announce Type: new  Abstract: State-space models (SSMs), Mamba in particular, are increasingly adopted for long-context sequence modeling, providing linear-time aggregation via an input-dependent, causal selective-scan operation. Along this line, recent \"Mamba-for-vision\" variants largely explore multiple scan orders to relax strict causality for non-sequential signals (e.g., images). Rather than preserving cross-block memory, the conventional formulation of the selective-scan operation in Mamba reinitializes each block's state-space dynamics from zero, discarding the terminal state-space representation (SSR) from the previous block. Arcee, a cross-block recurrent state chain, reuses each block's terminal state-space representation as the initial condition for the next block. Handoff across blocks is constructed as a differentiable boundary map whose Jacobian enables end-to-end gradient flow across terminal boundaries. Key to practicality, Arcee is compatible with all prior \"vision-mamba\" variants, parameter-free, and incurs constant, negligible cost. As a modeling perspective, we view terminal SSR as a mild directional prior induced by a causal pass over the input, rather than an estimator of the non-sequential signal itself. To quantify the impact, for unconditional generation on CelebA-HQ (256$\\times$256) with Flow Matching, Arcee reduces FID$\\downarrow$ from $82.81$ to $15.33$ ($5.4\\times$ lower) on a single scan-order Zigzag Mamba baseline. Efficient CUDA kernels and training code will be released to support rigorous and reproducible research.",
        "arxiv_id": "2511.11243",
        "ARXIVID": "2511.11243",
        "COMMENT": "Arcee introduces a differentiable recurrent state chain for generative vision modeling with Mamba SSMs, improving long-context sequence modeling for images. This is relevant to criterion 4 (vision foundation models and applications), especially for generative modeling in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.11045": {
        "authors": [
            "Wenrui Li",
            "Yidan Lu",
            "Yeyu Chai",
            "Rui Zhao",
            "Hengyu Man",
            "Xiaopeng Fan"
        ],
        "title": "Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval",
        "abstract": "arXiv:2511.11045v1 Announce Type: new  Abstract: With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention. However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD). HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model's ability to distinguish hard negatives. To address these challenges, we introduce the Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN) for text-3D retrieval. H$^{2}$ARN embeds both text and 3D data in a Lorentz-model hyperbolic space, where exponential volume growth inherently preserves hierarchical distances. A hierarchical ordering loss constructs a shrinking entailment cone around each text vector, ensuring that the matched 3D instance falls within the cone, while an instance-level contrastive loss jointly enforces separation from non-matching samples. To tackle RISD, we propose a contribution-aware hyperbolic aggregation module that leverages Lorentzian distance to assess the relevance of each local feature and applies contribution-weighted aggregation guided by hyperbolic geometry, enhancing discriminative regions while suppressing redundancy without additional supervision. We also release the expanded T3DR-HIT v2 benchmark, which contains 8,935 text-to-3D pairs, 2.6 times the original size, covering both fine-grained cultural artefacts and complex indoor scenes. Our codes are available at https://github.com/liwrui/H2ARN.",
        "arxiv_id": "2511.11045",
        "ARXIVID": "2511.11045",
        "COMMENT": "This paper proposes a new hyperbolic hierarchical alignment reasoning network for text-3D retrieval, addressing spatial and semantic hierarchy issues. It is relevant to spatial understanding (criterion 1) and vision foundation model applications (criterion 4), especially in 3D retrieval.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.10953": {
        "authors": [
            "Wenrui Li",
            "Wei Han",
            "Hengyu Man",
            "Wangmeng Zuo",
            "Xiaopeng Fan",
            "Yonghong Tian"
        ],
        "title": "Language-Guided Graph Representation Learning for Video Summarization",
        "abstract": "arXiv:2511.10953v1 Announce Type: new  Abstract: With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.",
        "arxiv_id": "2511.10953",
        "ARXIVID": "2511.10953",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in video summarization via graph representation learning and cross-modal embedding). The use of language-guided graph representation and dual-threshold graph convolution for semantic frame selection is a novel approach for spatial and temporal reasoning in video. Also relevant to criterion 4 (vision foundation models and applications) due to the cross-modal embedding, but the main contribution is in spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.11060": {
        "authors": [
            "Jiaxuan Chen",
            "Bo Zhang",
            "Qingdong He",
            "Jinlong Peng",
            "Li Niu"
        ],
        "title": "CareCom: Generative Image Composition with Calibrated Reference Features",
        "abstract": "arXiv:2511.11060v1 Announce Type: new  Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.",
        "arxiv_id": "2511.11060",
        "ARXIVID": "2511.11060",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a generative image composition model with calibrated reference features, improving detail preservation and pose adjustment, and supports multi-reference composition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10894": {
        "authors": [
            "Luciano Araujo Dourado Filho",
            "Almir Moreira da Silva Neto",
            "Anthony Miyaguchi",
            "Rodrigo Pereira David",
            "Rodrigo Tripodi Calumby",
            "Luk\\'a\\v{s} Picek"
        ],
        "title": "DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting",
        "abstract": "arXiv:2511.10894v1 Announce Type: new  Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\\approx$26\\% in effectiveness gain against the best 3D-UNET.",
        "arxiv_id": "2511.10894",
        "ARXIVID": "2511.10894",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Uses DINOv3, a vision foundation model, as a frozen encoder for probabilistic rainfall nowcasting, showing significant performance gains over baselines.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.11236": {
        "authors": [
            "Cong Cao",
            "Yujie Xu",
            "Xiaodong Xu"
        ],
        "title": "Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing",
        "abstract": "arXiv:2511.11236v1 Announce Type: new  Abstract: In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.",
        "arxiv_id": "2511.11236",
        "ARXIVID": "2511.11236",
        "COMMENT": "This paper proposes a parameter-efficient MoE LoRA for few-shot multi-style image editing, with a new benchmark and method for style transfer. It is relevant to vision foundation models and their applications (criterion 4), especially in generative modeling and efficient adaptation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.10721": {
        "authors": [
            "Sheng-Yu Wang",
            "Aaron Hertzmann",
            "Alexei A Efros",
            "Richard Zhang",
            "Jun-Yan Zhu"
        ],
        "title": "Fast Data Attribution for Text-to-Image Models",
        "abstract": "arXiv:2511.10721v1 Announce Type: new  Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.",
        "arxiv_id": "2511.10721",
        "ARXIVID": "2511.10721",
        "COMMENT": "This paper is related to vision foundation models and their applications (criterion 4), specifically focusing on data attribution for text-to-image models like Stable Diffusion. It proposes a scalable and efficient method for data attribution, which is a novel application area for vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.11162": {
        "authors": [
            "Zhanpeng Wang",
            "Shuting Cao",
            "Yuhang Lu",
            "Yuhan Li",
            "Na Lei",
            "Zhongxuan Luo"
        ],
        "title": "OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation",
        "abstract": "arXiv:2511.11162v1 Announce Type: new  Abstract: The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.",
        "arxiv_id": "2511.11162",
        "ARXIVID": "2511.11162",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications). Proposes a new image-to-image translation framework using optimal transport to align latent distributions, improving efficiency and quality. While not directly about foundation models, it is a novel generative modeling method in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.11062": {
        "authors": [
            "Dor Shmilovich",
            "Tony Wu",
            "Aviad Dahan",
            "Yuval Domb"
        ],
        "title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
        "abstract": "arXiv:2511.11062v1 Announce Type: new  Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+\\delta$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.",
        "arxiv_id": "2511.11062",
        "ARXIVID": "2511.11062",
        "COMMENT": "Relevant to generative modeling (diffusion models) and introduces a clever statistical trick (temporal sparse attention), but does not directly match any of the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2511.11253": {
        "authors": [
            "Hyemin Boo",
            "Hyoryung Kim",
            "Myungjin Lee",
            "Seunghyeon Lee",
            "Jiyoung Lee",
            "Jang-Hwan Choi",
            "Hyunsoo Cho"
        ],
        "title": "CountSteer: Steering Attention for Object Counting in Diffusion Models",
        "abstract": "arXiv:2511.11253v1 Announce Type: new  Abstract: Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the model's cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.",
        "arxiv_id": "2511.11253",
        "ARXIVID": "2511.11253",
        "COMMENT": "Relevant to generative modeling and vision foundation models (diffusion models), and introduces a clever method for object counting in text-to-image generation, but does not directly match the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.11048": {
        "authors": [
            "Sun Jo",
            "Seok Young Hong",
            "JinHyun Kim",
            "Seungmin Kang",
            "Ahjin Choi",
            "Don-Gwan An",
            "Simon Song",
            "Je Hyeong Hong"
        ],
        "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI",
        "abstract": "arXiv:2511.11048v1 Announce Type: new  Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.",
        "arxiv_id": "2511.11048",
        "ARXIVID": "2511.11048",
        "COMMENT": "This paper introduces a new method for efficient super-resolution of 4D Flow MRI using axes-aligned spatiotemporal Gaussian representations, with innovations in normalized Gaussian splatting and merging. While it is a methodological improvement in spatial modeling, it is focused on medical imaging rather than embodied agents or general spatial intelligence. Closest to criterion 1, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.11119": {
        "authors": [
            "Xinyue Zhang",
            "Haolong Li",
            "Jiawei Ma",
            "Chen Ye"
        ],
        "title": "Stroke Modeling Enables Vectorized Character Generation with Large Vectorized Glyph Model",
        "abstract": "arXiv:2511.11119v1 Announce Type: new  Abstract: Vectorized glyphs are widely used in poster design, network animation, art display, and various other fields due to their scalability and flexibility. In typography, they are often seen as special sequences composed of ordered strokes. This concept extends to the token sequence prediction abilities of large language models (LLMs), enabling vectorized character generation through stroke modeling. In this paper, we propose a novel Large Vectorized Glyph Model (LVGM) designed to generate vectorized Chinese glyphs by predicting the next stroke. Initially, we encode strokes into discrete latent variables called stroke embeddings. Subsequently, we train our LVGM via fine-tuning DeepSeek LLM by predicting the next stroke embedding. With limited strokes given, it can generate complete characters, semantically elegant words, and even unseen verses in vectorized form. Moreover, we release a new large-scale Chinese SVG dataset containing 907,267 samples based on strokes for dynamically vectorized glyph generation. Experimental results show that our model has scaling behaviors on data scales. Our generated vectorized glyphs have been validated by experts and relevant individuals.",
        "arxiv_id": "2511.11119",
        "ARXIVID": "2511.11119",
        "COMMENT": "Does not directly match any criterion, but is tangentially related to generative modeling and vectorized glyph generation. Not a direct match to vision-language or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.10799": {
        "authors": [
            "Manish Dhakal",
            "Venkat R. Dasari",
            "Raj Sunderraman",
            "Yi Ding"
        ],
        "title": "GFT: Graph Feature Tuning for Efficient Point Cloud Analysis",
        "abstract": "arXiv:2511.10799v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is at https://github.com/manishdhakal/GFT.",
        "arxiv_id": "2511.10799",
        "ARXIVID": "2511.10799",
        "COMMENT": "GFT proposes a parameter-efficient fine-tuning method for point cloud analysis using graph features. While it is a methodological improvement in spatial understanding for point clouds, it is not focused on embodied agents or spatial intelligence in embodied AI. Closest to criterion 1, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.11286": {
        "authors": [
            "Ruoqi Wang",
            "Haitao Wang",
            "Shaojie Guo",
            "Qiong Luo"
        ],
        "title": "D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces",
        "abstract": "arXiv:2511.11286v1 Announce Type: new  Abstract: Out-of-domain (OOD) robustness is challenging to achieve in real-world computer vision applications, where shifts in image background, style, and acquisition instruments always degrade model performance. Generic augmentations show inconsistent gains under such shifts, whereas dataset-specific augmentations require expert knowledge and prior analysis. Moreover, prior studies show that neural networks adapt poorly to domain shifts because they exhibit a learning bias to domain-specific frequency components. Perturbing frequency values can mitigate such bias but overlooks pixel-level details, leading to suboptimal performance. To address these problems, we propose D-GAP (Dataset-agnostic and Gradient-guided augmentation in Amplitude and Pixel spaces), improving OOD robustness by introducing targeted augmentation in both the amplitude space (frequency space) and pixel space. Unlike conventional handcrafted augmentations, D-GAP computes sensitivity maps in the frequency space from task gradients, which reflect how strongly the model responds to different frequency components, and uses the maps to adaptively interpolate amplitudes between source and target samples. This way, D-GAP reduces the learning bias in frequency space, while a complementary pixel-space blending procedure restores fine spatial details. Extensive experiments on four real-world datasets and three domain-adaptation benchmarks show that D-GAP consistently outperforms both generic and dataset-specific augmentations, improving average OOD performance by +5.3% on real-world datasets and +1.8% on benchmark datasets.",
        "arxiv_id": "2511.11286",
        "ARXIVID": "2511.11286",
        "COMMENT": "Presents a new augmentation method (D-GAP) for improving out-of-domain robustness in computer vision models, using gradient-guided augmentation in both amplitude and pixel spaces. While it is a clever statistical trick for vision models, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Relevant to general interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.11014": {
        "authors": [
            "Sumin Yu",
            "Taesup Moon"
        ],
        "title": "SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation",
        "abstract": "arXiv:2511.11014v1 Announce Type: new  Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.",
        "arxiv_id": "2511.11014",
        "ARXIVID": "2511.11014",
        "COMMENT": "This paper addresses safety in text-to-image generation, which is tangentially related to vision foundation models (criterion 4), but focuses more on prompt-adaptive guidance for safer image generation rather than core model or methodology improvements.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.11406": {
        "authors": [
            "Feng-Qi Cui",
            "Jinyang Huang",
            "Ziyu Jia",
            "Xinyu Li",
            "Xin Yan",
            "Xiaokang Zhou",
            "Meng Wang"
        ],
        "title": "Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis",
        "abstract": "arXiv:2511.11406v1 Announce Type: new  Abstract: Video-based Affective Computing (VAC), vital for emotion analysis and human-computer interaction, suffers from model instability and representational degradation due to complex emotional dynamics. Since the meaning of different emotional fluctuations may differ under different emotional contexts, the core limitation is the lack of a hierarchical structural mechanism to disentangle distinct affective components, i.e., emotional bases (the long-term emotional tone), and transient fluctuations (the short-term emotional fluctuations). To address this, we propose the Low-Rank Sparse Emotion Understanding Framework (LSEF), a unified model grounded in the Low-Rank Sparse Principle, which theoretically reframes affective dynamics as a hierarchical low-rank sparse compositional process. LSEF employs three plug-and-play modules, i.e., the Stability Encoding Module (SEM) captures low-rank emotional bases; the Dynamic Decoupling Module (DDM) isolates sparse transient signals; and the Consistency Integration Module (CIM) reconstructs multi-scale stability and reactivity coherence. This framework is optimized by a Rank Aware Optimization (RAO) strategy that adaptively balances gradient smoothness and sensitivity. Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination, which further validates the effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics.",
        "arxiv_id": "2511.11406",
        "ARXIVID": "2511.11406",
        "COMMENT": "Does not directly match any criterion. Focuses on video-based affective computing using a low-rank sparse decomposition approach, which is interesting for representation learning but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.11378": {
        "authors": [
            "Julian Grolig",
            "Lars Griem",
            "Michael Selzer",
            "Hans-Ulrich Kauczor",
            "Simon M. F. Triphan",
            "Britta Nestler",
            "Arnd Koeppe"
        ],
        "title": "Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net",
        "abstract": "arXiv:2511.11378v1 Announce Type: new  Abstract: Extracting digital material representations from images is a necessary prerequisite for a quantitative analysis of material properties. Different segmentation approaches have been extensively studied in the past to achieve this task, but were often lacking accuracy or speed. With the advent of machine learning, supervised convolutional neural networks (CNNs) have achieved state-of-the-art performance for different segmentation tasks. However, these models are often trained in a supervised manner, which requires large labeled datasets. Unsupervised approaches do not require ground-truth data for learning, but suffer from long segmentation times and often worse segmentation accuracy. Hidden Markov Random Fields (HMRF) are an unsupervised segmentation approach that incorporates concepts of neighborhood and class distributions. We present a method that integrates HMRF theory and CNN segmentation, leveraging the advantages of both areas: unsupervised learning and fast segmentation times. We investigate the contribution of different neighborhood terms and components for the unsupervised HMRF loss. We demonstrate that the HMRF-UNet enables high segmentation accuracy without ground truth on a Micro-Computed Tomography (${\\mu}$CT) image dataset of Polyurethane (PU) foam structures. Finally, we propose and demonstrate a pre-training strategy that considerably reduces the required amount of ground-truth data when training a segmentation model.",
        "arxiv_id": "2511.11378",
        "ARXIVID": "2511.11378",
        "COMMENT": "This paper presents an unsupervised segmentation method for micro-CT scans, combining HMRF and U-Net. While it is a computer vision paper, it does not match any of the specific criteria (spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.11393": {
        "authors": [
            "Zejiao Liu",
            "Yi Li",
            "Jiali Wang",
            "Junqi Tu",
            "Yitian Hong",
            "Fangfei Li",
            "Yang Liu",
            "Toshiharu Sugawara",
            "Yang Tang"
        ],
        "title": "Robust and Efficient Communication in Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2511.11393v1 Announce Type: new  Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.",
        "arxiv_id": "2511.11393",
        "ARXIVID": "2511.11393",
        "COMMENT": "Somewhat relevant to criterion 3 (embodied AI, multi-agent systems). This is a survey on robust and efficient communication in multi-agent reinforcement learning, with applications in autonomous driving and SLAM. However, it is a survey, not a new method or benchmark.",
        "RELEVANCE": 4,
        "NOVELTY": 3
    },
    "2511.10810": {
        "authors": [
            "Ran Elgedawy",
            "Sanjay Das",
            "Ethan Seefried",
            "Gavin Wiggins",
            "Ryan Burchfield",
            "Dana Hewit",
            "Sudarshan Srinivasan",
            "Todd Thomas",
            "Prasanna Balaprakash",
            "Tirthankar Ghosal"
        ],
        "title": "HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments",
        "abstract": "arXiv:2511.10810v1 Announce Type: new  Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.",
        "arxiv_id": "2511.10810",
        "ARXIVID": "2511.10810",
        "COMMENT": "Does not directly match any criterion. Focuses on risk forecasting in DOE environments using LLMs and human-in-the-loop, but not on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}