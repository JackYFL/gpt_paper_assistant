{
    "2511.09057": {
        "authors": [
            "PAN Team",
            "Jiannan Xiang",
            "Yi Gu",
            "Zihan Liu",
            "Zeyu Feng",
            "Qiyue Gao",
            "Yiyan Hu",
            "Benhao Huang",
            "Guangyi Liu",
            "Yichi Yang",
            "Kun Zhou",
            "Davit Abrahamyan",
            "Arif Ahmad",
            "Ganesh Bannur",
            "Junrong Chen",
            "Kimi Chen",
            "Mingkai Deng",
            "Ruobing Han",
            "Xinqi Huang",
            "Haoqiang Kang",
            "Zheqi Li",
            "Enze Ma",
            "Hector Ren",
            "Yashowardhan Shinde",
            "Rohan Shingre",
            "Ramsundar Tanikella",
            "Kaiming Tao",
            "Dequan Yang",
            "Xinle Yu",
            "Cong Zeng",
            "Binglin Zhou",
            "Hector Liu",
            "Zhiting Hu",
            "Eric P. Xing"
        ],
        "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
        "abstract": "arXiv:2511.09057v1 Announce Type: new  Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
        "arxiv_id": "2511.09057",
        "ARXIVID": "2511.09057",
        "COMMENT": "Matches criteria 1 and 3 very closely: introduces a new general, interactable, and long-horizon world model (PAN) for embodied agents, with a novel architecture combining LLM-based latent dynamics and video diffusion for action-conditioned simulation. Also relevant to vision foundation models (criterion 4) and multi-modal generative modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2511.09018": {
        "authors": [
            "Liu Yu",
            "Zhonghao Chen",
            "Ping Kuang",
            "Zhikun Feng",
            "Fan Zhou",
            "Lan Wang",
            "Gillian Dobbie"
        ],
        "title": "Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs",
        "abstract": "arXiv:2511.09018v1 Announce Type: new  Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL",
        "arxiv_id": "2511.09018",
        "ARXIVID": "2511.09018",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): proposes a causally-grounded dual-path attention intervention for object hallucination mitigation in LVLMs, with a novel metric (VTACR) and decoding strategy. Also of interest for clever statistical tricks and empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.08823": {
        "authors": [
            "Wonbong Jang",
            "Jonathan Tremblay",
            "Lourdes Agapito"
        ],
        "title": "DT-NVS: Diffusion Transformers for Novel View Synthesis",
        "abstract": "arXiv:2511.08823v1 Announce Type: new  Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.",
        "arxiv_id": "2511.08823",
        "ARXIVID": "2511.08823",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is also relevant to generative modeling in multi-modal learning. Proposes a 3D-aware diffusion transformer for novel view synthesis from a single image, with new transformer/self-attention architectures and camera conditioning strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.09397": {
        "authors": [
            "Haiyi Li",
            "Qi Chen",
            "Denis Kalkofen",
            "Hsiang-Ting Chen"
        ],
        "title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS",
        "abstract": "arXiv:2511.09397v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.",
        "arxiv_id": "2511.09397",
        "ARXIVID": "2511.09397",
        "COMMENT": "Matches criterion 3 (Embodied AI papers on building new benchmark or new methods, focusing on novel angles). Proposes a new object-aware uncertainty estimation method for active view selection in 3D Gaussian Splatting, with a principled, physically-grounded approach for object-centric tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.08704": {
        "authors": [
            "Xinchen Yan",
            "Chen Liang",
            "Lijun Yu",
            "Adams Wei Yu",
            "Yifeng Lu",
            "Quoc V. Le"
        ],
        "title": "Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?",
        "abstract": "arXiv:2511.08704v1 Announce Type: new  Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.",
        "arxiv_id": "2511.08704",
        "ARXIVID": "2511.08704",
        "COMMENT": "Matches criterion 4 (Vision foundation models related and its applications). Investigates scaling properties of autoregressive next-pixel prediction for unified vision models, with insights into compute/data scaling for vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.09147": {
        "authors": [
            "Jiayue Yuan",
            "Fangting Xie",
            "Guangwen Ouyang",
            "Changhai Ma",
            "Ziyu Wu",
            "Heyu Ding",
            "Quan Wan",
            "Yi Ke",
            "Yuchen Wu",
            "Xiaohui Cai"
        ],
        "title": "PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery",
        "abstract": "arXiv:2511.09147v1 Announce Type: new  Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \\textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \\textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \\& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.",
        "arxiv_id": "2511.09147",
        "ARXIVID": "2511.09147",
        "COMMENT": "Presents PressTrack-HMR, a new method and dataset for multi-person human mesh recovery using pressure-based signals, focusing on privacy-preserving, occlusion-free action recognition. This is a novel method and benchmark for embodied AI (criterion 3), with a unique angle using tactile data instead of vision.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.08833": {
        "authors": [
            "Jiaxun Guo",
            "Manar Amayri",
            "Nizar Bouguila",
            "Xin Liu",
            "Wentao Fan"
        ],
        "title": "Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms",
        "abstract": "arXiv:2511.08833v1 Announce Type: new  Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.",
        "arxiv_id": "2511.08833",
        "ARXIVID": "2511.08833",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in 3D learning). Proposes a novel method for rotation-invariant 3D point cloud learning with global pose awareness and attention mechanisms, addressing a key limitation in previous methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.09170": {
        "authors": [
            "Ethan Griffiths",
            "Maryam Haghighat",
            "Simon Denman",
            "Clinton Fookes",
            "Milad Ramezani"
        ],
        "title": "HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests",
        "abstract": "arXiv:2511.09170v1 Announce Type: new  Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.",
        "arxiv_id": "2511.09170",
        "ARXIVID": "2511.09170",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents) and criterion 3 (new benchmark for vision-based localization in forests, with a novel octree-based transformer and multi-scale geometric verification). The method is robust to challenging spatial scenarios and introduces a new approach to hierarchical place recognition and localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.08409": {
        "authors": [
            "Junxian Li",
            "Xinyue Xu",
            "Sai Ma",
            "Sichao Li"
        ],
        "title": "FaithAct: Faithfulness Planning and Acting in MLLMs",
        "abstract": "arXiv:2511.08409v1 Announce Type: new  Abstract: Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",
        "arxiv_id": "2511.08409",
        "ARXIVID": "2511.08409",
        "COMMENT": "Matches criterion 2 (Shows new VLLMs or MLLMs). Proposes a new framework for enforcing and evaluating faithfulness in multimodal reasoning, with experiments on MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.08666": {
        "authors": [
            "Joseph Fioresi",
            "Ishan Rajendrakumar Dave",
            "Mubarak Shah"
        ],
        "title": "Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding",
        "abstract": "arXiv:2511.08666v1 Announce Type: new  Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.",
        "arxiv_id": "2511.08666",
        "ARXIVID": "2511.08666",
        "COMMENT": "Proposes a latent-space anonymization method for privacy-preserving video understanding in video foundation models, with a new anonymizing adapter module and new evaluation protocols. This is a methodological improvement for vision foundation models (criterion 4), and is relevant to applications in privacy-preserving video analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.08246": {
        "authors": [
            "Ziyu Ma",
            "Chenhui Gou",
            "Yiming Hu",
            "Yong Wang",
            "Xiangxiang Chu",
            "Bohan Zhuang",
            "Jianfei Cai"
        ],
        "title": "Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning",
        "abstract": "arXiv:2511.08246v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.",
        "arxiv_id": "2511.08246",
        "ARXIVID": "2511.08246",
        "COMMENT": "Matches criterion 2 (new methods for many-shot multimodal in-context learning in LMMs). Proposes a sensitivity-aware task vector insertion framework for improved ICL in multimodal models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.08809": {
        "authors": [
            "Abu Taib Mohammed Shahjahan",
            "A. Ben Hamza"
        ],
        "title": "Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation",
        "abstract": "arXiv:2511.08809v1 Announce Type: new  Abstract: Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.",
        "arxiv_id": "2511.08809",
        "ARXIVID": "2511.08809",
        "COMMENT": "Matches criterion 1 (methodological improvement to spatial understanding in embodied agents) and criterion 4 (vision foundation models). Proposes an adaptive graph Kolmogorov-Arnold Network for 3D human pose estimation, improving spatial awareness.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.08633": {
        "authors": [
            "Assaf Singer",
            "Noam Rotstein",
            "Amir Mann",
            "Ron Kimmel",
            "Or Litany"
        ],
        "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising",
        "abstract": "arXiv:2511.08633v1 Announce Type: new  Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.",
        "arxiv_id": "2511.08633",
        "ARXIVID": "2511.08633",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a training-free, plug-and-play method for motion-controlled video generation using diffusion models, with strong empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2511.08977": {
        "authors": [
            "Zihua Wang",
            "Jiarui Wang",
            "Haiyang Xu",
            "Ming Yan",
            "Fei Huang",
            "Xu Yang",
            "Xiu-Shen Wei",
            "Siya Mi",
            "Yu Zhang"
        ],
        "title": "Efficient and Effective In-context Demonstration Selection with Coreset",
        "abstract": "arXiv:2511.08977v1 Announce Type: new  Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.",
        "arxiv_id": "2511.08977",
        "ARXIVID": "2511.08977",
        "COMMENT": "Matches criterion 2 (new method for in-context demonstration selection for LVLMs). Proposes a coreset-based dual retrieval framework for more effective ICL in visual language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.07690": {
        "authors": [
            "Soham Hans",
            "Volkan Ustun",
            "Benjamin Nye",
            "James Sterrett",
            "Matthew Green"
        ],
        "title": "Towards AI-Assisted Generation of Military Training Scenarios",
        "abstract": "arXiv:2511.07690v1 Announce Type: new  Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.",
        "arxiv_id": "2511.07690",
        "ARXIVID": "2511.07690",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for scenario generation in simulators, multi-agent LLM-based framework for military training). Novel approach to complex scenario generation with multi-modal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.09055": {
        "authors": [
            "Xingchi Chen",
            "Pu Wang",
            "Xuerui Li",
            "Chaopeng Li",
            "Juxiang Zhou",
            "Jianhou Gan",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Wenqi Ren",
            "Zhuoran Zheng"
        ],
        "title": "4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching",
        "abstract": "arXiv:2511.09055v1 Announce Type: new  Abstract: Ultra-High-Definition (UHD) image dehazing faces challenges such as limited scene adaptability in prior-based methods and high computational complexity with color distortion in deep learning approaches. To address these issues, we propose 4KDehazeFlow, a novel method based on Flow Matching and the Haze-Aware vector field. This method models the dehazing process as a progressive optimization of continuous vector field flow, providing efficient data-driven adaptive nonlinear color transformation for high-quality dehazing. Specifically, our method has the following advantages: 1) 4KDehazeFlow is a general method compatible with various deep learning networks, without relying on any specific network architecture. 2) We propose a learnable 3D lookup table (LUT) that encodes haze transformation parameters into a compact 3D mapping matrix, enabling efficient inference through precomputed mappings. 3) We utilize a fourth-order Runge-Kutta (RK4) ordinary differential equation (ODE) solver to stably solve the dehazing flow field through an accurate step-by-step iterative method, effectively suppressing artifacts. Extensive experiments show that 4KDehazeFlow exceeds seven state-of-the-art methods. It delivers a 2dB PSNR increase and better performance in dense haze and color fidelity.",
        "arxiv_id": "2511.09055",
        "ARXIVID": "2511.09055",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel flow-matching method for ultra-high-definition image dehazing, with a learnable 3D LUT and ODE solver for efficient and high-quality results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.09443": {
        "authors": [
            "Hongchao Shu",
            "Roger D. Soberanis-Mukul",
            "Jiru Xu",
            "Hao Ding",
            "Morgan Ringel",
            "Mali Shen",
            "Saif Iftekar Sayed",
            "Hedyeh Rafii-Tari",
            "Mathias Unberath"
        ],
        "title": "BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation",
        "abstract": "arXiv:2511.09443v1 Announce Type: new  Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.",
        "arxiv_id": "2511.09443",
        "ARXIVID": "2511.09443",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and method for vision-based pose optimization in bronchoscopy navigation). Introduces a synthetic benchmark and a robust, domain-invariant vision-based localization method using fine-tuned foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.08872": {
        "authors": [
            "Hu Cui",
            "Wenqiang Hua",
            "Renjing Huang",
            "Shurui Jia",
            "Tessai Hayama"
        ],
        "title": "SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation",
        "abstract": "arXiv:2511.08872v1 Announce Type: new  Abstract: Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.",
        "arxiv_id": "2511.08872",
        "ARXIVID": "2511.08872",
        "COMMENT": "Matches criterion 1 (New methodological improvements to spatial understanding, spatial intelligence on embodied agents). Proposes a structure-aware stride state space model for 3D human pose estimation, improving spatial modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.08997": {
        "authors": [
            "Jiazhou Zhou",
            "Qing Jiang",
            "Kanghao Chen",
            "Lutao Jiang",
            "Yuanhuiyi Lyu",
            "Ying-Cong Chen",
            "Lei Zhang"
        ],
        "title": "T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection",
        "abstract": "arXiv:2511.08997v1 Announce Type: new  Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.",
        "arxiv_id": "2511.08997",
        "ARXIVID": "2511.08997",
        "COMMENT": "Matches criterion 4 (Vision foundation models related and its applications). Proposes a novel object detection framework that incorporates negative visual prompts, advancing open-set visual recognition systems.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.09082": {
        "authors": [
            "Zhen Li",
            "Yuwei Wu",
            "Chenchen Jing",
            "Che Sun",
            "Chuanhao Li",
            "Yunde Jia"
        ],
        "title": "Composition-Incremental Learning for Compositional Generalization",
        "abstract": "arXiv:2511.09082v1 Announce Type: new  Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.",
        "arxiv_id": "2511.09082",
        "ARXIVID": "2511.09082",
        "COMMENT": "This paper introduces a new benchmark and method for composition-incremental learning in compositional generalization, including a pseudo-replay framework and new datasets. It matches criterion 3 (new benchmark and method for compositional generalization in vision, with a novel angle on incremental learning).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.09502": {
        "authors": [
            "Jerrin Bright",
            "Yuhao Chen",
            "John S. Zelek"
        ],
        "title": "DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation",
        "abstract": "arXiv:2511.09502v1 Announce Type: new  Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.",
        "arxiv_id": "2511.09502",
        "ARXIVID": "2511.09502",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Introduces a diffusion-based framework for 3D human pose estimation with prompt learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.08609": {
        "authors": [
            "I. Bailo",
            "F. Buonora",
            "G. Ciarfaglia",
            "L. T. Consoli",
            "A. Evangelista",
            "M. Gabusi",
            "M. Ghiani",
            "C. Petracca Ciavarella",
            "F. Picariello",
            "F. Sarcina",
            "F. Tuosto",
            "V. Zullo",
            "L. Airoldi",
            "G. Bruno",
            "D. D. Gobbo",
            "S. Pezzenati",
            "G. A. Tona"
        ],
        "title": "Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants",
        "abstract": "arXiv:2511.08609v1 Announce Type: new  Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\\%.",
        "arxiv_id": "2511.08609",
        "ARXIVID": "2511.08609",
        "COMMENT": "Describes a transformer-based solution for automatic digitization of gas plants using OCR, Vision LLM, object detection, and relational reasoning. This is an application of vision foundation models (criterion 4), and uses Vision LLMs, making it also relevant to criterion 2.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.07988": {
        "authors": [
            "Nico Policzer",
            "Cameron Braunstein",
            "Mariya Toneva"
        ],
        "title": "The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends",
        "abstract": "arXiv:2511.07988v1 Announce Type: new  Abstract: Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.",
        "arxiv_id": "2511.07988",
        "ARXIVID": "2511.07988",
        "COMMENT": "Some relevance to criterion 2 (multi-modal models) and general interest in multi-modal learning, but the focus is on brain alignment and social cognition rather than spatial intelligence or embodied AI. Novelty in extending brain-tuning to multi-modal models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.09139": {
        "authors": [
            "Zijian Chen",
            "Yuze Sun",
            "Yuan Tian",
            "Wenjun Zhang",
            "Guangtao Zhai"
        ],
        "title": "MACEval: A Multi-Agent Continual Evaluation Network for Large Models",
        "abstract": "arXiv:2511.09139v1 Announce Type: new  Abstract: Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \\Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.",
        "arxiv_id": "2511.09139",
        "ARXIVID": "2511.09139",
        "COMMENT": "Matches criterion 2 (Shows new VLLMs or MLLMs) and is tangentially related to criterion 4 (benchmarking large models, which may include VLLMs/MLLMs). Proposes a new multi-agent continual evaluation network for large models, which could be relevant for evaluating VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.08883": {
        "authors": [
            "Cheng Wang",
            "Shuisheng Zhou",
            "Fengjiao Peng",
            "Jin Sheng",
            "Feng Ye",
            "Yinli Dong"
        ],
        "title": "Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks",
        "abstract": "arXiv:2511.08883v1 Announce Type: new  Abstract: In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.",
        "arxiv_id": "2511.08883",
        "ARXIVID": "2511.08883",
        "COMMENT": "This paper proposes a new method for contrastive clustering using multiple fusing-augmenting ViT blocks, leveraging Vision Transformers and CLIP features. It is relevant to vision foundation models and their applications (criterion 4), and also presents a novel methodological improvement in spatial feature learning (criterion 1).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.09272": {
        "authors": [
            "Rui Wan",
            "Qi Zheng",
            "Ruoyu Zhang",
            "Bu Chen",
            "Jiaming Liu",
            "Min Li",
            "Minge Jing",
            "Jinjia Zhou",
            "Yibo Fan"
        ],
        "title": "GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow",
        "abstract": "arXiv:2511.09272v1 Announce Type: new  Abstract: The Animation-based Generative Codec (AGC) is an emerging paradigm for talking-face video compression. However, deploying its intricate decoder on resource and power-constrained edge devices presents challenges due to numerous parameters, the inflexibility to adapt to dynamically evolving algorithms, and the high power consumption induced by extensive computations and data transmission. This paper for the first time proposes a novel field programmable gate arrays (FPGAs)-oriented AGC deployment scheme for edge-computing video services. Initially, we analyze the AGC algorithm and employ network compression methods including post-training static quantization and layer fusion techniques. Subsequently, we design an overlapped accelerator utilizing the co-processor paradigm to perform computations through software-hardware co-design. The hardware processing unit comprises engines such as convolution, grid sampling, upsample, etc. Parallelization optimization strategies like double-buffered pipelines and loop unrolling are employed to fully exploit the resources of FPGA. Ultimately, we establish an AGC FPGA prototype on the PYNQ-Z1 platform using the proposed scheme, achieving \\textbf{24.9$\\times$} and \\textbf{4.1$\\times$} higher energy efficiency against commercial Central Processing Unit (CPU) and Graphic Processing Unit (GPU), respectively. Specifically, only \\textbf{11.7} microjoules ($\\upmu$J) are required for one pixel reconstructed by this FPGA system.",
        "arxiv_id": "2511.09272",
        "ARXIVID": "2511.09272",
        "COMMENT": "Somewhat relevant to criterion 4 (generative face video codec, hardware-centric workflow for vision foundation models). Focuses on efficient deployment of generative codecs for talking-face video compression.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.08901": {
        "authors": [
            "Riling Wei",
            "Kelu Yao",
            "Chuanguang Yang",
            "Jin Wang",
            "Zhuoyan Gao",
            "Chao Li"
        ],
        "title": "Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency",
        "abstract": "arXiv:2511.08901v1 Announce Type: new  Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.",
        "arxiv_id": "2511.08901",
        "ARXIVID": "2511.08901",
        "COMMENT": "Somewhat relevant to criterion 2 (cross-modal knowledge distillation is related to multi-modal learning, but not specifically about VLLMs/MLLMs). The paper introduces a new framework for asymmetric cross-modal knowledge distillation and a new benchmark for remote sensing scene classification.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.07912": {
        "authors": [
            "Jun-Young Kim",
            "Young-Seok Kweon",
            "Gi-Hwan Shin",
            "Seong-Whan Lee"
        ],
        "title": "Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy",
        "abstract": "arXiv:2511.07912v1 Announce Type: new  Abstract: Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.",
        "arxiv_id": "2511.07912",
        "ARXIVID": "2511.07912",
        "COMMENT": "Matches criterion 2 (comparison with a multimodal large language model) and provides insight into the limitations of current MLLMs in adaptive reasoning. Interesting for those studying the gap between human and AI reasoning in multi-modal settings.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.09298": {
        "authors": [
            "Shengqi Dang",
            "Fu Chai",
            "Jiaxin Li",
            "Chao Yuan",
            "Wei Ye",
            "Nan Cao"
        ],
        "title": "DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures",
        "abstract": "arXiv:2511.09298v1 Announce Type: new  Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.",
        "arxiv_id": "2511.09298",
        "ARXIVID": "2511.09298",
        "COMMENT": "Some relevance to criterion 4 (vision foundation models and applications) as it builds on pretrained 3D generative models, but the main focus is on physical constraints and fabrication rather than spatial intelligence or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.08151": {
        "authors": [
            "Xuchen Li",
            "Ruitao Wu",
            "Xuanbo Liu",
            "Xukai Wang",
            "Jinbo Hu",
            "Zhixin Bai",
            "Bohan Zeng",
            "Hao Liang",
            "Leheng Chen",
            "Mingrui Chen",
            "Haitian Zhong",
            "Xuanlin Yang",
            "Xu-Yao Zhang",
            "Liu Liu",
            "Jia Li",
            "Kaiqi Huang",
            "Jiahao Xu",
            "Haitao Mi",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "title": "SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning",
        "abstract": "arXiv:2511.08151v1 Announce Type: new  Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.",
        "arxiv_id": "2511.08151",
        "ARXIVID": "2511.08151",
        "COMMENT": "Relevant to multi-agent LLM systems for scientific reasoning, but does not match any specific criterion (not spatial, not vision, not multi-modal, not embodied AI).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07587": {
        "authors": [
            "Shreyas Rajesh",
            "Pavan Holur",
            "Chenda Duan",
            "David Chong",
            "Vwani Roychowdhury"
        ],
        "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
        "abstract": "arXiv:2511.07587v1 Announce Type: new  Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \\textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \\textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \\textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \\cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \\textbf{20\\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \\textbf{51\\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.",
        "arxiv_id": "2511.07587",
        "ARXIVID": "2511.07587",
        "COMMENT": "Relevant to LLMs and memory, but not to any specific criterion (not spatial, not vision, not multi-modal, not embodied AI).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07974": {
        "authors": [
            "Lintong Zhang",
            "Kang Yin",
            "Seong-Whan Lee"
        ],
        "title": "Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition",
        "abstract": "arXiv:2511.07974v1 Announce Type: new  Abstract: Attribution-based explanation techniques capture key patterns to enhance visual interpretability; however, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods.",
        "arxiv_id": "2511.07974",
        "ARXIVID": "2511.07974",
        "COMMENT": "Does not match any specific criterion. Focuses on interpretability and counterfactual explanations for misclassification, which is related to vision but not to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.09388": {
        "authors": [
            "Yang Chen",
            "Miaoge Li",
            "Zhijie Rao",
            "Deze Zeng",
            "Song Guo",
            "Jingcai Guo"
        ],
        "title": "Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition",
        "abstract": "arXiv:2511.09388v1 Announce Type: new  Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an \"align-then-classify\" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\\texttt{$\\textbf{Flora}$}$, which builds upon $\\textbf{F}$lexib$\\textbf{L}$e neighb$\\textbf{O}$r-aware semantic attunement and open-form dist$\\textbf{R}$ibution-aware flow cl$\\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.",
        "arxiv_id": "2511.09388",
        "ARXIVID": "2511.09388",
        "COMMENT": "Does not match any specific criterion. Focuses on zero-shot skeleton action recognition, which is related to computer vision but not specifically to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.08379": {
        "authors": [
            "Giorgio Piras",
            "Raffaele Mura",
            "Fabio Brau",
            "Luca Oneto",
            "Fabio Roli",
            "Battista Biggio"
        ],
        "title": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models",
        "abstract": "arXiv:2511.08379v1 Announce Type: new  Abstract: Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.",
        "arxiv_id": "2511.08379",
        "ARXIVID": "2511.08379",
        "COMMENT": "Does not match any specific criterion. Focuses on mechanistic interpretability and refusal suppression in LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07943": {
        "authors": [
            "Jun Xu",
            "Xinkai Du",
            "Yu Ao",
            "Peilong Zhao",
            "Yang Li",
            "Ling Zhong",
            "Lin Yuan",
            "Zhongpu Bo",
            "Xiaorui Wang",
            "Mengshu Sun",
            "Zhengke Gui",
            "Dalong Zhang",
            "Zhaoyang Wang",
            "Qiwei Wang",
            "Yangyang Hou",
            "Zhiying Yin",
            "Haofen Wang",
            "Huajun Chen",
            "Lei Liang",
            "Jun Zhou"
        ],
        "title": "Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction",
        "abstract": "arXiv:2511.07943v1 Announce Type: new  Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.",
        "arxiv_id": "2511.07943",
        "ARXIVID": "2511.07943",
        "COMMENT": "Introduces Thinker, a hierarchical thinking model for LLMs to improve deep search and reasoning via multi-turn interaction. While it is a new method for LLM reasoning, it does not focus on spatial intelligence, embodied agents, or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.08581": {
        "authors": [
            "Ying Jiao",
            "Rodrigo Castellano Ontiveros",
            "Luc De Raedt",
            "Marco Gori",
            "Francesco Giannini",
            "Michelangelo Diligenti",
            "Giuseppe Marra"
        ],
        "title": "DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs",
        "abstract": "arXiv:2511.08581v1 Announce Type: new  Abstract: Neurosymbolic (NeSy) AI aims to combine the strengths of neural architectures and symbolic reasoning to improve the accuracy, interpretability, and generalization capability of AI models. While logic inference on top of subsymbolic modules has been shown to effectively guarantee these properties, this often comes at the cost of reduced scalability, which can severely limit the usability of NeSy models. This paper introduces DeepProofLog (DPrL), a novel NeSy system based on stochastic logic programs, which addresses the scalability limitations of previous methods. DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. Additionally, we establish a formal mapping between the resolution process of our deep stochastic logic programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning techniques for efficient inference and learning. This theoretical connection improves scalability for complex proof spaces and large knowledge bases. Our experiments on standard NeSy benchmarks and knowledge graph reasoning tasks demonstrate that DPrL outperforms existing state-of-the-art NeSy systems, advancing scalability to larger and more complex settings than previously possible.",
        "arxiv_id": "2511.08581",
        "ARXIVID": "2511.08581",
        "COMMENT": "This paper introduces DeepProofLog, a neurosymbolic system for efficient proving in deep stochastic logic programs. While it is a methodological improvement in neurosymbolic AI, it is not directly related to spatial intelligence, embodied agents, or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07685": {
        "authors": [
            "Manasi Sharma",
            "Chen Bo Calvin Zhang",
            "Chaithanya Bandi",
            "Clinton Wang",
            "Ankit Aich",
            "Huy Nghiem",
            "Tahseen Rabbani",
            "Ye Htet",
            "Brian Jang",
            "Sumana Basu",
            "Aishwarya Balwani",
            "Denis Peskoff",
            "Marcos Ayestaran",
            "Sean M. Hendryx",
            "Brad Kenstler",
            "Bing Liu"
        ],
        "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
        "abstract": "arXiv:2511.07685v1 Announce Type: new  Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.",
        "arxiv_id": "2511.07685",
        "ARXIVID": "2511.07685",
        "COMMENT": "Presents a new benchmark (ResearchRubrics) for evaluating deep research agents, focusing on LLMs and multi-step reasoning. While it is a new benchmark, it is not focused on embodied AI or vision/multi-modal models, so it does not match the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.08484": {
        "authors": [
            "Huzaifa Arif",
            "Keerthiram Murugesan",
            "Ching-Yun Ko",
            "Pin-Yu Chen",
            "Payel Das",
            "Alex Gittens"
        ],
        "title": "Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models",
        "abstract": "arXiv:2511.08484v1 Announce Type: new  Abstract: We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This \"patch\" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be \"patched\" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.",
        "arxiv_id": "2511.08484",
        "ARXIVID": "2511.08484",
        "COMMENT": "Does not match any specific criterion. Focuses on safety policy patching for LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07991": {
        "authors": [
            "Hyojun Choi",
            "Seokju Hwang",
            "Kyong-Ho Lee"
        ],
        "title": "VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation",
        "abstract": "arXiv:2511.07991v1 Announce Type: new  Abstract: Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as \"Misusing allValuesFrom\". Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.",
        "arxiv_id": "2511.07991",
        "ARXIVID": "2511.07991",
        "COMMENT": "Does not match any specific criterion. Focuses on LLM-based competency question generation for ontology validation, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07719": {
        "authors": [
            "V\\'it R\\r{u}\\v{z}i\\v{c}ka",
            "Gonzalo Mateo-Garc\\'ia",
            "Itziar Irakulis-Loitxate",
            "Juan Emmanuel Johnson",
            "Manuel Montesino San Mart\\'in",
            "Anna Allen",
            "Luis Guanter",
            "David R. Thompson"
        ],
        "title": "Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources",
        "abstract": "arXiv:2511.07719v1 Announce Type: new  Abstract: Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. While satellite-based imaging spectrometers, such as EMIT, PRISMA, and EnMAP, can detect these point sources, current methane retrieval methods based on matched filters still produce a high number of false detections requiring laborious manual verification. This paper describes the operational deployment of a machine learning system for detecting methane emissions within the Methane Alert and Response System (MARS) of the United Nations Environment Programme's International Methane Emissions Observatory. We created the largest and most diverse global dataset of annotated methane plumes from three imaging spectrometer missions and quantitatively compared different deep learning model configurations. Focusing on the requirements for operational deployment, we extended prior evaluation methodologies from small tiled datasets to full granule evaluation. This revealed that deep learning models still produce a large number of false detections, a problem we address with model ensembling, which reduced false detections by over 74%. Deployed in the MARS pipeline, our system processes scenes and proposes plumes to analysts, accelerating the detection and analysis process. During seven months of operational deployment, it facilitated the verification of 1,351 distinct methane leaks, resulting in 479 stakeholder notifications. We further demonstrate the model's utility in verifying mitigation success through case studies in Libya, Argentina, Oman, and Azerbaijan. Our work represents a critical step towards a global AI-assisted methane leak detection system, which is required to process the dramatically higher data volumes expected from new and current imaging spectrometers.",
        "arxiv_id": "2511.07719",
        "ARXIVID": "2511.07719",
        "COMMENT": "Relevant to general computer vision and machine learning interests, but does not match any specific criterion. Focuses on operational deployment of deep learning for remote sensing, not spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.07568": {
        "authors": [
            "Vincent Hsiao",
            "Mark Roberts",
            "Leslie Smith"
        ],
        "title": "Procedural Knowledge Improves Agentic LLM Workflows",
        "abstract": "arXiv:2511.07568v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.",
        "arxiv_id": "2511.07568",
        "ARXIVID": "2511.07568",
        "COMMENT": "Relevant to general LLM workflow improvements, but does not match any specific criterion (not spatial, not vision, not multi-modal, not embodied AI).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.08301": {
        "authors": [
            "Valentin Tablan",
            "Scott Taylor",
            "Gabriel Hurtado",
            "Kristoffer Bernhem",
            "Anders Uhrenholt",
            "Gabriele Farei",
            "Karo Moilanen"
        ],
        "title": "Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning",
        "abstract": "arXiv:2511.08301v1 Announce Type: new  Abstract: The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.   In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.",
        "arxiv_id": "2511.08301",
        "ARXIVID": "2511.08301",
        "COMMENT": "Does not match any specific criterion. Focuses on agentic memory for code generation agents, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}