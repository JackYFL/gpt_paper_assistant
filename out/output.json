{
    "2602.04802": {
        "authors": [
            "Qing'an Liu",
            "Juntong Feng",
            "Yuhao Wang",
            "Xinzhe Han",
            "Yujie Cheng",
            "Yue Zhu",
            "Haiwen Diao",
            "Yunzhi Zhuge",
            "Huchuan Lu"
        ],
        "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
        "abstract": "arXiv:2602.04802v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.",
        "arxiv_id": "2602.04802",
        "ARXIVID": "2602.04802",
        "COMMENT": "Matches criterion 2 and 4: The paper introduces VISTA-Bench, a new benchmark for evaluating VLMs' understanding of visualized text versus pure text, revealing a modality gap. This is directly relevant to VLLMs and vision foundation models and their applications, and provides surprising empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.04304": {
        "authors": [
            "Zipeng Zhu",
            "Zhanghao Hu",
            "Qinglin Zhu",
            "Yuxi Hong",
            "Yijun Liu",
            "Jingyong Su",
            "Yulan He",
            "Lin Gui"
        ],
        "title": "Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement",
        "abstract": "arXiv:2602.04304v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static \"magic layer\" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.",
        "arxiv_id": "2602.04304",
        "ARXIVID": "2602.04304",
        "COMMENT": "This paper directly addresses criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). It proposes LASER, a layer-adaptive, training-free inference procedure for large vision-language models (LVLMs), improving visual localization and reasoning by dynamically selecting layers based on query sensitivity. The approach is novel in its dynamic, query-driven adaptation and provides surprising empirical results on VQA benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.03916": {
        "authors": [
            "Azmine Toushik Wasi",
            "Wahid Faisal",
            "Abdur Rahman",
            "Mahfuz Ahmed Anik",
            "Munem Shahriar",
            "Mohsin Mahmud Topu",
            "Sadia Tasnim Meem",
            "Rahatun Nesa Priti",
            "Sabrina Afroz Mitu",
            "Md. Iqramul Hoque",
            "Shahriyar Zaman Ridoy",
            "Mohammed Eunus Ali",
            "Majd Hawasly",
            "Mohammad Raza",
            "Md Rizwan Parvez"
        ],
        "title": "SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?",
        "abstract": "arXiv:2602.03916v1 Announce Type: new  Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.",
        "arxiv_id": "2602.03916",
        "ARXIVID": "2602.03916",
        "COMMENT": "Directly matches criterion 1 (spatial understanding in VLMs) and criterion 3 (new benchmark for spatial reasoning in the wild). Highly relevant for spatial intelligence and VLM evaluation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.03890": {
        "authors": [
            "Xindan Zhang",
            "Weilong Yan",
            "Yufei Shi",
            "Xuerui Qiu",
            "Tao He",
            "Ying Li",
            "Ming Li",
            "Hehe Fan"
        ],
        "title": "4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping",
        "abstract": "arXiv:2602.03890v1 Announce Type: new  Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.",
        "arxiv_id": "2602.03890",
        "ARXIVID": "2602.03890",
        "COMMENT": "Matches criterion 2: Presents 4DPC$^2$hat, a new MLLM for dynamic point cloud understanding, with a new large-scale cross-modal dataset and a novel failure-aware bootstrapping learning strategy. Also relevant to criterion 3 (new dataset and method for embodied/dynamic spatial understanding).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.04864": {
        "authors": [
            "Soumya Jahagirdar",
            "Walid Bousselham",
            "Anna Kukleva",
            "Hilde Kuehne"
        ],
        "title": "When LLaVA Meets Objects: Token Composition for Vision-Language-Models",
        "abstract": "arXiv:2602.04864v1 Announce Type: new  Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.",
        "arxiv_id": "2602.04864",
        "ARXIVID": "2602.04864",
        "COMMENT": "Matches criterion 2: Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models). The paper proposes Mask-LLaVA, a token-efficient VLM that combines object, global, and patch tokens, and allows dynamic token selection at inference. This is a methodological improvement in VLLMs and is directly relevant.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2602.04722": {
        "authors": [
            "Gon\\c{c}alo P. Matos",
            "Carlos Santiago",
            "Jo\\~ao P. Costeira",
            "Ricardo L. Saldanha",
            "Ernesto M. Morgado"
        ],
        "title": "How to rewrite the stars: Mapping your orchard over time through constellations of fruits",
        "abstract": "arXiv:2602.04722v1 Announce Type: new  Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.",
        "arxiv_id": "2602.04722",
        "ARXIVID": "2602.04722",
        "COMMENT": "This paper introduces a new paradigm for tracking fruits in orchards over time using 3D constellations, enabling mapping and autonomous navigation. It is directly relevant to criterion 1 (spatial understanding on embodied agents) and criterion 3 (embodied AI with a novel method for spatial mapping and tracking in real-world environments). The approach is novel in its use of sparse 3D point cloud constellations for matching and mapping.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.04454": {
        "authors": [
            "Tianming Liang",
            "Qirui Du",
            "Jian-Fang Hu",
            "Haichao Jiang",
            "Zicheng Lin",
            "Wei-Shi Zheng"
        ],
        "title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
        "abstract": "arXiv:2602.04454v1 Announce Type: new  Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \\textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.",
        "arxiv_id": "2602.04454",
        "ARXIVID": "2602.04454",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new segmentation paradigm with interleaved reasoning and external search, and introduces a new benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.04565": {
        "authors": [
            "Guanzhou Lan",
            "Chenyi Liao",
            "Yuqi Yang",
            "Qianli Ma",
            "Zhigang Wang",
            "Dong Wang",
            "Bin Zhao",
            "Xuelong Li"
        ],
        "title": "Understanding Degradation with Vision Language Model",
        "abstract": "arXiv:2602.04565v1 Announce Type: new  Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \\textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.",
        "arxiv_id": "2602.04565",
        "ARXIVID": "2602.04565",
        "COMMENT": "Matches criterion 2 and 4: Proposes DU-VLM, a vision-language model for structured degradation understanding, with a new large-scale dataset and a chain-of-thought approach. Also demonstrates zero-shot control of diffusion models, which is a clever application.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.04268": {
        "authors": [
            "Siyu Jiang",
            "Feiyang Chen",
            "Xiaojin Zhang",
            "Kun He"
        ],
        "title": "KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing",
        "abstract": "arXiv:2602.04268v1 Announce Type: new  Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.   To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.   Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\\mathit{CHAIR}_{S}$ from $41.8 \\rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \\rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.",
        "arxiv_id": "2602.04268",
        "ARXIVID": "2602.04268",
        "COMMENT": "Matches criterion 2: Proposes KVSmooth, a new method to mitigate hallucination in MLLMs, with a clever statistical trick (attention-entropy-guided adaptive smoothing) that is plug-and-play and training-free. Strong empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2602.04167": {
        "authors": [
            "Yu Zhou",
            "Xiaoyan Yang",
            "Bojia Zi",
            "Lihan Zhang",
            "Ruijie Sun",
            "Weishi Zheng",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Point2Insert: Video Object Insertion via Sparse Point Guidance",
        "abstract": "arXiv:2602.04167v1 Announce Type: new  Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\\times$10 more parameters.",
        "arxiv_id": "2602.04167",
        "ARXIVID": "2602.04167",
        "COMMENT": "Matches criterion 1: Presents a new method for spatial understanding in video object insertion using sparse point guidance, allowing fine-grained spatial control. The approach is a methodological improvement in spatial intelligence for generative models in vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04213": {
        "authors": [
            "Feiyu Gavin Zhu",
            "Jean Oh",
            "Reid Simmons"
        ],
        "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
        "abstract": "arXiv:2602.04213v1 Announce Type: new  Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy",
        "arxiv_id": "2602.04213",
        "ARXIVID": "2602.04213",
        "COMMENT": "Matches criterion 3: Embodied AI papers on building new methods for imitation learning, specifically enabling laypersons to interactively teach embodied agents. The paper introduces a novel interactive policy restructuring and training method, which is a new angle for embodied AI and user-in-the-loop learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.03918": {
        "authors": [
            "Peihao Xiang",
            "Kaida Wu",
            "Ou Bai"
        ],
        "title": "Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers",
        "abstract": "arXiv:2602.03918v1 Announce Type: new  Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.",
        "arxiv_id": "2602.03918",
        "ARXIVID": "2602.03918",
        "COMMENT": "This paper analyzes block importance in masked self-supervised vision transformers using information entropy, enabling data-free, one-shot pruning. It is relevant to vision foundation models (criterion 4) and uses a clever statistical trick (entropy-based analysis) for model compression. The empirical findings about redundancy and transfer performance are insightful.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04441": {
        "authors": [
            "Weiguang Zhao",
            "Haoran Xu",
            "Xingyu Miao",
            "Qin Zhao",
            "Rui Zhang",
            "Kaizhu Huang",
            "Ning Gao",
            "Peizhou Cao",
            "Mingze Sun",
            "Mulin Yu",
            "Tao Lu",
            "Linning Xu",
            "Junting Dong",
            "Jiangmiao Pang"
        ],
        "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
        "abstract": "arXiv:2602.04441v1 Announce Type: new  Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.",
        "arxiv_id": "2602.04441",
        "ARXIVID": "2602.04441",
        "COMMENT": "Presents a new large-scale synthetic dataset and benchmark for point tracking, including embodied manipulation and navigation. Matches criterion 3 (benchmark for embodied AI, simulator-related).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.03907": {
        "authors": [
            "Team Hunyuan3D",
            ":",
            "Bowen Zhang",
            "Chunchao Guo",
            "Dongyuan Guo",
            "Haolin Liu",
            "Hongyu Yan",
            "Huiwen Shi",
            "Jiaao Yu",
            "Jiachen Xu",
            "Jingwei Huang",
            "Kunhong Li",
            "Lifu Wang",
            "Linus",
            "Penghao Wang",
            "Qingxiang Lin",
            "Ruining Tang",
            "Xianghui Yang",
            "Yang Li",
            "Yirui Guan",
            "Yunfei Zhao",
            "Yunhan Yang",
            "Zeqiang Lai",
            "Zhihao Liang",
            "Zibo Zhao"
        ],
        "title": "HY3D-Bench: Generation of 3D Assets",
        "abstract": "arXiv:2602.03907v1 Announce Type: new  Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.",
        "arxiv_id": "2602.03907",
        "ARXIVID": "2602.03907",
        "COMMENT": "Matches criterion 3 (new benchmark for 3D asset generation, with a focus on robotics and digital content creation). Also relevant to criterion 4 (foundation models for 3D).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.03892": {
        "authors": [
            "Jinxing Zhou",
            "Yanghao Zhou",
            "Yaoting Wang",
            "Zongyan Han",
            "Jiaqi Ma",
            "Henghui Ding",
            "Rao Muhammad Anwer",
            "Hisham Cholakkal"
        ],
        "title": "Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation",
        "abstract": "arXiv:2602.03892v1 Announce Type: new  Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.",
        "arxiv_id": "2602.03892",
        "ARXIVID": "2602.03892",
        "COMMENT": "Matches criterion 2: Introduces a new MLLM-based auditor for mask quality assessment in language-referred audio-visual segmentation, and builds a new benchmark (also relevant to criterion 3).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.03913": {
        "authors": [
            "Qiuming Luo",
            "Tao Zeng",
            "Feng Li",
            "Heming Liu",
            "Rui Mao",
            "Chang Kong"
        ],
        "title": "Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition",
        "abstract": "arXiv:2602.03913v1 Announce Type: new  Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples.",
        "arxiv_id": "2602.03913",
        "ARXIVID": "2602.03913",
        "COMMENT": "This paper proposes an entropy-aware structural alignment network for zero-shot handwritten Chinese character recognition, using information-theoretic modeling and multi-granularity structural features. It is relevant to spatial understanding (criterion 1) as it models spatial structure in character recognition, and uses clever statistical tricks (entropy, information theory).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.04789": {
        "authors": [
            "Chengtao Lv",
            "Yumeng Shi",
            "Yushi Huang",
            "Ruihao Gong",
            "Shen Ren",
            "Wenya Wang"
        ],
        "title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
        "abstract": "arXiv:2602.04789v1 Announce Type: new  Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.",
        "arxiv_id": "2602.04789",
        "ARXIVID": "2602.04789",
        "COMMENT": "Proposes a new sparse attention method for autoregressive video diffusion. Relevant to generative modeling and vision foundation models (criterion 4), but not directly to spatial intelligence or embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.04712": {
        "authors": [
            "David F. Ramirez",
            "Tim Overman",
            "Kristen Jaskie",
            "Joe Marvin",
            "Andreas Spanias"
        ],
        "title": "SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation",
        "abstract": "arXiv:2602.04712v1 Announce Type: new  Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.",
        "arxiv_id": "2602.04712",
        "ARXIVID": "2602.04712",
        "COMMENT": "Presents a new method combining MLLMs with retrieval for visual question answering in SAR imagery. Matches criterion 2 (MLLMs) and is a novel application in a challenging domain.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.04300": {
        "authors": [
            "Jue Gong",
            "Zihan Zhou",
            "Jingkai Wang",
            "Xiaohong Liu",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "Light Up Your Face: A Physically Consistent Dataset and Diffusion Model for Face Fill-Light Enhancement",
        "abstract": "arXiv:2602.04300v1 Announce Type: new  Abstract: Face fill-light enhancement (FFE) brightens underexposed faces by adding virtual fill light while keeping the original scene illumination and background unchanged. Most face relighting methods aim to reshape overall lighting, which can suppress the input illumination or modify the entire scene, leading to foreground-background inconsistency and mismatching practical FFE needs. To support scalable learning, we introduce LightYourFace-160K (LYF-160K), a large-scale paired dataset built with a physically consistent renderer that injects a disk-shaped area fill light controlled by six disentangled factors, producing 160K before-and-after pairs. We first pretrain a physics-aware lighting prompt (PALP) that embeds the 6D parameters into conditioning tokens, using an auxiliary planar-light reconstruction objective. Building on a pretrained diffusion backbone, we then train a fill-light diffusion (FiLitDiff), an efficient one-step model conditioned on physically grounded lighting codes, enabling controllable and high-fidelity fill lighting at low computational cost. Experiments on held-out paired sets demonstrate strong perceptual quality and competitive full-reference metrics, while better preserving background illumination. The dataset and model will be at https://github.com/gobunu/Light-Up-Your-Face.",
        "arxiv_id": "2602.04300",
        "ARXIVID": "2602.04300",
        "COMMENT": "Matches criterion 4: Introduces a new physically consistent dataset and a diffusion model for face fill-light enhancement, leveraging vision foundation models for controllable image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.04405": {
        "authors": [
            "Yixin Zhu",
            "Long Lv",
            "Pingping Zhang",
            "Xuehu Liu",
            "Tongdan Tang",
            "Feng Tian",
            "Weibing Sun",
            "Huchuan Lu"
        ],
        "title": "Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion",
        "abstract": "arXiv:2602.04405v1 Announce Type: new  Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.",
        "arxiv_id": "2602.04405",
        "ARXIVID": "2602.04405",
        "COMMENT": "Matches criterion 4: Proposes a novel multi-modal image fusion framework (ISFM) with interactive spatial-frequency fusion, leveraging recent advances in vision foundation models (Mamba architecture).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.03915": {
        "authors": [
            "Levi Lingsch",
            "Georgios Kissas",
            "Johannes Jakubik",
            "Siddhartha Mishra"
        ],
        "title": "Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science",
        "abstract": "arXiv:2602.03915v1 Announce Type: new  Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.",
        "arxiv_id": "2602.03915",
        "ARXIVID": "2602.03915",
        "COMMENT": "Phaedra proposes a new tokenizer for scientific images, improving discrete tokenization for physical science data. While it is relevant to generative modeling and vision foundation models, it does not introduce a new VLLM/MLLM or focus on spatial intelligence or embodied AI. It is a novel approach to tokenization for scientific images.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.04328": {
        "authors": [
            "Jie Chen",
            "Zhu Wang",
            "Chuanbin Liu",
            "Xi Peng"
        ],
        "title": "Multiview Self-Representation Learning across Heterogeneous Views",
        "abstract": "arXiv:2602.04328v1 Announce Type: new  Abstract: Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches.",
        "arxiv_id": "2602.04328",
        "ARXIVID": "2602.04328",
        "COMMENT": "This paper proposes a multiview self-representation learning method for learning invariant representations from heterogeneous pretrained models. It is relevant to vision foundation models and representation learning, but does not introduce a new VLLM/MLLM or focus on spatial intelligence or embodied AI. It is a novel approach to leveraging multiple pretrained models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.04030": {
        "authors": [
            "Leeje Jang",
            "Yijun Lin",
            "Yao-Yi Chiang",
            "Jerod Weinman"
        ],
        "title": "TiCLS : Tightly Coupled Language Text Spotter",
        "abstract": "arXiv:2602.04030v1 Announce Type: new  Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.",
        "arxiv_id": "2602.04030",
        "ARXIVID": "2602.04030",
        "COMMENT": "This paper introduces TiCLS, an end-to-end text spotter that fuses visual and linguistic features using a pretrained language model. It is relevant to multi-modal learning and vision-language integration, but does not introduce a new VLLM/MLLM or focus on spatial intelligence or embodied AI. It is a strong application of vision-language modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.04439": {
        "authors": [
            "Xingyu Miao",
            "Weiguang Zhao",
            "Tao Lu",
            "Linning Yu",
            "Mulin Yu",
            "Yang Long",
            "Jiangmiao Pang",
            "Junting Dong"
        ],
        "title": "TrajVG: 3D Trajectory-Coupled Visual Geometry Learning",
        "abstract": "arXiv:2602.04439v1 Announce Type: new  Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.",
        "arxiv_id": "2602.04439",
        "ARXIVID": "2602.04439",
        "COMMENT": "Somewhat relevant to criterion 1: Proposes a new method for 3D trajectory-coupled visual geometry learning, which may be of interest for spatial understanding, but not directly about embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.04416": {
        "authors": [
            "Aavash Chhetri",
            "Bibek Niroula",
            "Pratik Shrestha",
            "Yash Raj Shrestha",
            "Lesley A Anderson",
            "Prashnna K Gyawali",
            "Loris Bazzani",
            "Binod Bhattarai"
        ],
        "title": "Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare",
        "abstract": "arXiv:2602.04416v1 Announce Type: new  Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .",
        "arxiv_id": "2602.04416",
        "ARXIVID": "2602.04416",
        "COMMENT": "Presents a new multimodal federated learning benchmark in healthcare. Closest to criterion 3 (benchmark), but not in embodied AI or spatial intelligence. Some relevance to multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.04271": {
        "authors": [
            "Lifan Wu",
            "Ruijie Zhu",
            "Yubo Ai",
            "Tianzhu Zhang"
        ],
        "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
        "abstract": "arXiv:2602.04271v1 Announce Type: new  Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/",
        "arxiv_id": "2602.04271",
        "ARXIVID": "2602.04271",
        "COMMENT": "Relevant to 4D generation and editability, but not directly about spatial intelligence in embodied agents or VLLMs/MLLMs. Some overlap with vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.04188": {
        "authors": [
            "Ning Zhang",
            "Zhengyu Li",
            "Kwong Weng Loh",
            "Mingxi Xu",
            "Qi Wang",
            "Zhengyu Wen",
            "Xiaoyu He",
            "Wei Zhao",
            "Kehong Gong",
            "Mingyuan Zhang"
        ],
        "title": "DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding",
        "abstract": "arXiv:2602.04188v1 Announce Type: new  Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.",
        "arxiv_id": "2602.04188",
        "ARXIVID": "2602.04188",
        "COMMENT": "Relevant to multi-modal generative modeling (motion and text), but does not directly address spatial intelligence or embodied agents. Closest to general interest in generative modeling.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.04634": {
        "authors": [
            "Zelai Xu",
            "Zhexuan Xu",
            "Ruize Zhang",
            "Chunyang Zhu",
            "Shi Yu",
            "Weilin Liu",
            "Quanlu Zhang",
            "Wenbo Ding",
            "Chao Yu",
            "Yu Wang"
        ],
        "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2602.04634v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
        "arxiv_id": "2602.04634",
        "ARXIVID": "2602.04634",
        "COMMENT": "WideSeek-R1 explores width scaling in multi-agent reinforcement learning for information seeking, using LLMs as agents. While it is relevant to multi-agent systems and LLM orchestration, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more NLP/agent-oriented.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.04154": {
        "authors": [
            "Mingjian Lu",
            "Pawan K. Tripathi",
            "Mark Shteyn",
            "Debargha Ganguly",
            "Roger H. French",
            "Vipin Chaudhary",
            "Yinghui Wu"
        ],
        "title": "Context Determines Optimal Architecture in Materials Segmentation",
        "abstract": "arXiv:2602.04154v1 Announce Type: new  Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.",
        "arxiv_id": "2602.04154",
        "ARXIVID": "2602.04154",
        "COMMENT": "Somewhat relevant to criterion 4: Proposes a cross-modal evaluation framework for segmentation architectures, but not directly about vision foundation models or their applications. More about benchmarking segmentation architectures in materials science.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.04260": {
        "authors": [
            "Yong Li",
            "Yuanzhi Wang",
            "Yi Ding",
            "Shiqing Zhang",
            "Ke Lu",
            "Cuntai Guan"
        ],
        "title": "Decoupled Hierarchical Distillation for Multimodal Emotion Recognition",
        "abstract": "arXiv:2602.04260v1 Announce Type: new  Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\\%/2.4\\% (ACC$_7$), 1.3\\%/1.9\\% (ACC$_2$) and 1.9\\%/1.8\\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.",
        "arxiv_id": "2602.04260",
        "ARXIVID": "2602.04260",
        "COMMENT": "This paper proposes a new hierarchical distillation framework for multimodal emotion recognition, which is a methodological improvement in multi-modal learning but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is relevant to multi-modal learning but not a direct match to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.03879": {
        "authors": [
            "Ali Bayeh",
            "Samira Sadaoui",
            "Malek Mouhoub"
        ],
        "title": "TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions",
        "abstract": "arXiv:2602.03879v1 Announce Type: new  Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.",
        "arxiv_id": "2602.03879",
        "ARXIVID": "2602.03879",
        "COMMENT": "This paper proposes TruKAN, a new architecture for Kolmogorov-Arnold Networks, and evaluates it on computer vision benchmarks. It focuses on architectural and interpretability improvements for neural networks, but does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.04227": {
        "authors": [
            "Hanuman Verma",
            "Kiho Im",
            "Pranabesh Maji",
            "Akshansh Gupta"
        ],
        "title": "An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation",
        "abstract": "arXiv:2602.04227v1 Announce Type: new  Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.",
        "arxiv_id": "2602.04227",
        "ARXIVID": "2602.04227",
        "COMMENT": "Somewhat relevant to general computer vision, but does not match any specific criterion. Focuses on fuzzy logic in UNet for brain image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}