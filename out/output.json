{
    "2506.05302": {
        "authors": [
            "Weifeng Lin",
            "Xinyu Wei",
            "Ruichuan An",
            "Tianhe Ren",
            "Tingwei Chen",
            "Renrui Zhang",
            "Ziyu Guo",
            "Wentao Zhang",
            "Lei Zhang",
            "Hongsheng Li"
        ],
        "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos",
        "abstract": "arXiv:2506.05302v1 Announce Type: new  Abstract: We present Perceive Anything Model (PAM), a conceptually straightforward and efficient framework for comprehensive region-level visual understanding in images and videos. Our approach extends the powerful segmentation model SAM 2 by integrating Large Language Models (LLMs), enabling simultaneous object segmentation with the generation of diverse, region-specific semantic outputs, including categories, label definition, functional explanations, and detailed captions. A key component, Semantic Perceiver, is introduced to efficiently transform SAM 2's rich visual features, which inherently carry general vision, localization, and semantic priors into multi-modal tokens for LLM comprehension. To support robust multi-granularity understanding, we also develop a dedicated data refinement and augmentation pipeline, yielding a high-quality dataset of 1.5M image and 0.6M video region-semantic annotations, including novel region-level streaming video caption data. PAM is designed for lightweightness and efficiency, while also demonstrates strong performance across a diverse range of region understanding tasks. It runs 1.2-2.4x faster and consumes less GPU memory than prior approaches, offering a practical solution for real-world applications. We believe that our effective approach will serve as a strong baseline for future research in region-level visual understanding.",
        "arxiv_id": "2506.05302",
        "ARXIVID": "2506.05302",
        "COMMENT": "Matches criterion 2 as it integrates SAM with LLMs for region-level visual understanding, aligning with visual large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.05331": {
        "authors": [
            "Xinyan Chen",
            "Renrui Zhang",
            "Dongzhi Jiang",
            "Aojun Zhou",
            "Shilin Yan",
            "Weifeng Lin",
            "Hongsheng Li"
        ],
        "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning",
        "abstract": "arXiv:2506.05331v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT",
        "arxiv_id": "2506.05331",
        "ARXIVID": "2506.05331",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal reasoning model (MINT-CoT) for mathematical problems, integrating visual and textual reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.04704": {
        "authors": [
            "Youngwan Lee",
            "Kangsan Kim",
            "Kwanyong Park",
            "Ilcahe Jung",
            "Soojin Jang",
            "Seanie Lee",
            "Yong-Ju Lee",
            "Sung Ju Hwang"
        ],
        "title": "HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model",
        "abstract": "arXiv:2506.04704v1 Announce Type: new  Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models (VLMs), current approaches face two main shortcomings. 1) Existing safety-tuning datasets and benchmarks only partially consider how image-text interactions can yield harmful content, often overlooking contextually unsafe outcomes from seemingly benign pairs. This narrow coverage leaves VLMs vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely primarily on data-centric tuning, with limited architectural innovations to intrinsically strengthen safety. We address these gaps by introducing a holistic safety dataset and benchmark, HoliSafe, that spans all five safe/unsafe image-text combinations, providing a more robust basis for both training and evaluation. We further propose SafeLLaVA, a novel VLM augmented with a learnable safety meta token and a dedicated safety head. The meta token encodes harmful visual cues during training, intrinsically guiding the language model toward safer responses, while the safety head offers interpretable harmfulness classification aligned with refusal rationales. Experiments show that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety performance across multiple VLM benchmarks. Additionally, the HoliSafe benchmark itself reveals critical vulnerabilities in existing models. We hope that HoliSafe and SafeLLaVA will spur further research into robust and interpretable VLM safety, expanding future avenues for multimodal alignment.",
        "arxiv_id": "2506.04704",
        "ARXIVID": "2506.04704",
        "COMMENT": "Matches criterion 2 as it introduces a novel VLM (SafeLLaVA) and a new safety benchmark (HoliSafe) for vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.05341": {
        "authors": [
            "Xingjian Ran",
            "Yixuan Li",
            "Linning Xu",
            "Mulin Yu",
            "Bo Dai"
        ],
        "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
        "abstract": "arXiv:2506.05341v1 Announce Type: new  Abstract: Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.",
        "arxiv_id": "2506.05341",
        "ARXIVID": "2506.05341",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 3D indoor scene synthesis with spatial reasoning, which is relevant for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05285": {
        "authors": [
            "Bardienus P. Duisterhof",
            "Jan Oberst",
            "Bowen Wen",
            "Stan Birchfield",
            "Deva Ramanan",
            "Jeffrey Ichnowski"
        ],
        "title": "RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion",
        "abstract": "arXiv:2506.05285v1 Announce Type: new  Abstract: 3D shape completion has broad applications in robotics, digital twin reconstruction, and extended reality (XR). Although recent advances in 3D object and scene completion have achieved impressive results, existing methods lack 3D consistency, are computationally expensive, and struggle to capture sharp object boundaries. Our work (RaySt3R) addresses these limitations by recasting 3D shape completion as a novel view synthesis problem. Specifically, given a single RGB-D image and a novel viewpoint (encoded as a collection of query rays), we train a feedforward transformer to predict depth maps, object masks, and per-pixel confidence scores for those query rays. RaySt3R fuses these predictions across multiple query views to reconstruct complete 3D shapes. We evaluate RaySt3R on synthetic and real-world datasets, and observe it achieves state-of-the-art performance, outperforming the baselines on all datasets by up to 44% in 3D chamfer distance. Project page: https://rayst3r.github.io",
        "arxiv_id": "2506.05285",
        "ARXIVID": "2506.05285",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in 3D shape completion using a transformer-based approach.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05260": {
        "authors": [
            "Xiaodong Wang",
            "Jinfa Huang",
            "Li Yuan",
            "Peixi Peng"
        ],
        "title": "LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs",
        "abstract": "arXiv:2506.05260v1 Announce Type: new  Abstract: Most Video Large Language Models (Video-LLMs) adopt preference alignment techniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin between a winning response ($y_w$) and a losing response ($y_l$). However, the likelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta (y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training, inadvertently boosting the probabilities of non-target responses. In this paper, we systematically revisit this phenomenon from LLMs to Video-LLMs, showing that it intensifies when dealing with the redundant complexity of video content. To alleviate the impact of this phenomenon, we propose \\emph{Lean Preference Optimization} (LeanPO), a reference-free approach that reformulates the implicit reward as the average likelihood of the response with respect to the policy model. A key component of LeanPO is the reward-trustworthiness correlated self-generated preference data pipeline, which carefully infuses relevant prior knowledge into the model while continuously refining the preference data via self-reflection. This allows the policy model to obtain high-quality paired data and accurately estimate the newly defined reward, thus mitigating the unintended drop. In addition, we introduce a dynamic label smoothing strategy that mitigates the impact of noise in responses from diverse video content, preventing the model from overfitting to spurious details. Extensive experiments demonstrate that LeanPO significantly enhances the performance of state-of-the-art Video-LLMs, consistently boosting baselines of varying capacities with minimal additional training overhead. Moreover, LeanPO offers a simple yet effective solution for aligning Video-LLM preferences with human trustworthiness, paving the way toward the reliable and efficient Video-LLMs.",
        "arxiv_id": "2506.05260",
        "ARXIVID": "2506.05260",
        "COMMENT": "Matches criterion 2 as it proposes a new optimization method for Video-LLMs, improving their alignment and performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04789": {
        "authors": [
            "Gaia Di Lorenzo",
            "Federico Tombari",
            "Marc Pollefeys",
            "Daniel Barath"
        ],
        "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
        "abstract": "arXiv:2506.04789v1 Announce Type: new  Abstract: Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.",
        "arxiv_id": "2506.04789",
        "ARXIVID": "2506.04789",
        "COMMENT": "Matches criterion 1 as it proposes a multi-modal 3D object representation framework with applications in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05204": {
        "authors": [
            "Yanbo Wang",
            "Ziyi Wang",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View",
        "abstract": "arXiv:2506.05204v1 Announce Type: new  Abstract: Reconstructing semantic-aware 3D scenes from sparse views is a challenging yet essential research direction, driven by the demands of emerging applications such as virtual reality and embodied AI. Existing per-scene optimization methods require dense input views and incur high computational costs, while generalizable approaches often struggle to reconstruct regions outside the input view cone. In this paper, we propose OGGSplat, an open Gaussian growing method that expands the field-of-view in generalizable 3D reconstruction. Our key insight is that the semantic attributes of open Gaussians provide strong priors for image extrapolation, enabling both semantic consistency and visual plausibility. Specifically, once open Gaussians are initialized from sparse views, we introduce an RGB-semantic consistent inpainting module applied to selected rendered views. This module enforces bidirectional control between an image diffusion model and a semantic diffusion model. The inpainted regions are then lifted back into 3D space for efficient and progressive Gaussian parameter optimization. To evaluate our method, we establish a Gaussian Outpainting (GO) benchmark that assesses both semantic and generative quality of reconstructed open-vocabulary scenes. OGGSplat also demonstrates promising semantic-aware scene reconstruction capabilities when provided with two view images captured directly from a smartphone camera.",
        "arxiv_id": "2506.05204",
        "ARXIVID": "2506.05204",
        "COMMENT": "Matches criterion 3 as it introduces a new method for semantic-aware 3D reconstruction and establishes a new benchmark (Gaussian Outpainting).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05287": {
        "authors": [
            "Yuqian Yuan",
            "Ronghao Dang",
            "Long Li",
            "Wentong Li",
            "Dian Jiao",
            "Xin Li",
            "Deli Zhao",
            "Fan Wang",
            "Wenqiao Zhang",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?",
        "abstract": "arXiv:2506.05287v1 Announce Type: new  Abstract: The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.",
        "arxiv_id": "2506.05287",
        "ARXIVID": "2506.05287",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (EOC-Bench) for evaluating multi-modal large language models in dynamic egocentric scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04590": {
        "authors": [
            "Yue Ma",
            "Kunyu Feng",
            "Xinhua Zhang",
            "Hongyu Liu",
            "David Junhao Zhang",
            "Jinbo Xing",
            "Yinhan Zhang",
            "Ayden Yang",
            "Zeyu Wang",
            "Qifeng Chen"
        ],
        "title": "Follow-Your-Creation: Empowering 4D Creation through Video Inpainting",
        "abstract": "arXiv:2506.04590v1 Announce Type: new  Abstract: We introduce Follow-Your-Creation, a novel 4D video creation framework capable of both generating and editing 4D content from a single monocular video input. By leveraging a powerful video inpainting foundation model as a generative prior, we reformulate 4D video creation as a video inpainting task, enabling the model to fill in missing content caused by camera trajectory changes or user edits. To facilitate this, we generate composite masked inpainting video data to effectively fine-tune the model for 4D video generation. Given an input video and its associated camera trajectory, we first perform depth-based point cloud rendering to obtain invisibility masks that indicate the regions that should be completed. Simultaneously, editing masks are introduced to specify user-defined modifications, and these are combined with the invisibility masks to create a composite masks dataset. During training, we randomly sample different types of masks to construct diverse and challenging inpainting scenarios, enhancing the model's generalization and robustness in various 4D editing and generation tasks. To handle temporal consistency under large camera motion, we design a self-iterative tuning strategy that gradually increases the viewing angles during training, where the model is used to generate the next-stage training data after each fine-tuning iteration. Moreover, we introduce a temporal packaging module during inference to enhance generation quality. Our method effectively leverages the prior knowledge of the base model without degrading its original performance, enabling the generation of 4D videos with consistent multi-view coherence. In addition, our approach supports prompt-based content editing, demonstrating strong flexibility and significantly outperforming state-of-the-art methods in both quality and versatility.",
        "arxiv_id": "2506.04590",
        "ARXIVID": "2506.04590",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 4D video creation and editing, focusing on multi-view coherence and user-defined modifications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05348": {
        "authors": [
            "Yifan Wang",
            "Peishan Yang",
            "Zhen Xu",
            "Jiaming Sun",
            "Zhanhua Zhang",
            "Yong Chen",
            "Hujun Bao",
            "Sida Peng",
            "Xiaowei Zhou"
        ],
        "title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction",
        "abstract": "arXiv:2506.05348v1 Announce Type: new  Abstract: This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin.",
        "arxiv_id": "2506.05348",
        "ARXIVID": "2506.05348",
        "COMMENT": "Matches criterion 1 as it introduces a novel 4D representation for dynamic 3D scene reconstruction, improving spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04953": {
        "authors": [
            "Hong Gao",
            "Yiming Bao",
            "Xuezhan Tu",
            "Bin Zhong",
            "Minling Zhang"
        ],
        "title": "APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval",
        "abstract": "arXiv:2506.04953v1 Announce Type: new  Abstract: Current video-based multimodal large language models struggle with hour-level video understanding due to computational constraints and inefficient information extraction from extensive temporal sequences. We propose APVR (Adaptive Pivot Visual information Retrieval), a training-free framework that addresses the memory wall limitation through hierarchical visual information retrieval. APVR operates via two complementary components: Pivot Frame Retrieval employs semantic expansion and multi-modal confidence scoring to identify semantically relevant video frames, while Pivot Token Retrieval performs query-aware attention-driven token selection within the pivot frames. This dual granularity approach enables processing of hour-long videos while maintaining semantic fidelity. Experimental validation on LongVideoBench and VideoMME demonstrates significant performance improvements, establishing state-of-the-art results for not only training-free but also training-based approaches while providing plug-and-play integration capability with existing MLLM architectures.",
        "arxiv_id": "2506.04953",
        "ARXIVID": "2506.04953",
        "COMMENT": "Matches criterion 3 as it introduces a training-free framework (APVR) for long video understanding, focusing on novel hierarchical visual information retrieval.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04353": {
        "authors": [
            "Ankit Pal",
            "Jung-Oh Lee",
            "Xiaoman Zhang",
            "Malaikannan Sankarasubbu",
            "Seunghyeon Roh",
            "Won Jung Kim",
            "Meesun Lee",
            "Pranav Rajpurkar"
        ],
        "title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding",
        "abstract": "arXiv:2506.04353v1 Announce Type: new  Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA",
        "arxiv_id": "2506.04353",
        "ARXIVID": "2506.04353",
        "COMMENT": "Matches criterion 4 as it introduces a large-scale VQA benchmark (ReXVQA) for chest X-ray understanding and evaluates state-of-the-art multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04614": {
        "authors": [
            "Yuyang Wanyan",
            "Xi Zhang",
            "Haiyang Xu",
            "Haowei Liu",
            "Junyang Wang",
            "Jiabo Ye",
            "Yutong Kou",
            "Ming Yan",
            "Fei Huang",
            "Xiaoshan Yang",
            "Weiming Dong",
            "Changsheng Xu"
        ],
        "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation",
        "abstract": "arXiv:2506.04614v1 Announce Type: new  Abstract: In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.",
        "arxiv_id": "2506.04614",
        "ARXIVID": "2506.04614",
        "COMMENT": "Matches criterion 2 as it discusses a multimodal large language model (GUI-Critic-R1) for GUI automation with novel pre-operative error diagnosis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04612": {
        "authors": [
            "Jinyoung Jun",
            "Lei Chu",
            "Jiahao Li",
            "Yan Lu",
            "Chang-Su Kim"
        ],
        "title": "Perfecting Depth: Uncertainty-Aware Enhancement of Metric Depth",
        "abstract": "arXiv:2506.04612v1 Announce Type: new  Abstract: We propose a novel two-stage framework for sensor depth enhancement, called Perfecting Depth. This framework leverages the stochastic nature of diffusion models to automatically detect unreliable depth regions while preserving geometric cues. In the first stage (stochastic estimation), the method identifies unreliable measurements and infers geometric structure by leveraging a training-inference domain gap. In the second stage (deterministic refinement), it enforces structural consistency and pixel-level accuracy using the uncertainty map derived from the first stage. By combining stochastic uncertainty modeling with deterministic refinement, our method yields dense, artifact-free depth maps with improved reliability. Experimental results demonstrate its effectiveness across diverse real-world scenarios. Furthermore, theoretical analysis, various experiments, and qualitative visualizations validate its robustness and scalability. Our framework sets a new baseline for sensor depth enhancement, with potential applications in autonomous driving, robotics, and immersive technologies.",
        "arxiv_id": "2506.04612",
        "ARXIVID": "2506.04612",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for sensor depth enhancement, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.04559": {
        "authors": [
            "Yunhao Gou",
            "Kai Chen",
            "Zhili Liu",
            "Lanqing Hong",
            "Xin Jin",
            "Zhenguo Li",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "title": "Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning",
        "abstract": "arXiv:2506.04559v1 Announce Type: new  Abstract: Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment.",
        "arxiv_id": "2506.04559",
        "ARXIVID": "2506.04559",
        "COMMENT": "Matches criterion 2 as it proposes a reasoning-aligned perceptual decoupling strategy for multi-modal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.05318": {
        "authors": [
            "Haoyuan Li",
            "Yanpeng Zhou",
            "Yufei Gao",
            "Tao Tang",
            "Jianhua Han",
            "Yujie Yuan",
            "Dave Zhenyu Chen",
            "Jiawang Bian",
            "Hang Xu",
            "Xiaodan Liang"
        ],
        "title": "Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets 3D VLMs",
        "abstract": "arXiv:2506.05318v1 Announce Type: new  Abstract: Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest in extending them to 3D settings for tasks like 3D Question Answering, Dense Captioning, and Visual Grounding. Unlike 2D VLMs that typically process images through an image encoder, 3D scenes, with their intricate spatial structures, allow for diverse model architectures. Based on their encoder design, this paper categorizes recent 3D VLMs into 3D object-centric, 2D image-based, and 3D scene-centric approaches. Despite the architectural similarity of 3D scene-centric VLMs to their 2D counterparts, they have exhibited comparatively lower performance compared with the latest 3D object-centric and 2D image-based approaches. To understand this gap, we conduct an in-depth analysis, revealing that 3D scene-centric VLMs show limited reliance on the 3D scene encoder, and the pre-train stage appears less effective than in 2D VLMs. Furthermore, we observe that data scaling benefits are less pronounced on larger datasets. Our investigation suggests that while these models possess cross-modal alignment capabilities, they tend to over-rely on linguistic cues and overfit to frequent answer distributions, thereby diminishing the effective utilization of the 3D encoder. To address these limitations and encourage genuine 3D scene understanding, we introduce a novel 3D Relevance Discrimination QA dataset designed to disrupt shortcut learning and improve 3D understanding. Our findings highlight the need for advanced evaluation and improved strategies for better 3D understanding in 3D VLMs.",
        "arxiv_id": "2506.05318",
        "ARXIVID": "2506.05318",
        "COMMENT": "Matches criterion 2 as it discusses extending 2D vision-language models to 3D settings and introduces a novel dataset for evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.05280": {
        "authors": [
            "Nan Wang",
            "Yuantao Chen",
            "Lixing Xiao",
            "Weiqing Xiao",
            "Bohan Li",
            "Zhaoxi Chen",
            "Chongjie Ye",
            "Shaocong Xu",
            "Saining Zhang",
            "Ziyang Yan",
            "Pierre Merriaux",
            "Lei Lei",
            "Tianfan Xue",
            "Hao Zhao"
        ],
        "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting",
        "abstract": "arXiv:2506.05280v1 Announce Type: new  Abstract: Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
        "arxiv_id": "2506.05280",
        "ARXIVID": "2506.05280",
        "COMMENT": "Matches criterion 4 as it focuses on improving neural rendering techniques for autonomous driving, which is an application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05338": {
        "authors": [
            "Alan Dolhasz",
            "Chen Ma",
            "Dave Gausebeck",
            "Kevin Chen",
            "Gregor Miller",
            "Lucas Hayne",
            "Gunnar Hovden",
            "Azwad Sabik",
            "Olaf Brandt",
            "Mira Slavcheva"
        ],
        "title": "Defurnishing with X-Ray Vision: Joint Removal of Furniture from Panoramas and Mesh",
        "abstract": "arXiv:2506.05338v1 Announce Type: new  Abstract: We present a pipeline for generating defurnished replicas of indoor spaces represented as textured meshes and corresponding multi-view panoramic images. To achieve this, we first segment and remove furniture from the mesh representation, extend planes, and fill holes, obtaining a simplified defurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying structure, guiding the defurnishing process. We extract Canny edges from depth and normal images rendered from the SDM. We then use these as a guide to remove the furniture from panorama images via ControlNet inpainting. This control signal ensures the availability of global geometric information that may be hidden from a particular panoramic view by the furniture being removed. The inpainted panoramas are used to texture the mesh. We show that our approach produces higher quality assets than methods that rely on neural radiance fields, which tend to produce blurry low-resolution images, or RGB-D inpainting, which is highly susceptible to hallucinations.",
        "arxiv_id": "2506.05338",
        "ARXIVID": "2506.05338",
        "COMMENT": "Matches criterion 3 as it focuses on a novel pipeline for defurnishing indoor spaces, which could be relevant for embodied AI benchmarks or simulators.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05313": {
        "authors": [
            "Ta-Ying Cheng",
            "Prafull Sharma",
            "Mark Boss",
            "Varun Jampani"
        ],
        "title": "MARBLE: Material Recomposition and Blending in CLIP-Space",
        "abstract": "arXiv:2506.05313v1 Announce Type: new  Abstract: Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/",
        "arxiv_id": "2506.05313",
        "ARXIVID": "2506.05313",
        "COMMENT": "Matches criterion 4 as it focuses on material editing using vision foundation models and CLIP-space.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05198": {
        "authors": [
            "Yuxin Wen",
            "Yangsibo Huang",
            "Tom Goldstein",
            "Ravi Kumar",
            "Badih Ghazi",
            "Chiyuan Zhang"
        ],
        "title": "Quantifying Cross-Modality Memorization in Vision-Language Models",
        "abstract": "arXiv:2506.05198v1 Announce Type: new  Abstract: Understanding what and how neural networks memorize during training is crucial, both from the perspective of unintentional memorization of potentially sensitive information and from the standpoint of effective knowledge acquisition for real-world, knowledge-intensive tasks. While previous studies primarily investigate memorization within a single modality, such as text memorization in large language models or image memorization in diffusion models, unified multimodal models are becoming increasingly prevalent in practical applications. In this work, we focus on the unique characteristics of cross-modality memorization and conduct a systematic study centered on vision-language models. To facilitate controlled experiments, we first introduce a synthetic persona dataset comprising diverse synthetic person images and textual descriptions. We quantify factual knowledge memorization and cross-modal transferability by training models on a single modality and evaluating their performance in the other. Our results reveal that facts learned in one modality transfer to the other, but a significant gap exists between recalling information in the source and target modalities. Furthermore, we observe that this gap exists across various scenarios, including more capable models, machine unlearning, and the multi-hop case. At the end, we propose a baseline method to mitigate this challenge. We hope our study can inspire future research on developing more robust multimodal learning techniques to enhance cross-modal transferability.",
        "arxiv_id": "2506.05198",
        "ARXIVID": "2506.05198",
        "COMMENT": "Matches criterion 2 as it systematically studies cross-modality memorization in vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.04606": {
        "authors": [
            "Alexander Huang-Menders",
            "Xinhang Liu",
            "Andy Xu",
            "Yuyao Zhang",
            "Chi-Keung Tang",
            "Yu-Wing Tai"
        ],
        "title": "SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents",
        "abstract": "arXiv:2506.04606v1 Announce Type: new  Abstract: SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware.",
        "arxiv_id": "2506.04606",
        "ARXIVID": "2506.04606",
        "COMMENT": "Matches criterion 2 as it leverages VLMs for avatar generation and introduces an innovative refinement process.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05146": {
        "authors": [
            "Massimo Rizzoli",
            "Simone Alghisi",
            "Olha Khomyn",
            "Gabriel Roccabruna",
            "Seyed Mahed Mousavi",
            "Giuseppe Riccardi"
        ],
        "title": "CIVET: Systematic Evaluation of Understanding in VLMs",
        "abstract": "arXiv:2506.05146v1 Announce Type: new  Abstract: While Vision-Language Models (VLMs) have achieved competitive performance in various tasks, their comprehension of the underlying structure and semantics of a scene remains understudied. To investigate the understanding of VLMs, we study their capability regarding object properties and relations in a controlled and interpretable manner. To this scope, we introduce CIVET, a novel and extensible framework for systematiC evaluatIon Via controllEd sTimuli. CIVET addresses the lack of standardized systematic evaluation for assessing VLMs' understanding, enabling researchers to test hypotheses with statistical rigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of stimuli, free from annotation noise, dataset-specific biases, and uncontrolled scene complexity. Our findings reveal that 1) current VLMs can accurately recognize only a limited set of basic object properties; 2) their performance heavily depends on the position of the object in the scene; 3) they struggle to understand basic relations among objects. Furthermore, a comparative evaluation with human annotators reveals that VLMs still fall short of achieving human-level accuracy.",
        "arxiv_id": "2506.05146",
        "ARXIVID": "2506.05146",
        "COMMENT": "Matches criterion 2 as it evaluates VLMs systematically and provides insights into their limitations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.04444": {
        "authors": [
            "Zhaoyang Lv",
            "Maurizio Monge",
            "Ka Chen",
            "Yufeng Zhu",
            "Michael Goesele",
            "Jakob Engel",
            "Zhao Dong",
            "Richard Newcombe"
        ],
        "title": "Photoreal Scene Reconstruction from an Egocentric Device",
        "abstract": "arXiv:2506.04444v1 Announce Type: new  Abstract: In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at http://www.projectaria.com/photoreal-reconstruction/",
        "arxiv_id": "2506.04444",
        "ARXIVID": "2506.04444",
        "COMMENT": "Matches criterion 3 as it focuses on photorealistic scene reconstruction using egocentric devices, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.04641": {
        "authors": [
            "Qiming Hu",
            "Linlong Fan",
            "Yiyan Luo",
            "Yuhang Yu",
            "Xiaojie Guo",
            "Qingnan Fan"
        ],
        "title": "Text-Aware Real-World Image Super-Resolution via Diffusion Model with Joint Segmentation Decoders",
        "abstract": "arXiv:2506.04641v1 Announce Type: new  Abstract: The introduction of generative models has significantly advanced image super-resolution (SR) in handling real-world degradations. However, they often incur fidelity-related issues, particularly distorting textual structures. In this paper, we introduce a novel diffusion-based SR framework, namely TADiSR, which integrates text-aware attention and joint segmentation decoders to recover not only natural details but also the structural fidelity of text regions in degraded real-world images. Moreover, we propose a complete pipeline for synthesizing high-quality images with fine-grained full-image text masks, combining realistic foreground text regions with detailed background content. Extensive experiments demonstrate that our approach substantially enhances text legibility in super-resolved images, achieving state-of-the-art performance across multiple evaluation metrics and exhibiting strong generalization to real-world scenarios. Our code is available at \\href{https://github.com/mingcv/TADiSR}{here}.",
        "arxiv_id": "2506.04641",
        "ARXIVID": "2506.04641",
        "COMMENT": "Matches criterion 4 as it introduces a novel diffusion-based framework (TADiSR) for text-aware image super-resolution, which is an application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.04999": {
        "authors": [
            "Gengluo Li",
            "Huawen Shen",
            "Yu Zhou"
        ],
        "title": "Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts",
        "abstract": "arXiv:2506.04999v1 Announce Type: new  Abstract: Chinese scene text retrieval is a practical task that aims to search for images containing visual instances of a Chinese query text. This task is extremely challenging because Chinese text often features complex and diverse layouts in real-world scenes. Current efforts tend to inherit the solution for English scene text retrieval, failing to achieve satisfactory performance. In this paper, we establish a Diversified Layout benchmark for Chinese Street View Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval performance across various text layouts, including vertical, cross-line, and partial alignments. To address the limitations in existing methods, we propose Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates global visual information with multi-granularity alignment training. CSTR-CLIP applies a two-stage training process to overcome previous limitations, such as the exclusion of visual features outside the text region and reliance on single-granularity alignment, thereby enabling the model to effectively handle diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP outperforms the previous state-of-the-art model by 18.82% accuracy and also provides faster inference speed. Further analysis on DL-CSVTR confirms the superior performance of CSTR-CLIP in handling various text layouts. The dataset and code will be publicly available to facilitate research in Chinese scene text retrieval.",
        "arxiv_id": "2506.04999",
        "ARXIVID": "2506.04999",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (DL-CSVTR) and a novel method (CSTR-CLIP) for Chinese scene text retrieval, focusing on diverse layouts.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05218": {
        "authors": [
            "Zhang Li",
            "Yuliang Liu",
            "Qiang Liu",
            "Zhiyin Ma",
            "Ziyang Zhang",
            "Shuo Zhang",
            "Zidun Guo",
            "Jiarui Zhang",
            "Xinyu Wang",
            "Xiang Bai"
        ],
        "title": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm",
        "abstract": "arXiv:2506.05218v1 Announce Type: new  Abstract: We introduce MonkeyOCR, a vision-language model for document parsing that advances the state of the art by leveraging a Structure-Recognition-Relation (SRR) triplet paradigm. This design simplifies what would otherwise be a complex multi-tool pipeline (as in MinerU's modular approach) and avoids the inefficiencies of processing full pages with giant end-to-end models (e.g., large multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted into three fundamental questions - \"Where is it?\" (structure), \"What is it?\" (recognition), and \"How is it organized?\" (relation) - corresponding to layout analysis, content identification, and logical ordering. This focused decomposition balances accuracy and speed: it enables efficient, scalable processing without sacrificing precision. To train and evaluate this approach, we introduce the MonkeyDoc (the most comprehensive document parsing dataset to date), with 3.9 million instances spanning over ten document types in both Chinese and English. Experiments show that MonkeyOCR outperforms MinerU by an average of 5.1%, with particularly notable improvements on challenging content such as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter model surpasses much larger and top-performing models, including Qwen2.5-VL (72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on English document parsing tasks. In addition, MonkeyOCR processes multi-page documents significantly faster (0.84 pages per second compared to 0.65 for MinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed for inference on a single NVIDIA 3090 GPU. Code and models will be released at https://github.com/Yuliang-Liu/MonkeyOCR.",
        "arxiv_id": "2506.05218",
        "ARXIVID": "2506.05218",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model (MonkeyOCR) for document parsing with a novel triplet paradigm.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.05343": {
        "authors": [
            "Wenfeng Lin",
            "Renjie Chen",
            "Boyuan Liu",
            "Shiyue Yan",
            "Ruoyu Feng",
            "Jiangchuan Wei",
            "Yichen Zhang",
            "Yimeng Zhou",
            "Chao Feng",
            "Jiao Ran",
            "Qi Wu",
            "Zuotao Liu",
            "Mingyu Guo"
        ],
        "title": "ContentV: Efficient Training of Video Generation Models with Limited Compute",
        "abstract": "arXiv:2506.05343v1 Announce Type: new  Abstract: Recent advances in video generation demand increasingly efficient training recipes to mitigate escalating computational costs. In this report, we present ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art performance (85.14 on VBench) after training on 256 x 64GB Neural Processing Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality videos across multiple resolutions and durations from text prompts, enabled by three key innovations: (1) A minimalist architecture that maximizes reuse of pre-trained image generation models for video generation; (2) A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency; and (3) A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations. All the code and models are available at: https://contentv.github.io.",
        "arxiv_id": "2506.05343",
        "ARXIVID": "2506.05343",
        "COMMENT": "Matches criterion 4 as it focuses on efficient training of video generation models, which are related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05096": {
        "authors": [
            "Haosong Liu",
            "Yuge Cheng",
            "Zihan Liu",
            "Aiyue Chen",
            "Yiwu Yao",
            "Chen Chen",
            "Jingwen Leng",
            "Yu Feng",
            "Minyi Guo"
        ],
        "title": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers",
        "abstract": "arXiv:2506.05096v1 Announce Type: new  Abstract: Video diffusion transformers (vDiTs) have made impressive progress in text-to-video generation, but their high computational demands present major challenges for practical deployment. While existing acceleration methods reduce workload at various granularities, they often rely on heuristics, limiting their applicability.   We introduce ASTRAEA, an automatic framework that searches for near-optimal configurations for vDiT-based video generation. At its core, ASTRAEA proposes a lightweight token selection mechanism and a memory-efficient, GPU-parallel sparse attention strategy, enabling linear reductions in execution time with minimal impact on generation quality. To determine optimal token reduction for different timesteps, we further design a search framework that leverages a classic evolutionary algorithm to automatically determine the distribution of the token budget effectively. Together, ASTRAEA achieves up to 2.4x inference speedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs) while retaining better video quality compared to the state-of-the-art methods (<0.5% loss on the VBench score compared to the baseline vDiT models).",
        "arxiv_id": "2506.05096",
        "ARXIVID": "2506.05096",
        "COMMENT": "Matches criterion 4 as it focuses on improving computational efficiency for video diffusion transformers, which are related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05289": {
        "authors": [
            "Pingyu Wu",
            "Kai Zhu",
            "Yu Liu",
            "Longxiang Tang",
            "Jian Yang",
            "Yansong Peng",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "AliTok: Towards Sequence Modeling Alignment between Tokenizer and Autoregressive Model",
        "abstract": "arXiv:2506.05289v1 Announce Type: new  Abstract: Autoregressive image generation aims to predict the next token based on previous ones. However, existing image tokenizers encode tokens with bidirectional dependencies during the compression process, which hinders the effective modeling by autoregressive models. In this paper, we propose a novel Aligned Tokenizer (AliTok), which utilizes a causal decoder to establish unidirectional dependencies among encoded tokens, thereby aligning the token modeling approach between the tokenizer and autoregressive model. Furthermore, by incorporating prefix tokens and employing two-stage tokenizer training to enhance reconstruction consistency, AliTok achieves great reconstruction performance while being generation-friendly. On ImageNet-256 benchmark, using a standard decoder-only autoregressive model as the generator with only 177M parameters, AliTok achieves a gFID score of 1.50 and an IS of 305.9. When the parameter count is increased to 662M, AliTok achieves a gFID score of 1.35, surpassing the state-of-the-art diffusion method with 10x faster sampling speed. The code and weights are available at https://github.com/ali-vilab/alitok.",
        "arxiv_id": "2506.05289",
        "ARXIVID": "2506.05289",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05109": {
        "authors": [
            "Tennison Liu",
            "Mihaela van der Schaar"
        ],
        "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning",
        "abstract": "arXiv:2506.05109v1 Announce Type: new  Abstract: Self-improving agents aim to continuously acquire new capabilities with minimal supervision. However, current approaches face two key limitations: their self-improvement processes are often rigid, fail to generalize across tasks domains, and struggle to scale with increasing agent capabilities. We argue that effective self-improvement requires intrinsic metacognitive learning, defined as an agent's intrinsic ability to actively evaluate, reflect on, and adapt its own learning processes. Drawing inspiration from human metacognition, we introduce a formal framework comprising three components: metacognitive knowledge (self-assessment of capabilities, tasks, and learning strategies), metacognitive planning (deciding what and how to learn), and metacognitive evaluation (reflecting on learning experiences to improve future learning). Analyzing existing self-improving agents, we find they rely predominantly on extrinsic metacognitive mechanisms, which are fixed, human-designed loops that limit scalability and adaptability. Examining each component, we contend that many ingredients for intrinsic metacognition are already present. Finally, we explore how to optimally distribute metacognitive responsibilities between humans and agents, and robustly evaluate and improve intrinsic metacognitive learning, key challenges that must be addressed to enable truly sustained, generalized, and aligned self-improvement.",
        "arxiv_id": "2506.05109",
        "ARXIVID": "2506.05109",
        "COMMENT": "Does not match any specific criterion but is tangentially related to embodied AI through self-improving agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05008": {
        "authors": [
            "Fuyi Zhang",
            "Zhu Yu",
            "Chunhao Li",
            "Runmin Zhang",
            "Xiaokai Bai",
            "Zili Zhou",
            "Si-Yuan Cao",
            "Wang Wang",
            "Hui-Liang Shen"
        ],
        "title": "Structure-Aware Radar-Camera Depth Estimation",
        "abstract": "arXiv:2506.05008v1 Announce Type: new  Abstract: Monocular depth estimation aims to determine the depth of each pixel from an RGB image captured by a monocular camera. The development of deep learning has significantly advanced this field by facilitating the learning of depth features from some well-annotated datasets \\cite{Geiger_Lenz_Stiller_Urtasun_2013,silberman2012indoor}. Eigen \\textit{et al.} \\cite{eigen2014depth} first introduce a multi-scale fusion network for depth regression. Following this, subsequent improvements have come from reinterpreting the regression task as a classification problem \\cite{bhat2021adabins,Li_Wang_Liu_Jiang_2022}, incorporating additional priors \\cite{shao2023nddepth,yang2023gedepth}, and developing more effective objective function \\cite{xian2020structure,Yin_Liu_Shen_Yan_2019}. Despite these advances, generalizing to unseen domains remains a challenge. Recently, several methods have employed affine-invariant loss to enable multi-dataset joint training \\cite{MiDaS,ZeroDepth,guizilini2023towards,Dany}. Among them, Depth Anything \\cite{Dany} has shown leading performance in zero-shot monocular depth estimation. While it struggles to estimate accurate metric depth due to the lack of explicit depth cues, it excels at extracting structural information from unseen images, producing structure-detailed monocular depth.",
        "arxiv_id": "2506.05008",
        "ARXIVID": "2506.05008",
        "COMMENT": "Does not match any specific criteria but focuses on radar-camera depth estimation, which is tangentially related to spatial understanding.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2506.04830": {
        "authors": [
            "Shuo Cao",
            "Yihao Liu",
            "Xiaohui Li. Yuanting Gao. Yu Zhou",
            "Chao Dong"
        ],
        "title": "DualX-VSR: Dual Axial Spatial$\\times$Temporal Transformer for Real-World Video Super-Resolution without Motion Compensation",
        "abstract": "arXiv:2506.04830v1 Announce Type: new  Abstract: Transformer-based models like ViViT and TimeSformer have advanced video understanding by effectively modeling spatiotemporal dependencies. Recent video generation models, such as Sora and Vidu, further highlight the power of transformers in long-range feature extraction and holistic spatiotemporal modeling. However, directly applying these models to real-world video super-resolution (VSR) is challenging, as VSR demands pixel-level precision, which can be compromised by tokenization and sequential attention mechanisms. While recent transformer-based VSR models attempt to address these issues using smaller patches and local attention, they still face limitations such as restricted receptive fields and dependence on optical flow-based alignment, which can introduce inaccuracies in real-world settings. To overcome these issues, we propose Dual Axial Spatial$\\times$Temporal Transformer for Real-World Video Super-Resolution (DualX-VSR), which introduces a novel dual axial spatial$\\times$temporal attention mechanism that integrates spatial and temporal information along orthogonal directions. DualX-VSR eliminates the need for motion compensation, offering a simplified structure that provides a cohesive representation of spatiotemporal information. As a result, DualX-VSR achieves high fidelity and superior performance in real-world VSR task.",
        "arxiv_id": "2506.04830",
        "ARXIVID": "2506.04830",
        "COMMENT": "Does not match any specific criteria but proposes a novel transformer-based model for video super-resolution.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05108": {
        "authors": [
            "Revant Teotia",
            "Candace Ross",
            "Karen Ullrich",
            "Sumit Chopra",
            "Adriana Romero-Soriano",
            "Melissa Hall",
            "Matthew J. Muckley"
        ],
        "title": "DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models",
        "abstract": "arXiv:2506.05108v1 Announce Type: new  Abstract: Recent advances in text-to-image (T2I) models have achieved impressive quality and consistency. However, this has come at the cost of representation diversity. While automatic evaluation methods exist for benchmarking model diversity, they either require reference image datasets or lack specificity about the kind of diversity measured, limiting their adaptability and interpretability. To address this gap, we introduce the Does-it/Can-it framework, DIM-CIM, a reference-free measurement of default-mode diversity (\"Does\" the model generate images with expected attributes?) and generalization capacity (\"Can\" the model generate diverse attributes for a particular concept?). We construct the COCO-DIMCIM benchmark, which is seeded with COCO concepts and captions and augmented by a large language model. With COCO-DIMCIM, we find that widely-used models improve in generalization at the cost of default-mode diversity when scaling from 1.5B to 8.1B parameters. DIMCIM also identifies fine-grained failure cases, such as attributes that are generated with generic prompts but are rarely generated when explicitly requested. Finally, we use DIMCIM to evaluate the training data of a T2I model and observe a correlation of 0.85 between diversity in training images and default-mode diversity. Our work provides a flexible and interpretable framework for assessing T2I model diversity and generalization, enabling a more comprehensive understanding of model performance.",
        "arxiv_id": "2506.05108",
        "ARXIVID": "2506.05108",
        "COMMENT": "Does not match any specific criteria but introduces a framework for evaluating diversity in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05083": {
        "authors": [
            "Peng Wang",
            "Yichun Shi",
            "Xiaochen Lian",
            "Zhonghua Zhai",
            "Xin Xia",
            "Xuefeng Xiao",
            "Weilin Huang",
            "Jianchao Yang"
        ],
        "title": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing",
        "abstract": "arXiv:2506.05083v1 Announce Type: new  Abstract: We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0 [22], which significantly improves over our previous version [27] in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and a reward loss. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).",
        "arxiv_id": "2506.05083",
        "ARXIVID": "2506.05083",
        "COMMENT": "Does not match any specific criteria but focuses on generative image editing improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.04648": {
        "authors": [
            "Akide Liu",
            "Zeyu Zhang",
            "Zhexin Li",
            "Xuehai Bai",
            "Yizeng Han",
            "Jiasheng Tang",
            "Yuanjie Xing",
            "Jichao Wu",
            "Mingyang Yang",
            "Weihua Chen",
            "Jiahao He",
            "Yuanyu He",
            "Fan Wang",
            "Gholamreza Haffari",
            "Bohan Zhuang"
        ],
        "title": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion",
        "abstract": "arXiv:2506.04648v1 Announce Type: new  Abstract: Diffusion generative models have become the standard for producing high-quality, coherent video content, yet their slow inference speeds and high computational demands hinder practical deployment. Although both quantization and sparsity can independently accelerate inference while maintaining generation quality, naively combining these techniques in existing training-free approaches leads to significant performance degradation due to the lack of joint optimization.We introduce FPSAttention, a novel training-aware co-design of FP8 quantization and sparsity for video generation, with a focus on the 3D bi-directional attention mechanism. Our approach features three key innovations: 1) A unified 3D tile-wise granularity that simultaneously supports both quantization and sparsity; 2) A denoising step-aware strategy that adapts to the noise schedule, addressing the strong correlation between quantization/sparsity errors and denoising steps; 3) A native, hardware-friendly kernel that leverages FlashAttention and is implemented with optimized Hopper architecture features for highly efficient execution. Trained on Wan2.1's 1.3B and 14B models and evaluated on the VBench benchmark, FPSAttention achieves a 7.09x kernel speedup for attention operations and a 4.96x end-to-end speedup for video generation compared to the BF16 baseline at 720p resolution-without sacrificing generation quality.",
        "arxiv_id": "2506.04648",
        "ARXIVID": "2506.04648",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in video diffusion, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04501": {
        "authors": [
            "Guangyu Shen",
            "Zhihua Li",
            "Xiang Xu",
            "Tianchen Zhao",
            "Zheng Zhang",
            "Dongsheng An",
            "Zhuowen Tu",
            "Yifan Xing",
            "Qin Zhang"
        ],
        "title": "AuthGuard: Generalizable Deepfake Detection via Language Guidance",
        "abstract": "arXiv:2506.04501v1 Announce Type: new  Abstract: Existing deepfake detection techniques struggle to keep-up with the ever-evolving novel, unseen forgeries methods. This limitation stems from their reliance on statistical artifacts learned during training, which are often tied to specific generation processes that may not be representative of samples from new, unseen deepfake generation methods encountered at test time. We propose that incorporating language guidance can improve deepfake detection generalization by integrating human-like commonsense reasoning -- such as recognizing logical inconsistencies and perceptual anomalies -- alongside statistical cues. To achieve this, we train an expert deepfake vision encoder by combining discriminative classification with image-text contrastive learning, where the text is generated by generalist MLLMs using few-shot prompting. This allows the encoder to extract both language-describable, commonsense deepfake artifacts and statistical forgery artifacts from pixel-level distributions. To further enhance robustness, we integrate data uncertainty learning into vision-language contrastive learning, mitigating noise in image-text supervision. Our expert vision encoder seamlessly interfaces with an LLM, further enabling more generalized and interpretable deepfake detection while also boosting accuracy. The resulting framework, AuthGuard, achieves state-of-the-art deepfake detection accuracy in both in-distribution and out-of-distribution settings, achieving AUC gains of 6.15% on the DFDC dataset and 16.68% on the DF40 dataset. Additionally, AuthGuard significantly enhances deepfake reasoning, improving performance by 24.69% on the DDVQA dataset.",
        "arxiv_id": "2506.04501",
        "ARXIVID": "2506.04501",
        "COMMENT": "Does not match any specific criterion but is related to vision-language models and their application in deepfake detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05184": {
        "authors": [
            "Neeraj Kumar",
            "Swaraj Nanda",
            "Siddharth Singi",
            "Jamal Benhamida",
            "David Kim",
            "Jie-Fu Chen",
            "Amir Momeni-Boroujeni",
            "Gregory M. Goldgof",
            "Gabriele Campanella",
            "Chad Vanderbilt"
        ],
        "title": "Single GPU Task Adaptation of Pathology Foundation Models for Whole Slide Image Analysis",
        "abstract": "arXiv:2506.05184v1 Announce Type: new  Abstract: Pathology foundation models (PFMs) have emerged as powerful tools for analyzing whole slide images (WSIs). However, adapting these pretrained PFMs for specific clinical tasks presents considerable challenges, primarily due to the availability of only weak (WSI-level) labels for gigapixel images, necessitating multiple instance learning (MIL) paradigm for effective WSI analysis. This paper proposes a novel approach for single-GPU \\textbf{T}ask \\textbf{A}daptation of \\textbf{PFM}s (TAPFM) that uses vision transformer (\\vit) attention for MIL aggregation while optimizing both for feature representations and attention weights. The proposed approach maintains separate computational graphs for MIL aggregator and the PFM to create stable training dynamics that align with downstream task objectives during end-to-end adaptation. Evaluated on mutation prediction tasks for bladder cancer and lung adenocarcinoma across institutional and TCGA cohorts, TAPFM consistently outperforms conventional approaches, with H-Optimus-0 (TAPFM) outperforming the benchmarks. TAPFM effectively handles multi-label classification of actionable mutations as well. Thus, TAPFM makes adaptation of powerful pre-trained PFMs practical on standard hardware for various clinical applications.",
        "arxiv_id": "2506.05184",
        "ARXIVID": "2506.05184",
        "COMMENT": "Does not match any specific criterion but is related to adapting foundation models for a specific domain (pathology).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04956": {
        "authors": [
            "Huihan Wang",
            "Zhiwen Yang",
            "Hui Zhang",
            "Dan Zhao",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation",
        "abstract": "arXiv:2506.04956v1 Announce Type: new  Abstract: Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT.",
        "arxiv_id": "2506.04956",
        "ARXIVID": "2506.04956",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in a specific domain (medical video generation).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04743": {
        "authors": [
            "Shuhan Xu",
            "Siyuan Liang",
            "Hongling Zheng",
            "Yong Luo",
            "Aishan Liu",
            "Dacheng Tao"
        ],
        "title": "SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs",
        "abstract": "arXiv:2506.04743v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved remarkable performance in image captioning, but recent studies show they are vulnerable to backdoor attacks. Attackers can inject imperceptible perturbations-such as local pixel triggers or global semantic phrases-into the training data, causing the model to generate malicious, attacker-controlled captions for specific inputs. These attacks are hard to detect and defend due to their stealthiness and cross-modal nature. By analyzing attack samples, we identify two key vulnerabilities: (1) abnormal attention concentration on specific image regions, and (2) semantic drift and incoherence in generated captions. To counter this, we propose Semantic Reward Defense (SRD), a reinforcement learning framework that mitigates backdoor behavior without prior knowledge of triggers. SRD uses a Deep Q-Network to learn policies for applying discrete perturbations (e.g., occlusion, color masking) to sensitive image regions, aiming to disrupt the activation of malicious pathways. We design a semantic fidelity score as the reward signal, which jointly evaluates semantic consistency and linguistic fluency of the output, guiding the agent toward generating robust yet faithful captions. Experiments across mainstream VLMs and datasets show SRD reduces attack success rates to 5.6%, while preserving caption quality on clean inputs with less than 10% performance drop. SRD offers a trigger-agnostic, interpretable defense paradigm against stealthy backdoor threats in multimodal generative models.",
        "arxiv_id": "2506.04743",
        "ARXIVID": "2506.04743",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of vision-language models and their robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05350": {
        "authors": [
            "George Stoica",
            "Vivek Ramanujan",
            "Xiang Fan",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Judy Hoffman"
        ],
        "title": "Contrastive Flow Matching",
        "abstract": "arXiv:2506.05350v1 Announce Type: new  Abstract: Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.",
        "arxiv_id": "2506.05350",
        "ARXIVID": "2506.05350",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04481": {
        "authors": [
            "Jiayu Liu",
            "Zhenya Huang",
            "Wei Dai",
            "Cheng Cheng",
            "Jinze Wu",
            "Jing Sha",
            "Song Li",
            "Qi Liu",
            "Shijin Wang",
            "Enhong Chen"
        ],
        "title": "CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective",
        "abstract": "arXiv:2506.04481v1 Announce Type: new  Abstract: Although large language models (LLMs) show promise in solving complex mathematical tasks, existing evaluation paradigms rely solely on a coarse measure of overall answer accuracy, which are insufficient for assessing their authentic capabilities. In this paper, we propose \\textbf{CogMath}, which comprehensively assesses LLMs' mathematical abilities through the lens of human cognition. Specifically, inspired by psychological theories, CogMath formalizes human reasoning process into 3 stages: \\emph{problem comprehension}, \\emph{problem solving}, and \\emph{solution summarization}. Within these stages, we investigate perspectives such as numerical calculation, knowledge, and counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In each dimension, we develop an ``\\emph{Inquiry}-\\emph{Judge}-\\emph{Reference}'' multi-agent system to generate inquiries that assess LLMs' mastery from this dimension. An LLM is considered to truly master a problem only when excelling in all inquiries from the 9 dimensions. By applying CogMath on three benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs are overestimated by 30\\%-40\\%. Moreover, we locate their strengths and weaknesses across specific stages/dimensions, offering in-depth insights to further enhance their reasoning abilities.",
        "arxiv_id": "2506.04481",
        "ARXIVID": "2506.04481",
        "COMMENT": "Does not match any specific criteria but evaluates LLMs' mathematical reasoning abilities from a cognitive perspective.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05296": {
        "authors": [
            "Mikhail Terekhov",
            "Zhen Ning David Liu",
            "Caglar Gulcehre",
            "Samuel Albanie"
        ],
        "title": "Control Tax: The Price of Keeping AI in Check",
        "abstract": "arXiv:2506.05296v1 Announce Type: new  Abstract: The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts.",
        "arxiv_id": "2506.05296",
        "ARXIVID": "2506.05296",
        "COMMENT": "Does not match any specific criteria but discusses AI control mechanisms and their economic feasibility.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04950": {
        "authors": [
            "Yong Sun",
            "Yipeng Wang",
            "Junyu Shi",
            "Zhiyuan Zhang",
            "Yanmei Xiao",
            "Lei Zhu",
            "Manxi Jiang",
            "Qiang Nie"
        ],
        "title": "Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining",
        "abstract": "arXiv:2506.04950v1 Announce Type: new  Abstract: Artificial intelligence has recently shown promise in automated embryo selection for In-Vitro Fertilization (IVF). However, current approaches either address partial embryo evaluation lacking holistic quality assessment or target clinical outcomes inevitably confounded by extra-embryonic factors, both limiting clinical utility. To bridge this gap, we propose a new task called Video-Based Embryo Grading - the first paradigm that directly utilizes full-length time-lapse monitoring (TLM) videos to predict embryologists' overall quality assessments. To support this task, we curate a real-world clinical dataset comprising over 2,500 TLM videos, each annotated with a grading label indicating the overall quality of embryos. Grounded in clinical decision-making principles, we propose a Complementary Spatial-Temporal Pattern Mining (CoSTeM) framework that conceptually replicates embryologists' evaluation process. The CoSTeM comprises two branches: (1) a morphological branch using a Mixture of Cross-Attentive Experts layer and a Temporal Selection Block to select discriminative local structural features, and (2) a morphokinetic branch employing a Temporal Transformer to model global developmental trajectories, synergistically integrating static and dynamic determinants for grading embryos. Extensive experimental results demonstrate the superiority of our design. This work provides a valuable methodological framework for AI-assisted embryo selection. The dataset and source code will be publicly available upon acceptance.",
        "arxiv_id": "2506.04950",
        "ARXIVID": "2506.04950",
        "COMMENT": "Does not match any specific criteria but introduces a novel framework for video-based embryo grading, which is tangentially related to vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.04619": {
        "authors": [
            "Shihua Zhang",
            "Zizhuo Li",
            "Kaining Zhang",
            "Yifan Lu",
            "Yuxin Deng",
            "Linfeng Tang",
            "Xingyu Jiang",
            "Jiayi Ma"
        ],
        "title": "Deep Learning Reforms Image Matching: A Survey and Outlook",
        "abstract": "arXiv:2506.04619v1 Announce Type: new  Abstract: Image matching, which establishes correspondences between two-view images to recover 3D structure and camera geometry, serves as a cornerstone in computer vision and underpins a wide range of applications, including visual localization, 3D reconstruction, and simultaneous localization and mapping (SLAM). Traditional pipelines composed of ``detector-descriptor, feature matcher, outlier filter, and geometric estimator'' falter in challenging scenarios. Recent deep-learning advances have significantly boosted both robustness and accuracy. This survey adopts a unique perspective by comprehensively reviewing how deep learning has incrementally transformed the classical image matching pipeline. Our taxonomy highly aligns with the traditional pipeline in two key aspects: i) the replacement of individual steps in the traditional pipeline with learnable alternatives, including learnable detector-descriptor, outlier filter, and geometric estimator; and ii) the merging of multiple steps into end-to-end learnable modules, encompassing middle-end sparse matcher, end-to-end semi-dense/dense matcher, and pose regressor. We first examine the design principles, advantages, and limitations of both aspects, and then benchmark representative methods on relative pose recovery, homography estimation, and visual localization tasks. Finally, we discuss open challenges and outline promising directions for future research. By systematically categorizing and evaluating deep learning-driven strategies, this survey offers a clear overview of the evolving image matching landscape and highlights key avenues for further innovation.",
        "arxiv_id": "2506.04619",
        "ARXIVID": "2506.04619",
        "COMMENT": "Does not match any specific criteria but is a survey on deep learning for image matching, which is relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.04734": {
        "authors": [
            "Lin Sun",
            "Weihong Lin",
            "Jinzhu Wu",
            "Yongfu Zhu",
            "Xiaoqi Jian",
            "Guangxiang Zhao",
            "Change Jia",
            "Linglin Zhang",
            "Sai-er Hu",
            "Yuhan Wu",
            "Xiangzheng Zhang"
        ],
        "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design",
        "abstract": "arXiv:2506.04734v1 Announce Type: new  Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.",
        "arxiv_id": "2506.04734",
        "ARXIVID": "2506.04734",
        "COMMENT": "Does not match any specific criteria but discusses evaluation design for reasoning models, which is tangentially related to your friend's interest in empirical insights.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}