{
    "2511.16624": {
        "authors": [
            "SAM 3D Team",
            "Xingyu Chen",
            "Fu-Jen Chu",
            "Pierre Gleize",
            "Kevin J Liang",
            "Alexander Sax",
            "Hao Tang",
            "Weiyao Wang",
            "Michelle Guo",
            "Thibaut Hardin",
            "Xiang Li",
            "Aohan Lin",
            "Jiawei Liu",
            "Ziqi Ma",
            "Anushka Sagar",
            "Bowen Song",
            "Xiaodong Wang",
            "Jianing Yang",
            "Bowen Zhang",
            "Piotr Doll\\'ar",
            "Georgia Gkioxari",
            "Matt Feiszli",
            "Jitendra Malik"
        ],
        "title": "SAM 3D: 3Dfy Anything in Images",
        "abstract": "arXiv:2511.16624v1 Announce Type: new  Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
        "arxiv_id": "2511.16624",
        "ARXIVID": "2511.16624",
        "COMMENT": "Matches criteria 1 (new methodological improvements to spatial understanding on embodied agents), 3 (new benchmark for 3D object reconstruction), and 4 (vision foundation models and applications). Introduces a generative model for 3D object reconstruction from images, a new large-scale dataset, and a challenging benchmark.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2511.16160": {
        "authors": [
            "Yibin Huang",
            "Wang Xu",
            "Wanyue Zhang",
            "Helu Zhi",
            "Jingjing Huang",
            "Yangbin Xu",
            "Yangang Sun",
            "Conghui Zhu",
            "Tiejun Zhao"
        ],
        "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning",
        "abstract": "arXiv:2511.16160v1 Announce Type: new  Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.",
        "arxiv_id": "2511.16160",
        "ARXIVID": "2511.16160",
        "COMMENT": "Matches criterion 1 (new methodological improvement to spatial understanding/spatial intelligence on embodied agents) and criterion 3 (new benchmark: QVS-Bench, and new method for metric-grounded cognitive maps). Proposes a framework for reconstructing metric-grounded spatial layouts from video, with a new benchmark and analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.16567": {
        "authors": [
            "Ye Mao",
            "Weixun Luo",
            "Ranran Huang",
            "Junpeng Jing",
            "Krystian Mikolajczyk"
        ],
        "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
        "abstract": "arXiv:2511.16567v1 Announce Type: new  Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
        "arxiv_id": "2511.16567",
        "ARXIVID": "2511.16567",
        "COMMENT": "Matches criteria 1 (methodological improvements to spatial understanding on embodied agents) and 3 (new methods and dataset for 3D scene understanding). Introduces a new self-supervised 3D representation model, a new dataset, and demonstrates benefits for embodied navigation and localization.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.16175": {
        "authors": [
            "Yi Yang",
            "Xueqi Li",
            "Yiyang Chen",
            "Jin Song",
            "Yihan Wang",
            "Zipeng Xiao",
            "Jiadi Su",
            "You Qiaoben",
            "Pengfei Liu",
            "Zhijie Deng"
        ],
        "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
        "abstract": "arXiv:2511.16175v1 Announce Type: new  Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\\pi_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
        "arxiv_id": "2511.16175",
        "ARXIVID": "2511.16175",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 3 (embodied AI, new methods for vision-language-action models). Proposes Mantis, a new VLA model with disentangled visual foresight, and demonstrates strong empirical results on embodied AI benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.16301": {
        "authors": [
            "Minseok Seo",
            "Mark Hamilton",
            "Changick Kim"
        ],
        "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
        "abstract": "arXiv:2511.16301v1 Announce Type: new  Abstract: We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.",
        "arxiv_id": "2511.16301",
        "ARXIVID": "2511.16301",
        "COMMENT": "This paper introduces 'Upsample Anything', a lightweight test-time optimization framework for feature upsampling, enabling high-resolution outputs from vision foundation models without retraining. This is a direct match for criterion 4 (vision foundation models and applications), as it addresses a key limitation of ViT-based models and provides a general-purpose solution.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.16091": {
        "authors": [
            "Renxiang Xiao",
            "Wei Liu",
            "Yuanfan Zhang",
            "Yushuai Chen",
            "Jinming Chen",
            "Zilu Wang",
            "Liang Hu"
        ],
        "title": "Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
        "abstract": "arXiv:2511.16091v1 Announce Type: new  Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.",
        "arxiv_id": "2511.16091",
        "ARXIVID": "2511.16091",
        "COMMENT": "Matches criterion 1 (New methodological improvements to spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI): This paper presents a radar-vision integration system for 3D Gaussian Splatting SLAM in large-scale outdoor environments, introducing new methods for dynamic object masking, memory efficiency, and robust mapping. The use of radar and differentiable spatial representations is a novel approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16166": {
        "authors": [
            "Zeting Liu",
            "Zida Yang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
        "abstract": "arXiv:2511.16166v1 Announce Type: new  Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
        "arxiv_id": "2511.16166",
        "ARXIVID": "2511.16166",
        "COMMENT": "Matches criterion 3 (embodied AI, new method for VLA models with novel angle: addressing stage hallucination and sim-to-real transfer). Introduces self-evolving VLA model with new reward, exploration, and memory mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.15351": {
        "authors": [
            "Yifu Guo",
            "Zishan Xu",
            "Zhiyuan Yao",
            "Yuquan Lu",
            "Jiaye Lin",
            "Sen Hu",
            "Zhenheng Tang",
            "Yingchao Li",
            "Huacan Wang",
            "Ronghao Chen"
        ],
        "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
        "abstract": "arXiv:2511.15351v1 Announce Type: new  Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
        "arxiv_id": "2511.15351",
        "ARXIVID": "2511.15351",
        "COMMENT": "Matches criteria 2 (new MLLMs) and 4 (vision foundation models and applications). Proposes a new agentic multimodal reasoning paradigm and benchmark, with dynamic capability orchestration for multimodal agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16221": {
        "authors": [
            "Caixin Kang",
            "Yifei Huang",
            "Liangyang Ouyang",
            "Mingfang Zhang",
            "Ruicong Liu",
            "Yoichi Sato"
        ],
        "title": "Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions",
        "abstract": "arXiv:2511.16221v1 Announce Type: new  Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.",
        "arxiv_id": "2511.16221",
        "ARXIVID": "2511.16221",
        "COMMENT": "Matches criteria 2 (new MLLMs) and 4 (vision foundation models and applications). Introduces a new multimodal benchmark for deception detection in social interactions, evaluates state-of-the-art MLLMs, and proposes new modules for social reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16521": {
        "authors": [
            "Fan Yang",
            "Sosuke Yamao",
            "Ikuo Kusajima",
            "Atsunori Moteki",
            "Shoichi Masui",
            "Shan Jiang"
        ],
        "title": "YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras",
        "abstract": "arXiv:2511.16521v1 Announce Type: new  Abstract: Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.",
        "arxiv_id": "2511.16521",
        "ARXIVID": "2511.16521",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (embodied AI, new benchmark and method). Proposes a novel method for joint scene mapping and camera registration using a mobile agent, and introduces a new benchmark dataset.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16671": {
        "authors": [
            "Ziyu Guo",
            "Renrui Zhang",
            "Hongyu Li",
            "Manyuan Zhang",
            "Xinyan Chen",
            "Sifan Wang",
            "Yan Feng",
            "Peng Pei",
            "Pheng-Ann Heng"
        ],
        "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
        "abstract": "arXiv:2511.16671v1 Announce Type: new  Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
        "arxiv_id": "2511.16671",
        "ARXIVID": "2511.16671",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel interleaved framework (TwiG) for integrating textual reasoning during visual generation, which is a new approach for vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16140": {
        "authors": [
            "Chenyu Zhao",
            "Xianwei Zheng",
            "Zimin Xia",
            "Linwei Yue",
            "Nan Xue"
        ],
        "title": "Real-Time 3D Object Detection with Inference-Aligned Learning",
        "abstract": "arXiv:2511.16140v1 Announce Type: new  Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.",
        "arxiv_id": "2511.16140",
        "ARXIVID": "2511.16140",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents): The paper proposes a new framework for real-time 3D object detection from point clouds, introducing spatial-prioritized optimal transport and rank-aware self-distillation to bridge the training-inference gap. The focus on spatial reliability and ranking awareness is a novel methodological improvement.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16170": {
        "authors": [
            "Jiahao Li",
            "Yang Lu",
            "Yachao Zhang",
            "Yong Xie",
            "Fangyong Wang",
            "Yuan Xie",
            "Yanyun Qu"
        ],
        "title": "Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective",
        "abstract": "arXiv:2511.16170v1 Announce Type: new  Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.",
        "arxiv_id": "2511.16170",
        "ARXIVID": "2511.16170",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications) and partially criterion 2 (VLLMs/MLLMs): The paper proposes a novel, training-free method to improve CLIP's dense prediction for open-vocabulary semantic segmentation by analyzing and redirecting attention, with strong empirical results. The focus on CLIP's internal mechanisms and explainability is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16670": {
        "authors": [
            "Chenyu Lin",
            "Cheng Chi",
            "Jinlin Wu",
            "Sharon Li",
            "Kaiyang Zhou"
        ],
        "title": "Learning to Think Fast and Slow for Visual Language Models",
        "abstract": "arXiv:2511.16670v1 Announce Type: new  Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
        "arxiv_id": "2511.16670",
        "ARXIVID": "2511.16670",
        "COMMENT": "Matches criterion 2 (new VLLM with dual-mode reasoning). Proposes a VLM that can switch between fast and slow thinking modes, improving efficiency and performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.15831": {
        "authors": [
            "Wei Zhang",
            "Yeying Jin",
            "Xin Li",
            "Yan Zhang",
            "Xiaofeng Cong",
            "Cong Wang",
            "Fengcai Qiao",
            "zhichao Lian"
        ],
        "title": "UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment",
        "abstract": "arXiv:2511.15831v1 Announce Type: new  Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.",
        "arxiv_id": "2511.15831",
        "ARXIVID": "2511.15831",
        "COMMENT": "Matches criterion 2 (new MLLM for virtual try-on) and criterion 4 (application of vision foundation models). Proposes a universal VTON framework using an MLLM-guided semantic alignment module, with a novel training strategy for complex tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.15984": {
        "authors": [
            "Xinyu Nan",
            "Lingtao Mao",
            "Huangyu Dai",
            "Zexin Zheng",
            "Xinyu Sun",
            "Zihan Liang",
            "Ben Chen",
            "Yuqing Ding",
            "Chenyi Lei",
            "Wenwu Ou",
            "Han Li"
        ],
        "title": "UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition",
        "abstract": "arXiv:2511.15984v1 Announce Type: new  Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.",
        "arxiv_id": "2511.15984",
        "ARXIVID": "2511.15984",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications). Proposes a unified detection-to-generation framework for hierarchical object recognition, with generative modeling and fine-grained recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.15923": {
        "authors": [
            "Meilong Xu",
            "Di Fu",
            "Jiaxing Zhang",
            "Gong Yu",
            "Jiayu Zheng",
            "Xiaoling Hu",
            "Dongdi Zhao",
            "Feiyang Li",
            "Chao Chen",
            "Yong Cao"
        ],
        "title": "RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification",
        "abstract": "arXiv:2511.15923v1 Announce Type: new  Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \\textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.",
        "arxiv_id": "2511.15923",
        "ARXIVID": "2511.15923",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications). Proposes a new fine-tuning paradigm for VLMs in video classification using self-generated rationales, which is a clever statistical trick.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16618": {
        "authors": [
            "Haofeng Liu",
            "Ziyue Wang",
            "Sudhanshu Mishra",
            "Mingqi Gao",
            "Guanyi Qin",
            "Chang Han Low",
            "Alex Y. W. Kong",
            "Yueming Jin"
        ],
        "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
        "abstract": "arXiv:2511.16618v1 Announce Type: new  Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
        "arxiv_id": "2511.16618",
        "ARXIVID": "2511.16618",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications). Presents a foundation model for surgical video segmentation, introduces a new benchmark, and proposes new methods for long-term tracking and ambiguity-resilient learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16378": {
        "authors": [
            "Pan Yang",
            "Cheng Deng",
            "Jing Yang",
            "Han Zhao",
            "Yun Liu",
            "Yuling Chen",
            "Xiaoli Ruan",
            "Yanping Chen"
        ],
        "title": "CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement",
        "abstract": "arXiv:2511.16378v1 Announce Type: new  Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.",
        "arxiv_id": "2511.16378",
        "ARXIVID": "2511.16378",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method (CAMS) for compositional zero-shot learning using CLIP, with novel cross-attention and disentanglement strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16227": {
        "authors": [
            "Boyue Xu",
            "Ruichao Hou",
            "Tongwei Ren",
            "Dongming Zhou",
            "Gangshan Wu",
            "Jinde Cao"
        ],
        "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking",
        "abstract": "arXiv:2511.16227v1 Announce Type: new  Abstract: Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.",
        "arxiv_id": "2511.16227",
        "ARXIVID": "2511.16227",
        "COMMENT": "This paper proposes SwiTrack, a tri-state switch framework for cross-modal object tracking, with specialized streams for RGB, NIR, and invalid modalities. It is a methodological improvement in cross-modal tracking, which is relevant to spatial intelligence in embodied agents (criterion 1) and introduces a novel method for robust tracking (criterion 3).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16143": {
        "authors": [
            "Quanqing Ma",
            "Jiaen Chen",
            "Peng Wang",
            "Yao Zheng",
            "Qingzhan Zhao",
            "Yuchen Zheng"
        ],
        "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
        "abstract": "arXiv:2511.16143v1 Announce Type: new  Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
        "arxiv_id": "2511.16143",
        "ARXIVID": "2511.16143",
        "COMMENT": "This paper introduces a new high spatial resolution dataset for water body change detection and proposes a spatial semantics and continuity perception attention module. It matches criterion 1 (spatial understanding) and criterion 3 (new benchmark for spatial change detection), as it provides both a new dataset and a novel attention mechanism for spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16430": {
        "authors": [
            "Yihan Li",
            "Nikhil Churamani",
            "Maria Robu",
            "Imanol Luengo",
            "Danail Stoyanov"
        ],
        "title": "Graph Neural Networks for Surgical Scene Segmentation",
        "abstract": "arXiv:2511.16430v1 Announce Type: new  Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.   Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.   Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.   Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.",
        "arxiv_id": "2511.16430",
        "ARXIVID": "2511.16430",
        "COMMENT": "This paper introduces graph-based segmentation models that combine Vision Transformers and Graph Neural Networks to enhance spatial and semantic understanding in surgical scene segmentation. This is a clear match for criterion 1 (new methodological improvements to spatial understanding), as it explicitly models spatial relationships and improves spatial intelligence in a vision context.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16635": {
        "authors": [
            "Guolin Huang",
            "Wenting Chen",
            "Jiaqi Yang",
            "Xinheng Lyu",
            "Xiaoling Luo",
            "Sen Yang",
            "Xiaohan Xing",
            "Linlin Shen"
        ],
        "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction",
        "abstract": "arXiv:2511.16635v1 Announce Type: new  Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.",
        "arxiv_id": "2511.16635",
        "ARXIVID": "2511.16635",
        "COMMENT": "Matches criterion 2 (new MLLMs) and partially criterion 4 (vision foundation models in application). Introduces a hierarchical chain-of-thought enhanced multi-agent system for multimodal survival prediction, with a focus on explainable AI and multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.16136": {
        "authors": [
            "Jiazhen Yan",
            "Ziqiang Li",
            "Fan Wang",
            "Kai Zeng",
            "Zhangjie Fu"
        ],
        "title": "How Noise Benefits AI-generated Image Detection",
        "abstract": "arXiv:2511.16136v1 Announce Type: new  Abstract: The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.",
        "arxiv_id": "2511.16136",
        "ARXIVID": "2511.16136",
        "COMMENT": "This paper proposes a new method (PiN-CLIP) for detecting AI-generated images by injecting positive-incentive noise into the feature space of CLIP. It is relevant to vision foundation models (criterion 4), as it leverages CLIP and addresses a key challenge in generative model forensics with a clever statistical trick.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.16524": {
        "authors": [
            "Rahul Kumar",
            "Vipul Baghel",
            "Sudhanshu Singh",
            "Bikash Kumar Badatya",
            "Shivam Yadav",
            "Babji Srinivasan",
            "Ravi Hegde"
        ],
        "title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization",
        "abstract": "arXiv:2511.16524v1 Announce Type: new  Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.",
        "arxiv_id": "2511.16524",
        "ARXIVID": "2511.16524",
        "COMMENT": "Matches criterion 3 (Embodied AI papers on new benchmarks): This paper introduces a new, well-annotated multi-modal benchmark for boxing action recognition and localization, specifically designed for real-time vision-based action recognition in unconstrained environments. The focus on combat sports and the diversity of the dataset is a novel contribution.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16435": {
        "authors": [
            "Jin Wang",
            "Bingfeng Zhang",
            "Jian Pang",
            "Mengyu Liu",
            "Honglong Chen",
            "Weifeng Liu"
        ],
        "title": "Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation",
        "abstract": "arXiv:2511.16435v1 Announce Type: new  Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.",
        "arxiv_id": "2511.16435",
        "ARXIVID": "2511.16435",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Uses LLMs to generate attribute descriptions for few-shot segmentation, introducing new modules for multi-modal alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.15967": {
        "authors": [
            "Muyao Yuan",
            "Yuanhong Zhang",
            "Weizhan Zhang",
            "Lan Ma",
            "Yuan Gao",
            "Jiangyong Ying",
            "Yudeng Xin"
        ],
        "title": "InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer",
        "abstract": "arXiv:2511.15967v1 Announce Type: new  Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.",
        "arxiv_id": "2511.15967",
        "ARXIVID": "2511.15967",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes an information-theoretic approach to transfer CLIP alignment for open-vocabulary semantic segmentation, with new mutual information objectives.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16024": {
        "authors": [
            "Xiao He",
            "Zhijun Tu",
            "Kun Cheng",
            "Mingrui Zhu",
            "Jie Hu",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution",
        "abstract": "arXiv:2511.16024v1 Announce Type: new  Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.",
        "arxiv_id": "2511.16024",
        "ARXIVID": "2511.16024",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications). Proposes a Mixture-of-Experts architecture for image super-resolution, leveraging CLIP embeddings for degradation estimation, which is a clever use of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.16186": {
        "authors": [
            "Deniz Sayin Mercadier",
            "Hieu Le",
            "Yihong Chen",
            "Jiancheng Yang",
            "Udaranga Wickramasinghe",
            "Pascal Fua"
        ],
        "title": "PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction",
        "abstract": "arXiv:2511.16186v1 Announce Type: new  Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.",
        "arxiv_id": "2511.16186",
        "ARXIVID": "2511.16186",
        "COMMENT": "Somewhat related to criteria 1 (spatial understanding) as it focuses on anatomically plausible 3D organ mesh reconstruction, but is more specialized to medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.16673": {
        "authors": [
            "Jing Wen",
            "Alexander G. Schwing",
            "Shenlong Wang"
        ],
        "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
        "abstract": "arXiv:2511.16673v1 Announce Type: new  Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
        "arxiv_id": "2511.16673",
        "ARXIVID": "2511.16673",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications). Proposes a method for animatable 3D avatars from sparse images, but does not focus on VLLMs/MLLMs or embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.16037": {
        "authors": [
            "Qing Wang",
            "Chong-Wah Ngo",
            "Ee-Peng Lim",
            "Qianru Sun"
        ],
        "title": "LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets",
        "abstract": "arXiv:2511.16037v1 Announce Type: new  Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.",
        "arxiv_id": "2511.16037",
        "ARXIVID": "2511.16037",
        "COMMENT": "This paper uses large language models (LLMs) to augment domain adaptation for food recognition, projecting generated text and images into a shared embedding space. It is relevant to criterion 2 (new VLLMs/MLLMs), as it leverages LLMs for multi-modal learning, though it does not introduce a new VLLM/MLLM architecture.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.14780": {
        "authors": [
            "Keith Moore",
            "Jun W. Kim",
            "David Lyu",
            "Jeffrey Heo",
            "Ehsan Adeli"
        ],
        "title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents",
        "abstract": "arXiv:2511.14780v1 Announce Type: new  Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.",
        "arxiv_id": "2511.14780",
        "ARXIVID": "2511.14780",
        "COMMENT": "This paper introduces a systems-level framework for probing belief formation in multi-agent LLMs, with a focus on role-primed agents and a medical case simulator. While it involves multi-agent LLMs and simulation, it does not focus on spatial intelligence, VLLMs, or embodied AI benchmarks. It is most closely related to multi-agent reasoning and interpretability, which is tangential to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.16162": {
        "authors": [
            "Yuting Lu",
            "Ziliang Wang",
            "Weixin Xu",
            "Wei Zhang",
            "Yongqiang Zhao",
            "Yang Yu",
            "Xiaohong Zhang"
        ],
        "title": "Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation",
        "abstract": "arXiv:2511.16162v1 Announce Type: new  Abstract: Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \\emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.",
        "arxiv_id": "2511.16162",
        "ARXIVID": "2511.16162",
        "COMMENT": "This paper introduces a layer-wise noise-guided selective wavelet reconstruction method for robust medical image segmentation. While it is a methodological improvement in segmentation robustness, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.16659": {
        "authors": [
            "Zhaoning Wang",
            "Xinyue Wei",
            "Ruoxi Shi",
            "Xiaoshuai Zhang",
            "Hao Su",
            "Minghua Liu"
        ],
        "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
        "abstract": "arXiv:2511.16659v1 Announce Type: new  Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
        "arxiv_id": "2511.16659",
        "ARXIVID": "2511.16659",
        "COMMENT": "This paper introduces a new part-based UV unwrapping pipeline for 3D meshes, with a focus on handling AI-generated, noisy meshes. While it is a methodological improvement in 3D geometry processing, it does not directly address spatial intelligence in embodied agents or vision-language models. It is most relevant to 3D vision and graphics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.16264": {
        "authors": [
            "Sinan Mutlu",
            "Georgios F. Angelis",
            "Savas Ozkan",
            "Paul Wisbey",
            "Anastasios Drosou",
            "Mete Ozay"
        ],
        "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs",
        "abstract": "arXiv:2511.16264v1 Announce Type: new  Abstract: Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.",
        "arxiv_id": "2511.16264",
        "ARXIVID": "2511.16264",
        "COMMENT": "Does not directly match any specific criterion. Focuses on real-time 3D human motion generation from sparse inputs, which is more about motion synthesis than spatial intelligence or VLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16298": {
        "authors": [
            "Md Musfiqur Rahman Sanim",
            "Zhihao Shu",
            "Bahram Afsharmanesh",
            "AmirAli Mirian",
            "Jiexiong Guan",
            "Wei Niu",
            "Bin Ren",
            "Gagan Agrawal"
        ],
        "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
        "abstract": "arXiv:2511.16298v1 Announce Type: new  Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
        "arxiv_id": "2511.16298",
        "ARXIVID": "2511.16298",
        "COMMENT": "Does not directly match any specific criterion. Focuses on optimizing 3D Gaussian Splatting for mobile GPUs, which is more about efficient implementation than new spatial intelligence or embodied AI methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16532": {
        "authors": [
            "Fan Yang",
            "Shigeyuki Odashima",
            "Shoichi Masui",
            "Ikuo Kusajima",
            "Sosuke Yamao",
            "Shan Jiang"
        ],
        "title": "Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration",
        "abstract": "arXiv:2511.16532v1 Announce Type: new  Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \\revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.",
        "arxiv_id": "2511.16532",
        "ARXIVID": "2511.16532",
        "COMMENT": "Does not directly match any specific criterion. Focuses on multi-camera tracking for gymnastics using domain knowledge, not on embodied AI benchmarks, VLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16418": {
        "authors": [
            "Hai Lan",
            "Zongyan Li",
            "Jianmin Hu",
            "Jialing Yang",
            "Houde Dai"
        ],
        "title": "End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss",
        "abstract": "arXiv:2511.16418v1 Announce Type: new  Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.",
        "arxiv_id": "2511.16418",
        "ARXIVID": "2511.16418",
        "COMMENT": "Does not directly match any specific criterion. Focuses on motion capture with geodesic loss and new marker type, which is more about graphics and biomechanics than spatial intelligence or VLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16555": {
        "authors": [
            "Junpeng Jing",
            "Weixun Luo",
            "Ye Mao",
            "Krystian Mikolajczyk"
        ],
        "title": "Lite Any Stereo: Efficient Zero-Shot Stereo Matching",
        "abstract": "arXiv:2511.16555v1 Announce Type: new  Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.",
        "arxiv_id": "2511.16555",
        "ARXIVID": "2511.16555",
        "COMMENT": "Does not match any specific criterion. Focuses on efficient stereo matching, which is not directly related to spatial intelligence on embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16574": {
        "authors": [
            "Nirjhor Datta",
            "Md. Golam Rabiul Alam"
        ],
        "title": "Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks",
        "abstract": "arXiv:2511.16574v1 Announce Type: new  Abstract: The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.   For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.   These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.",
        "arxiv_id": "2511.16574",
        "ARXIVID": "2511.16574",
        "COMMENT": "Does not match any specific criterion. Focuses on selective unlearning in medical segmentation networks using LoRA, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16137": {
        "authors": [
            "Li Yu",
            "Yingbo Zhao",
            "Shiyu Wu",
            "Siyue Yu",
            "Moncef Gabbouj",
            "Qingshan Liu"
        ],
        "title": "Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video",
        "abstract": "arXiv:2511.16137v1 Announce Type: new  Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.",
        "arxiv_id": "2511.16137",
        "ARXIVID": "2511.16137",
        "COMMENT": "This paper proposes a degradation-aware hierarchical termination mechanism for blind quality enhancement of compressed video. While it introduces a new method for video enhancement, it does not directly relate to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16184": {
        "authors": [
            "Nianchang Huang",
            "Yi Xu",
            "Ruida Xi",
            "Ruida Xi",
            "Qiang Zhang"
        ],
        "title": "Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification",
        "abstract": "arXiv:2511.16184v1 Announce Type: new  Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.",
        "arxiv_id": "2511.16184",
        "ARXIVID": "2511.16184",
        "COMMENT": "This paper proposes a new method for unsupervised domain adaptation in visible-infrared person re-identification. While it involves cross-modal learning and domain adaptation, it does not introduce a new VLLM/MLLM or focus on embodied agents or vision foundation models. It is more about domain adaptation in person re-ID.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16084": {
        "authors": [
            "Meihua Zhou",
            "Liping Yu",
            "Jiawei Cai",
            "Wai Kin Fung",
            "Ruiguo Hu",
            "Jiarui Zhao",
            "Wenzhuo Liu",
            "Nan Wan"
        ],
        "title": "SpectralTrain: A Universal Framework for Hyperspectral Image Classification",
        "abstract": "arXiv:2511.16084v1 Announce Type: new  Abstract: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.",
        "arxiv_id": "2511.16084",
        "ARXIVID": "2511.16084",
        "COMMENT": "Does not match any specific criterion. Focuses on hyperspectral image classification with curriculum learning and PCA, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}