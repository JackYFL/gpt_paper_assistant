{
    "2506.17202": {
        "authors": [
            "Teng Li",
            "Quanfeng Lu",
            "Lirui Zhao",
            "Hao Li",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jun Zhang",
            "Wenqi Shao"
        ],
        "title": "UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2506.17202v1 Announce Type: new  Abstract: Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.",
        "arxiv_id": "2506.17202",
        "ARXIVID": "2506.17202",
        "COMMENT": "Matches criterion 2 as it explores a novel architecture for unified multimodal understanding and generation, focusing on modality alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.16806": {
        "authors": [
            "Fan Yang",
            "Yousong Zhu",
            "Xin Li",
            "Yufei Zhan",
            "Hongyin Zhao",
            "Shurong Zheng",
            "Yaowei Wang",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation",
        "abstract": "arXiv:2506.16806v1 Announce Type: new  Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat \"what to see\" and \"how to edit\" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.",
        "arxiv_id": "2506.16806",
        "ARXIVID": "2506.16806",
        "COMMENT": "Matches criterion 2 as it introduces FOCUS, a unified vision-language model for interactive editing, combining segmentation-aware perception and controllable generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.15757": {
        "authors": [
            "Ruoyu Wang",
            "Tong Yu",
            "Junda Wu",
            "Yao Liu",
            "Julian McAuley",
            "Lina Yao"
        ],
        "title": "Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation",
        "abstract": "arXiv:2506.15757v1 Announce Type: new  Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.",
        "arxiv_id": "2506.15757",
        "ARXIVID": "2506.15757",
        "COMMENT": "Matches criterion 3 as it proposes a weakly-supervised method for VLN, addressing dynamic viewpoints and integrating pre-trained VLM knowledge.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17221": {
        "authors": [
            "Zhangyang Qi",
            "Zhixiong Zhang",
            "Yizhou Yu",
            "Jiaqi Wang",
            "Hengshuang Zhao"
        ],
        "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2506.17221v1 Announce Type: new  Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.",
        "arxiv_id": "2506.17221",
        "ARXIVID": "2506.17221",
        "COMMENT": "Matches criterion 3 as it introduces a new method for Vision-Language Navigation (VLN) using LVLMs and a novel dataset (VLN-Ego) in a 3D simulator.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16112": {
        "authors": [
            "Yuan Zhang",
            "Chun-Kai Fan",
            "Tao Huang",
            "Ming Lu",
            "Sicheng Yu",
            "Junwen Pan",
            "Kuan Cheng",
            "Qi She",
            "Shanghang Zhang"
        ],
        "title": "AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models",
        "abstract": "arXiv:2506.16112v1 Announce Type: new  Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \\textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we developed an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experimental results indicate that AutoV enhances the performance of various LVLMs across multiple popular image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\\textbf{1.7}\\%$ accuracy gain on LLaVA$^{\\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\\textbf{1.9}\\%$ on MMMU, highlighting its potential as an optimal visual prompting method for LVLMs.",
        "arxiv_id": "2506.16112",
        "ARXIVID": "2506.16112",
        "COMMENT": "Matches criterion 2. Proposes AutoV, a method for learning optimal visual prompts for LVLMs, enhancing their reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16402": {
        "authors": [
            "Xiaoya Lu",
            "Zeren Chen",
            "Xuhao Hu",
            "Yijin Zhou",
            "Weichen Zhang",
            "Dongrui Liu",
            "Lu Sheng",
            "Jing Shao"
        ],
        "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks",
        "abstract": "arXiv:2506.16402v1 Announce Type: new  Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.",
        "arxiv_id": "2506.16402",
        "ARXIVID": "2506.16402",
        "COMMENT": "Matches criterion 3. Proposes a new benchmark (IS-Bench) for evaluating interactive safety in embodied agents, focusing on a novel angle of safety-aware procedural evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16826": {
        "authors": [
            "Sattwik Sahu",
            "Agamdeep Singh",
            "Karthik Nambiar",
            "Srikanth Saripalli",
            "P. B. Sujit"
        ],
        "title": "AnyTraverse: An off-road traversability framework with VLM and human operator in the loop",
        "abstract": "arXiv:2506.16826v1 Announce Type: new  Abstract: Off-road traversability segmentation enables autonomous navigation with applications in search-and-rescue, military operations, wildlife exploration, and agriculture. Current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes, and are not adaptive to be used for different robot types. We present AnyTraverse, a framework combining natural language-based prompts with human-operator assistance to determine navigable regions for diverse robotic vehicles. The system segments scenes for a given set of prompts and calls the operator only when encountering previously unexplored scenery or unknown class not part of the prompt in its region-of-interest, thus reducing active supervision load while adapting to varying outdoor scenes. Our zero-shot learning approach eliminates the need for extensive data collection or retraining. Our experimental validation includes testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate real-world deployment on multiple robot platforms. The results show that AnyTraverse performs better than GA-NAV and Off-seg while offering a vehicle-agnostic approach to off-road traversability that balances automation with targeted human supervision.",
        "arxiv_id": "2506.16826",
        "ARXIVID": "2506.16826",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for off-road traversability with VLMs and human-in-the-loop.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17004": {
        "authors": [
            "Hanlin Wu",
            "Pengfei Lin",
            "Ehsan Javanmardi",
            "Naren Bao",
            "Bo Qian",
            "Hao Si",
            "Manabu Tsukada"
        ],
        "title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving",
        "abstract": "arXiv:2506.17004v1 Announce Type: new  Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in autonomous driving, providing a voxel-level representation of both geometric details and semantic categories. However, the perception capability of a single vehicle is inherently constrained by occlusion, restricted sensor range, and narrow viewpoints. To address these limitations, collaborative perception enables the exchange of complementary information, thereby enhancing the completeness and accuracy. In the absence of a dedicated dataset for collaborative 3D semantic occupancy prediction, we augment an existing collaborative perception dataset by replaying it in CARLA with a high-resolution semantic voxel sensor to provide dense and comprehensive occupancy annotations. In addition, we establish benchmarks with varying prediction ranges designed to systematically assess the impact of spatial extent on collaborative prediction. We further develop a baseline model that performs inter-agent feature fusion via spatial alignment and attention aggregation. Experimental results demonstrate that our baseline model consistently outperforms single-agent models, with increasing gains observed as the prediction range expands.",
        "arxiv_id": "2506.17004",
        "ARXIVID": "2506.17004",
        "COMMENT": "Matches criterion 3 as it introduces a synthetic benchmark for collaborative 3D semantic occupancy prediction in autonomous driving, focusing on a novel angle of collaborative perception.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16940": {
        "authors": [
            "Annika Thomas",
            "Robaire Galliath",
            "Aleksander Garbuz",
            "Luke Anger",
            "Cormac O'Neill",
            "Trevor Johst",
            "Dami Thomas",
            "George Lordos",
            "Jonathan P. How"
        ],
        "title": "LunarLoc: Segment-Based Global Localization on the Moon",
        "abstract": "arXiv:2506.16940v1 Announce Type: new  Abstract: Global localization is necessary for autonomous operations on the lunar surface where traditional Earth-based navigation infrastructure, such as GPS, is unavailable. As NASA advances toward sustained lunar presence under the Artemis program, autonomous operations will be an essential component of tasks such as robotic exploration and infrastructure deployment. Tasks such as excavation and transport of regolith require precise pose estimation, but proposed approaches such as visual-inertial odometry (VIO) accumulate odometry drift over long traverses. Precise pose estimation is particularly important for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on autonomous agents to operate over extended timescales and varied terrain. To help overcome odometry drift over long traverses, we propose LunarLoc, an approach to global localization that leverages instance segmentation for zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment detections are used to construct a graph-based representation of the terrain, which is then aligned with a reference map of the environment captured during a previous session using graph-theoretic data association. This method enables accurate and drift-free global localization in visually ambiguous settings. LunarLoc achieves sub-cm level accuracy in multi-session global localization experiments, significantly outperforming the state of the art in lunar global localization. To encourage the development of further methods for global localization on the Moon, we release our datasets publicly with a playback module: https://github.com/mit-acl/lunarloc-data.",
        "arxiv_id": "2506.16940",
        "ARXIVID": "2506.16940",
        "COMMENT": "Matches criterion 3 as it introduces a new method (LunarLoc) for global localization on the Moon, focusing on a novel angle of using instance segmentation and graph-based data association.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.16673": {
        "authors": [
            "Ruiming Chen",
            "Junming Yang",
            "Shiyu Xia",
            "Xu Yang",
            "Jing Wang",
            "Xin Geng"
        ],
        "title": "Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge",
        "abstract": "arXiv:2506.16673v1 Announce Type: new  Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread attention for its multimodal generalizable knowledge, which is significant for downstream tasks. However, the computational overhead of a large number of parameters and large-scale pre-training poses challenges of pre-training a different scale of CLIP. Learngene extracts the generalizable components termed as learngene from an ancestry model and initializes diverse descendant models with it. Previous Learngene paradigms fail to handle the generalizable knowledge in multimodal scenarios. In this paper, we put forward the idea of utilizing a multimodal block to extract the multimodal generalizable knowledge, which inspires us to propose MM-LG (Multimodal Learngene), a novel framework designed to extract and leverage generalizable components from CLIP. Specifically, we first establish multimodal and unimodal blocks to extract the multimodal and unimodal generalizable knowledge in a weighted-sum manner. Subsequently, we employ these components to numerically initialize descendant models of varying scales and modalities. Extensive experiments demonstrate MM-LG's effectiveness, which achieves performance gains over existing learngene approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and comparable or superior results to the pre-training and fine-tuning paradigm (e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG requires only around 25% of the parameter storage while reducing around 2.8 times pre-training costs for diverse model scales compared to the pre-training and fine-tuning paradigm, making it particularly suitable for efficient deployment across diverse downstream tasks.",
        "arxiv_id": "2506.16673",
        "ARXIVID": "2506.16673",
        "COMMENT": "Matches criterion 2 as it discusses a novel framework (MM-LG) for extracting multimodal generalizable knowledge from CLIP, which is a visual large language model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15741": {
        "authors": [
            "He Zhu",
            "Tianrui Qin",
            "King Zhu",
            "Heyuan Huang",
            "Yeyi Guan",
            "Jinxiang Xia",
            "Yi Yao",
            "Hanhao Li",
            "Ningning Wang",
            "Pai Liu",
            "Tianhao Peng",
            "Xin Gui",
            "Xiaowan Li",
            "Yuhui Liu",
            "Yuchen Eleanor Jiang",
            "Jun Wang",
            "Changwang Zhang",
            "Xiangru Tang",
            "Ge Zhang",
            "Jian Yang",
            "Minghao Liu",
            "Xitong Gao",
            "Wangchunshu Zhou",
            "Jiaheng Liu"
        ],
        "title": "OAgents: An Empirical Study of Building Effective Agents",
        "abstract": "arXiv:2506.15741v1 Announce Type: new  Abstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.",
        "arxiv_id": "2506.15741",
        "ARXIVID": "2506.15741",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and framework for evaluating agentic AI systems.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.16962": {
        "authors": [
            "Haoran Sun",
            "Yankai Jiang",
            "Wenjie Lou",
            "Yujie Zhang",
            "Wenjie Li",
            "Lilong Wang",
            "Mianxin Liu",
            "Lei Liu",
            "Xiaosong Wang"
        ],
        "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
        "abstract": "arXiv:2506.16962v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
        "arxiv_id": "2506.16962",
        "ARXIVID": "2506.16962",
        "COMMENT": "Matches criterion 2. Proposes a novel reasoning-path searching scheme for MLLMs in the medical domain, enhancing their reasoning capabilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.16931": {
        "authors": [
            "Jiaqi Chen",
            "Mingfeng Fan",
            "Xuefeng Zhang",
            "Jingsong Liang",
            "Yuhong Cao",
            "Guohua Wu",
            "Guillaume Adrien Sartoretti"
        ],
        "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
        "abstract": "arXiv:2506.16931v1 Announce Type: new  Abstract: Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
        "arxiv_id": "2506.16931",
        "ARXIVID": "2506.16931",
        "COMMENT": "Matches criterion 3 as it proposes a novel multimodal learning framework for robotic task planning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.17074": {
        "authors": [
            "Wang Zhao",
            "Yan-Pei Cao",
            "Jiale Xu",
            "Yuejiang Dong",
            "Ying Shan"
        ],
        "title": "Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion",
        "abstract": "arXiv:2506.17074v1 Announce Type: new  Abstract: We present Assembler, a scalable and generalizable framework for 3D part assembly that reconstructs complete objects from input part meshes and a reference image. Unlike prior approaches that mostly rely on deterministic part pose prediction and category-specific training, Assembler is designed to handle diverse, in-the-wild objects with varying part counts, geometries, and structures. It addresses the core challenges of scaling to general 3D part assembly through innovations in task formulation, representation, and data. First, Assembler casts part assembly as a generative problem and employs diffusion models to sample plausible configurations, effectively capturing ambiguities arising from symmetry, repeated parts, and multiple valid assemblies. Second, we introduce a novel shape-centric representation based on sparse anchor point clouds, enabling scalable generation in Euclidean space rather than SE(3) pose prediction. Third, we construct a large-scale dataset of over 320K diverse part-object assemblies using a synthesis and filtering pipeline built on existing 3D shape repositories. Assembler achieves state-of-the-art performance on PartNet and is the first to demonstrate high-quality assembly for complex, real-world objects. Based on Assembler, we further introduce an interesting part-aware 3D modeling system that generates high-resolution, editable objects from images, demonstrating potential for interactive and compositional design. Project page: https://assembler3d.github.io",
        "arxiv_id": "2506.17074",
        "ARXIVID": "2506.17074",
        "COMMENT": "Matches criterion 4 as it discusses a scalable framework for 3D part assembly and its applications in part-aware 3D modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.16371": {
        "authors": [
            "Yunhao Hou",
            "Bochao Zou",
            "Min Zhang",
            "Ran Chen",
            "Shangdong Yang",
            "Yanmei Zhang",
            "Junbao Zhuo",
            "Siheng Chen",
            "Jiansheng Chen",
            "Huimin Ma"
        ],
        "title": "AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios",
        "abstract": "arXiv:2506.16371v1 Announce Type: new  Abstract: By sharing information across multiple agents, collaborative perception helps autonomous vehicles mitigate occlusions and improve overall perception accuracy. While most previous work focus on vehicle-to-vehicle and vehicle-to-infrastructure collaboration, with limited attention to aerial perspectives provided by UAVs, which uniquely offer dynamic, top-down views to alleviate occlusions and monitor large-scale interactive environments. A major reason for this is the lack of high-quality datasets for aerial-ground collaborative scenarios. To bridge this gap, we present AGC-Drive, the first large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The data collection platform consists of two vehicles, each equipped with five cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception. Consisting of approximately 120K LiDAR frames and 440K images, the dataset covers 14 diverse real-world driving scenarios, including urban roundabouts, highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic interaction events, including vehicle cut-ins, cut-outs, and frequent lane changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and fully annotated 3D bounding boxes covering 13 object categories. We provide benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative perception and vehicle-to-UAV collaborative perception. Additionally, we release an open-source toolkit, including spatiotemporal alignment verification tools, multi-agent visualization systems, and collaborative annotation utilities. The dataset and code are available at https://github.com/PercepX/AGC-Drive.",
        "arxiv_id": "2506.16371",
        "ARXIVID": "2506.16371",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for aerial-ground collaborative perception in driving scenarios.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.15755": {
        "authors": [
            "Xiasi Wang",
            "Tianliang Yao",
            "Simin Chen",
            "Runqi Wang",
            "Lei YE",
            "Kuofeng Gao",
            "Yi Huang",
            "Yuan Yao"
        ],
        "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
        "abstract": "arXiv:2506.15755v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters -- an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community's awareness about the efficiency robustness of VLMs.",
        "arxiv_id": "2506.15755",
        "ARXIVID": "2506.15755",
        "COMMENT": "Matches criterion 2. Evaluates the efficiency robustness of VLMs, which is related to VLLMs and their performance in real-world applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.15854": {
        "authors": [
            "Abdolazim Rezaei",
            "Mehdi Sookhak",
            "Ahmad Patooghy"
        ],
        "title": "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation",
        "abstract": "arXiv:2506.15854v1 Announce Type: new  Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77\\% and Detail Density by around 50\\% compared to existing approaches.",
        "arxiv_id": "2506.15854",
        "ARXIVID": "2506.15854",
        "COMMENT": "Matches criterion 2 as it leverages vision-language models (VLMs) for privacy-preserving transformations.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.16157": {
        "authors": [
            "Xingbai Chen",
            "Tingchao Fu",
            "Renyang Liu",
            "Wei Zhou",
            "Chao Yi"
        ],
        "title": "MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models",
        "abstract": "arXiv:2506.16157v1 Announce Type: new  Abstract: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tasks. Despite its impressive performance, the robustness of RES models against adversarial examples remains largely unexplored. While prior adversarial attack methods have explored adversarial robustness on conventional segmentation models, they perform poorly when directly applied to RES, failing to expose vulnerabilities in its multimodal structure. Moreover, in practical open-world scenarios, users typically issue multiple, diverse referring expressions to interact with the same image, highlighting the need for adversarial examples that generalize across varied textual inputs. To address these multimodal challenges, we propose a novel adversarial attack strategy termed \\textbf{Multimodal Bidirectional Attack}, tailored for RES models. Our method introduces learnable proxy textual embedding perturbation and jointly performs visual-aligned optimization on the image modality and textual-adversarial optimization on the textual modality during attack generation. This dual optimization framework encourages adversarial images to actively adapt to more challenging text embedding during optimization, thereby enhancing their cross-text transferability, which refers to the ability of adversarial examples to remain effective under a variety of unseen or semantically diverse textual inputs. Extensive experiments conducted on multiple RES models and benchmark datasets demonstrate the superior effectiveness of our method compared to existing methods.",
        "arxiv_id": "2506.16157",
        "ARXIVID": "2506.16157",
        "COMMENT": "Matches criterion 2 as it explores adversarial robustness in multimodal models for referring expression segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.16784": {
        "authors": [
            "Xiaoyu Shi",
            "Rahul Kumar Jain",
            "Yinhao Li",
            "Ruibo Hou",
            "Jingliang Cheng",
            "Jie Bai",
            "Guohua Zhao",
            "Lanfen Lin",
            "Rui Xu",
            "Yen-wei Chen"
        ],
        "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration",
        "abstract": "arXiv:2506.16784v1 Announce Type: new  Abstract: Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data.   To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques.   Our dataset, implementation code, and pre-trained models are publicly available at https://github.com/Jupitern52/TextBraTS.",
        "arxiv_id": "2506.16784",
        "ARXIVID": "2506.16784",
        "COMMENT": "Matches criterion 2 as it explores multimodal integration with textual and visual data for brain tumor segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.16504": {
        "authors": [
            "Zeqiang Lai",
            "Yunfei Zhao",
            "Haolin Liu",
            "Zibo Zhao",
            "Qingxiang Lin",
            "Huiwen Shi",
            "Xianghui Yang",
            "Mingxin Yang",
            "Shuhui Yang",
            "Yifei Feng",
            "Sheng Zhang",
            "Xin Huang",
            "Di Luo",
            "Fan Yang",
            "Fang Yang",
            "Lifu Wang",
            "Sicong Liu",
            "Yixuan Tang",
            "Yulin Cai",
            "Zebin He",
            "Tian Liu",
            "Yuhong Liu",
            "Jie Jiang",
            "Linus",
            "Jingwei Huang",
            "Chunchao Guo"
        ],
        "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details",
        "abstract": "arXiv:2506.16504v1 Announce Type: new  Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
        "arxiv_id": "2506.16504",
        "ARXIVID": "2506.16504",
        "COMMENT": "Matches criterion 4 as it discusses advancements in 3D diffusion models and their applications in generating high-fidelity 3D assets.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.16499": {
        "authors": [
            "Zexi Liu",
            "Yuzhu Cai",
            "Xinyu Zhu",
            "Yujie Zheng",
            "Runkun Chen",
            "Ying Wen",
            "Yanfeng Wang",
            "Weinan E",
            "Siheng Chen"
        ],
        "title": "ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning",
        "abstract": "arXiv:2506.16499v1 Announce Type: new  Abstract: As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.",
        "arxiv_id": "2506.16499",
        "ARXIVID": "2506.16499",
        "COMMENT": "Does not match any specific criterion but is related to AI-for-AI and optimization, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2506.15733": {
        "authors": [
            "Mert Cemri",
            "Nived Rajaraman",
            "Rishabh Tiwari",
            "Xiaoxuan Liu",
            "Kurt Keutzer",
            "Ion Stoica",
            "Kannan Ramchandran",
            "Ahmad Beirami",
            "Ziteng Sun"
        ],
        "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
        "abstract": "arXiv:2506.15733v1 Announce Type: new  Abstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",
        "arxiv_id": "2506.15733",
        "ARXIVID": "2506.15733",
        "COMMENT": "Does not match any specific criteria but focuses on latency-aware test-time scaling for LLMs, which is tangentially related to computational efficiency in embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17201": {
        "authors": [
            "Jiaqi Li",
            "Junshu Tang",
            "Zhiyong Xu",
            "Longhuang Wu",
            "Yuan Zhou",
            "Shuai Shao",
            "Tianbao Yu",
            "Zhiguo Cao",
            "Qinglin Lu"
        ],
        "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition",
        "abstract": "arXiv:2506.17201v1 Announce Type: new  Abstract: Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.",
        "arxiv_id": "2506.17201",
        "ARXIVID": "2506.17201",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and video synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16679": {
        "authors": [
            "Manuel Brack",
            "Sudeep Katakol",
            "Felix Friedrich",
            "Patrick Schramowski",
            "Hareesh Ravi",
            "Kristian Kersting",
            "Ajinkya Kale"
        ],
        "title": "How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions",
        "abstract": "arXiv:2506.16679v1 Announce Type: new  Abstract: Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.",
        "arxiv_id": "2506.16679",
        "ARXIVID": "2506.16679",
        "COMMENT": "Does not match any specific criterion but is related to text-to-image models and synthetic training captions, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16016": {
        "authors": [
            "William Sharpless",
            "Dylan Hirsch",
            "Sander Tonkens",
            "Nikhil Shinde",
            "Sylvia Herbert"
        ],
        "title": "Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations",
        "abstract": "arXiv:2506.16016v1 Announce Type: new  Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the reward function or the model architecture, often degrade policy performance. Lagrangian methods offer a way to blend objectives with constraints, but often require intricate reward engineering and parameter tuning. In this work, we extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to propose two novel value functions for dual-objective satisfaction. Namely, we address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds of two distinct rewards. In contrast with temporal logic approaches, which typically involve representing an automaton, we derive explicit, tractable Bellman forms in this context by decomposing our problem into reach, avoid, and reach-avoid problems, as to leverage these aforementioned recent advances. From a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are complementary and fundamentally different from standard sum-of-rewards problems and temporal logic problems, providing a new perspective on constrained decision-making. We leverage our analysis to propose a variation of Proximal Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of tasks for safe-arrival and multi-target achievement, we demonstrate that DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches and out-competes a number of baselines in various metrics.",
        "arxiv_id": "2506.16016",
        "ARXIVID": "2506.16016",
        "COMMENT": "Does not match any specific criterion but is related to reinforcement learning and constrained decision-making, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16690": {
        "authors": [
            "Yun Xing",
            "Yue Cao",
            "Nhat Chung",
            "Jie Zhang",
            "Ivor Tsang",
            "Ming-Ming Cheng",
            "Yang Liu",
            "Lei Ma",
            "Qing Guo"
        ],
        "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches",
        "abstract": "arXiv:2506.16690v1 Announce Type: new  Abstract: Stereo Depth estimation is a critical task in autonomous driving and robotics, where inaccuracies (such as misidentifying nearby objects as distant) can lead to dangerous situations. Adversarial attacks against stereo depth estimation can help reveal vulnerabilities before deployment. Previous work has shown that repeating optimized textures can effectively mislead stereo depth estimation in digital settings. However, our research reveals that these naively repeated texture structures perform poorly in physical-world implementations, i.e., when deployed as patches, limiting their practical utility for testing stereo depth estimation systems. In this work, for the first time, we discover that introducing regular intervals between repeated textures, creating a striped structure, significantly enhances the patch attack effectiveness. Through extensive experimentation, we analyze how variations of this novel structure influence the performance. Based on these insights, we develop a novel stereo depth attack that jointly optimizes both the striped structure and texture elements. Our generated adversarial patches can be inserted into any scenes and successfully attack state-of-the-art stereo depth estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can also attack commercial RGB-D cameras (Intel RealSense) in real-world conditions, demonstrating their practical relevance for security assessment of stereo systems.",
        "arxiv_id": "2506.16690",
        "ARXIVID": "2506.16690",
        "COMMENT": "Does not match any specific criterion but is related to stereo depth estimation and adversarial attacks, which are tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16160": {
        "authors": [
            "Jiyao Wang",
            "Xiao Yang",
            "Hao Lu",
            "Dengbo He",
            "Kaishun Wu"
        ],
        "title": "Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization",
        "abstract": "arXiv:2506.16160v1 Announce Type: new  Abstract: Multi-source synsemantic domain generalization (MSSDG) for multi-task remote physiological measurement seeks to enhance the generalizability of these metrics and attracts increasing attention. However, challenges like partial labeling and environmental noise may disrupt task-specific accuracy. Meanwhile, given that real-time adaptation is necessary for personalized products, the test-time personalized adaptation (TTPA) after MSSDG is also worth exploring, while the gap between previous generalization and personalization methods is significant and hard to fuse. Thus, we proposed a unified framework for MSSD\\textbf{G} and TTP\\textbf{A} employing \\textbf{P}riors (\\textbf{GAP}) in biometrics and remote photoplethysmography (rPPG). We first disentangled information from face videos into invariant semantics, individual bias, and noise. Then, multiple modules incorporating priors and our observations were applied in different stages and for different facial information. Then, based on the different principles of achieving generalization and personalization, our framework could simultaneously address MSSDG and TTPA under multi-task remote physiological estimation with minimal adjustments. We expanded the MSSDG benchmark to the TTPA protocol on six publicly available datasets and introduced a new real-world driving dataset with complete labeling. Extensive experiments that validated our approach, and the codes along with the new dataset will be released.",
        "arxiv_id": "2506.16160",
        "ARXIVID": "2506.16160",
        "COMMENT": "Does not match any specific criteria but focuses on domain generalization and personalization for remote physiological measurement, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16994": {
        "authors": [
            "Yasir Ali Farrukh",
            "Syed Wali",
            "Irfan Khan",
            "Nathaniel D. Bastian"
        ],
        "title": "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments",
        "abstract": "arXiv:2506.16994v1 Announce Type: new  Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world vision systems, especially in resource-constrained environments like drones, where memory and computation are limited. Existing prompt-driven UDA methods typically rely on large vision-language models and require full access to source-domain data during adaptation, limiting their applicability. In this work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain adaptation framework built around a teacher-student paradigm guided by prompt-based feature alignment. At the core of our method is a distilled and fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A small set of low-level source features is aligned to the target domain semantics-specified only through a natural language prompt-via Prompt-driven Instance Normalization (PIN). These semantically steered features are used to briefly fine-tune the detection head of the teacher model. The adapted teacher then generates high-quality pseudo-labels, which guide the on-the-fly adaptation of a compact student model. Experiments on the MDS-A dataset demonstrate that Prmpt2Adpt achieves competitive detection performance compared to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x faster inference speed using few source images-making it a practical and scalable solution for real-time adaptation in low-resource domains.",
        "arxiv_id": "2506.16994",
        "ARXIVID": "2506.16994",
        "COMMENT": "Does not match any specific criteria but focuses on domain adaptation in vision systems, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16819": {
        "authors": [
            "Yuchu Jiang",
            "Jiaming Chu",
            "Jian Zhao",
            "Xin Zhang",
            "Xu Yang",
            "Lei Jin",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection",
        "abstract": "arXiv:2506.16819v1 Announce Type: new  Abstract: The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at https://github.com/Kamichanw/Loupe.",
        "arxiv_id": "2506.16819",
        "ARXIVID": "2506.16819",
        "COMMENT": "Does not match any specific criteria but focuses on image forgery detection, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15787": {
        "authors": [
            "Lukas Helff",
            "Ahmad Omar",
            "Felix Friedrich",
            "Wolfgang Stammer",
            "Antonia W\\\"ust",
            "Tim Woydt",
            "Rupert Mitchell",
            "Patrick Schramowski",
            "Kristian Kersting"
        ],
        "title": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning",
        "abstract": "arXiv:2506.15787v1 Announce Type: new  Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.",
        "arxiv_id": "2506.15787",
        "ARXIVID": "2506.15787",
        "COMMENT": "Does not match any specific criteria but focuses on logical reasoning in LLMs, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16596": {
        "authors": [
            "Vinay K Chaudhri",
            "Chaitan Baru",
            "Brandon Bennett",
            "Mehul Bhatt",
            "Darion Cassel",
            "Anthony G Cohn",
            "Rina Dechter",
            "Esra Erdem",
            "Dave Ferrucci",
            "Ken Forbus",
            "Gregory Gelfond",
            "Michael Genesereth",
            "Andrew S. Gordon",
            "Benjamin Grosof",
            "Gopal Gupta",
            "Jim Hendler",
            "Sharat Israni",
            "Tyler R. Josephson",
            "Patrick Kyllonen",
            "Yuliya Lierler",
            "Vladimir Lifschitz",
            "Clifton McFate",
            "Hande K. McGinty",
            "Leora Morgenstern",
            "Alessandro Oltramari",
            "Praveen Paritosh",
            "Dan Roth",
            "Blake Shepard",
            "Cogan Shimzu",
            "Denny Vrande\\v{c}i\\'c",
            "Mark Whiting",
            "Michael Witbrock"
        ],
        "title": "A Community-driven vision for a new Knowledge Resource for AI",
        "abstract": "arXiv:2506.16596v1 Announce Type: new  Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.",
        "arxiv_id": "2506.16596",
        "ARXIVID": "2506.16596",
        "COMMENT": "Does not match any specific criteria. Focuses on a community-driven vision for a new knowledge resource for AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16273": {
        "authors": [
            "Xin Jiang",
            "Meiqi Cao",
            "Hao Tang",
            "Fei Shen",
            "Zechao Li"
        ],
        "title": "Fine-grained Image Retrieval via Dual-Vision Adaptation",
        "abstract": "arXiv:2506.16273v1 Announce Type: new  Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning discriminative visual representations to retrieve images with similar fine-grained features. Current leading FGIR solutions typically follow two regimes: enforce pairwise similarity constraints in the semantic embedding space, or incorporate a localization sub-network to fine-tune the entire model. However, such two regimes tend to overfit the training data while forgetting the knowledge gained from large-scale pre-training, thus reducing their generalization ability. In this paper, we propose a Dual-Vision Adaptation (DVA) approach for FGIR, which guides the frozen pre-trained model to perform FGIR through collaborative sample and feature adaptation. Specifically, we design Object-Perceptual Adaptation, which modifies input samples to help the pre-trained model perceive critical objects and elements within objects that are helpful for category prediction. Meanwhile, we propose In-Context Adaptation, which introduces a small set of parameters for feature adaptation without modifying the pre-trained parameters. This makes the FGIR task using these adjusted features closer to the task solved during the pre-training. Additionally, to balance retrieval efficiency and performance, we propose Discrimination Perception Transfer to transfer the discriminative knowledge in the object-perceptual adaptation to the image encoder using the knowledge distillation mechanism. Extensive experiments show that DVA has fewer learnable parameters and performs well on three in-distribution and three out-of-distribution fine-grained datasets.",
        "arxiv_id": "2506.16273",
        "ARXIVID": "2506.16273",
        "COMMENT": "Does not match any specific criteria. Focuses on fine-grained image retrieval with dual-vision adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16209": {
        "authors": [
            "Annajoyce Mariani",
            "Kira Maag",
            "Hanno Gottschalk"
        ],
        "title": "VideoGAN-based Trajectory Proposal for Automated Vehicles",
        "abstract": "arXiv:2506.16209v1 Announce Type: new  Abstract: Being able to generate realistic trajectory options is at the core of increasing the degree of automation of road vehicles. While model-driven, rule-based, and classical learning-based methods are widely used to tackle these tasks at present, they can struggle to effectively capture the complex, multimodal distributions of future trajectories. In this paper we investigate whether a generative adversarial network (GAN) trained on videos of bird's-eye view (BEV) traffic scenarios can generate statistically accurate trajectories that correctly capture spatial relationships between the agents. To this end, we propose a pipeline that uses low-resolution BEV occupancy grid videos as training data for a video generative model. From the generated videos of traffic scenarios we extract abstract trajectory data using single-frame object detection and frame-to-frame object matching. We particularly choose a GAN architecture for the fast training and inference times with respect to diffusion models. We obtain our best results within 100 GPU hours of training, with inference times under 20\\,ms. We demonstrate the physical realism of the proposed trajectories in terms of distribution alignment of spatial and dynamic parameters with respect to the ground truth videos from the Waymo Open Motion Dataset.",
        "arxiv_id": "2506.16209",
        "ARXIVID": "2506.16209",
        "COMMENT": "Does not match any specific criteria. Focuses on trajectory generation for automated vehicles using GANs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17137": {
        "authors": [
            "Zhuonan Liang",
            "Dongnan Liu",
            "Jianan Fan",
            "Yaxuan Song",
            "Qiang Qu",
            "Yu Yao",
            "Peng Fu",
            "Weidong Cai"
        ],
        "title": "On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting",
        "abstract": "arXiv:2506.17137v1 Announce Type: new  Abstract: Object counting models suffer when deployed across domains with differing density variety, since density shifts are inherently task-relevant and violate standard domain adaptation assumptions. To address this, we propose a theoretical framework of conditional feature alignment. We first formalize the notion of conditional divergence by partitioning each domain into subsets (e.g., object vs. background) and measuring divergences per condition. We then derive a joint error bound showing that, under discrete label spaces treated as condition sets, aligning distributions conditionally leads to tighter bounds on the combined source-target decision error than unconditional alignment. These insights motivate a general conditional adaptation principle: by preserving task-relevant variations while filtering out nuisance shifts, one can achieve superior cross-domain generalization for counting. We provide both defining conditional divergence then proving its benefit in lowering joint error and a practical adaptation strategy that preserves task-relevant information in unsupervised domain-adaptive counting. We demonstrate the effectiveness of our approach through extensive experiments on multiple counting datasets with varying density distributions. The results show that our method outperforms existing unsupervised domain adaptation methods, empirically validating the theoretical insights on conditional feature alignment.",
        "arxiv_id": "2506.17137",
        "ARXIVID": "2506.17137",
        "COMMENT": "Does not match any specific criteria. Focuses on conditional feature alignment for domain-adaptive counting, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16163": {
        "authors": [
            "Hao Li",
            "Gengrui Zhang",
            "Petter Holme",
            "Shuyue Hu",
            "Zhen Wang"
        ],
        "title": "Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior",
        "abstract": "arXiv:2506.16163v1 Announce Type: new  Abstract: Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.",
        "arxiv_id": "2506.16163",
        "ARXIVID": "2506.16163",
        "COMMENT": "Does not match any specific criteria. Focuses on decision-making behavior of LLMs compared to humans, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16745": {
        "authors": [
            "Qi-Ying Sun",
            "Wan-Lei Zhao",
            "Yi-Bo Miao",
            "Chong-Wah Ngo"
        ],
        "title": "Class Agnostic Instance-level Descriptor for Visual Instance Search",
        "abstract": "arXiv:2506.16745v1 Announce Type: new  Abstract: Despite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance level feature representation. Supervised or weakly supervised object detection methods are not among the options due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of feature subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the various instance regions in an image of different semantic scales. The hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in the real scenarios. The features derived from the nodes on the hierarchy make up a comprehensive representation for the latent instances in the image. Our instance-level descriptor remains effective on both the known and unknown object categories. Empirical studies on three instance search benchmarks show that it outperforms state-of-the-art methods considerably.",
        "arxiv_id": "2506.16745",
        "ARXIVID": "2506.16745",
        "COMMENT": "Does not match any specific criteria. Focuses on instance-level descriptors for visual instance search, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16369": {
        "authors": [
            "Pallabi Dutta",
            "Anubhab Maity",
            "Sushmita Mitra"
        ],
        "title": "Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation",
        "abstract": "arXiv:2506.16369v1 Announce Type: new  Abstract: The high computational demands of Vision Transformers (ViTs), in processing a huge number of tokens, often constrain their practical application in analyzing medical images. This research proposes an adaptive prompt-guided pruning method to selectively reduce the processing of irrelevant tokens in the segmentation pipeline. The prompt-based spatial prior helps to rank the tokens according to their relevance. Tokens with low-relevance scores are down-weighted, ensuring that only the relevant ones are propagated for processing across subsequent stages. This data-driven pruning strategy facilitates end-to-end training, maintains gradient flow, and improves segmentation accuracy by focusing computational resources on essential regions. The proposed framework is integrated with several state-of-the-art models to facilitate the elimination of irrelevant tokens; thereby, enhancing computational efficiency while preserving segmentation accuracy. The experimental results show a reduction of $\\sim$ 35-55\\% tokens; thus reducing the computational costs relative to the baselines. Cost-effective medical image processing, using our framework, facilitates real-time diagnosis by expanding its applicability in resource-constrained environments.",
        "arxiv_id": "2506.16369",
        "ARXIVID": "2506.16369",
        "COMMENT": "Does not match any specific criteria. Focuses on token pruning for efficient segmentation in medical imaging, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16852": {
        "authors": [
            "Chaonan Ji",
            "Jinwei Qi",
            "Peng Zhang",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "title": "Controllable and Expressive One-Shot Video Head Swapping",
        "abstract": "arXiv:2506.16852v1 Announce Type: new  Abstract: In this paper, we propose a novel diffusion-based multi-condition controllable framework for video head swapping, which seamlessly transplant a human head from a static image into a dynamic video, while preserving the original body and background of target video, and further allowing to tweak head expressions and movements during swapping as needed. Existing face-swapping methods mainly focus on localized facial replacement neglecting holistic head morphology, while head-swapping approaches struggling with hairstyle diversity and complex backgrounds, and none of these methods allow users to modify the transplanted head expressions after swapping. To tackle these challenges, our method incorporates several innovative strategies through a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We propose a shape-agnostic mask strategy to explicitly disentangle foreground head identity features from background/body contexts, combining hair enhancement strategy to achieve robust holistic head identity preservation across diverse hair types and complex backgrounds. 2) Expression-aware landmark retargeting and editing: We propose a disentangled 3DMM-driven retargeting module that decouples identity, expression, and head poses, minimizing the impact of original expressions in input images and supporting expression editing. While a scale-aware retargeting strategy is further employed to minimize cross-identity expression distortion for higher transfer precision. Experimental results demonstrate that our method excels in seamless background integration while preserving the identity of the source portrait, as well as showcasing superior expression transfer capabilities applicable to both real and virtual characters.",
        "arxiv_id": "2506.16852",
        "ARXIVID": "2506.16852",
        "COMMENT": "Does not match any specific criteria. Focuses on video head swapping, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15980": {
        "authors": [
            "Cong Wang",
            "Zexuan Deng",
            "Zhiwei Jiang",
            "Fei Shen",
            "Yafeng Yin",
            "Shiwei Gan",
            "Zifeng Cheng",
            "Shiping Ge",
            "Qing Gu"
        ],
        "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization",
        "abstract": "arXiv:2506.15980v1 Announce Type: new  Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.",
        "arxiv_id": "2506.15980",
        "ARXIVID": "2506.15980",
        "COMMENT": "Does not match any specific criteria. Focuses on sign language video generation, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16743": {
        "authors": [
            "Weinan Guan",
            "Wei Wang",
            "Bo Peng",
            "Ziwen He",
            "Jing Dong",
            "Haonan Cheng"
        ],
        "title": "Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention",
        "abstract": "arXiv:2506.16743v1 Announce Type: new  Abstract: With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective countermeasure.However, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art performance.Our code is available at https://github.com/WeinanGuan/NASA-Swin.",
        "arxiv_id": "2506.16743",
        "ARXIVID": "2506.16743",
        "COMMENT": "Does not match any specific criterion but is relevant to diffusion models and detection techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16407": {
        "authors": [
            "Dong Nguyen Tien",
            "Dung D. Le"
        ],
        "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks",
        "abstract": "arXiv:2506.16407v1 Announce Type: new  Abstract: Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility.   Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.",
        "arxiv_id": "2506.16407",
        "ARXIVID": "2506.16407",
        "COMMENT": "Does not match any specific criterion but is relevant to robustness in vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15976": {
        "authors": [
            "Jingwei Zhang",
            "Xi Han",
            "Hong Qin",
            "Mahdi S. Hosseini",
            "Dimitris Samaras"
        ],
        "title": "LBMamba: Locally Bi-directional Mamba",
        "abstract": "arXiv:2506.15976v1 Announce Type: new  Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting recurrence as a parallel selective scan, has recently emerged as a linearly-scaling, efficient alternative to self-attention. Because of its unidirectional nature, each state in Mamba only has information of its previous states and is blind to states after. Current Mamba-based computer-vision methods typically overcome this limitation by augmenting Mamba's global forward scan with a global backward scan, forming a bi-directional scan that restores a full receptive field. However, this operation doubles the computational load, eroding much of the efficiency advantage that originally Mamba have. To eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM block that embeds a lightweight locally backward scan inside the forward selective scan and executes it entirely in per-thread registers. Building on LBMamba, we present LBVim, a scalable vision backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. We validate the versatility of our approach on both natural images and whole slide images (WSIs). We show that our LBVim constantly offers a superior performance-throughput trade-off. That is under the same throughput, LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection dataset. We also integrate LBMamba into the SOTA pathology multiple instance learning (MIL) approach, MambaMIL, which uses single directional scan. Experiments on 3 public WSI classification datasets for show that our method achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1, 1.67% better accuracy.",
        "arxiv_id": "2506.15976",
        "ARXIVID": "2506.15976",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and efficient model design.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16265": {
        "authors": [
            "Zhaoyi Wang",
            "Jemil Avers Butt",
            "Shengyu Huang",
            "Tomislav Medic",
            "Andreas Wieser"
        ],
        "title": "Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images",
        "abstract": "arXiv:2506.16265v1 Announce Type: new  Abstract: Landslide monitoring is essential for understanding geohazards and mitigating associated risks. However, existing point cloud-based methods typically rely on either geometric or radiometric information and often yield sparse or non-3D displacement estimates. In this paper, we propose a hierarchical partition-based coarse-to-fine approach that fuses 3D point clouds and co-registered RGB images to estimate dense 3D displacement vector fields. We construct patch-level matches using both 3D geometry and 2D image features. These matches are refined via geometric consistency checks, followed by rigid transformation estimation per match. Experimental results on two real-world landslide datasets demonstrate that our method produces 3D displacement estimates with high spatial coverage (79% and 97%) and high accuracy. Deviations in displacement magnitude with respect to external measurements (total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets, respectively, and only 0.07 m and 0.20 m compared to manually derived references. These values are below the average scan resolutions (0.08 m and 0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial coverage while maintaining comparable accuracy. Our approach offers a practical and adaptable solution for TLS-based landslide monitoring and is extensible to other types of point clouds and monitoring tasks. Our example data and source code are publicly available at https://github.com/zhaoyiww/fusion4landslide.",
        "arxiv_id": "2506.16265",
        "ARXIVID": "2506.16265",
        "COMMENT": "Does not match any specific criterion but is related to 3D displacement estimation and landslide monitoring, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16842": {
        "authors": [
            "Chaehyeon Song",
            "Dongjae Lee",
            "Jongwoo Lim",
            "Ayoung Kim"
        ],
        "title": "Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model",
        "abstract": "arXiv:2506.16842v1 Announce Type: new  Abstract: Camera calibration using planar targets has been widely favored, and two types of control points have been mainly considered as measurements: the corners of the checkerboard and the centroid of circles. Since a centroid is derived from numerous pixels, the circular pattern provides more precise measurements than the checkerboard. However, the existing projection model of circle centroids is biased under lens distortion, resulting in low performance. To surmount this limitation, we propose an unbiased projection model of the circular pattern and demonstrate its superior accuracy compared to the checkerboard. Complementing this, we introduce uncertainty into circular patterns to enhance calibration robustness and completeness. Defining centroid uncertainty improves the performance of calibration components, including pattern detection, optimization, and evaluation metrics. We also provide guidelines for performing good camera calibration based on the evaluation metric. The core concept of this approach is to model the boundary points of a two-dimensional shape as a Markov random field, considering its connectivity. The shape distribution is propagated to the centroid uncertainty through an appropriate shape representation based on the Green theorem. Consequently, the resulting framework achieves marked gains in calibration accuracy and robustness. The complete source code and demonstration video are available at https://github.com/chaehyeonsong/discocal.",
        "arxiv_id": "2506.16842",
        "ARXIVID": "2506.16842",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and camera calibration, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16531": {
        "authors": [
            "Mei Qi Tang",
            "Sean Sedwards",
            "Chengjie Huang",
            "Krzysztof Czarnecki"
        ],
        "title": "How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+",
        "abstract": "arXiv:2506.16531v1 Announce Type: new  Abstract: The impact of snowfall on 3D object detection performance remains underexplored. Conducting such an evaluation requires a dataset with sufficient labelled data from both weather conditions, ideally captured in the same driving environment. Current driving datasets with LiDAR point clouds either do not provide enough labelled data in both snowy and clear weather conditions, or rely on de-snowing methods to generate synthetic clear weather. Synthetic data often lacks realism and introduces an additional domain shift that confounds accurate evaluations. To address these challenges, we present CADC+, the first paired weather domain adaptation dataset for autonomous driving in winter conditions. CADC+ extends the Canadian Adverse Driving Conditions dataset (CADC) using clear weather data that was recorded on the same roads and in the same period as CADC. To create CADC+, we pair each CADC sequence with a clear weather sequence that matches the snowy sequence as closely as possible. CADC+ thus minimizes the domain shift resulting from factors unrelated to the presence of snow. We also present some preliminary results using CADC+ to evaluate the effect of snow on 3D object detection performance. We observe that snow introduces a combination of aleatoric and epistemic uncertainties, acting as both noise and a distinct data domain.",
        "arxiv_id": "2506.16531",
        "ARXIVID": "2506.16531",
        "COMMENT": "Does not match any specific criteria. Focuses on domain adaptation for snowy and clear weather in autonomous driving.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.17027": {
        "authors": [
            "Yiyang Tie",
            "Hong Zhu",
            "Yunyun Luo",
            "Jing Shi"
        ],
        "title": "Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns",
        "abstract": "arXiv:2506.17027v1 Announce Type: new  Abstract: The training of real-world super-resolution reconstruction models heavily relies on datasets that reflect real-world degradation patterns. Extracting and modeling degradation patterns for super-resolution reconstruction using only real-world low-resolution (LR) images remains a challenging task. When synthesizing datasets to simulate real-world degradation, relying solely on degradation extraction methods fails to capture both blur and diverse noise characteristics across varying LR distributions, as well as more implicit degradations such as color gamut shifts. Conversely, domain translation alone cannot accurately approximate real-world blur characteristics due to the significant degradation domain gap between synthetic and real data. To address these challenges, we propose a novel TripleGAN framework comprising two strategically designed components: The FirstGAN primarily focuses on narrowing the domain gap in blur characteristics, while the SecondGAN performs domain-specific translation to approximate target-domain blur properties and learn additional degradation patterns. The ThirdGAN is trained on pseudo-real data generated by the FirstGAN and SecondGAN to reconstruct real-world LR images. Extensive experiments on the RealSR and DRealSR datasets demonstrate that our method exhibits clear advantages in quantitative metrics while maintaining sharp reconstructions without over-smoothing artifacts. The proposed framework effectively learns real-world degradation patterns from LR observations and synthesizes aligned datasets with corresponding degradation characteristics, thereby enabling the trained network to achieve superior performance in reconstructing high-quality SR images from real-world LR inputs.",
        "arxiv_id": "2506.17027",
        "ARXIVID": "2506.17027",
        "COMMENT": "Does not match any specific criteria. Focuses on super-resolution reconstruction and degradation patterns.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.15988": {
        "authors": [
            "Connor Malone",
            "Owen Claxton",
            "Iman Shames",
            "Michael Milford"
        ],
        "title": "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation",
        "abstract": "arXiv:2506.15988v1 Announce Type: new  Abstract: Stand-alone Visual Place Recognition (VPR) systems have little defence against a well-designed adversarial attack, which can lead to disastrous consequences when deployed for robot navigation. This paper extensively analyzes the effect of four adversarial attacks common in other perception tasks and four novel VPR-specific attacks on VPR localization performance. We then propose how to close the loop between VPR, an Adversarial Attack Detector (AAD), and active navigation decisions by demonstrating the performance benefit of simulated AADs in a novel experiment paradigm -- which we detail for the robotics community to use as a system framework. In the proposed experiment paradigm, we see the addition of AADs across a range of detection accuracies can improve performance over baseline; demonstrating a significant improvement -- such as a ~50% reduction in the mean along-track localization error -- can be achieved with True Positive and False Positive detection rates of only 75% and up to 25% respectively. We examine a variety of metrics including: Along-Track Error, Percentage of Time Attacked, Percentage of Time in an `Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on these results, we provide the first investigation into the efficacy of the Fast Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this work highlights the need for AADs in real-world systems for trustworthy navigation, and informs quantitative requirements for system design.",
        "arxiv_id": "2506.15988",
        "ARXIVID": "2506.15988",
        "COMMENT": "Does not match any specific criterion but is relevant to adversarial robustness in visual place recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.16773": {
        "authors": [
            "Shuchen Sun",
            "Ligen Shi",
            "Chang Liu",
            "Lina Wu",
            "Jun Qiu"
        ],
        "title": "Infrared and Visible Image Fusion Based on Implicit Neural Representations",
        "abstract": "arXiv:2506.16773v1 Announce Type: new  Abstract: Infrared and visible light image fusion aims to combine the strengths of both modalities to generate images that are rich in information and fulfill visual or computational requirements. This paper proposes an image fusion method based on Implicit Neural Representations (INR), referred to as INRFuse. This method parameterizes a continuous function through a neural network to implicitly represent the multimodal information of the image, breaking through the traditional reliance on discrete pixels or explicit features. The normalized spatial coordinates of the infrared and visible light images serve as inputs, and multi-layer perceptrons is utilized to adaptively fuse the features of both modalities, resulting in the output of the fused image. By designing multiple loss functions, the method jointly optimizes the similarity between the fused image and the original images, effectively preserving the thermal radiation information of the infrared image while maintaining the texture details of the visible light image. Furthermore, the resolution-independent characteristic of INR allows for the direct fusion of images with varying resolutions and achieves super-resolution reconstruction through high-density coordinate queries. Experimental results indicate that INRFuse outperforms existing methods in both subjective visual quality and objective evaluation metrics, producing fused images with clear structures, natural details, and rich information without the necessity for a training dataset.",
        "arxiv_id": "2506.16773",
        "ARXIVID": "2506.16773",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and image fusion techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}