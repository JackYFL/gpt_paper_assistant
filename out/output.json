{
    "2508.03691": {
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Xin Li",
            "Ao Liang",
            "Runnan Chen",
            "Ben Fei",
            "Tongliang Liu"
        ],
        "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data",
        "abstract": "arXiv:2508.03691v1 Announce Type: new  Abstract: Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.",
        "arxiv_id": "2508.03691",
        "ARXIVID": "2508.03691",
        "COMMENT": "Strongly matches criterion 3 (new benchmark and method for controllable 3D scene generation in embodied AI/simulation) and criterion 4 (vision foundation models and their application to LiDAR scene generation). Introduces new datasets (Waymo-SG, nuScenes-SG) and evaluation metrics, and proposes a novel layout-guided generative framework for LiDAR data.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.03177": {
        "authors": [
            "Zhaoxu Li",
            "Chenqi Kong",
            "Yi Yu",
            "Qiangqiang Wu",
            "Xinghao Jiang",
            "Ngai-Man Cheung",
            "Bihan Wen",
            "Alex Kot",
            "Xudong Jiang"
        ],
        "title": "SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision",
        "abstract": "arXiv:2508.03177v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) recently achieve significant breakthroughs in understanding complex visual-textual contexts. However, hallucination issues still limit their real-world applicability. Although previous mitigation methods effectively reduce hallucinations in photographic images, they largely overlook the potential risks posed by stylized images, which play crucial roles in critical scenarios such as game scene understanding, art education, and medical analysis. In this work, we first construct a dataset comprising photographic images and their corresponding stylized versions with carefully annotated caption labels. We then conduct head-to-head comparisons on both discriminative and generative tasks by benchmarking 13 advanced LVLMs on the collected datasets. Our findings reveal that stylized images tend to induce significantly more hallucinations than their photographic counterparts. To address this issue, we propose Style-Aware Visual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs' final outputs based on the token-level visual attention patterns, leveraging early-layer feedback to mitigate hallucinations caused by stylized images. Extensive experiments demonstrate that SAVER achieves state-of-the-art performance in hallucination mitigation across various models, datasets, and tasks.",
        "arxiv_id": "2508.03177",
        "ARXIVID": "2508.03177",
        "COMMENT": "This paper directly addresses criterion 2 by benchmarking 13 large vision-language models (LVLMs) and proposing a new hallucination mitigation method (SAVER) for stylized images. It also provides surprising empirical results about hallucination rates in stylized vs. photographic images, which matches your friend's interest in empirical insights in VLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.03320": {
        "authors": [
            "Peiyu Wang",
            "Yi Peng",
            "Yimeng Gan",
            "Liang Hu",
            "Tianyidan Xie",
            "Xiaokun Wang",
            "Yichen Wei",
            "Chuanxin Tang",
            "Bo Zhu",
            "Changshi Li",
            "Hongyang Wei",
            "Eric Li",
            "Xuchen Song",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation",
        "abstract": "arXiv:2508.03320v1 Announce Type: new  Abstract: We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
        "arxiv_id": "2508.03320",
        "ARXIVID": "2508.03320",
        "COMMENT": "This paper introduces Skywork UniPic, a unified autoregressive model for image understanding, text-to-image generation, and image editing, with a compact multimodal architecture. This is a direct match to criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.03643": {
        "authors": [
            "Xiangyu Sun",
            "Haoyi jiang",
            "Liu Liu",
            "Seungtae Nam",
            "Gyeongjin Kang",
            "Xinjie wang",
            "Wei Sui",
            "Zhizhong Su",
            "Wenyu Liu",
            "Xinggang Wang",
            "Eunbyung Park"
        ],
        "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images",
        "abstract": "arXiv:2508.03643v1 Announce Type: new  Abstract: Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding. The code is available at https://github.com/HorizonRobotics/Uni3R.",
        "arxiv_id": "2508.03643",
        "ARXIVID": "2508.03643",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Uni3R proposes a unified 3D scene representation with open-vocabulary semantics from unposed multi-view images, using a Cross-View Transformer and Gaussian splatting, which is highly relevant to spatial intelligence and vision foundation models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.03692": {
        "authors": [
            "Ao Liang",
            "Youquan Liu",
            "Yu Yang",
            "Dongyue Lu",
            "Linfeng Li",
            "Lingdong Kong",
            "Huaici Zhao",
            "Wei Tsang Ooi"
        ],
        "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences",
        "abstract": "arXiv:2508.03692v1 Announce Type: new  Abstract: Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.",
        "arxiv_id": "2508.03692",
        "ARXIVID": "2508.03692",
        "COMMENT": "LiDARCrafter presents a unified framework for 4D LiDAR generation and editing, with a new benchmark for dynamic world modeling and evaluation. It enables language-conditioned scene generation and editing, and introduces a comprehensive benchmark. This matches criterion 3 (embodied AI, new benchmark/simulator, novel angle) and is also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.03680": {
        "authors": [
            "Xufang Luo",
            "Yuge Zhang",
            "Zhiyuan He",
            "Zilong Wang",
            "Siyun Zhao",
            "Dongsheng Li",
            "Luna K. Qiu",
            "Yuqing Yang"
        ],
        "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
        "abstract": "arXiv:2508.03680v1 Announce Type: new  Abstract: We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.",
        "arxiv_id": "2508.03680",
        "ARXIVID": "2508.03680",
        "COMMENT": "This paper introduces Agent Lightning, a framework for RL-based training of LLMs for any AI agent, with a unified data interface and hierarchical RL algorithm. It is a new method for embodied AI agent training and benchmarking, matching criterion 3 (embodied AI, new methods/benchmarks).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.03469": {
        "authors": [
            "Jiabing Yang",
            "Chenhang Cui",
            "Yiyang Zhou",
            "Yixiang Chen",
            "Peng Xia",
            "Ying Wei",
            "Tao Yu",
            "Yan Huang",
            "Liang Wang"
        ],
        "title": "IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models",
        "abstract": "arXiv:2508.03469v1 Announce Type: new  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress across multiple domains. However, these models still face the inherent challenge of integrating vision and language for collaborative inference, which often leads to \"hallucinations\", outputs that are not grounded in the corresponding images. Many efforts have been made to address these issues, but each comes with its own limitations, such as high computational cost or expensive dataset annotation. Recent research shows that LVLMs exhibit a long-term bias where hallucinations increase as the sequence length grows, yet the underlying cause remains poorly understood. Building on extensive research into attention mechanisms in LVLMs, we analyze the relationship between this long-term bias and visual attention. In our research, we identify a consistent phenomenon in current LVLMs: the model's attention to visual input diminishes as the generated sequence grows, which we hypothesize to be a key factor contributing to observed increasing hallucinations. Based on these insights, we propose Image attention-guided Key-value merging cOllaborative Decoding (IKOD), a collaborative decoding strategy generating more image-focused sequences. This method derives logits from shorter sequences with higher image attention through key-value merging and combines them with those from the original decoding, effectively mitigating attention degradation and suppressing hallucinations while not incurring too much inference cost. Extensive experiments on both hallucination and comprehensive benchmarks demonstrate IKOD's superior effectiveness in mitigating hallucinations and improving comprehensive capacities for LVLMs. Importantly, IKOD requires no additional training or external tools, making it a lightweight and efficient framework applicable to various models.",
        "arxiv_id": "2508.03469",
        "ARXIVID": "2508.03469",
        "COMMENT": "This paper addresses hallucination in Large Vision-Language Models (LVLMs) by proposing a new decoding strategy (IKOD) that mitigates visual attention degradation. This is a direct match to criterion 2 (new VLLMs/MLLMs) and also provides a clever statistical trick for improving VLLM performance.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.03690": {
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Ao Liang",
            "Jianxiong Gao",
            "Yang Wu",
            "Xiang Xu",
            "Xin Li",
            "Linfeng Li",
            "Runnan Chen",
            "Ben Fei"
        ],
        "title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image",
        "abstract": "arXiv:2508.03690v1 Announce Type: new  Abstract: Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.",
        "arxiv_id": "2508.03690",
        "ARXIVID": "2508.03690",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications) and criterion 3 (novel method for cross-modal generation and evaluation in embodied AI). Proposes a new conditional diffusion framework for generating panoramic LiDAR from monocular RGB, with new metrics for cross-modal consistency and a new benchmark (KITTI-Weather).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.03039": {
        "authors": [
            "Yiran Meng",
            "Junhong Ye",
            "Wei Zhou",
            "Guanghui Yue",
            "Xudong Mao",
            "Ruomei Wang",
            "Baoquan Zhao"
        ],
        "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering",
        "abstract": "arXiv:2508.03039v1 Announce Type: new  Abstract: Cross-video question answering presents significant challenges beyond traditional single-video understanding, particularly in establishing meaningful connections across video streams and managing the complexity of multi-source information retrieval. We introduce VideoForest, a novel framework that addresses these challenges through person-anchored hierarchical reasoning. Our approach leverages person-level features as natural bridge points between videos, enabling effective cross-video understanding without requiring end-to-end training. VideoForest integrates three key innovations: 1) a human-anchored feature extraction mechanism that employs ReID and tracking algorithms to establish robust spatiotemporal relationships across multiple video sources; 2) a multi-granularity spanning tree structure that hierarchically organizes visual content around person-level trajectories; and 3) a multi-agent reasoning framework that efficiently traverses this hierarchical structure to answer complex cross-video queries. To evaluate our approach, we develop CrossVideoQA, a comprehensive benchmark dataset specifically designed for person-centric cross-video analysis. Experimental results demonstrate VideoForest's superior performance in cross-video reasoning tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior analysis, and 51.67% in summarization and reasoning, significantly outperforming existing methods. Our work establishes a new paradigm for cross-video understanding by unifying multiple video streams through person-level features, enabling sophisticated reasoning across distributed visual information while maintaining computational efficiency.",
        "arxiv_id": "2508.03039",
        "ARXIVID": "2508.03039",
        "COMMENT": "This paper introduces VideoForest, a new framework for cross-video question answering using person-anchored hierarchical reasoning, and presents a new benchmark (CrossVideoQA). This matches criterion 3 (embodied AI papers on new benchmarks or methods with novel angles), as it focuses on multi-video reasoning and person-centric analysis, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.03402": {
        "authors": [
            "Pingchuan Ma",
            "Xiaopei Yang",
            "Yusong Li",
            "Ming Gui",
            "Felix Krause",
            "Johannes Schusterbauer",
            "Bj\\\"orn Ommer"
        ],
        "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models",
        "abstract": "arXiv:2508.03402v1 Announce Type: new  Abstract: Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51 styles $\\times$ 10,000 content samples) was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process.",
        "arxiv_id": "2508.03402",
        "ARXIVID": "2508.03402",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). SCFlow proposes a flow-matching framework for implicit style and content disentanglement in vision models, with applications to controllable generation and generalization to large datasets, which is relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.03009": {
        "authors": [
            "Xuyi Yang",
            "Wenhao Zhang",
            "Hongbo Jin",
            "Lin Liu",
            "Hongbo Xu",
            "Yongwei Nie",
            "Fei Yu",
            "Fei Ma"
        ],
        "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping",
        "abstract": "arXiv:2508.03009v1 Announce Type: new  Abstract: Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding, primarily due to resource limitations that prevent them from processing all video frames and their associated information. Efficiently extracting relevant information becomes a challenging task. Existing frameworks and evaluation tasks focus on identifying specific frames containing core objects from a large number of irrelevant frames, which does not align with the practical needs of real-world applications. To address this issue, we propose a new scenario under the video question-answering task, SceneQA, which emphasizes scene-based detail perception and reasoning abilities. And we develop the LVSQA dataset to support the SceneQA task, which is built upon carefully selected videos from LVBench and contains a new collection of question-answer pairs to promote a more fair evaluation of MLLMs' scene perception abilities in long videos. Inspired by human cognition, we introduce a novel method called SLFG. The core idea of SLFG is to combine individual frames into semantically coherent scene frames. By leveraging scene localization methods and dynamic frame reassembly mechanisms, SLFG significantly enhances the understanding capabilities of existing MLLMs in long videos. SLFG requires no modification to the original model architecture and boasts excellent plug-and-play usability. Experimental results show that this method performs exceptionally well in several long video benchmark tests. Code and dataset will be released at http://www.slfg.pkuzwh.cn.",
        "arxiv_id": "2508.03009",
        "ARXIVID": "2508.03009",
        "COMMENT": "Matches criterion 2: Proposes a new method (SLFG) to enhance MLLMs' long video understanding and introduces a new dataset (LVSQA) for scene-based video QA, which is a novel angle for MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03118": {
        "authors": [
            "Heng Jia",
            "Linchao Zhu",
            "Na Zhao"
        ],
        "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction",
        "abstract": "arXiv:2508.03118v1 Announce Type: new  Abstract: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a fundamental trade-off: explicit methods achieve geometric precision but struggle with ambiguous regions, while implicit methods provide robustness but suffer from slow convergence. We present H3R, a hybrid framework that addresses this limitation by integrating volumetric latent fusion with attention-based feature aggregation. Our framework consists of two complementary components: an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages Pl\\\"ucker coordinates for adaptive correspondence refinement. By integrating both paradigms, our approach enhances generalization while converging 2$\\times$ faster than existing methods. Furthermore, we show that spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2), resolving the mismatch between semantic representations and spatial reconstruction requirements. Our method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively. Code is available at https://github.com/JiaHeng-DLUT/H3R.",
        "arxiv_id": "2508.03118",
        "ARXIVID": "2508.03118",
        "COMMENT": "H3R proposes a hybrid multi-view correspondence framework for generalizable 3D reconstruction, integrating volumetric latent fusion and attention-based feature aggregation. It also provides insights into spatial-aligned vs semantic-aligned foundation models. This matches criterion 4 (vision foundation models and applications) and is relevant to criterion 1 (spatial understanding).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03277": {
        "authors": [
            "Hang Guo",
            "Qing Zhang",
            "Zixuan Gao",
            "Siyuan Yang",
            "Shulin Peng",
            "Xiang Tao",
            "Ting Yu",
            "Yan Wang",
            "Qingli Li"
        ],
        "title": "Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification",
        "abstract": "arXiv:2508.03277v1 Announce Type: new  Abstract: Accurate prediction of placental diseases via whole slide images (WSIs) is critical for preventing severe maternal and fetal complications. However, WSI analysis presents significant computational challenges due to the massive data volume. Existing WSI classification methods encounter critical limitations: (1) inadequate patch selection strategies that either compromise performance or fail to sufficiently reduce computational demands, and (2) the loss of global histological context resulting from patch-level processing approaches. To address these challenges, we propose an Efficient multimodal framework for Patient-level placental disease Diagnosis, named EmmPD. Our approach introduces a two-stage patch selection module that combines parameter-free and learnable compression strategies, optimally balancing computational efficiency with critical feature preservation. Additionally, we develop a hybrid multimodal fusion module that leverages adaptive graph learning to enhance pathological feature representation and incorporates textual medical reports to enrich global contextual understanding. Extensive experiments conducted on both a self-constructed patient-level Placental dataset and two public datasets demonstrating that our method achieves state-of-the-art diagnostic performance. The code is available at https://github.com/ECNU-MultiDimLab/EmmPD.",
        "arxiv_id": "2508.03277",
        "ARXIVID": "2508.03277",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a multimodal framework for placental disease classification using visual and textual data, with adaptive graph learning and multimodal fusion, which is relevant to multi-modal large language models and vision foundation model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03127": {
        "authors": [
            "Sai Ma",
            "Zhuang Li",
            "John A Taylor"
        ],
        "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery",
        "abstract": "arXiv:2508.03127v1 Announce Type: new  Abstract: Vision language models (VLMs) that enable natural language interaction with satellite imagery can democratize Earth observation by accelerating expert workflows, making data accessible to non-specialists, and enabling planet-scale automation. However, existing datasets focus mainly on short-term, high-resolution imagery from a limited number of satellites, overlooking low-resolution, multi-satellite, long-term archives, such as Landsat, that are essential for affordable and bias-robust global monitoring. We address this gap with Landsat30-AU, a large-scale vision-language dataset built from 30-meter resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over Australia, spanning more than 36 years. The dataset includes two components: Landsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA, comprising 17,725 human-verified visual question answering (VQA) samples across eight remote sensing domains. Both datasets are curated through a bootstrapped pipeline that leverages generic VLMs with iterative refinement and human verification to ensure quality. Our evaluation of eight VLMs on our benchmark reveals that off-the-shelf models struggle to understand satellite imagery. The open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in captioning and a VQA accuracy of 0.48, highlighting the limitations of current approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and boosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at https://github.com/papersubmit1/landsat30-au.",
        "arxiv_id": "2508.03127",
        "ARXIVID": "2508.03127",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 3 (new benchmark for VLMs on satellite imagery). Introduces a large-scale vision-language dataset for remote sensing and evaluates VLMs, revealing their limitations and improvements via fine-tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03447": {
        "authors": [
            "Qiyu Chen",
            "Zhen Qu",
            "Wei Luo",
            "Haiming Yao",
            "Yunkang Cao",
            "Yuxin Jiang",
            "Yinan Duan",
            "Huiyuan Luo",
            "Chengkan Lv",
            "Zhengtao Zhang"
        ],
        "title": "CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection",
        "abstract": "arXiv:2508.03447v1 Announce Type: new  Abstract: Recently, large pre-trained vision-language models have shown remarkable performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single auxiliary dataset, the model enables cross-category anomaly detection on diverse datasets covering industrial defects and medical lesions. Compared to manually designed prompts, prompt learning eliminates the need for expert knowledge and trial-and-error. However, it still faces the following challenges: (i) static learnable tokens struggle to capture the continuous and diverse patterns of normal and anomalous states, limiting generalization to unseen categories; (ii) fixed textual labels provide overly sparse category information, making the model prone to overfitting to a specific semantic subspace. To address these issues, we propose Conditional Prompt Synthesis (CoPS), a novel framework that synthesizes dynamic prompts conditioned on visual features to enhance ZSAD performance. Specifically, we extract representative normal and anomaly prototypes from fine-grained patch features and explicitly inject them into prompts, enabling adaptive state modeling. Given the sparsity of class labels, we leverage a variational autoencoder to model semantic image features and implicitly fuse varied class tokens into prompts. Additionally, integrated with our spatially-aware alignment mechanism, extensive experiments demonstrate that CoPS surpasses state-of-the-art methods by 2.5% AUROC in both classification and segmentation across 13 industrial and medical datasets. Code will be available at https://github.com/cqylunlun/CoPS.",
        "arxiv_id": "2508.03447",
        "ARXIVID": "2508.03447",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (improves VLM-based zero-shot anomaly detection with conditional prompt synthesis and spatial alignment). The use of dynamic, visually-conditioned prompts is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03102": {
        "authors": [
            "Tianjiao Jiang",
            "Zhen Zhang",
            "Yuhang Liu",
            "Javen Qinfeng Shi"
        ],
        "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning",
        "abstract": "arXiv:2508.03102v1 Announce Type: new  Abstract: Few-shot learning (FSL) often requires effective adaptation of models using limited labeled data. However, most existing FSL methods rely on entangled representations, requiring the model to implicitly recover the unmixing process to obtain disentangled representations using only limited supervision, which hinders effective adaptation. Recent theoretical studies show that multimodal contrastive learning methods, such as CLIP, can disentangle latent representations up to linear transformations. In light of this, we propose the Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles visual features extracted from CLIP using unsupervised Independent Component Analysis (ICA). This removes the need to learn the unmixing process from the labeled data, thereby reducing the number of trainable parameters and mitigating overfitting. Taking a step further, while ICA can obtain visual disentangled representations, it may also disrupt CLIP's intra- and inter-modal alignment. To counteract this, CCA further leverages CLIP's inherent cross-modal alignment by enhancing it in two ways: unidirectionally, through fine-tuning a CLIP-based text classifier, and bidirectionally, via a cross-attention mechanism that enriches visual and textual representations through mutual interaction. Both unimodal and cross-modal classification outputs can be effectively combined linearly to improve classification accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches in terms of few-shot performance and robustness to distributional shifts, while maintaining computational efficiency. Code will be available at https://github.com/tianjiao-j/CCA.",
        "arxiv_id": "2508.03102",
        "ARXIVID": "2508.03102",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (improves CLIP-based few-shot learning with causal disentanglement and cross-modal alignment). The use of ICA for disentanglement and cross-modal alignment is a novel methodological improvement.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03618": {
        "authors": [
            "Nassim Ali Ousalah",
            "Peyman Rostami",
            "Anis Kacem",
            "Enjie Ghorbel",
            "Emmanuel Koumandakis",
            "Djamila Aouada"
        ],
        "title": "FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation",
        "abstract": "arXiv:2508.03618v1 Announce Type: new  Abstract: We introduce FPG-NAS, a FLOPs-aware Gated Differentiable Neural Architecture Search framework for efficient 6DoF object pose estimation. Estimating 3D rotation and translation from a single image has been widely investigated yet remains computationally demanding, limiting applicability in resource-constrained scenarios. FPG-NAS addresses this by proposing a specialized differentiable NAS approach for 6DoF pose estimation, featuring a task-specific search space and a differentiable gating mechanism that enables discrete multi-candidate operator selection, thus improving architectural diversity. Additionally, a FLOPs regularization term ensures a balanced trade-off between accuracy and efficiency. The framework explores a vast search space of approximately 10\\textsuperscript{92} possible architectures. Experiments on the LINEMOD and SPEED+ datasets demonstrate that FPG-NAS-derived models outperform previous methods under strict FLOPs constraints. To the best of our knowledge, FPG-NAS is the first differentiable NAS framework specifically designed for 6DoF object pose estimation.",
        "arxiv_id": "2508.03618",
        "ARXIVID": "2508.03618",
        "COMMENT": "This paper introduces FPG-NAS, a differentiable neural architecture search framework for efficient 6DoF pose estimation, with FLOPs-aware gating. It is a methodological improvement in spatial understanding for pose estimation (criterion 1), and the NAS approach is novel for this task.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03373": {
        "authors": [
            "Ni Tang",
            "Xiaotong Luo",
            "Zihan Cheng",
            "Liangtai Zhou",
            "Dongxiao Zhang",
            "Yanyun Qu"
        ],
        "title": "Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration",
        "abstract": "arXiv:2508.03373v1 Announce Type: new  Abstract: Diffusion models have revealed powerful potential in all-in-one image restoration (AiOIR), which is talented in generating abundant texture details. The existing AiOIR methods either retrain a diffusion model or fine-tune the pretrained diffusion model with extra conditional guidance. However, they often suffer from high inference costs and limited adaptability to diverse degradation types. In this paper, we propose an efficient AiOIR method, Diffusion Once and Done (DOD), which aims to achieve superior restoration performance with only one-step sampling of Stable Diffusion (SD) models. Specifically, multi-degradation feature modulation is first introduced to capture different degradation prompts with a pretrained diffusion model. Then, parameter-efficient conditional low-rank adaptation integrates the prompts to enable the fine-tuning of the SD model for adapting to different degradation types. Besides, a high-fidelity detail enhancement module is integrated into the decoder of SD to improve structural and textural details. Experiments demonstrate that our method outperforms existing diffusion-based restoration approaches in both visual quality and inference efficiency.",
        "arxiv_id": "2508.03373",
        "ARXIVID": "2508.03373",
        "COMMENT": "This paper proposes an efficient all-in-one image restoration method using diffusion models and LoRA, with a focus on parameter-efficient adaptation and high-fidelity detail enhancement. It is relevant to criterion 4 (vision foundation models and applications), as it leverages Stable Diffusion for restoration tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03578": {
        "authors": [
            "Jonas Leo Mueller",
            "Lukas Engel",
            "Eva Dorschky",
            "Daniel Krauss",
            "Ingrid Ullmann",
            "Martin Vossiek",
            "Bjoern M. Eskofier"
        ],
        "title": "RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data",
        "abstract": "arXiv:2508.03578v1 Announce Type: new  Abstract: Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications.",
        "arxiv_id": "2508.03578",
        "ARXIVID": "2508.03578",
        "COMMENT": "This paper introduces RadProPoser, a probabilistic encoder-decoder for human pose estimation from raw radar data, with explicit uncertainty quantification. It is a methodological improvement in spatial understanding (criterion 1), especially in embodied sensing scenarios, and introduces a new dataset. The uncertainty modeling and radar-based approach are novel for pose estimation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03441": {
        "authors": [
            "Ning Zhu",
            "Xiaochuan Ma",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "title": "MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis",
        "abstract": "arXiv:2508.03441v1 Announce Type: new  Abstract: Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insufficient feature representation. Recently, pre-trained Foundation Models (FMs) have shown powerful feature extraction ability with a potential for better CSAL. However, this paradigm has been rarely investigated, with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets under different annotation budgets, covering classification and segmentation tasks from diverse medical modalities. It is also the first CSAL benchmark that evaluates both the feature extraction and sample selection stages. Our experimental results reveal that: 1) Most FMs are effective feature extractors for CSAL, with DINO family performing the best in segmentation; 2) The performance differences of these FMs are large in segmentation tasks, while small for classification; 3) Different sample selection strategies should be considered in CSAL on different datasets, with Active Learning by Processing Surprisal (ALPS) performing the best in segmentation while RepDiv leading for classification. The code is available at https://github.com/HiLab-git/MedCAL-Bench.",
        "arxiv_id": "2508.03441",
        "ARXIVID": "2508.03441",
        "COMMENT": "MedCAL-Bench is a new benchmark for cold-start active learning with foundation models in medical image analysis. It evaluates 14 foundation models and 7 strategies across multiple datasets. This matches criterion 4 (vision foundation models and applications) and criterion 3 (new benchmark, though in medical imaging).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03180": {
        "authors": [
            "Weihang Liu",
            "Yuke Li",
            "Yuxuan Li",
            "Jingyi Yu",
            "Xin Lou"
        ],
        "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting",
        "abstract": "arXiv:2508.03180v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.",
        "arxiv_id": "2508.03180",
        "ARXIVID": "2508.03180",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Duplex-GS proposes a new real-time, order-independent Gaussian Splatting method for 3D rendering, which is a core technique in vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.02806": {
        "authors": [
            "Zongyou Yang",
            "Jonathan Loo"
        ],
        "title": "PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation",
        "abstract": "arXiv:2508.02806v1 Announce Type: new  Abstract: Recently, a significant improvement in the accuracy of 3D human pose estimation has been achieved by combining convolutional neural networks (CNNs) with pyramid grid alignment feedback loops. Additionally, innovative breakthroughs have been made in the field of computer vision through the adoption of Transformer-based temporal analysis architectures. Given these advancements, this study aims to deeply optimize and improve the existing Pymaf network architecture. The main innovations of this paper include: (1) Introducing a Transformer feature extraction network layer based on self-attention mechanisms to enhance the capture of low-level features; (2) Enhancing the understanding and capture of temporal signals in video sequences through feature temporal fusion techniques; (3) Implementing spatial pyramid structures to achieve multi-scale feature fusion, effectively balancing feature representations differences across different scales. The new PyCAT4 model obtained in this study is validated through experiments on the COCO and 3DPW datasets. The results demonstrate that the proposed improvement strategies significantly enhance the network's detection capability in human pose estimation, further advancing the development of human pose estimation technology.",
        "arxiv_id": "2508.02806",
        "ARXIVID": "2508.02806",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents). PyCAT4 introduces a hierarchical vision transformer-based framework for 3D human pose estimation, with innovations in temporal and spatial feature fusion, directly improving spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.03356": {
        "authors": [
            "Matteo Caligiuri",
            "Francesco Barbato",
            "Donald Shenaj",
            "Umberto Michieli",
            "Pietro Zanuttigh"
        ],
        "title": "FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models",
        "abstract": "arXiv:2508.03356v1 Announce Type: new  Abstract: Federated Learning (FL) is an established paradigm for training deep learning models on decentralized data. However, as the size of the models grows, conventional FL approaches often require significant computational resources on client devices, which may not be feasible. We introduce FedPromo, a novel framework that enables efficient adaptation of large-scale foundation models stored on a central server to new domains encountered only by remote clients. Instead of directly training the large model on client devices, FedPromo optimizes lightweight proxy models via FL, significantly reducing computational overhead while maintaining privacy. Our method follows a two-stage process: first, server-side knowledge distillation aligns the representations of a large-scale foundation model (e.g., a transformer) with those of a compact counterpart (e.g., a CNN). Then, the compact model encoder is deployed to client devices, where trainable classifiers are learned locally. These classifiers are subsequently aggregated and seamlessly transferred back to the foundation model, facilitating personalized adaptation without requiring direct access to user data. Through novel regularization strategies, our framework enables decentralized multi-domain learning, balancing performance, privacy, and resource efficiency. Extensive experiments on five image classification benchmarks demonstrate that FedPromo outperforms existing methods while assuming limited-resource clients.",
        "arxiv_id": "2508.03356",
        "ARXIVID": "2508.03356",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). FedPromo is about adapting large-scale foundation models to new domains via federated learning with lightweight proxy models, which is a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03338": {
        "authors": [
            "Tongshun Zhang",
            "Pingping Liu",
            "Zhe Zhang",
            "Qiuzhan Zhou"
        ],
        "title": "CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement",
        "abstract": "arXiv:2508.03338v1 Announce Type: new  Abstract: Images captured in nighttime scenes suffer from severely reduced visibility, hindering effective content perception. Current low-light image enhancement (LLIE) methods face significant challenges: data-driven end-to-end mapping networks lack interpretability or rely on unreliable prior guidance, struggling under extremely dark conditions, while physics-based methods depend on simplified assumptions that often fail in complex real-world scenarios. To address these limitations, we propose CIVQLLIE, a novel framework that leverages the power of discrete representation learning through causal reasoning. We achieve this through Vector Quantization (VQ), which maps continuous image features to a discrete codebook of visual tokens learned from large-scale high-quality images. This codebook serves as a reliable prior, encoding standardized brightness and color patterns that are independent of degradation. However, direct application of VQ to low-light images fails due to distribution shifts between degraded inputs and the learned codebook. Therefore, we propose a multi-level causal intervention approach to systematically correct these shifts. First, during encoding, our Pixel-level Causal Intervention (PCI) module intervenes to align low-level features with the brightness and color distributions expected by the codebook. Second, a Feature-aware Causal Intervention (FCI) mechanism with Low-frequency Selective Attention Gating (LSAG) identifies and enhances channels most affected by illumination degradation, facilitating accurate codebook token matching while enhancing the encoder's generalization performance through flexible feature-level intervention. Finally, during decoding, the High-frequency Detail Reconstruction Module (HDRM) leverages structural information preserved in the matched codebook representations to reconstruct fine details using deformable convolution techniques.",
        "arxiv_id": "2508.03338",
        "ARXIVID": "2508.03338",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel causal intervention framework using vector quantization for low-light image enhancement, leveraging discrete representations learned from large-scale image data.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03334": {
        "authors": [
            "Xunzhi Xiang",
            "Yabo Chen",
            "Guiyu Zhang",
            "Zhongyu Wang",
            "Zhe Gao",
            "Quanming Xiang",
            "Gonghu Shang",
            "Junqi Liu",
            "Haibin Huang",
            "Yang Gao",
            "Chi Zhang",
            "Qi Fan",
            "Xuelong Li"
        ],
        "title": "Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation",
        "abstract": "arXiv:2508.03334v1 Announce Type: new  Abstract: Current autoregressive diffusion models excel at video generation but are generally limited to short temporal durations. Our theoretical analysis indicates that the autoregressive modeling typically suffers from temporal drift caused by error accumulation and hinders parallelization in long video synthesis. To address these limitations, we propose a novel planning-then-populating framework centered on Macro-from-Micro Planning (MMPL) for long video generation. MMPL sketches a global storyline for the entire video through two hierarchical stages: Micro Planning and Macro Planning. Specifically, Micro Planning predicts a sparse set of future keyframes within each short video segment, offering motion and appearance priors to guide high-quality video segment generation. Macro Planning extends the in-segment keyframes planning across the entire video through an autoregressive chain of micro plans, ensuring long-term consistency across video segments. Subsequently, MMPL-based Content Populating generates all intermediate frames in parallel across segments, enabling efficient parallelization of autoregressive generation. The parallelization is further optimized by Adaptive Workload Scheduling for balanced GPU execution and accelerated autoregressive video generation. Extensive experiments confirm that our method outperforms existing long video generation models in quality and stability. Generated videos and comparison results are in our project page.",
        "arxiv_id": "2508.03334",
        "ARXIVID": "2508.03334",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in video. Proposes a new planning-then-populating framework for long video generation, which is a novel approach to autoregressive video synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03484": {
        "authors": [
            "Zhiyao Xu",
            "Dan Zhao",
            "Qingsong Zou",
            "Qing Li",
            "Yong Jiang",
            "Yuhang Wang",
            "Jingyu Xiao"
        ],
        "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes",
        "abstract": "arXiv:2508.03484v1 Announce Type: new  Abstract: As smart homes become increasingly prevalent, intelligent models are widely used for tasks such as anomaly detection and behavior prediction. These models are typically trained on static datasets, making them brittle to behavioral drift caused by seasonal changes, lifestyle shifts, or evolving routines. However, collecting new behavior data for retraining is often impractical due to its slow pace, high cost, and privacy concerns. In this paper, we propose SmartGen, an LLM-based framework that synthesizes context-aware user behavior data to support continual adaptation of downstream smart home models. SmartGen consists of four key components. First, we design a Time and Semantic-aware Split module to divide long behavior sequences into manageable, semantically coherent subsequences under dual time-span constraints. Second, we propose Semantic-aware Sequence Compression to reduce input length while preserving representative semantics by clustering behavior mapping in latent space. Third, we introduce Graph-guided Sequence Synthesis, which constructs a behavior relationship graph and encodes frequent transitions into prompts, guiding the LLM to generate data aligned with contextual changes while retaining core behavior patterns. Finally, we design a Two-stage Outlier Filter to identify and remove implausible or semantically inconsistent outputs, aiming to improve the factual coherence and behavioral validity of the generated sequences. Experiments on three real-world datasets demonstrate that SmartGen significantly enhances model performance on anomaly detection and behavior prediction tasks under behavioral drift, with anomaly detection improving by 85.43% and behavior prediction by 70.51% on average. The code is available at https://github.com/horizonsinzqs/SmartGen.",
        "arxiv_id": "2508.03484",
        "ARXIVID": "2508.03484",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel methods for behavior sequence generation in smart home environments, with semantic-aware graph-guided LLM-based synthesis and new filtering techniques). The paper proposes a new framework for generating context-aware user behavior data, which is relevant for embodied AI and spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.02841": {
        "authors": [
            "Ziruo Yi",
            "Jinyu Liu",
            "Ting Xiao",
            "Mark V. Albert"
        ],
        "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering",
        "abstract": "arXiv:2508.02841v1 Announce Type: new  Abstract: Radiology visual question answering (RVQA) provides precise answers to questions about chest X-ray images, alleviating radiologists' workload. While recent methods based on multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have shown promising progress in RVQA, they still face challenges in factual accuracy, hallucinations, and cross-modal misalignment. We introduce a multi-agent system (MAS) designed to support complex reasoning in RVQA, with specialized agents for context understanding, multimodal reasoning, and answer validation. We evaluate our system on a challenging RVQA set curated via model disagreement filtering, comprising consistently hard cases across multiple MLLMs. Extensive experiments demonstrate the superiority and effectiveness of our system over strong MLLM baselines, with a case study illustrating its reliability and interpretability. This work highlights the potential of multi-agent approaches to support explainable and trustworthy clinical AI applications that require complex reasoning.",
        "arxiv_id": "2508.02841",
        "ARXIVID": "2508.02841",
        "COMMENT": "Matches criterion 2: Proposes a multi-agent system for complex reasoning in radiology VQA, leveraging MLLMs and showing empirical improvements over strong baselines.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03079": {
        "authors": [
            "Zaiying Zhao",
            "Toshihiko Yamasaki"
        ],
        "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models",
        "abstract": "arXiv:2508.03079v1 Announce Type: new  Abstract: The rapid expansion of applications using Large Vision-Language Models (LVLMs), such as GPT-4o, has raised significant concerns about their fairness. While existing studies primarily focus on demographic attributes such as race and gender, fairness across a broader range of attributes remains largely unexplored. In this study, we construct an open-set knowledge base of bias attributes leveraging Large Language Models (LLMs) and evaluate the fairness of LVLMs across finer-grained attributes. Our experimental results reveal that LVLMs exhibit biased outputs across a diverse set of attributes and further demonstrate that cultural, environmental, and behavioral factors have a more pronounced impact on LVLM decision-making than traditional demographic attributes.",
        "arxiv_id": "2508.03079",
        "ARXIVID": "2508.03079",
        "COMMENT": "Matches criterion 2: The paper evaluates fairness in Large Vision-Language Models (LVLMs) and constructs a new knowledge base for bias attributes, providing empirical insights into model behavior.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03243": {
        "authors": [
            "Lukas Ranftl",
            "Felix Brendel",
            "Bertram Drost",
            "Carsten Steger"
        ],
        "title": "MVTOP: Multi-View Transformer-based Object Pose-Estimation",
        "abstract": "arXiv:2508.03243v1 Announce Type: new  Abstract: We present MVTOP, a novel transformer-based method for multi-view rigid object pose estimation. Through an early fusion of the view-specific features, our method can resolve pose ambiguities that would be impossible to solve with a single view or with a post-processing of single-view poses. MVTOP models the multi-view geometry via lines of sight that emanate from the respective camera centers. While the method assumes the camera interior and relative orientations are known for a particular scene, they can vary for each inference. This makes the method versatile. The use of the lines of sight enables MVTOP to correctly predict the correct pose with the merged multi-view information. To show the model's capabilities, we provide a synthetic data set that can only be solved with such holistic multi-view approaches since the poses in the dataset cannot be solved with just one view. Our method outperforms single-view and all existing multi-view approaches on our dataset and achieves competitive results on the YCB-V dataset. To the best of our knowledge, no holistic multi-view method exists that can resolve such pose ambiguities reliably. Our model is end-to-end trainable and does not require any additional data, e.g., depth.",
        "arxiv_id": "2508.03243",
        "ARXIVID": "2508.03243",
        "COMMENT": "MVTOP is a transformer-based method for multi-view object pose estimation, using early fusion and lines of sight for holistic pose prediction. This is a methodological improvement in spatial understanding and spatial intelligence, matching criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03266": {
        "authors": [
            "Huaihai Lyu",
            "Chaofan Chen",
            "Yuheng Ji",
            "Changsheng Xu"
        ],
        "title": "EgoPrompt: Prompt Pool Learning for Egocentric Action Recognition",
        "abstract": "arXiv:2508.03266v1 Announce Type: new  Abstract: Driven by the increasing demand for applications in augmented and virtual reality, egocentric action recognition has emerged as a prominent research area. It is typically divided into two subtasks: recognizing the performed behavior (i.e., verb component) and identifying the objects being acted upon (i.e., noun component) from the first-person perspective. However, most existing approaches treat these two components as independent classification tasks, focusing on extracting component-specific knowledge while overlooking their inherent semantic and contextual relationships, leading to fragmented representations and sub-optimal generalization capability. To address these challenges, we propose a prompt learning-based framework, EgoPrompt, to conduct the egocentric action recognition task. Building on the existing prompting strategy to capture the component-specific knowledge, we construct a Unified Prompt Pool space to establish interaction between the two types of component representations. Specifically, the component representations (from verbs and nouns) are first decomposed into fine-grained patterns with the prompt pair form. Then, these pattern-level representations are fused through an attention-based mechanism to facilitate cross-component interaction. To ensure the prompt pool is informative, we further introduce a novel training objective, Diverse Pool Criteria. This objective realizes our goals from two perspectives: Prompt Selection Frequency Regularization and Prompt Knowledge Orthogonalization. Extensive experiments are conducted on the Ego4D, EPIC-Kitchens, and EGTEA datasets. The results consistently show that EgoPrompt achieves state-of-the-art performance across within-dataset, cross-dataset, and base-to-novel generalization benchmarks.",
        "arxiv_id": "2508.03266",
        "ARXIVID": "2508.03266",
        "COMMENT": "EgoPrompt introduces a prompt learning-based framework for egocentric action recognition, focusing on cross-component (verb/noun) interaction and generalization. This is a new method for spatial intelligence in embodied agents (egocentric vision), matching criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03227": {
        "authors": [
            "Hongyu Shen",
            "Junfeng Ni",
            "Yixin Chen",
            "Weishuo Li",
            "Mingtao Pei",
            "Siyuan Huang"
        ],
        "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
        "abstract": "arXiv:2508.03227v1 Announce Type: new  Abstract: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.",
        "arxiv_id": "2508.03227",
        "ARXIVID": "2508.03227",
        "COMMENT": "This paper proposes Trace3D, a method for consistent 2D-to-3D segmentation lifting using Gaussian Instance Tracing. It is a methodological improvement in spatial understanding and segmentation, but not specifically for embodied agents. It is relevant to vision foundation models and spatial intelligence.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.03625": {
        "authors": [
            "Daniel DeAlcala",
            "Aythami Morales",
            "Julian Fierrez",
            "Ruben Tolosana"
        ],
        "title": "AttZoom: Attention Zoom for Better Visual Features",
        "abstract": "arXiv:2508.03625v1 Announce Type: new  Abstract: We present Attention Zoom, a modular and model-agnostic spatial attention mechanism designed to improve feature extraction in convolutional neural networks (CNNs). Unlike traditional attention approaches that require architecture-specific integration, our method introduces a standalone layer that spatially emphasizes high-importance regions in the input. We evaluated Attention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet, showing consistent improvements in Top-1 and Top-5 classification accuracy. Visual analyses using Grad-CAM and spatial warping reveal that our method encourages fine-grained and diverse attention patterns. Our results confirm the effectiveness and generality of the proposed layer for improving CCNs with minimal architectural overhead.",
        "arxiv_id": "2508.03625",
        "ARXIVID": "2508.03625",
        "COMMENT": "This paper proposes a new spatial attention mechanism (Attention Zoom) for CNNs, which is a methodological improvement to spatial understanding in visual models. This matches criterion 1 (new methodological improvements to spatial understanding).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03069": {
        "authors": [
            "Bo Zhang",
            "Yifan Zhang",
            "Shuo Yan",
            "Yu Bai",
            "Zheng Zhang",
            "Wu Liu",
            "Xiuzhuang Zhou",
            "Wendong Wang"
        ],
        "title": "SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation",
        "abstract": "arXiv:2508.03069v1 Announce Type: new  Abstract: In light of the spatial domain's limited capacity for modeling global context in 3D medical image segmentation, emerging approaches have begun to incorporate frequency domain representations. However, straightforward feature extraction strategies often overlook the unique properties of frequency domain information, such as conjugate symmetry. They also fail to account for the fundamental differences in data distribution between the spatial and frequency domains, which can ultimately dilute or obscure the complementary strengths that frequency-based representations offer. In this paper, we propose SSFMamba, a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D medical image segmentation. SSFMamba employs a complementary dual-branch architecture that extracts features from both the spatial and frequency domains, and leverages a Mamba block to fuse these heterogeneous features to preserve global context while reinforcing local details. In the frequency domain branch, we harness Mamba's exceptional capability to extract global contextual information in conjunction with the synergistic effect of frequency domain features to further enhance global modeling. Moreover, we design a 3D multi-directional scanning mechanism to strengthen the fusion of local and global cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets demonstrate that our approach consistently outperforms state-of-the-art methods across various evaluation metrics.",
        "arxiv_id": "2508.03069",
        "ARXIVID": "2508.03069",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). SSFMamba introduces a symmetry-driven spatial-frequency feature fusion network for 3D medical image segmentation, leveraging Mamba blocks for global context, which is a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.02807": {
        "authors": [
            "Tongchun Zuo",
            "Zaiyu Huang",
            "Shuliang Ning",
            "Ente Lin",
            "Chao Liang",
            "Zerong Zheng",
            "Jianwen Jiang",
            "Yuan Zhang",
            "Mingyuan Gao",
            "Xin Dong"
        ],
        "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework",
        "abstract": "arXiv:2508.02807v1 Announce Type: new  Abstract: Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \\textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/",
        "arxiv_id": "2508.02807",
        "ARXIVID": "2508.02807",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (uses vision-language models in a generative video try-on pipeline). The use of a stage-wise diffusion transformer and integration of VLMs for video try-on is a novel application.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03483": {
        "authors": [
            "Dasol Choi Jihwan Lee",
            "Minjae Lee",
            "Minsuk Kahng"
        ],
        "title": "When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models",
        "abstract": "arXiv:2508.03483v1 Announce Type: new  Abstract: While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., \"for young people'') to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in today's generative models. We see this as an essential step toward more systematic and responsible AI development.",
        "arxiv_id": "2508.03483",
        "ARXIVID": "2508.03483",
        "COMMENT": "Related to criterion 4: The paper audits demographic bias in text-to-image models (vision foundation models) and introduces a new framework (SODA) for systematic measurement.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.03343": {
        "authors": [
            "Junlong Ren",
            "Gangjian Zhang",
            "Honghao Fu",
            "Pengcheng Wu",
            "Hao Wang"
        ],
        "title": "WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval",
        "abstract": "arXiv:2508.03343v1 Announce Type: new  Abstract: Text-Motion Retrieval (TMR) aims to retrieve 3D motion sequences semantically relevant to text descriptions. However, matching 3D motions with text remains highly challenging, primarily due to the intricate structure of human body and its spatial-temporal dynamics. Existing approaches often overlook these complexities, relying on general encoding methods that fail to distinguish different body parts and their dynamics, limiting precise semantic alignment. To address this, we propose WaMo, a novel wavelet-based multi-frequency feature extraction framework. It fully captures part-specific and time-varying motion details across multiple resolutions on body joints, extracting discriminative motion features to achieve fine-grained alignment with texts. WaMo has three key components: (1) Trajectory Wavelet Decomposition decomposes motion signals into frequency components that preserve both local kinematic details and global motion semantics. (2) Trajectory Wavelet Reconstruction uses learnable inverse wavelet transforms to reconstruct original joint trajectories from extracted features, ensuring the preservation of essential spatial-temporal information. (3) Disordered Motion Sequence Prediction reorders shuffled motion sequences to improve the learning of inherent temporal coherence, enhancing motion-text alignment. Extensive experiments demonstrate WaMo's superiority, achieving 17.0\\% and 18.2\\% improvements in $Rsum$ on HumanML3D and KIT-ML datasets, respectively, outperforming existing state-of-the-art (SOTA) methods.",
        "arxiv_id": "2508.03343",
        "ARXIVID": "2508.03343",
        "COMMENT": "WaMo introduces a wavelet-based multi-frequency feature extraction framework for fine-grained text-motion retrieval, focusing on spatial-temporal alignment. This is a new method for multi-modal learning and spatial understanding, relevant to criterion 1 and 2 (if the model is large-scale), but mainly criterion 1.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.03388": {
        "authors": [
            "Yizhe Xiong",
            "Zihan Zhou",
            "Yiwen Liang",
            "Hui Chen",
            "Zijia Lin",
            "Tianxiang Hao",
            "Fan Zhang",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "title": "Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation",
        "abstract": "arXiv:2508.03388v1 Announce Type: new  Abstract: Test-Time Adaptation (TTA) has emerged as an effective solution for adapting Vision Transformers (ViT) to distribution shifts without additional training data. However, existing TTA methods often incur substantial computational overhead, limiting their applicability in resource-constrained real-world scenarios. To reduce inference cost, plug-and-play token aggregation methods merge redundant tokens in ViTs to reduce total processed tokens. Albeit efficient, it suffers from significant performance degradation when directly integrated with existing TTA methods. We formalize this problem as Efficient Test-Time Adaptation (ETTA), seeking to preserve the adaptation capability of TTA while reducing inference latency. In this paper, we first provide a theoretical analysis from a novel mutual information perspective, showing that token aggregation inherently leads to information loss, which cannot be fully mitigated by conventional norm-tuning-based TTA methods. Guided by this insight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation \\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}). Specifically, we directly augment the [CLS] token embedding and incorporate adaptive biases into the [CLS] token in shallow layers of ViTs. We theoretically demonstrate that these augmentations, when optimized via entropy minimization, recover the information lost due to token aggregation. Extensive experiments across various out-of-distribution benchmarks demonstrate that NAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while achieving an inference latency reduction of more than 20\\%, effectively addressing the ETTA challenge.",
        "arxiv_id": "2508.03388",
        "ARXIVID": "2508.03388",
        "COMMENT": "This paper proposes a new method (NAVIA) for efficient test-time adaptation of Vision Transformers, focusing on token aggregation and information augmentation. It introduces a mutual information perspective and a novel augmentation strategy. This is a methodological improvement in spatial understanding for vision models, matching criterion 1.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.02923": {
        "authors": [
            "Minh-Hai Nguyen",
            "Edouard Pauwels",
            "Pierre Weiss"
        ],
        "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution",
        "abstract": "arXiv:2508.02923v1 Announce Type: new  Abstract: The Maximum A Posteriori (MAP) estimation is a widely used framework in blind deconvolution to recover sharp images from blurred observations. The estimated image and blur filter are defined as the maximizer of the posterior distribution. However, when paired with sparsity-promoting image priors, MAP estimation has been shown to favors blurry solutions, limiting its effectiveness. In this paper, we revisit this result using diffusion-based priors, a class of models that capture realistic image distributions. Through an empirical examination of the prior's likelihood landscape, we uncover two key properties: first, blurry images tend to have higher likelihoods; second, the landscape contains numerous local minimizers that correspond to natural images. Building on these insights, we provide a theoretical analysis of the blind deblurring posterior. This reveals that the MAP estimator tends to produce sharp filters (close to the Dirac delta function) and blurry solutions. However local minimizers of the posterior, which can be obtained with gradient descent, correspond to realistic, natural images, effectively solving the blind deconvolution problem. Our findings suggest that overcoming MAP's limitations requires good local initialization to local minima in the posterior landscape. We validate our analysis with numerical experiments, demonstrating the practical implications of our insights for designing improved priors and optimization techniques.",
        "arxiv_id": "2508.02923",
        "ARXIVID": "2508.02923",
        "COMMENT": "Related to generative modeling and optimization in computer vision, but does not directly match any specific criterion. Provides empirical and theoretical insights into diffusion priors for blind deconvolution.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.03017": {
        "authors": [
            "Liheng Zhang",
            "Weihao Yu",
            "Zubo Lu",
            "Haozhi Gu",
            "Jin Huang"
        ],
        "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting",
        "abstract": "arXiv:2508.03017v1 Announce Type: new  Abstract: Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.",
        "arxiv_id": "2508.03017",
        "ARXIVID": "2508.03017",
        "COMMENT": "Related to criterion 4: Proposes a new self-adaptive compression method for 3D Gaussian Splatting, which is relevant to vision foundation models and their efficient deployment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03442": {
        "authors": [
            "Shangwen Zhu",
            "Qianyu Peng",
            "Yuting Hu",
            "Zhantao Yang",
            "Han Zhang",
            "Zhao Pu",
            "Ruili Feng",
            "Fan Cheng"
        ],
        "title": "RAAG: Ratio Aware Adaptive Guidance",
        "abstract": "arXiv:2508.03442v1 Announce Type: new  Abstract: Flow-based generative models have recently achieved remarkable progress in image and video synthesis, with classifier-free guidance (CFG) becoming the standard tool for high-fidelity, controllable generation. However, despite their practical success, little is known about how guidance interacts with different stages of the sampling process-especially in the fast, low-step regimes typical of modern flow-based pipelines. In this work, we uncover and analyze a fundamental instability: the earliest reverse steps are acutely sensitive to the guidance scale, owing to a pronounced spike in the relative strength (RATIO) of conditional to unconditional predictions. Through rigorous theoretical analysis and empirical validation, we show that this RATIO spike is intrinsic to the data distribution, independent of the model architecture, and causes exponential error amplification when paired with strong guidance. To address this, we propose a simple, theoretically grounded, RATIO-aware adaptive guidance schedule that automatically dampens the guidance scale at early steps based on the evolving RATIO, using a closed-form exponential decay. Our method is lightweight, requires no additional inference overhead, and is compatible with standard flow frameworks. Experiments across state-of-the-art image (SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables up to 3x faster sampling while maintaining or improving generation quality, robustness, and semantic alignment. Extensive ablation studies further confirm the generality and stability of our schedule across models, datasets, and hyperparameters. Our findings highlight the critical role of stepwise guidance adaptation in unlocking the full potential of fast flow-based generative models.",
        "arxiv_id": "2508.03442",
        "ARXIVID": "2508.03442",
        "COMMENT": "Related to generative modeling in computer vision, but does not directly match any specific criterion. The paper proposes a new guidance schedule for flow-based generative models, which is a clever statistical trick.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.02987": {
        "authors": [
            "Zachary Yahn",
            "Selim Furkan Tekin",
            "Fatih Ilhan",
            "Sihao Hu",
            "Tiansheng Huang",
            "Yichang Xu",
            "Margaret Loper",
            "Ling Liu"
        ],
        "title": "Adversarial Attention Perturbations for Large Object Detection Transformers",
        "abstract": "arXiv:2508.02987v1 Announce Type: new  Abstract: Adversarial perturbations are useful tools for exposing vulnerabilities in neural networks. Existing adversarial perturbation methods for object detection are either limited to attacking CNN-based detectors or weak against transformer-based detectors. This paper presents an Attention-Focused Offensive Gradient (AFOG) attack against object detection transformers. By design, AFOG is neural-architecture agnostic and effective for attacking both large transformer-based object detectors and conventional CNN-based detectors with a unified adversarial attention framework. This paper makes three original contributions. First, AFOG utilizes a learnable attention mechanism that focuses perturbations on vulnerable image regions in multi-box detection tasks, increasing performance over non-attention baselines by up to 30.6%. Second, AFOG's attack loss is formulated by integrating two types of feature loss through learnable attention updates with iterative injection of adversarial perturbations. Finally, AFOG is an efficient and stealthy adversarial perturbation method. It probes the weak spots of detection transformers by adding strategically generated and visually imperceptible perturbations which can cause well-trained object detection models to fail. Extensive experiments conducted with twelve large detection transformers on COCO demonstrate the efficacy of AFOG. Our empirical results also show that AFOG outperforms existing attacks on transformer-based and CNN-based object detectors by up to 83% with superior speed and imperceptibility. Code is available at https://github.com/zacharyyahn/AFOG.",
        "arxiv_id": "2508.02987",
        "ARXIVID": "2508.02987",
        "COMMENT": "This paper presents AFOG, an adversarial attack method for object detection transformers using attention-focused perturbations. While it is a novel method for vision transformers, it does not directly address spatial intelligence, VLLMs, or embodied AI. It is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03143": {
        "authors": [
            "Yanshu Wang",
            "Xichen Xu",
            "Xiaoning Lei",
            "Guoyang Xie"
        ],
        "title": "SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance",
        "abstract": "arXiv:2508.03143v1 Announce Type: new  Abstract: Synthesizing realistic and spatially precise anomalies is essential for enhancing the robustness of industrial anomaly detection systems. While recent diffusion-based methods have demonstrated strong capabilities in modeling complex defect patterns, they often struggle with spatial controllability and fail to maintain fine-grained regional fidelity. To overcome these limitations, we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained Diffusion with discriminative mask Guidance), a novel diffusion-based framework specifically designed for anomaly generation. Our approach introduces a Region-Constrained Diffusion (RCD) process that preserves the background by freezing it and selectively updating only the foreground anomaly regions during the reverse denoising phase, thereby effectively reducing background artifacts. Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into the discriminator, enabling joint evaluation of both global realism and local anomaly fidelity, guided by pixel-level masks. Extensive experiments on the MVTec-AD and BTAD datasets show that SARD surpasses existing methods in segmentation accuracy and visual quality, setting a new state-of-the-art for pixel-level anomaly synthesis.",
        "arxiv_id": "2508.03143",
        "ARXIVID": "2508.03143",
        "COMMENT": "This paper introduces SARD, a segmentation-aware diffusion framework for anomaly synthesis with region-constrained diffusion and discriminative mask guidance. It is a generative modeling paper with spatial control, but does not directly address spatial intelligence in embodied agents, VLLMs, or vision foundation models. It is relevant to generative modeling in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03596": {
        "authors": [
            "Wuyang Li",
            "Wentao Pan",
            "Xiaoyuan Liu",
            "Zhendong Luo",
            "Chenxin Li",
            "Hengyu Liu",
            "Din Ping Tsai",
            "Mu Ku Chen",
            "Yixuan Yuan"
        ],
        "title": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy",
        "abstract": "arXiv:2508.03596v1 Announce Type: new  Abstract: Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.",
        "arxiv_id": "2508.03596",
        "ARXIVID": "2508.03596",
        "COMMENT": "This paper introduces a new optics-driven neural network (MetaScope) for metalens endoscopy, focusing on physical priors and novel neural designs for biomedical imaging. While it is a vision foundation model application, it is not directly about spatial intelligence, VLLMs, or embodied AI. It is relevant to computer vision and novel imaging, but not a direct match to the criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.03186": {
        "authors": [
            "Heng Wu",
            "Qian Zhang",
            "Guixu Zhang"
        ],
        "title": "Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling",
        "abstract": "arXiv:2508.03186v1 Announce Type: new  Abstract: Accurate monocular depth estimation remains a challenging problem due to the inherent ambiguity that stems from the ill-posed nature of recovering 3D structure from a single view, where multiple plausible depth configurations can produce identical 2D projections. In this paper, we present a novel depth estimation method that combines both local and global cues to improve prediction accuracy. Specifically, we propose the Gated Large Kernel Attention Module (GLKAM) to effectively capture multi-scale local structural information by leveraging large kernel convolutions with a gated mechanism. To further enhance the global perception of the network, we introduce the Global Bin Prediction Module (GBPM), which estimates the global distribution of depth bins and provides structural guidance for depth regression. Extensive experiments on the NYU-V2 and KITTI dataset demonstrate that our method achieves competitive performance and outperforms existing approaches, validating the effectiveness of each proposed component.",
        "arxiv_id": "2508.03186",
        "ARXIVID": "2508.03186",
        "COMMENT": "This paper presents a new monocular depth estimation method combining local and global cues. While it is a methodological improvement in spatial understanding, it does not introduce a new benchmark or foundation model, but is relevant to criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.03397": {
        "authors": [
            "Xinzhu Li",
            "Juepeng Zheng",
            "Yikun Chen",
            "Xudong Mao",
            "Guanghui Yue",
            "Wei Zhou",
            "Chenlei Lv",
            "Ruomei Wang",
            "Fan Zhou",
            "Baoquan Zhao"
        ],
        "title": "DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition",
        "abstract": "arXiv:2508.03397v1 Announce Type: new  Abstract: Robust gait recognition requires highly discriminative representations, which are closely tied to input modalities. While binary silhouettes and skeletons have dominated recent literature, these 2D representations fall short of capturing sufficient cues that can be exploited to handle viewpoint variations, and capture finer and meaningful details of gait. In this paper, we introduce a novel framework, termed DepthGait, that incorporates RGB-derived depth maps and silhouettes for enhanced gait recognition. Specifically, apart from the 2D silhouette representation of the human body, the proposed pipeline explicitly estimates depth maps from a given RGB image sequence and uses them as a new modality to capture discriminative features inherent in human locomotion. In addition, a novel multi-scale and cross-level fusion scheme has also been developed to bridge the modality gap between depth maps and silhouettes. Extensive experiments on standard benchmarks demonstrate that the proposed DepthGait achieves state-of-the-art performance compared to peer methods and attains an impressive mean rank-1 accuracy on the challenging datasets.",
        "arxiv_id": "2508.03397",
        "ARXIVID": "2508.03397",
        "COMMENT": "This paper proposes DepthGait, a new framework for gait recognition using RGB-derived depth and silhouette sequences, with a novel multi-scale fusion scheme. While it is a methodological improvement in spatial understanding, it is not for embodied agents or spatial intelligence in agents, but rather for biometric recognition.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.03360": {
        "authors": [
            "Feng Rui",
            "Zhiyao Luo",
            "Wei Wang",
            "Yuting Song",
            "Yong Liu",
            "Tingting Zhu",
            "Jianqing Li",
            "Xingyao Wang"
        ],
        "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment",
        "abstract": "arXiv:2508.03360v1 Announce Type: new  Abstract: Automatic assessment of cognitive impairment from spontaneous speech offers a promising, non-invasive avenue for early cognitive screening. However, current approaches often lack generalizability when deployed across different languages and clinical settings, limiting their practical utility. In this study, we propose CogBench, the first benchmark designed to evaluate the cross-lingual and cross-site generalizability of large language models (LLMs) for speech-based cognitive impairment assessment. Using a unified multimodal pipeline, we evaluate model performance on three speech datasets spanning English and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set, CIR-E. Our results show that conventional deep learning models degrade substantially when transferred across domains. In contrast, LLMs equipped with chain-of-thought prompting demonstrate better adaptability, though their performance remains sensitive to prompt design. Furthermore, we explore lightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which significantly improves generalization in target domains. These findings offer a critical step toward building clinically useful and linguistically robust speech-based cognitive assessment tools.",
        "arxiv_id": "2508.03360",
        "ARXIVID": "2508.03360",
        "COMMENT": "Partially matches criterion 2 (evaluates LLMs for speech-based cognitive assessment in a multi-modal pipeline) and is relevant to multi-modal learning benchmarks. The focus is on speech and language, not vision, but the multi-modal aspect is present.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.03197": {
        "authors": [
            "Tao Chen",
            "Dan Zhang",
            "Da Chen",
            "Huazhu Fu",
            "Kai Jin",
            "Shanshan Wang",
            "Laurent D. Cohen",
            "Yitian Zhao",
            "Quanyong Yi",
            "Jiong Zhang"
        ],
        "title": "Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network",
        "abstract": "arXiv:2508.03197v1 Announce Type: new  Abstract: Choroidal neovascularization (CNV), a primary characteristic of wet age-related macular degeneration (wet AMD), represents a leading cause of blindness worldwide. In clinical practice, optical coherence tomography angiography (OCTA) is commonly used for studying CNV-related pathological changes, due to its micron-level resolution and non-invasive nature. Thus, accurate segmentation of CNV regions and vessels in OCTA images is crucial for clinical assessment of wet AMD. However, challenges existed due to irregular CNV shapes and imaging limitations like projection artifacts, noises and boundary blurring. Moreover, the lack of publicly available datasets constraints the CNV analysis. To address these challenges, this paper constructs the first publicly accessible CNV dataset (CNVSeg), and proposes a novel multilateral graph convolutional interaction-enhanced CNV segmentation network (MTG-Net). This network integrates both region and vessel morphological information, exploring semantic and geometric duality constraints within the graph domain. Specifically, MTG-Net consists of a multi-task framework and two graph-based cross-task modules: Multilateral Interaction Graph Reasoning (MIGR) and Multilateral Reinforcement Graph Reasoning (MRGR). The multi-task framework encodes rich geometric features of lesion shapes and surfaces, decoupling the image into three task-specific feature maps. MIGR and MRGR iteratively reason about higher-order relationships across tasks through a graph mechanism, enabling complementary optimization for task-specific objectives. Additionally, an uncertainty-weighted loss is proposed to mitigate the impact of artifacts and noise on segmentation accuracy. Experimental results demonstrate that MTG-Net outperforms existing methods, achieving a Dice socre of 87.21\\% for region segmentation and 88.12\\% for vessel segmentation.",
        "arxiv_id": "2508.03197",
        "ARXIVID": "2508.03197",
        "COMMENT": "This paper proposes a new graph convolutional network for neovascularization segmentation in OCTA images, along with a new public dataset. While it is a methodological improvement in medical image segmentation, it does not directly address the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03609": {
        "authors": [
            "Rodrigo Verschae",
            "Ignacio Bugueno-Cordova"
        ],
        "title": "evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition",
        "abstract": "arXiv:2508.03609v1 Announce Type: new  Abstract: Event-based cameras are bio-inspired vision sensors that asynchronously capture per-pixel intensity changes with microsecond latency, high temporal resolution, and high dynamic range, providing valuable information about the spatio-temporal dynamics of the scene. In the present work, we propose evTransFER, a transfer learning-based framework and architecture for face expression recognition using event-based cameras. The main contribution is a feature extractor designed to encode the spatio-temporal dynamics of faces, built by training an adversarial generative method on a different problem (facial reconstruction) and then transferring the trained encoder weights to the face expression recognition system. We show that this proposed transfer learning method greatly improves the ability to recognize facial expressions compared to training a network from scratch. In addition, we propose an architecture that incorporates an LSTM to capture longer-term facial expression dynamics, and we introduce a new event-based representation, referred to as TIE, both of which further improve the results. We evaluate the proposed framework on the event-based facial expression database e-CK+ and compare it to state-of-the-art methods. The results show that the proposed framework evTransFER achieves a 93.6\\% recognition rate on the e-CK+ database, significantly improving the accuracy (25.9\\% points or more) when compared to state-of-the-art performance for similar problems.",
        "arxiv_id": "2508.03609",
        "ARXIVID": "2508.03609",
        "COMMENT": "This paper proposes a transfer learning framework for event-based facial expression recognition, using adversarial generative methods and LSTM architectures. While it is a methodological improvement in spatio-temporal vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03411": {
        "authors": [
            "Diana-Nicoleta Grigore",
            "Neelu Madan",
            "Andreas Mogelmose",
            "Thomas B. Moeslund",
            "Radu Tudor Ionescu"
        ],
        "title": "SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation",
        "abstract": "arXiv:2508.03411v1 Announce Type: new  Abstract: Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on two datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running 1.9x faster. Moreover, our student surpasses previous unsupervised video segmentation models.",
        "arxiv_id": "2508.03411",
        "ARXIVID": "2508.03411",
        "COMMENT": "SlotMatch proposes a knowledge distillation framework for unsupervised video segmentation using object-centric representations. While it is a new method in video segmentation, it does not directly address spatial intelligence in embodied agents or VLLMs/MLLMs, but is relevant to general computer vision interests.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.03542": {
        "authors": [
            "Dmitrii Korzh",
            "Dmitrii Tarasov",
            "Artyom Iudin",
            "Elvir Karimov",
            "Matvey Skripkin",
            "Nikita Kuzmin",
            "Andrey Kuznetsov",
            "Oleg Y. Rogov",
            "Ivan Oseledets"
        ],
        "title": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences",
        "abstract": "arXiv:2508.03542v1 Announce Type: new  Abstract: Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.",
        "arxiv_id": "2508.03542",
        "ARXIVID": "2508.03542",
        "COMMENT": "Does not match any specific criterion. Focuses on speech-to-LaTeX conversion and mathematical content recognition, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03081": {
        "authors": [
            "Bo Zhang",
            "Xu Xinan",
            "Shuo Yan",
            "Yu Bai",
            "Zheng Zhang",
            "Wufan Wang",
            "Wendong Wang"
        ],
        "title": "Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification",
        "abstract": "arXiv:2508.03081v1 Announce Type: new  Abstract: Recent pseudo-bag augmentation methods for Multiple Instance Learning (MIL)-based Whole Slide Image (WSI) classification sample instances from a limited number of bags, resulting in constrained diversity. To address this issue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample instances from all bags with the same class to increase the diversity of pseudo-bags. However, introducing new instances into the pseudo-bag increases the number of critical instances (e.g., tumor instances). This increase results in a reduced occurrence of pseudo-bags containing few critical instances, thereby limiting model performance, particularly on test slides with small tumor areas. To address this, we introduce a bag-level and group-level contrastive learning framework to enhance the discrimination of features with distinct semantic meanings, thereby improving model performance. Experimental results demonstrate that $C^2Aug$ consistently outperforms state-of-the-art approaches across multiple evaluation metrics.",
        "arxiv_id": "2508.03081",
        "ARXIVID": "2508.03081",
        "COMMENT": "Related to computer vision and machine learning, but does not directly match any specific criterion. Proposes a new augmentation and contrastive learning method for WSI classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02694": {
        "authors": [
            "Ningning Wang",
            "Xavier Hu",
            "Pai Liu",
            "He Zhu",
            "Yue Hou",
            "Heyuan Huang",
            "Shengyu Zhang",
            "Jian Yang",
            "Jiaheng Liu",
            "Ge Zhang",
            "Changwang Zhang",
            "Jun Wang",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
        "abstract": "arXiv:2508.02694v1 Announce Type: new  Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.",
        "arxiv_id": "2508.02694",
        "ARXIVID": "2508.02694",
        "COMMENT": "Somewhat related to criterion 3: Discusses agent frameworks and efficiency in LLM-driven agents, but not specifically embodied AI or spatial intelligence. Focuses on cost-effective agent design.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03374": {
        "authors": [
            "Keyi Li",
            "Alexander Jaus",
            "Jens Kleesiek",
            "Rainer Stiefelhagen"
        ],
        "title": "GRASPing Anatomy to Improve Pathology Segmentation",
        "abstract": "arXiv:2508.03374v1 Announce Type: new  Abstract: Radiologists rely on anatomical understanding to accurately delineate pathologies, yet most current deep learning approaches use pure pattern recognition and ignore the anatomical context in which pathologies develop. To narrow this gap, we introduce GRASP (Guided Representation Alignment for the Segmentation of Pathologies), a modular plug-and-play framework that enhances pathology segmentation models by leveraging existing anatomy segmentation models through pseudolabel integration and feature alignment. Unlike previous approaches that obtain anatomical knowledge via auxiliary training, GRASP integrates into standard pathology optimization regimes without retraining anatomical components. We evaluate GRASP on two PET/CT datasets, conduct systematic ablation studies, and investigate the framework's inner workings. We find that GRASP consistently achieves top rankings across multiple evaluation metrics and diverse architectures. The framework's dual anatomy injection strategy, combining anatomical pseudo-labels as input channels with transformer-guided anatomical feature fusion, effectively incorporates anatomical context.",
        "arxiv_id": "2508.03374",
        "ARXIVID": "2508.03374",
        "COMMENT": "This paper introduces GRASP, a framework that injects anatomical knowledge into pathology segmentation models via feature alignment and pseudo-labels. While it is a clever method for improving segmentation, it does not directly address any of the four criteria, though it is relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03516": {
        "authors": [
            "Shiben Liu",
            "Mingyue Xu",
            "Huijie Fan",
            "Qiang Wang",
            "Yandong Tang",
            "Zhi Han"
        ],
        "title": "Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification",
        "abstract": "arXiv:2508.03516v1 Announce Type: new  Abstract: Lifelong person re-identification (LReID) encounters a key challenge: balancing the preservation of old knowledge with adaptation to new information. Existing LReID methods typically employ knowledge distillation to enforce representation alignment. However, these approaches ignore two crucial aspects: specific distribution awareness and cross-domain unified knowledge learning, both of which are essential for addressing this challenge. To overcome these limitations, we propose a novel distribution-aware knowledge unification and association (DKUA) framework where domain-style modeling is performed for each instance to propagate domain-specific representations, enhancing anti-forgetting and generalization capacity. Specifically, we design a distribution-aware model to transfer instance-level representations of the current domain into the domain-specific representations with the different domain styles, preserving learned knowledge without storing old samples. Next, we propose adaptive knowledge consolidation (AKC) to dynamically generate the unified representation as a cross-domain representation center. To further mitigate forgetting, we develop a unified knowledge association (UKA) mechanism, which explores the unified representation as a bridge to explicitly model inter-domain associations, reducing inter-domain gaps. Finally, distribution-based knowledge transfer (DKT) is proposed to prevent the current domain distribution from deviating from the cross-domain distribution center, improving adaptation capacity. Experimental results show our DKUA outperforms the existing methods by 7.6%/5.3% average mAP/R@1 improvement on anti-forgetting and generalization capacity, respectively. Our code will be publicly released.",
        "arxiv_id": "2508.03516",
        "ARXIVID": "2508.03516",
        "COMMENT": "This paper proposes a new framework for lifelong person re-identification with distribution-aware knowledge unification and association. While it introduces new methods for continual learning in vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about representation learning and knowledge transfer in re-ID.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03189": {
        "authors": [
            "Tianshuo Zhang",
            "Siran Peng",
            "Li Gao",
            "Haoyuan Zhang",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "title": "Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection",
        "abstract": "arXiv:2508.03189v1 Announce Type: new  Abstract: The rapid advancements in face forgery techniques necessitate that detectors continuously adapt to new forgery methods, thus situating face forgery detection within a continual learning paradigm. However, when detectors learn new forgery types, their performance on previous types often degrades rapidly, a phenomenon known as catastrophic forgetting. Kolmogorov-Arnold Networks (KANs) utilize locally plastic splines as their activation functions, enabling them to learn new tasks by modifying only local regions of the functions while leaving other areas unaffected. Therefore, they are naturally suitable for addressing catastrophic forgetting. However, KANs have two significant limitations: 1) the splines are ineffective for modeling high-dimensional images, while alternative activation functions that are suitable for images lack the essential property of locality; 2) in continual learning, when features from different domains overlap, the mapping of different domains to distinct curve regions always collapses due to repeated modifications of the same regions. In this paper, we propose a KAN-based Continual Face Forgery Detection (KAN-CFD) framework, which includes a Domain-Group KAN Detector (DG-KD) and a data-free replay Feature Separation strategy via KAN Drift Compensation Projection (FS-KDCP). DG-KD enables KANs to fit high-dimensional image inputs while preserving locality and local plasticity. FS-KDCP avoids the overlap of the KAN input spaces without using data from prior tasks. Experimental results demonstrate that the proposed method achieves superior performance while notably reducing forgetting.",
        "arxiv_id": "2508.03189",
        "ARXIVID": "2508.03189",
        "COMMENT": "This paper introduces a KAN-based continual learning method for face forgery detection, focusing on catastrophic forgetting and high-dimensional image modeling. While it is a novel continual learning method, it does not directly match the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02927": {
        "authors": [
            "Srikanth Muralidharan",
            "Heitor R. Medeiros",
            "Masih Aminbeidokhti",
            "Eric Granger",
            "Marco Pedersoli"
        ],
        "title": "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?",
        "abstract": "arXiv:2508.02927v1 Announce Type: new  Abstract: Many real-world applications require recognition models that are robust to different operational conditions and modalities, but at the same time run on small embedded devices, with limited hardware. While for normal size models, pre-training is known to be very beneficial in accuracy and robustness, for small models, that can be employed for embedded and edge devices, its effect is not clear. In this work, we investigate the effect of ImageNet pretraining on increasingly small backbone architectures (ultra-small models, with $<$1M parameters) with respect to robustness in downstream object detection tasks in the infrared visual modality. Using scaling laws derived from standard object recognition architectures, we construct two ultra-small backbone families and systematically study their performance. Our experiments on three different datasets reveal that while ImageNet pre-training is still useful, beyond a certain capacity threshold, it offers diminishing returns in terms of out-of-distribution detection robustness. Therefore, we advise practitioners to still use pre-training and, when possible avoid too small models as while they might work well for in-domain problems, they are brittle when working conditions are different.",
        "arxiv_id": "2508.02927",
        "ARXIVID": "2508.02927",
        "COMMENT": "Does not match any specific criterion. Focuses on the effect of ImageNet pretraining for ultra-small ConvNets in infrared object detection, which is not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03054": {
        "authors": [
            "Rui Pu",
            "Chaozhuo Li",
            "Rui Ha",
            "Litian Zhang",
            "Lirong Qiu",
            "Xi Zhang"
        ],
        "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning",
        "abstract": "arXiv:2508.03054v1 Announce Type: new  Abstract: Defending large language models (LLMs) against jailbreak attacks is essential for their safe and reliable deployment. Existing defenses often rely on shallow pattern matching, which struggles to generalize to novel and unseen attack strategies. To address this challenge, we propose the Cognitive-Driven Defense (CDD) framework, which targets the underlying structure of jailbreak prompts by applying meta-operations, defined as basic manipulations that conceal harmful intent.CDD emulates human cognitive reasoning through a structured reasoning chain. It begins with a global perception of the prompt and follows with a localized analysis to uncover hidden manipulations. By applying supervised fine-tuning on this structured chain, the model learns to identify and reason about known manipulation patterns. To enhance generalization to unseen threats, an entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to encourage exploration of new types and variants of meta-operations. Experiments demonstrate that CDD can achieve state-of-the-art defense performance and exhibit strong generalization to unseen jailbreak attacks.",
        "arxiv_id": "2508.03054",
        "ARXIVID": "2508.03054",
        "COMMENT": "Does not match any specific criterion. Focuses on LLM jailbreak defense, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}