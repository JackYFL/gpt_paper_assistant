{
    "2512.08016": {
        "authors": [
            "Jiyoon Pyo",
            "Yuankun Jiao",
            "Dongwon Jung",
            "Zekun Li",
            "Leeje Jang",
            "Sofia Kirsanova",
            "Jina Kim",
            "Yijun Lin",
            "Qin Liu",
            "Junyi Xie",
            "Hadi Askari",
            "Nan Xu",
            "Muhao Chen",
            "Yao-Yi Chiang"
        ],
        "title": "FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models",
        "abstract": "arXiv:2512.08016v1 Announce Type: new  Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.",
        "arxiv_id": "2512.08016",
        "ARXIVID": "2512.08016",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (new benchmark for spatial reasoning in LVLMs). Introduces FRIEDA, a benchmark for multi-step cartographic reasoning, focusing on spatial relations and multi-step inference, and evaluates state-of-the-art LVLMs, revealing a significant gap in spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.08889": {
        "authors": [
            "Damiano Marsili",
            "Georgia Gkioxari"
        ],
        "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers",
        "abstract": "arXiv:2512.08889v1 Announce Type: new  Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",
        "arxiv_id": "2512.08889",
        "ARXIVID": "2512.08889",
        "COMMENT": "This paper introduces an annotation-free training framework for visual reasoning using multimodal verifiers, focusing on spatial reasoning and grounding. It matches criterion 1 (spatial understanding in embodied agents) and criterion 2 (new VLLMs/MLLMs), as it leverages both LLMs and VLMs for spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.08410": {
        "authors": [
            "Tao Chen",
            "Shaobo Ju",
            "Qiong Wu",
            "Chenxin Fang",
            "Kun Zhang",
            "Jun Peng",
            "Hui Li",
            "Yiyi Zhou",
            "Rongrong Ji"
        ],
        "title": "Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval",
        "abstract": "arXiv:2512.08410v1 Announce Type: new  Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.",
        "arxiv_id": "2512.08410",
        "ARXIVID": "2512.08410",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes OneClip-RAG, a new paradigm for efficient long video understanding in MLLMs, with a novel query-guided chunking and retrieval algorithm, and demonstrates significant efficiency and performance gains.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08358": {
        "authors": [
            "Jiahao Lu",
            "Weitao Xiong",
            "Jiacheng Deng",
            "Peng Li",
            "Tianyu Huang",
            "Zhiyang Dou",
            "Cheng Lin",
            "Sai-Kit Yeung",
            "Yuan Liu"
        ],
        "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
        "abstract": "arXiv:2512.08358v1 Announce Type: new  Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
        "arxiv_id": "2512.08358",
        "ARXIVID": "2512.08358",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 3 (new method for 3D tracking in a world-centric coordinate system). Proposes TrackingWorld, a pipeline for dense 3D tracking of almost all pixels, separating camera and dynamic motion, and generalizing to new objects.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08228": {
        "authors": [
            "Jusheng Zhang",
            "Kaitong Cai",
            "Xiaoyang Guo",
            "Sidi Liu",
            "Qinhan Lv",
            "Ruiqi Chen",
            "Jing Yang",
            "Yijia Fan",
            "Xiaofei Sun",
            "Jian Wang",
            "Ziliang Chen",
            "Liang Lin",
            "Keze Wang"
        ],
        "title": "MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models",
        "abstract": "arXiv:2512.08228v1 Announce Type: new  Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.",
        "arxiv_id": "2512.08228",
        "ARXIVID": "2512.08228",
        "COMMENT": "This paper introduces MM-CoT, a benchmark for probing visual chain-of-thought reasoning in multimodal models, focusing on visual grounding and logical coherence. This is a direct match for criterion 2 (new MLLMs/VLLMs) and criterion 3 (new benchmarks for embodied AI/multimodal reasoning).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08923": {
        "authors": [
            "Angela van Sprang",
            "Laurens Samson",
            "Ana Lucic",
            "Erman Acar",
            "Sennay Ghebreab",
            "Yuki M. Asano"
        ],
        "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
        "abstract": "arXiv:2512.08923v1 Announce Type: new  Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",
        "arxiv_id": "2512.08923",
        "ARXIVID": "2512.08923",
        "COMMENT": "This paper introduces two new benchmarks (REST and REST+) for evaluating cross-modal inconsistency in MLLMs, and provides empirical analysis of modality gaps in state-of-the-art models. This is a direct match for criterion 2 (new MLLMs/VLLMs) and also provides surprising empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08411": {
        "authors": [
            "Mingwei Li",
            "Xiaoyuan Zhang",
            "Chengwei Yang",
            "Zilong Zheng",
            "Yaodong Yang"
        ],
        "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems",
        "abstract": "arXiv:2512.08411v1 Announce Type: new  Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.",
        "arxiv_id": "2512.08411",
        "ARXIVID": "2512.08411",
        "COMMENT": "This paper proposes the Prismatic World Model (PRISM-WM), a Mixture-of-Experts world model for planning in hybrid robotic systems, focusing on compositional dynamics and mode transitions. It matches criterion 3 (embodied AI, new methods for simulators/benchmarks, novel angles in model-based planning).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.08240": {
        "authors": [
            "Jusheng Zhang",
            "Xiaoyang Guo",
            "Kaitong Cai",
            "Qinhan Lv",
            "Yijia Fan",
            "Wenhao Chai",
            "Jian Wang",
            "Keze Wang"
        ],
        "title": "HybridToken-VLM: Hybrid Token Compression for Vision-Language Models",
        "abstract": "arXiv:2512.08240v1 Announce Type: new  Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.",
        "arxiv_id": "2512.08240",
        "ARXIVID": "2512.08240",
        "COMMENT": "This paper presents HTC-VLM, a hybrid token compression framework for vision-language models, directly addressing the efficiency-fidelity trade-off in VLMs. It matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2512.08269": {
        "authors": [
            "Taewoong Kang",
            "Kinam Kim",
            "Dohyeon Kim",
            "Minho Park",
            "Junha Hyung",
            "Jaegul Choo"
        ],
        "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
        "abstract": "arXiv:2512.08269v1 Announce Type: new  Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
        "arxiv_id": "2512.08269",
        "ARXIVID": "2512.08269",
        "COMMENT": "EgoX introduces a novel framework for generating egocentric videos from exocentric input using large-scale video diffusion models and geometry-guided self-attention. This is a new generative modeling approach in multi-modal learning and vision foundation models (criterion 4), and also relates to spatial intelligence (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.08881": {
        "authors": [
            "Aysim Toker",
            "Andreea-Maria Oncescu",
            "Roy Miles",
            "Ismail Elezi",
            "Jiankang Deng"
        ],
        "title": "SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing",
        "abstract": "arXiv:2512.08881v1 Announce Type: new  Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.",
        "arxiv_id": "2512.08881",
        "ARXIVID": "2512.08881",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications). Proposes SATGround, a spatially-aware approach for visual grounding in remote sensing using VLMs, with a novel structured localization mechanism and improved spatial reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.08498": {
        "authors": [
            "Yijia Guo",
            "Tong Hu",
            "Zhiwei Li",
            "Liwen Hu",
            "Keming Qian",
            "Xitong Lin",
            "Shengbo Chen",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs",
        "abstract": "arXiv:2512.08498v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.",
        "arxiv_id": "2512.08498",
        "ARXIVID": "2512.08498",
        "COMMENT": "This paper presents a new on-the-fly 3D reconstruction framework for multi-camera rigs, with novel methods for alignment and optimization. It is a strong match for criterion 3 (embodied AI, new benchmarks/methods, simulator-related, novel angles).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.08924": {
        "authors": [
            "Chuhan Zhang",
            "Guillaume Le Moing",
            "Skanda Koppula",
            "Ignacio Rocco",
            "Liliane Momeni",
            "Junyu Xie",
            "Shuyang Sun",
            "Rahul Sukthankar",
            "Jo\\\"elle K Barral",
            "Raia Hadsell",
            "Zoubin Ghahramani",
            "Andrew Zisserman",
            "Junlin Zhang",
            "Mehdi SM Sajjadi"
        ],
        "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "abstract": "arXiv:2512.08924v1 Announce Type: new  Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.",
        "arxiv_id": "2512.08924",
        "ARXIVID": "2512.08924",
        "COMMENT": "This paper introduces D4RT, a transformer-based model for efficient 4D dynamic scene reconstruction from video, with a novel querying mechanism for spatio-temporal geometry. This is a strong match for criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.08854": {
        "authors": [
            "Jack Brady",
            "Bernhard Sch\\\"olkopf",
            "Thomas Kipf",
            "Simon Buchholz",
            "Wieland Brendel"
        ],
        "title": "Generation is Required for Data-Efficient Perception",
        "abstract": "arXiv:2512.08854v1 Announce Type: new  Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.",
        "arxiv_id": "2512.08854",
        "ARXIVID": "2512.08854",
        "COMMENT": "Matches criterion 4 (vision foundation models and generative modeling in perception). The paper provides theoretical and empirical analysis on the necessity of generative models for compositional generalization in vision.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.08296": {
        "authors": [
            "Yubin Kim",
            "Ken Gu",
            "Chanwoo Park",
            "Chunjong Park",
            "Samuel Schmidgall",
            "A. Ali Heydari",
            "Yao Yan",
            "Zhihan Zhang",
            "Yuchen Zhuang",
            "Mark Malhotra",
            "Paul Pu Liang",
            "Hae Won Park",
            "Yuzhe Yang",
            "Xuhai Xu",
            "Yilun Du",
            "Shwetak Patel",
            "Tim Althoff",
            "Daniel McDuff",
            "Xin Liu"
        ],
        "title": "Towards a Science of Scaling Agent Systems",
        "abstract": "arXiv:2512.08296v1 Announce Type: new  Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
        "arxiv_id": "2512.08296",
        "ARXIVID": "2512.08296",
        "COMMENT": "Matches criterion 1 (methodological insights into scaling agent systems, with empirical and statistical analysis of agent coordination and performance). Also relevant to criterion 3 (novel benchmark and evaluation framework for agentic scaling).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.08529": {
        "authors": [
            "Yunzhu Zhang",
            "Zeyu Pan",
            "Zhengwen Zeng",
            "Shuheng Shen",
            "Changhua Meng",
            "Linchao Zhu"
        ],
        "title": "MVP: Multiple View Prediction Improves GUI Grounding",
        "abstract": "arXiv:2512.08529v1 Announce Type: new  Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.",
        "arxiv_id": "2512.08529",
        "ARXIVID": "2512.08529",
        "COMMENT": "MVP proposes a multi-view prediction framework for GUI grounding, improving spatial localization in vision-language models. This is a methodological improvement in spatial understanding for embodied agents (criterion 1), and also relates to vision-language models (criterion 2).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08868": {
        "authors": [
            "Rui Min (May)",
            "Zile Qiao (May)",
            "Ze Xu (May)",
            "Jiawen Zhai (May)",
            "Wenyu Gao (May)",
            "Xuanzhong Chen (May)",
            "Haozhen Sun (May)",
            "Zhen Zhang (May)",
            "Xinyu Wang (May)",
            "Hong Zhou (May)",
            "Wenbiao Yin (May)",
            "Xuan Zhou (May)",
            "Yong Jiang (May)",
            "Haicheng Liu (May)",
            "Liang Ding (May)",
            "Ling Zou (May)",
            "Yi R. (May)",
            "Fung",
            "Yalong Li",
            "Pengjun Xie"
        ],
        "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
        "abstract": "arXiv:2512.08868v1 Announce Type: new  Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
        "arxiv_id": "2512.08868",
        "ARXIVID": "2512.08868",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied/agentic AI in a real-world e-commerce setting). Introduces EcomBench, a holistic benchmark for evaluating foundation agents in realistic e-commerce environments, focusing on multi-step reasoning and cross-source knowledge integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08503": {
        "authors": [
            "Jiaming Zhang",
            "Che Wang",
            "Yang Cao",
            "Longtao Huang",
            "Wei Yang Bryan Lim"
        ],
        "title": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models",
        "abstract": "arXiv:2512.08503v1 Announce Type: new  Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.",
        "arxiv_id": "2512.08503",
        "ARXIVID": "2512.08503",
        "COMMENT": "This paper introduces ReasonBreak, an adversarial framework to disrupt hierarchical reasoning in MLRMs for geographic privacy, and a new dataset. It is a direct match for criterion 2 (new MLLMs/MLRMs) and criterion 3 (novel methods for embodied/multimodal reasoning).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08524": {
        "authors": [
            "Jawad Ibn Ahad",
            "Maisha Rahman",
            "Amrijit Biswas",
            "Muhammad Rafsan Kabir",
            "Robin Krambroeckers",
            "Sifat Momen",
            "Nabeel Mohammed",
            "Shafin Rahman"
        ],
        "title": "Beyond Real Weights: Hypercomplex Representations for Stable Quantization",
        "abstract": "arXiv:2512.08524v1 Announce Type: new  Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.",
        "arxiv_id": "2512.08524",
        "ARXIVID": "2512.08524",
        "COMMENT": "Matches criterion 2 (new method for compressing and improving efficiency of MLLMs/VLLMs using hypercomplex representations and progressive reparameterization). The paper is directly about improving vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08743": {
        "authors": [
            "Shuyue Hu",
            "Haoyang Yan",
            "Yiqun Zhang",
            "Yang Chen",
            "Dongzhan Zhou",
            "Lei Bai"
        ],
        "title": "Towards Foundation Models with Native Multi-Agent Intelligence",
        "abstract": "arXiv:2512.08743v1 Announce Type: new  Abstract: Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.",
        "arxiv_id": "2512.08743",
        "ARXIVID": "2512.08743",
        "COMMENT": "Matches criterion 1 (multi-agent intelligence as a new methodological improvement for foundation models in agent settings) and criterion 2 (discussion of foundation models and their multi-agent capabilities). The paper provides empirical evidence and outlines research directions for native multi-agent intelligence in foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08645": {
        "authors": [
            "Young Kyung Kim",
            "Oded Schlesinger",
            "Yuzhou Zhao",
            "J. Matias Di Martino",
            "Guillermo Sapiro"
        ],
        "title": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation",
        "abstract": "arXiv:2512.08645v1 Announce Type: new  Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",
        "arxiv_id": "2512.08645",
        "ARXIVID": "2512.08645",
        "COMMENT": "This paper introduces the Chain-of-Image Generation (CoIG) framework, which brings chain-of-thought style reasoning to image generation for improved monitorability and control. It matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), as it proposes a novel, model-agnostic framework for image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.08294": {
        "authors": [
            "Yexin Liu",
            "Manyuan Zhang",
            "Yueze Wang",
            "Hongyu Li",
            "Dian Zheng",
            "Weiming Zhang",
            "Changsheng Lu",
            "Xunliang Cai",
            "Yan Feng",
            "Peng Pei",
            "Harry Yang"
        ],
        "title": "OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation",
        "abstract": "arXiv:2512.08294v1 Announce Type: new  Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.",
        "arxiv_id": "2512.08294",
        "ARXIVID": "2512.08294",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (VLLMs/MLLMs) as it uses vision-language models for dataset construction and evaluation. Also introduces a new benchmark for subject-driven generation and manipulation, which is related to criterion 3.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08511": {
        "authors": [
            "Wenxi Yang",
            "Yuzhong Zhao",
            "Fang Wan",
            "Qixiang Ye"
        ],
        "title": "Thinking with Images via Self-Calling Agent",
        "abstract": "arXiv:2512.08511v1 Announce Type: new  Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\\%$ with $\\sim 75\\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
        "arxiv_id": "2512.08511",
        "ARXIVID": "2512.08511",
        "COMMENT": "This paper proposes Self-Calling Chain-of-Thought (sCoT), a new visual reasoning paradigm for interleaved multimodal CoT, improving reasoning in vision-language models (criterion 2).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08535": {
        "authors": [
            "Xinyue Liang",
            "Zhinyuan Ma",
            "Lingchen Sun",
            "Yanjun Guo",
            "Lei Zhang"
        ],
        "title": "Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement",
        "abstract": "arXiv:2512.08535v1 Announce Type: new  Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.",
        "arxiv_id": "2512.08535",
        "ARXIVID": "2512.08535",
        "COMMENT": "Photo3D introduces a new framework for photorealistic 3D generation using GPT-4o-Image model data, with a structure-aligned multi-view synthesis pipeline and detail enhancement. This is relevant to vision foundation models and their applications (criterion 4), and also touches on generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08237": {
        "authors": [
            "Yuanpeng Chen",
            "Hui Song",
            "Wei Tao",
            "ShanHui Mo",
            "Shuang Zhang",
            "Xiao Hua",
            "TianKun Zhao"
        ],
        "title": "FastBEV++: Fast by Algorithm, Deployable by Design",
        "abstract": "arXiv:2512.08237v1 Announce Type: new  Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the \"Deployable by Design\" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is \"Fast by Algorithm\", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev",
        "arxiv_id": "2512.08237",
        "ARXIVID": "2512.08237",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). FastBEV++ is a new framework for efficient BEV perception, introducing a deployable, high-performance algorithm for camera-only BEV, with a novel view transformation paradigm and depth-aware fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08221": {
        "authors": [
            "Ziwei Yao",
            "Qiyang Wan",
            "Ruiping Wang",
            "Xilin Chen"
        ],
        "title": "VisKnow: Constructing Visual Knowledge Base for Object Understanding",
        "abstract": "arXiv:2512.08221v1 Announce Type: new  Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.",
        "arxiv_id": "2512.08221",
        "ARXIVID": "2512.08221",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes VisKnow, a framework for constructing a visual knowledge base for object understanding, integrating multi-modal data and supporting tasks like zero-shot recognition and VQA.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08478": {
        "authors": [
            "Yuning Gong",
            "Yifei Liu",
            "Yifan Zhan",
            "Muyao Niu",
            "Xueying Li",
            "Yuanjun Liao",
            "Jiaming Chen",
            "Yuanyuan Gao",
            "Jiaqi Chen",
            "Minming Chen",
            "Li Zhou",
            "Yuning Zhang",
            "Wei Wang",
            "Xiaoqing Hou",
            "Huaxi Huang",
            "Shixiang Tang",
            "Le Ma",
            "Dingwen Zhang",
            "Xue Yang",
            "Junchi Yan",
            "Yanchi Zhang",
            "Yinqiang Zheng",
            "Xiao Sun",
            "Zhihang Zhong"
        ],
        "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
        "abstract": "arXiv:2512.08478v1 Announce Type: new  Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
        "arxiv_id": "2512.08478",
        "ARXIVID": "2512.08478",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications, specifically in neural rendering and world models). The paper introduces a web-native platform for 3D Gaussian Splatting and generative models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.08537": {
        "authors": [
            "Zhen Zou",
            "Xiaoxiao Ma",
            "Jie Huang",
            "Zichao Yu",
            "Feng Zhao"
        ],
        "title": "Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation",
        "abstract": "arXiv:2512.08537v1 Announce Type: new  Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\\times$256, TransDiff attains 4.3$\\times$ lossless speedup, and NextStep-1 achieves 3$\\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.",
        "arxiv_id": "2512.08537",
        "ARXIVID": "2512.08537",
        "COMMENT": "Relevant to generative modeling and vision foundation models (criterion 4). Proposes Fast-ARDiff, an acceleration framework for AR-diffusion models, introducing entropy-informed speculative decoding and joint distillation for faster, high-quality synthesis.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.08648": {
        "authors": [
            "Shaofeng Zhang",
            "Xuanqi Chen",
            "Ning Liao",
            "Haoxiang Zhao",
            "Xiaoxing Wang",
            "Haoru Tan",
            "Sitong Wu",
            "Xiaosong Jia",
            "Qi Fan",
            "Junchi Yan"
        ],
        "title": "Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank",
        "abstract": "arXiv:2512.08648v1 Announce Type: new  Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\\mname} achieves a state-of-the-art FID of \\textbf{2.40} within 400k steps, significantly outperforming comparable methods.",
        "arxiv_id": "2512.08648",
        "ARXIVID": "2512.08648",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling. Proposes a new contrastive memory bank method for generative models, improving efficiency and quality without relying on pretrained vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.08317": {
        "authors": [
            "Xuhui Li",
            "Zhengquan Luo",
            "Zihui Cui",
            "Zhiqiang Xu"
        ],
        "title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation",
        "abstract": "arXiv:2512.08317v1 Announce Type: new  Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \\textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.",
        "arxiv_id": "2512.08317",
        "ARXIVID": "2512.08317",
        "COMMENT": "This paper proposes a geometry-aware distribution matching framework for dataset distillation, introducing manifold-aware methods for better data representation. This is a methodological improvement relevant to spatial understanding (criterion 1), as it considers intrinsic data geometry.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.08673": {
        "authors": [
            "Shaofeng Zhang",
            "Xuanqi Chen",
            "Xiangdong Zhang",
            "Sitong Wu",
            "Junchi Yan"
        ],
        "title": "Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds",
        "abstract": "arXiv:2512.08673v1 Announce Type: new  Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \\textbf{C}enter-\\textbf{S}urrounding \\textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \\textbf{7.9\\%}, \\textbf{6.7\\%}, and \\textbf{10.3\\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.",
        "arxiv_id": "2512.08673",
        "ARXIVID": "2512.08673",
        "COMMENT": "Matches criterion 1 (new contrastive learning method for 3D point clouds, improving spatial understanding in 3D data). The dual-branch center-surrounding contrast is a novel approach for 3D representation learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.08247": {
        "authors": [
            "Haowen Zheng",
            "Hu Zhu",
            "Lu Deng",
            "Weihao Gu",
            "Yang Yang",
            "Yanyan Liang"
        ],
        "title": "Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection",
        "abstract": "arXiv:2512.08247v1 Announce Type: new  Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.",
        "arxiv_id": "2512.08247",
        "ARXIVID": "2512.08247",
        "COMMENT": "Matches criterion 1 (methodological improvement in spatial-temporal understanding for embodied agents, specifically in 3D object detection with future temporal knowledge distillation). The paper introduces a new distillation method for temporal 3D object detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.08730": {
        "authors": [
            "Kaiyu Li",
            "Shengqi Zhang",
            "Yupeng Deng",
            "Zhi Wang",
            "Deyu Meng",
            "Xiangyong Cao"
        ],
        "title": "SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images",
        "abstract": "arXiv:2512.08730v1 Announce Type: new  Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.",
        "arxiv_id": "2512.08730",
        "ARXIVID": "2512.08730",
        "COMMENT": "This paper explores the use of SAM 3 (Segment Anything Model 3) for open-vocabulary semantic segmentation in remote sensing, adapting a vision foundation model to a new domain. It matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.08564": {
        "authors": [
            "Mahmoud Afifi",
            "Zhongling Wang",
            "Ran Zhang",
            "Michael S. Brown"
        ],
        "title": "Modular Neural Image Signal Processing",
        "abstract": "arXiv:2512.08564v1 Announce Type: new  Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
        "arxiv_id": "2512.08564",
        "ARXIVID": "2512.08564",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it presents a modular neural image signal processing framework with a learning-based approach and foundation model-like flexibility. Also relevant to computer vision and generative modeling interests.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.08337": {
        "authors": [
            "Jianwei Wang",
            "Qing Wang",
            "Menglan Ruan",
            "Rongjun Ge",
            "Chunfeng Yang",
            "Yang Chen",
            "Chunming Xie"
        ],
        "title": "DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation",
        "abstract": "arXiv:2512.08337v1 Announce Type: new  Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.",
        "arxiv_id": "2512.08337",
        "ARXIVID": "2512.08337",
        "COMMENT": "DINO-BOLDNet uses a DINOv3-guided attention network for generating BOLD images from T1w images, leveraging vision foundation models (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.08254": {
        "authors": [
            "Yun Liu",
            "Tao Li",
            "Cosmin Ancuti",
            "Wenqi Ren",
            "Weisi Lin"
        ],
        "title": "SFP: Real-World Scene Recovery Using Spatial and Frequency Priors",
        "abstract": "arXiv:2512.08254v1 Announce Type: new  Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.",
        "arxiv_id": "2512.08254",
        "ARXIVID": "2512.08254",
        "COMMENT": "This paper proposes new spatial and frequency priors for real-world scene recovery, which is a methodological improvement in spatial understanding (criterion 1).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.08329": {
        "authors": [
            "Michael R. Martin",
            "Garrick Chan",
            "Kwan-Liu Ma"
        ],
        "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models",
        "abstract": "arXiv:2512.08329v1 Announce Type: new  Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.",
        "arxiv_id": "2512.08329",
        "ARXIVID": "2512.08329",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications) as it analyzes image protection methods for diffusion models, which are a type of vision foundation model. Provides empirical insights into adversarial protection mechanisms.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.08477": {
        "authors": [
            "Huiguo He",
            "Pengyu Yan",
            "Ziqi Yi",
            "Weizhi Zhong",
            "Zheng Liu",
            "Yejun Tang",
            "Huan Yang",
            "Kun Gai",
            "Guanbin Li",
            "Lianwen Jin"
        ],
        "title": "ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention",
        "abstract": "arXiv:2512.08477v1 Announce Type: new  Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.",
        "arxiv_id": "2512.08477",
        "ARXIVID": "2512.08477",
        "COMMENT": "This paper presents ContextDrag, a new drag-based image editing paradigm leveraging context-preserving token injection and position-consistent attention. It is an application of vision foundation models but does not introduce a new VLLM/MLLM or address spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.08534": {
        "authors": [
            "Zhangli Hu",
            "Ye Chen",
            "Jiajun Yao",
            "Bingbing Ni"
        ],
        "title": "PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation",
        "abstract": "arXiv:2512.08534v1 Announce Type: new  Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.",
        "arxiv_id": "2512.08534",
        "ARXIVID": "2512.08534",
        "COMMENT": "This paper presents a unified multimodal framework for oil painting generation and editing, leveraging spatial alignment, semantic enhancement, and self-supervised style transfer. It matches criterion 4 (vision foundation models and applications) and partially criterion 2 (multimodal models for image generation/editing).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.08445": {
        "authors": [
            "Madhav Gupta",
            "Vishak Prasad C",
            "Ganesh Ramakrishnan"
        ],
        "title": "Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts",
        "abstract": "arXiv:2512.08445v1 Announce Type: new  Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.",
        "arxiv_id": "2512.08445",
        "ARXIVID": "2512.08445",
        "COMMENT": "This paper proposes an uncertainty-aware subset selection framework for robust visual explainability under distribution shifts. While it is a methodological improvement in vision explainability, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.08751": {
        "authors": [
            "Kuniko Paxton",
            "Koorosh Aslansefat",
            "Dhavalkumar Thakker",
            "Yiannis Papadopoulos"
        ],
        "title": "Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices",
        "abstract": "arXiv:2512.08751v1 Announce Type: new  Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.",
        "arxiv_id": "2512.08751",
        "ARXIVID": "2512.08751",
        "COMMENT": "This paper proposes a skewness-guided pruning method for multimodal Swin Transformers in federated learning for medical imaging. While it involves multimodal models, it is not a new VLLM/MLLM or vision foundation model, nor does it focus on spatial intelligence or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.08253": {
        "authors": [
            "YiLin Zhou",
            "Lili Wei",
            "Zheming Xu",
            "Ziyi Chen",
            "Congyan Lang"
        ],
        "title": "Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation",
        "abstract": "arXiv:2512.08253v1 Announce Type: new  Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.",
        "arxiv_id": "2512.08253",
        "ARXIVID": "2512.08253",
        "COMMENT": "This paper introduces Query-aware Hub Prototype learning for few-shot 3D point cloud semantic segmentation, focusing on prototype learning and semantic alignment. While it is relevant to spatial understanding, it does not directly address embodied agents or spatial intelligence in the context of embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.08334": {
        "authors": [
            "Chang Liu",
            "Hongliang Yuan",
            "Lianghao Zhang",
            "Sichao Wang",
            "Jianwei Guo",
            "Shi-Sheng Huang"
        ],
        "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting",
        "abstract": "arXiv:2512.08334v1 Announce Type: new  Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.",
        "arxiv_id": "2512.08334",
        "ARXIVID": "2512.08334",
        "COMMENT": "HybridSplat presents a new method for fast, high-fidelity scene reconstruction using hybrid splatting for Gaussian primitives, focusing on rendering and memory efficiency. While it is a novel rendering method, it does not directly match the listed criteria but is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.08163": {
        "authors": [
            "Yuki Kubota",
            "Taiki Fukiage"
        ],
        "title": "Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators",
        "abstract": "arXiv:2512.08163v1 Announce Type: new  Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.",
        "arxiv_id": "2512.08163",
        "ARXIVID": "2512.08163",
        "COMMENT": "This paper analyzes the relationship between accuracy and human-likeness in monocular depth estimators, providing empirical insights into model behavior. While it is insightful, it does not propose new methods or models, so it is only tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.08378": {
        "authors": [
            "Jing Tao",
            "You Li",
            "Banglei Guan",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "title": "Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions",
        "abstract": "arXiv:2512.08378v1 Announce Type: new  Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.",
        "arxiv_id": "2512.08378",
        "ARXIVID": "2512.08378",
        "COMMENT": "This paper proposes a new framework for simultaneous image enhancement and noise suppression under complex illumination, using a gradient-domain filter and Retinex model. It is a methodological improvement in image processing, but not directly related to spatial intelligence on embodied agents or the other criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.08227": {
        "authors": [
            "Md Eimran Hossain Eimon",
            "Ashan Perera",
            "Juan Merlos",
            "Velibor Adzic",
            "Hari Kalva"
        ],
        "title": "New VVC profiles targeting Feature Coding for Machines",
        "abstract": "arXiv:2512.08227v1 Announce Type: new  Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.",
        "arxiv_id": "2512.08227",
        "ARXIVID": "2512.08227",
        "COMMENT": "This paper introduces new VVC profiles for feature coding in machine vision, focusing on efficient compression for downstream vision tasks. While relevant to computer vision, it does not directly address any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.07926": {
        "authors": [
            "Arvind Agarwal",
            "Lisa Amini",
            "Sameep Mehta",
            "Horst Samulowitz",
            "Kavitha Srinivas"
        ],
        "title": "Can AI autonomously build, operate, and use the entire data stack?",
        "abstract": "arXiv:2512.07926v1 Announce Type: new  Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.",
        "arxiv_id": "2512.07926",
        "ARXIVID": "2512.07926",
        "COMMENT": "This paper discusses autonomous data management using AI agents, but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about data systems automation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.07951": {
        "authors": [
            "Zekai Luo",
            "Zongze Du",
            "Zhouhang Zhu",
            "Hao Zhong",
            "Muzhi Zhu",
            "Wen Wang",
            "Yuling Xi",
            "Chenchen Jing",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
        "abstract": "arXiv:2512.07951v1 Announce Type: new  Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
        "arxiv_id": "2512.07951",
        "ARXIVID": "2512.07951",
        "COMMENT": "This paper introduces LivingSwap, a video reference-guided face swapping model for high-fidelity, temporally consistent video editing. While it is an application of vision models, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.08697": {
        "authors": [
            "Athena Psalta",
            "Vasileios Tsironis",
            "Konstantinos Karantzalos"
        ],
        "title": "What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance",
        "abstract": "arXiv:2512.08697v1 Announce Type: new  Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID",
        "arxiv_id": "2512.08697",
        "ARXIVID": "2512.08697",
        "COMMENT": "This paper proposes a Mixture-of-Experts framework for analyzing semantic attribute importance in person re-identification. While it uses interpretable models and statistical analysis, it does not directly match any of the four criteria, but is relevant to computer vision and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.08769": {
        "authors": [
            "Eranga Bandara",
            "Ross Gore",
            "Peter Foytik",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Abdul Rahman",
            "Xueping Liang",
            "Safdar H. Bouk",
            "Amin Hass",
            "Sachini Rajapakse",
            "Ng Wee Keong",
            "Kasun De Zoysa",
            "Aruna Withanage",
            "Nilaan Loganathan"
        ],
        "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
        "abstract": "arXiv:2512.08769v1 Announce Type: new  Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
        "arxiv_id": "2512.08769",
        "ARXIVID": "2512.08769",
        "COMMENT": "Relevant to agentic AI workflows but does not directly match any specific criterion. Focuses on practical engineering and deployment of agentic AI, with a case study on multimodal news analysis, but lacks a novel methodological or empirical contribution in spatial intelligence, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.08198": {
        "authors": [
            "Yubo Chen",
            "Di Zhao",
            "Yun Sing Koh",
            "Talia Xu"
        ],
        "title": "Animal Re-Identification on Microcontrollers",
        "abstract": "arXiv:2512.08198v1 Announce Type: new  Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.",
        "arxiv_id": "2512.08198",
        "ARXIVID": "2512.08198",
        "COMMENT": "Does not match any specific criterion. The paper is about animal re-identification on microcontrollers, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.08038": {
        "authors": [
            "Elifnur Sunger",
            "Tales Imbiriba",
            "Peter Campbell",
            "Deniz Erdogmus",
            "Stratis Ioannidis",
            "Jennifer Dy"
        ],
        "title": "SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification",
        "abstract": "arXiv:2512.08038v1 Announce Type: new  Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.",
        "arxiv_id": "2512.08038",
        "ARXIVID": "2512.08038",
        "COMMENT": "Does not match any specific criterion. The paper is about explainability in medical imaging, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}