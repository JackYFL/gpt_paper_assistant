{
    "2508.16763": {
        "authors": [
            "Rabiul Awal",
            "Mahsa Massoud",
            "Aarash Feizi",
            "Zichao Li",
            "Suyuchen Wang",
            "Christopher Pal",
            "Aishwarya Agrawal",
            "David Vazquez",
            "Siva Reddy",
            "Juan A. Rodriguez",
            "Perouz Taslakian",
            "Spandana Gella",
            "Sai Rajeswar"
        ],
        "title": "WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation",
        "abstract": "arXiv:2508.16763v1 Announce Type: new  Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web tasks: (1) website visual question answering, (2) code editing involving HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks that treat these tasks separately, WebMMU unifies them using expert-annotated, real-world web data to assess models' abilities in complex multi-step reasoning, precise element grounding, and functional UI comprehension and coding. Our evaluation shows that while multimodal large language models (MLLMs) perform well on basic information extraction, they struggle with reasoning and grounding, editing code to preserve functionality, and generating design-to-code that maintains hierarchy and supports multilingual content. These findings reveal key limitations in current MLLMs and underscore the need for improved multimodal and cross-lingual reasoning to build future web agents capable of automating diverse web development tasks.",
        "arxiv_id": "2508.16763",
        "ARXIVID": "2508.16763",
        "COMMENT": "WebMMU is a new benchmark for multimodal, multilingual website understanding and code generation, evaluating MLLMs on complex web tasks. This directly matches criterion 2 (new MLLMs) and criterion 3 (new benchmarks for embodied/agentic tasks with novel evaluation angles).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.17270": {
        "authors": [
            "Xu Sun",
            "Yunqing He",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "title": "Spatial-Temporal Human-Object Interaction Detection",
        "abstract": "arXiv:2508.17270v1 Announce Type: new  Abstract: In this paper, we propose a new instance-level human-object interaction detection task on videos called ST-HOID, which aims to distinguish fine-grained human-object interactions (HOIs) and the trajectories of subjects and objects. It is motivated by the fact that HOI is crucial for human-centric video content understanding. To solve ST-HOID, we propose a novel method consisting of an object trajectory detection module and an interaction reasoning module. Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct extensive experiments to evaluate the effectiveness of our method. The experimental results demonstrate that our method outperforms the baselines generated by the state-of-the-art methods of image human-object interaction detection, video visual relation detection and video human-object interaction recognition.",
        "arxiv_id": "2508.17270",
        "ARXIVID": "2508.17270",
        "COMMENT": "This paper introduces a new instance-level spatial-temporal human-object interaction detection task (ST-HOID) and a new dataset (VidOR-HOID), directly matching criterion 3 (new embodied AI benchmarks and methods with novel angles in spatial-temporal reasoning).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.17050": {
        "authors": [
            "Xianjing Cheng",
            "Lintai Wu",
            "Zuowen Wang",
            "Junhui Hou",
            "Jie Wen",
            "Yong Xu"
        ],
        "title": "PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models",
        "abstract": "arXiv:2508.17050v1 Announce Type: new  Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on high-quality point clouds. However, LiDAR-scanned data often suffer from extreme sparsity, severely hindering downstream 3D perception tasks. Existing point cloud upsampling methods primarily focus on individual objects, thus demonstrating limited generalization capability for complex outdoor scenes. To address this issue, we propose PVNet, a diffusion model-based point-voxel interaction framework to perform LiDAR point cloud upsampling without dense supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to guide the generation, in which we employ a sparse point cloud as the guiding condition and the synthesized point clouds derived from its nearby frames as the input. Moreover, we design a voxel completion module to refine and complete the coarse voxel features for enriching the feature representation. In addition, we propose a point-voxel interaction module to integrate features from both points and voxels, which efficiently improves the environmental perception capability of each upsampled point. To the best of our knowledge, our approach is the first scene-level point cloud upsampling method supporting arbitrary upsampling rates. Extensive experiments on various benchmarks demonstrate that our method achieves state-of-the-art performance. The source code will be available at https://github.com/chengxianjing/PVNet.",
        "arxiv_id": "2508.17050",
        "ARXIVID": "2508.17050",
        "COMMENT": "Introduces PVNet, a diffusion model-based point-voxel interaction framework for LiDAR point cloud upsampling, supporting arbitrary upsampling rates and improving 3D scene understanding. This matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications), and is highly relevant for embodied AI and spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.18265": {
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lixin Gu",
            "Hengjun Pu",
            "Long Cui",
            "Xingguang Wei",
            "Zhaoyang Liu",
            "Linglin Jing",
            "Shenglong Ye",
            "Jie Shao",
            "Zhaokai Wang",
            "Zhe Chen",
            "Hongjie Zhang",
            "Ganlin Yang",
            "Haomin Wang",
            "Qi Wei",
            "Jinhui Yin",
            "Wenhao Li",
            "Erfei Cui",
            "Guanzhou Chen",
            "Zichen Ding",
            "Changyao Tian",
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Yuchen Duan",
            "Xuehui Wang",
            "Songze Li",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Nianchen Deng",
            "Bin Fu",
            "Yinan He",
            "Yi Wang",
            "Conghui He",
            "Botian Shi",
            "Junjun He",
            "Yingtong Xiong",
            "Han Lv",
            "Lijun Wu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Huipeng Deng",
            "Biqing Qi",
            "Jiaye Ge",
            "Qipeng Guo",
            "Wenwei Zhang",
            "Wanli Ouyang",
            "Limin Wang",
            "Min Dou",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Jifeng Dai",
            "Bowen Zhou",
            "Weijie Su",
            "Kai Chen",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "abstract": "arXiv:2508.18265v1 Announce Type: new  Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05$\\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
        "arxiv_id": "2508.18265",
        "ARXIVID": "2508.18265",
        "COMMENT": "Matches criterion 2 (introduces a new open-source MLLM, InternVL3.5, with novel training and deployment strategies) and criterion 4 (vision foundation model with new applications, including embodied agency). Cascade RL and ViR are notable innovations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.18040": {
        "authors": [
            "Xin Wang",
            "Zhiyao Cui",
            "Hao Li",
            "Ya Zeng",
            "Chenxu Wang",
            "Ruiqi Song",
            "Yihang Chen",
            "Kun Shao",
            "Qiaosheng Zhang",
            "Jinzhuo Liu",
            "Siyue Ren",
            "Shuyue Hu",
            "Zhen Wang"
        ],
        "title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration",
        "abstract": "arXiv:2508.18040v1 Announce Type: new  Abstract: Vision language model (VLM)-based mobile agents show great potential for assisting users in performing instruction-driven tasks. However, these agents typically struggle with personalized instructions -- those containing ambiguous, user-specific context -- a challenge that has been largely overlooked in previous research. In this paper, we define personalized instructions and introduce PerInstruct, a novel human-annotated dataset covering diverse personalized instructions across various mobile scenarios. Furthermore, given the limited personalization capabilities of existing mobile agents, we propose PerPilot, a plug-and-play framework powered by large language models (LLMs) that enables mobile agents to autonomously perceive, understand, and execute personalized user instructions. PerPilot identifies personalized elements and autonomously completes instructions via two complementary approaches: memory-based retrieval and reasoning-based exploration. Experimental results demonstrate that PerPilot effectively handles personalized tasks with minimal user intervention and progressively improves its performance with continued use, underscoring the importance of personalization-aware reasoning for next-generation mobile agents. The dataset and code are available at: https://github.com/xinwang-nwpu/PerPilot",
        "arxiv_id": "2508.18040",
        "ARXIVID": "2508.18040",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents, as it focuses on VLM-based mobile agents understanding personalized instructions in spatially grounded tasks) and criterion 2 (VLM-based agents). Also relevant to criterion 3 (novel method for embodied AI with a new dataset and framework for personalization). The PerPilot framework and PerInstruct dataset are new contributions for personalized, instruction-driven mobile agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.17012": {
        "authors": [
            "Diram Tabaa",
            "Gianni Di Caro"
        ],
        "title": "Fiducial Marker Splatting for High-Fidelity Robotics Simulations",
        "abstract": "arXiv:2508.17012v1 Announce Type: new  Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.",
        "arxiv_id": "2508.17012",
        "ARXIVID": "2508.17012",
        "COMMENT": "Matches criterion 3: Proposes a new simulator-related method for high-fidelity robotics simulation by combining Gaussian Splatting with fiducial marker rendering, addressing a novel challenge in embodied AI simulation environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.16812": {
        "authors": [
            "Xinhao Xiang",
            "Kuan-Chuan Peng",
            "Suhas Lohit",
            "Michael J. Jones",
            "Jiawei Zhang"
        ],
        "title": "Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes",
        "abstract": "arXiv:2508.16812v1 Announce Type: new  Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing methods are limited by closed-set assumptions and struggle to recognize novel objects and their attributes in real-world scenarios. We propose OVODA, a novel framework enabling both open-vocabulary 3D object and attribute detection with no need to know the novel class anchor size. OVODA uses foundation models to bridge the semantic gap between 3D features and texts while jointly detecting attributes, e.g., spatial relationships, motion states, etc. To facilitate such research direction, we propose OVAD, a new dataset that supplements existing 3D object detection benchmarks with comprehensive attribute annotations. OVODA incorporates several key innovations, including foundation model feature concatenation, prompt tuning strategies, and specialized techniques for attribute detection, including perspective-specified prompts and horizontal flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets show that under the condition of no given anchor sizes of novel classes, OVODA outperforms the state-of-the-art methods in open-vocabulary 3D object detection while successfully recognizing object attributes. Our OVAD dataset is released here: https://doi.org/10.5281/zenodo.16904069 .",
        "arxiv_id": "2508.16812",
        "ARXIVID": "2508.16812",
        "COMMENT": "This paper introduces OVODA, a framework for open-vocabulary multimodal 3D object and attribute detection, leveraging foundation models to bridge 3D features and text, and proposes a new dataset (OVAD) with attribute annotations. This matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), and partially criterion 3 (new benchmark for embodied/spatial AI).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.17932": {
        "authors": [
            "Zixuan Dong",
            "Baoyun Peng",
            "Yufei Wang",
            "Lin Liu",
            "Xinxin Dong",
            "Yunlong Cao",
            "Xiaodong Wang"
        ],
        "title": "See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops",
        "abstract": "arXiv:2508.17932v1 Announce Type: new  Abstract: Human video comprehension demonstrates dynamic coordination between reasoning and visual attention, adaptively focusing on query-relevant details. However, current long-form video question answering systems employ rigid pipelines that decouple reasoning from perception, leading to either information loss through premature visual abstraction or computational inefficiency through exhaustive processing. The core limitation lies in the inability to adapt visual extraction to specific reasoning requirements, different queries demand fundamentally different visual evidence from the same video content. In this work, we present CAVIA, a training-free framework that revolutionizes video understanding through reasoning, perception coordination. Unlike conventional approaches where visual processing operates independently of reasoning, CAVIA creates a closed-loop system where reasoning continuously guides visual extraction based on identified information gaps. CAVIA introduces three innovations: (1) hierarchical reasoning, guided localization to precise frames; (2) cross-modal semantic bridging for targeted extraction; (3) confidence-driven iterative synthesis. CAVIA achieves state-of-the-art performance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA (76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic reasoning-perception coordination provides a scalable paradigm for video understanding.",
        "arxiv_id": "2508.17932",
        "ARXIVID": "2508.17932",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (novel method for video understanding with reasoning-perception loops, focusing on dynamic, query-aware visual extraction). Proposes a new framework (CAVIA) for video question answering with reasoning-perception coordination.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.17614": {
        "authors": [
            "Aowen Wang",
            "Wei Li",
            "Hao Luo",
            "Mengxing Ao",
            "Chenyu Zhu",
            "Xinyang Li",
            "Fan Wang"
        ],
        "title": "JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on",
        "abstract": "arXiv:2508.17614v1 Announce Type: new  Abstract: Virtual try-on systems have long been hindered by heavy reliance on human body masks, limited fine-grained control over garment attributes, and poor generalization to real-world, in-the-wild scenarios. In this paper, we propose JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-On), a novel framework that overcomes these limitations by integrating diffusion-based image generation with multi-modal conditional fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our approach directly incorporates diverse control signals -- such as the reference person image and the target garment image -- into the denoising process through dedicated conditional pathways that fuse features within the self-attention layers. This fusion is further enhanced with refined positional encodings and attention masks, enabling precise spatial alignment and improved garment-person integration. To address data scarcity and quality, we introduce a bidirectional generation strategy for dataset construction: one pipeline uses a mask-based model to generate realistic reference images, while a symmetric ``Try-Off'' model, trained in a self-supervised manner, recovers the corresponding garment images. The synthesized dataset undergoes rigorous manual curation, allowing iterative improvement in visual fidelity and diversity. Experiments demonstrate that JCo-MVTON achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Moreover, it shows strong generalization in real-world applications, surpassing commercial systems.",
        "arxiv_id": "2508.17614",
        "ARXIVID": "2508.17614",
        "COMMENT": "Matches criterion 2 (multi-modal large language models: multi-modal diffusion transformer for virtual try-on) and criterion 4 (vision foundation models and their application in generative modeling). Proposes a new mask-free, controllable, multi-modal diffusion transformer for virtual try-on.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.17380": {
        "authors": [
            "Jiaqi Liu",
            "Songning Lai",
            "Pengze Li",
            "Di Yu",
            "Wenjie Zhou",
            "Yiyang Zhou",
            "Peng Xia",
            "Zijun Wang",
            "Xi Chen",
            "Shixiang Tang",
            "Lei Bai",
            "Wanli Ouyang",
            "Mingyu Ding",
            "Huaxiu Yao",
            "Aoran Wang"
        ],
        "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery",
        "abstract": "arXiv:2508.17380v1 Announce Type: new  Abstract: Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This \"sensory deprivation\" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist's perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws. Project page: https://jiaaqiliu.github.io/VIPER-R1/",
        "arxiv_id": "2508.17380",
        "ARXIVID": "2508.17380",
        "COMMENT": "Matches criterion 2 (introduces a new multimodal model for visual induction and symbolic reasoning, i.e., a VLM-centric approach) and criterion 4 (applies vision-language models to scientific discovery). Also introduces a new multimodal dataset (PhysSymbol), which is a novel angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.16974": {
        "authors": [
            "Leilei Guo",
            "Antonio Carlos Rivera",
            "Peiyu Tang",
            "Haoxuan Ren",
            "Zheyu Song"
        ],
        "title": "Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding",
        "abstract": "arXiv:2508.16974v1 Announce Type: new  Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have achieved remarkable progress in natural language processing and multimodal understanding. Despite their impressive generalization capabilities, current LVLMs often exhibit insufficient robustness, proneness to hallucination, and reasoning errors in complex real-world scenarios, particularly when precise image region localization and fine-grained visual reasoning are required. To address these limitations, we propose the Hierarchical Contextual Grounding LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine cognitive processing. HCG-LVLM employs a two-layered approach: a Global Contextual Perception layer for initial broad understanding and a Fine-grained Local Grounding layer. The latter incorporates a Local Detail Enhancement Module to extract high-resolution features and a Semantic Consistency Validator to ensure accurate, hallucination-free visual-language alignment. Through an adaptive fusion mechanism, information from both layers is integrated for robust and precise outputs. Extensive experiments on challenging datasets, including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model achieves superior accuracy and significantly reduces hallucination, validating the effectiveness of its hierarchical design in enhancing fine-grained visual-language understanding and precise grounding capabilities.",
        "arxiv_id": "2508.16974",
        "ARXIVID": "2508.16974",
        "COMMENT": "Matches criterion 2: Proposes a new LVLM (HCG-LVLM) with hierarchical contextual grounding for fine-grained visual-language understanding and robust grounding, directly addressing VLLM/MLLM improvements. Also relevant to criterion 4.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.18213": {
        "authors": [
            "Ayce Idil Aytekin",
            "Helge Rhodin",
            "Rishabh Dabral",
            "Christian Theobalt"
        ],
        "title": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance",
        "abstract": "arXiv:2508.18213v1 Announce Type: new  Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry of hand-held objects from monocular RGB images by leveraging hand-object interaction as geometric guidance. Our method conditions a latent diffusion model on an inpainted object appearance and uses inference-time guidance to optimize the object reconstruction, while simultaneously ensuring plausible hand-object interactions. Unlike prior methods that rely on extensive post-processing or produce low-quality reconstructions, our approach directly generates high-quality object geometry during the diffusion process by introducing guidance with an optimization-in-the-loop design. Specifically, we guide the diffusion model by applying supervision to the velocity field while simultaneously optimizing the transformations of both the hand and the object being reconstructed. This optimization is driven by multi-modal geometric cues, including normal and depth alignment, silhouette consistency, and 2D keypoint reprojection. We further incorporate signed distance field supervision and enforce contact and non-intersection constraints to ensure physical plausibility of hand-object interaction. Our method yields accurate, robust and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios.",
        "arxiv_id": "2508.18213",
        "ARXIVID": "2508.18213",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel method for hand-object interaction reconstruction with geometric guidance, focusing on spatial reasoning and physical plausibility). Uses a diffusion-based framework for 3D hand-object interaction reconstruction from monocular images.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.17638": {
        "authors": [
            "Xinyu Wei",
            "Guoli Yang",
            "Jialu Zhou",
            "Mingyue Yang",
            "Leqian Li",
            "Kedi Zhang",
            "Chunping Qiu"
        ],
        "title": "Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning",
        "abstract": "arXiv:2508.17638v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects visual features and then concatenates them with text tokens to form a unified sequence input for Large Language Models (LLMs). However, this paradigm leads to a significant increase in the length of the input sequence, resulting in substantial computational overhead. Existing methods attempt to fuse visual information into the intermediate layers of LLMs, which alleviate the sequence length issue but often neglect the hierarchical semantic representations within the model and the fine-grained visual information available in the shallower visual encoding layers. To address this limitation, we propose DEHVF, an efficient vision-language fine-tuning method based on dynamic embedding and fusion of hierarchical visual features. Its core lies in leveraging the inherent hierarchical representation characteristics of visual encoders and language models. Through a lightweight hierarchical visual fuser, it dynamically selects and fuses hierarchical features corresponding to semantic granularity based on the internal representations of each layer in LLMs. The fused layer-related visual features are then projected and aligned before being directly embedded into the Feed-Forward Network (FFN) of the corresponding layer in LLMs. This approach not only avoids sequence expansion but also dynamically fuses multi-layer visual information. By fine-tuning only a small number of parameters, DEHVF achieves precise alignment and complementarity of cross-modal information at the same semantic granularity. We conducted experiments across various VL benchmarks, including visual question answering on ScienceQA and image captioning on COCO Captions. The results demonstrate that DEHVF achieves higher accuracy than existing parameter-efficient fine-tuning (PEFT) baselines while maintaining efficient training and inference.",
        "arxiv_id": "2508.17638",
        "ARXIVID": "2508.17638",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new method for efficient vision-language fine-tuning by dynamically fusing hierarchical visual features into LLMs, improving parameter efficiency and accuracy.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.18227": {
        "authors": [
            "Lianming Huang",
            "Haibo Hu",
            "Qiao Li",
            "Xin He",
            "Nan Guan",
            "Chun Jason Xue"
        ],
        "title": "GM-Skip: Metric-Guided Transformer Block Skipping for Efficient Vision-Language Models",
        "abstract": "arXiv:2508.18227v1 Announce Type: new  Abstract: Transformer-based Vision-Language Models (VLMs) have achieved impressive performance on tasks such as image captioning, object recognition, and visual reasoning, but their high computational cost hinders deployment in latency-sensitive applications like autonomous driving. We introduce GM-Skip, a flexible and metric-adaptive framework for Transformer block skipping that accelerates VLM inference while preserving output quality. GM-Skip features a greedy, metric-guided block selection strategy that uses metric feedback (e.g., accuracy, CIDEr) to identify redundant layers, along with a reverse-order deletion mechanism that preserves early foundational blocks to avoid performance collapse. To support diverse deployment needs, it incorporates a tunable trade-off between sparsity and performance via a score-sparsity balance objective. Experiments across multiple tasks and datasets, including COCO and CODA, show that GM-Skip consistently improves inference speed while maintaining task performance. On the COCO dataset, GM-Skip improves single-object classification accuracy on the Person category from 19.1 percent to 87.3 percent while skipping more than 40 percent of Transformer blocks. In real-world deployment, it achieves up to 45.4 percent latency reduction on single-object detection when integrated into an autonomous vehicle running Autoware.Universe, validating the effectiveness of its skip configurations and confirming its practical value in accelerating real-world inference.",
        "arxiv_id": "2508.18227",
        "ARXIVID": "2508.18227",
        "COMMENT": "Matches criterion 2 (new VLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel, metric-guided transformer block skipping method for efficient vision-language model inference, with strong empirical results and practical deployment in autonomous vehicles.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.17243": {
        "authors": [
            "Zicong Tang",
            "Ziyang Ma",
            "Suqing Wang",
            "Zuchao Li",
            "Lefei Zhang",
            "Hai Zhao",
            "Yun Li",
            "Qianren Wang"
        ],
        "title": "CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models",
        "abstract": "arXiv:2508.17243v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.",
        "arxiv_id": "2508.17243",
        "ARXIVID": "2508.17243",
        "COMMENT": "Matches criterion 2 (new methodological improvement for VLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel, model-agnostic, plug-and-play visual token pruning method for large vision-language models, improving efficiency without accuracy loss.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.17437": {
        "authors": [
            "Long Le",
            "Ryan Lucas",
            "Chen Wang",
            "Chuhao Chen",
            "Dinesh Jayaraman",
            "Eric Eaton",
            "Lingjie Liu"
        ],
        "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels",
        "abstract": "arXiv:2508.17437v1 Announce Type: new  Abstract: Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/",
        "arxiv_id": "2508.17437",
        "ARXIVID": "2508.17437",
        "COMMENT": "This paper introduces PIXIE, a new method for fast, generalizable supervised learning of 3D physics from pixels, and presents a new large-scale dataset (PIXIEVERSE). It matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (new benchmark/dataset for embodied AI with a novel angle: learning 3D physics from pixels).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17972": {
        "authors": [
            "Junyuan Deng",
            "Heng Li",
            "Tao Xie",
            "Weiqiang Ren",
            "Qian Zhang",
            "Ping Tan",
            "Xiaoyang Guo"
        ],
        "title": "SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization",
        "abstract": "arXiv:2508.17972v1 Announce Type: new  Abstract: Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM) problem by directly regressing camera poses and 3D scene structures from input images. They demonstrate impressive performance in handling images under extreme viewpoint changes. However, these methods struggle to handle a large number of input images. To address this problem, we introduce SAIL-Recon, a feed-forward Transformer for large scale SfM, by augmenting the scene regression network with visual localization capabilities. Specifically, our method first computes a neural scene representation from a subset of anchor images. The regression network is then fine-tuned to reconstruct all input images conditioned on this neural scene representation. Comprehensive experiments show that our method not only scales efficiently to large-scale scenes, but also achieves state-of-the-art results on both camera pose estimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and Tanks & Temples. We will publish our model and code. Code and models are publicly available at: https://hkust-sail.github.io/ sail-recon/.",
        "arxiv_id": "2508.17972",
        "ARXIVID": "2508.17972",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications, as it proposes a transformer-based method for large-scale structure-from-motion and scene regression). Also relevant to criterion 1 (spatial understanding in large-scale 3D reconstruction). The SAIL-Recon method augments scene regression with localization for scalable SfM.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17971": {
        "authors": [
            "Pu Feng",
            "Size Wang",
            "Yuhong Cao",
            "Junkang Liang",
            "Rongye Shi",
            "Wenjun Wu"
        ],
        "title": "Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding",
        "abstract": "arXiv:2508.17971v1 Announce Type: new  Abstract: The development and application of large language models (LLM) have demonstrated that foundational models can be utilized to solve a wide array of tasks. However, their performance in multi-agent path finding (MAPF) tasks has been less than satisfactory, with only a few studies exploring this area. MAPF is a complex problem requiring both planning and multi-agent coordination. To improve the performance of LLM in MAPF tasks, we propose a novel framework, LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained graph neural network-based NAR, and a cross-attention mechanism. This is the first work to propose using a neural algorithmic reasoner to integrate GNNs with the map information for MAPF, thereby guiding LLM to achieve superior performance. LLM-NAR can be easily adapted to various LLM models. Both simulation and real-world experiments demonstrate that our method significantly outperforms existing LLM-based approaches in solving MAPF problems.",
        "arxiv_id": "2508.17971",
        "ARXIVID": "2508.17971",
        "COMMENT": "Matches criterion 2 (LLM-based approach) and criterion 1 (spatial intelligence for multi-agent path finding). The integration of neural algorithmic reasoners (GNNs) with LLMs for MAPF is a novel approach for spatial reasoning in multi-agent systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17832": {
        "authors": [
            "Xiping Wang",
            "Yuxi Wang",
            "Mengqi Zhou",
            "Junsong Fan",
            "Zhaoxiang Zhang"
        ],
        "title": "HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation",
        "abstract": "arXiv:2508.17832v1 Announce Type: new  Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior design, embodied intelligence, and scene understanding. While existing methods have made progress in coarse-scale furniture arrangement, they struggle to capture fine-grained object placements, limiting the realism and utility of generated environments. This gap hinders immersive virtual experiences and detailed scene comprehension for embodied AI applications. To address these issues, we propose Hierarchical Layout Generation (HLG), a novel method for fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine hierarchical approach, refining scene layouts from large-scale furniture placement to intricate object arrangements. Specifically, our fine-grained layout alignment module constructs a hierarchical layout through vertical and horizontal decoupling, effectively decomposing complex 3D indoor scenes into multiple levels of granularity. Additionally, our trainable layout optimization network addresses placement issues, such as incorrect positioning, orientation errors, and object intersections, ensuring structurally coherent and physically plausible scene generation. We demonstrate the effectiveness of our approach through extensive experiments, showing superior performance in generating realistic indoor scenes compared to existing methods. This work advances the field of scene generation and opens new possibilities for applications requiring detailed 3D environments. We will release our code upon publication to encourage future research.",
        "arxiv_id": "2508.17832",
        "ARXIVID": "2508.17832",
        "COMMENT": "Matches criterion 1 (methodological improvement for spatial understanding in 3D scene generation) and criterion 3 (novel method for embodied AI, as it proposes a hierarchical, fine-grained 3D scene generation method that could benefit embodied agents and simulators). The hierarchical layout generation and fine-grained alignment are new angles for 3D scene construction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16917": {
        "authors": [
            "Qing Zhang",
            "Jinguang Tong",
            "Jie Hong",
            "Jing Zhang",
            "Xuesong Li"
        ],
        "title": "Structural Energy-Guided Sampling for View-Consistent Text-to-3D",
        "abstract": "arXiv:2508.16917v1 Announce Type: new  Abstract: Text-to-3D generation often suffers from the Janus problem, where objects look correct from the front but collapse into duplicated or distorted geometry from other angles. We attribute this failure to viewpoint bias in 2D diffusion priors, which propagates into 3D optimization. To address this, we propose Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play framework that enforces multi-view consistency entirely at sampling time. SEGS defines a structural energy in a PCA subspace of intermediate U-Net features and injects its gradients into the denoising trajectory, steering geometry toward the intended viewpoint while preserving appearance fidelity. Integrated seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts, achieving improved geometric alignment and viewpoint consistency without retraining or weight modification.",
        "arxiv_id": "2508.16917",
        "ARXIVID": "2508.16917",
        "COMMENT": "Matches criterion 1 (methodological improvement for spatial consistency in text-to-3D generation) and criterion 4 (vision foundation models and applications, as it plugs into SDS/VSD pipelines). The proposed SEGS method is a plug-and-play, training-free approach for enforcing multi-view consistency in text-to-3D, addressing the Janus problem, which is a key spatial intelligence issue for generative models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16850": {
        "authors": [
            "Anku Rani",
            "Aparna Garimella",
            "Apoorv Saxena",
            "Balaji Vasan Srinivasan",
            "Paul Pu Liang"
        ],
        "title": "RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis",
        "abstract": "arXiv:2508.16850v1 Announce Type: new  Abstract: Data visualizations like charts are fundamental tools for quantitative analysis and decision-making across fields, requiring accurate interpretation and mathematical reasoning. The emergence of Multimodal Large Language Models (MLLMs) offers promising capabilities for automated visual data analysis, such as processing charts, answering questions, and generating summaries. However, they provide no visibility into which parts of the visual data informed their conclusions; this black-box nature poses significant challenges to real-world trust and adoption. In this paper, we take the first major step towards evaluating and enhancing the capabilities of MLLMs to attribute their reasoning process by highlighting the specific regions in charts and graphs that justify model answers. To this end, we contribute RADAR, a semi-automatic approach to obtain a benchmark dataset comprising 17,819 diverse samples with charts, questions, reasoning steps, and attribution annotations. We also introduce a method that provides attribution for chart-based mathematical reasoning. Experimental results demonstrate that our reasoning-guided approach improves attribution accuracy by 15% compared to baseline methods, and enhanced attribution capabilities translate to stronger answer generation, achieving an average BERTScore of $\\sim$ 0.90, indicating high alignment with ground truth responses. This advancement represents a significant step toward more interpretable and trustworthy chart analysis systems, enabling users to verify and understand model decisions through reasoning and attribution.",
        "arxiv_id": "2508.16850",
        "ARXIVID": "2508.16850",
        "COMMENT": "Matches criterion 2: Introduces a reasoning-guided attribution framework for explainable visual data analysis using MLLMs, and provides a new benchmark (RADAR) for chart-based reasoning and attribution. Also relevant to criterion 4 as it advances interpretability in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17427": {
        "authors": [
            "Zhao Zheng",
            "Jingfan Fan",
            "Long Shao",
            "Hong Song",
            "Danni Ai",
            "Tianyu Fu",
            "Deqiang Xiao",
            "Yongtian Wang",
            "Jian Yang"
        ],
        "title": "Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search",
        "abstract": "arXiv:2508.17427v1 Announce Type: new  Abstract: Point cloud registration based on correspondences computes the rigid transformation that maximizes the number of inliers constrained within the noise threshold. Current state-of-the-art (SOTA) methods employing spatial compatibility graphs or branch-and-bound (BnB) search mainly focus on registration under high outlier ratios. However, graph-based methods require at least quadratic space and time complexity for graph construction, while multi-stage BnB search methods often suffer from inaccuracy due to local optima between decomposed stages. This paper proposes a geometric maximum overlapping registration framework via rotation-only BnB search. The rigid transformation is decomposed using Chasles' theorem into a translation along rotation axis and a 2D rigid transformation. The optimal rotation axis and angle are searched via BnB, with residual parameters formulated as range maximum query (RMQ) problems. Firstly, the top-k candidate rotation axes are searched within a hemisphere parameterized by cube mapping, and the translation along each axis is estimated through interval stabbing of the correspondences projected onto that axis. Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D RMQ of geometric overlapping for axis-aligned rectangles, which is solved deterministically in polynomial time using sweep line algorithm with segment tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets demonstrate superior accuracy and efficiency over SOTA methods, while the time complexity is polynomial and the space complexity increases linearly with the number of points, even in the worst case.",
        "arxiv_id": "2508.17427",
        "ARXIVID": "2508.17427",
        "COMMENT": "Matches criterion 1: Presents a new method for robust point cloud registration using geometric overlapping and rotation search, which is a methodological improvement in spatial understanding for 3D data. Also relevant to embodied AI (criterion 3) as point cloud registration is fundamental for spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17714": {
        "authors": [
            "Hanbo Bi",
            "Zhiqiang Yuan",
            "Zexi Jia",
            "Jiapei Zhang",
            "Chongyang Li",
            "Peixiang Luo",
            "Ying Deng",
            "Xiaoyue Duan",
            "Jinchao Zhang"
        ],
        "title": "F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model",
        "abstract": "arXiv:2508.17714v1 Announce Type: new  Abstract: Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users' actual needs for revisiting semantically coherent content scattered across long-form conversations. To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues. As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns. Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments. To this end, we propose F2RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards promoting semantic precision, relevance, and contextual coherence. To handle varying intra-fragment complexity, from locally dense to sparsely distributed, we introduce difficulty-aware curriculum sampling that ranks training instances by model-predicted difficulty and gradually exposes the model to harder samples. This boosts reasoning ability in long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain and real-domain settings, demonstrating superior retrieval performance.",
        "arxiv_id": "2508.17714",
        "ARXIVID": "2508.17714",
        "COMMENT": "Matches criterion 2: Shows a new VLLM (F2RVLM) for fine-grained fragment retrieval in multi-modal long-form dialogue, with a new dataset and training paradigm. Also relevant to criterion 4 as it applies vision-language models to a novel retrieval task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17890": {
        "authors": [
            "Qipeng Zhu",
            "Yanzhe Chen",
            "Huasong Zhong",
            "Yan Li",
            "Jie Chen",
            "Zhixin Zhang",
            "Junping Zhang",
            "Zhenheng Yang"
        ],
        "title": "UniAPO: Unified Multimodal Automated Prompt Optimization",
        "abstract": "arXiv:2508.17890v1 Announce Type: new  Abstract: Prompting is fundamental to unlocking the full potential of large language models. To automate and enhance this process, automatic prompt optimization (APO) has been developed, demonstrating effectiveness primarily in text-only input scenarios. However, extending existing APO methods to multimodal tasks, such as video-language generation introduces two core challenges: (i) visual token inflation, where long visual token sequences restrict context capacity and result in insufficient feedback signals; (ii) a lack of process-level supervision, as existing methods focus on outcome-level supervision and overlook intermediate supervision, limiting prompt optimization. We present UniAPO: Unified Multimodal Automated Prompt Optimization, the first framework tailored for multimodal APO. UniAPO adopts an EM-inspired optimization process that decouples feedback modeling and prompt refinement, making the optimization more stable and goal-driven. To further address the aforementioned challenges, we introduce a short-long term memory mechanism: historical feedback mitigates context limitations, while historical prompts provide directional guidance for effective prompt optimization. UniAPO achieves consistent gains across text, image, and video benchmarks, establishing a unified framework for efficient and transferable prompt optimization.",
        "arxiv_id": "2508.17890",
        "ARXIVID": "2508.17890",
        "COMMENT": "UniAPO proposes a unified multimodal automated prompt optimization framework for text, image, and video tasks, addressing prompt optimization in MLLMs. This matches criterion 2 (new MLLMs/VLLMs) and is relevant for multi-modal generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16644": {
        "authors": [
            "Anindya Mondal",
            "Ayan Banerjee",
            "Sauradip Nag",
            "Josep Llad\\'os",
            "Xiatian Zhu",
            "Anjan Dutta"
        ],
        "title": "CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance",
        "abstract": "arXiv:2508.16644v1 Announce Type: new  Abstract: Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.",
        "arxiv_id": "2508.16644",
        "ARXIVID": "2508.16644",
        "COMMENT": "This paper introduces CountLoop, a training-free framework for high-instance image generation with spatial control, using iterative agent guidance and multimodal evaluation. It matches criterion 1 (spatial understanding in generative models) and partially criterion 4 (application of vision foundation models in generative modeling).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17408": {
        "authors": [
            "Bin Huang",
            "Zhong Liu",
            "Huiying Wen",
            "Bingsheng Huang",
            "Xin Chen",
            "Shuo Li"
        ],
        "title": "E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation",
        "abstract": "arXiv:2508.17408v1 Announce Type: new  Abstract: Although the Segment Anything Model (SAM) has advanced medical image segmentation, its Bayesian adaptation for uncertainty-aware segmentation remains hindered by three key issues: (1) instability in Bayesian fine-tuning of large pre-trained SAMs; (2) high computation cost due to SAM's massive parameters; (3) SAM's black-box design limits interpretability. To overcome these, we propose E-BayesSAM, an efficient framework combining Token-wise Variational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving interpretability. T-VBI innovatively reinterprets SAM's output tokens as dynamic probabilistic weights and reparameterizes them as latent variables without auxiliary training, enabling training-free VBI for uncertainty estimation. SO-KAN improves token prediction with learnable spline activations via self-supervised learning, providing insight to prune redundant tokens to boost efficiency and accuracy. Experiments on five ultrasound datasets demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image), (ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\\% vs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four critical tokens governing SAM's decisions. By unifying efficiency, reliability, and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs, advancing deployment in safety-critical medical applications. The source code is available at https://github.com/mp31192/E-BayesSAM.",
        "arxiv_id": "2508.17408",
        "ARXIVID": "2508.17408",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a Bayesian adaptation of SAM (Segment Anything Model) for uncertainty-aware segmentation, with interpretability and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17876": {
        "authors": [
            "Lulu Hao",
            "Lipu Zhou",
            "Zhenzhong Wei",
            "Xu Wang"
        ],
        "title": "Camera Pose Refinement via 3D Gaussian Splatting",
        "abstract": "arXiv:2508.17876v1 Announce Type: new  Abstract: Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.",
        "arxiv_id": "2508.17876",
        "ARXIVID": "2508.17876",
        "COMMENT": "Matches criterion 1 (spatial understanding: camera pose refinement using 3D Gaussian Splatting) and criterion 4 (application of vision foundation models in 3D vision). Proposes a new method for pose refinement leveraging 3DGS, with strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17188": {
        "authors": [
            "Zhilin Zhang",
            "Xiang Zhang",
            "Jiaqi Wei",
            "Yiwei Xu",
            "Chenyu You"
        ],
        "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs",
        "abstract": "arXiv:2508.17188v1 Announce Type: new  Abstract: Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.",
        "arxiv_id": "2508.17188",
        "ARXIVID": "2508.17188",
        "COMMENT": "Matches criterion 2 (multi-agent LLMs for vision-language task) and criterion 4 (application of vision-language models for poster generation). Uses a VLM-based rubric for evaluation and multi-agent LLMs for compositional visual design.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16942": {
        "authors": [
            "Junhao Wu",
            "Xiuer Gu",
            "Zhiying Li",
            "Yeying Jin",
            "Yunfeng Diao",
            "Zhiyu Li",
            "Zhenbo Song",
            "Xiaomei Zhang",
            "Zhaoxin Fan"
        ],
        "title": "HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis",
        "abstract": "arXiv:2508.16942v1 Announce Type: new  Abstract: Evaluating human actions with clear and detailed feedback is important in areas such as sports, healthcare, and robotics, where decisions rely not only on final outcomes but also on interpretable reasoning. However, most existing methods provide only a final score without explanation or detailed analysis, limiting their practical applicability. To address this, we introduce HieroAction, a vision-language model that delivers accurate and structured assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise Action Reasoning, a tailored chain of thought process designed specifically for action assessment, which guides the model to evaluate actions step by step, from overall recognition through sub action analysis to final scoring, thus enhancing interpretability and structured understanding; and (2) Hierarchical Policy Learning, a reinforcement learning strategy that enables the model to learn fine grained sub action dynamics and align them with high level action quality, thereby improving scoring precision. The reasoning pathway structures the evaluation process, while policy learning refines each stage through reward based optimization. Their integration ensures accurate and interpretable assessments, as demonstrated by superior performance across multiple benchmark datasets. Code will be released upon acceptance.",
        "arxiv_id": "2508.16942",
        "ARXIVID": "2508.16942",
        "COMMENT": "Matches criterion 2 (proposes a new vision-language model for fine-grained action analysis) and criterion 4 (applies VLMs to structured action assessment). The stepwise action reasoning and hierarchical policy learning are novel for interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.17488": {
        "authors": [
            "Zhiwen Chen",
            "Jinjian Wu",
            "Zhiyu Zhu",
            "Yifan Zhang",
            "Guangming Shi",
            "Junhui Hou"
        ],
        "title": "Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning",
        "abstract": "arXiv:2508.17488v1 Announce Type: new  Abstract: This paper tackles the critical challenge of optimizing multi-modal trackers by effectively adapting the pre-trained models for RGB data. Existing fine-tuning paradigms oscillate between excessive freedom and over-restriction, both leading to a suboptimal plasticity-stability trade-off. To mitigate this dilemma, we propose a novel sensitivity-aware regularized tuning framework, which delicately refines the learning process by incorporating intrinsic parameter sensitivities. Through a comprehensive investigation from pre-trained to multi-modal contexts, we identify that parameters sensitive to pivotal foundational patterns and cross-domain shifts are primary drivers of this issue. Specifically, we first analyze the tangent space of pre-trained weights to measure and orient prior sensitivities, dedicated to preserving generalization. Then, we further explore transfer sensitivities during the tuning phase, emphasizing adaptability and stability. By incorporating these sensitivities as regularization terms, our method significantly enhances the transferability across modalities. Extensive experiments showcase the superior performance of the proposed method, surpassing current state-of-the-art techniques across various multi-modal tracking. The source code and models will be publicly available at https://github.com/zhiwen-xdu/SRTrack.",
        "arxiv_id": "2508.17488",
        "ARXIVID": "2508.17488",
        "COMMENT": "Matches criterion 1 (methodological improvements to multi-modal trackers, which are key for spatial understanding in embodied agents) and criterion 4 (uses pre-trained multi-modal models, relevant to vision foundation models). The sensitivity-aware regularized tuning is a novel regularization approach for multi-modal transfer.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.18271": {
        "authors": [
            "Haitang Feng",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Beiqi Chen",
            "Jianhuang Lai",
            "Guangcong Wang"
        ],
        "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models",
        "abstract": "arXiv:2508.18271v1 Announce Type: new  Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .",
        "arxiv_id": "2508.18271",
        "ARXIVID": "2508.18271",
        "COMMENT": "Proposes ObjFiller-3D, a method for consistent multi-view 3D inpainting using video diffusion models, and adapts video inpainting for 3D scene completion. This is relevant to criterion 4 (vision foundation models and applications), and also involves generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.16783": {
        "authors": [
            "Stefania L. Moroianu",
            "Christian Bluethgen",
            "Pierre Chambon",
            "Mehdi Cherti",
            "Jean-Benoit Delbrouck",
            "Magdalini Paschali",
            "Brandon Price",
            "Judy Gichoya",
            "Jenia Jitsev",
            "Curtis P. Langlotz",
            "Akshay S. Chaudhari"
        ],
        "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data",
        "abstract": "arXiv:2508.16783v1 Announce Type: new  Abstract: Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .",
        "arxiv_id": "2508.16783",
        "ARXIVID": "2508.16783",
        "COMMENT": "This paper introduces a new text-to-image diffusion model (RoentGen-v2) for generating synthetic radiographs with fine-grained demographic control, and demonstrates a novel training strategy for improving fairness and robustness in medical imaging models. This matches criterion 4 (vision foundation models and applications) and is also of interest for generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.17254": {
        "authors": [
            "Xiao Zhang",
            "Kai-Fu Yang",
            "Xian-Shi Zhang",
            "Hong-Zhi You",
            "Hong-Mei Yan",
            "Yong-Jie Li"
        ],
        "title": "A biological vision inspired framework for machine perception of abutting grating illusory contours",
        "abstract": "arXiv:2508.17254v1 Announce Type: new  Abstract: Higher levels of machine intelligence demand alignment with human perception and cognition. Deep neural networks (DNN) dominated machine intelligence have demonstrated exceptional performance across various real-world tasks. Nevertheless, recent evidence suggests that DNNs fail to perceive illusory contours like the abutting grating, a discrepancy that misaligns with human perception patterns. Departing from previous works, we propose a novel deep network called illusory contour perception network (ICPNet) inspired by the circuits of the visual cortex. In ICPNet, a multi-scale feature projection (MFP) module is designed to extract multi-scale representations. To boost the interaction between feedforward and feedback features, a feature interaction attention module (FIAM) is introduced. Moreover, drawing inspiration from the shape bias observed in human perception, an edge detection task conducted via the edge fusion module (EFM) injects shape constraints that guide the network to concentrate on the foreground. We assess our method on the existing AG-MNIST test set and the AG-Fashion-MNIST test sets constructed by this work. Comprehensive experimental results reveal that ICPNet is significantly more sensitive to abutting grating illusory contours than state-of-the-art models, with notable improvements in top-1 accuracy across various subsets. This work is expected to make a step towards human-level intelligence for DNN-based models.",
        "arxiv_id": "2508.17254",
        "ARXIVID": "2508.17254",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a biologically inspired deep network for illusory contour perception, improving alignment with human vision and advancing DNN-based perception.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.17760": {
        "authors": [
            "Mingyue Yang",
            "Dianxi Shi",
            "Jialu Zhou",
            "Xinyu Wei",
            "Leqian Li",
            "Shaowu Yang",
            "Chunping Qiu"
        ],
        "title": "CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation",
        "abstract": "arXiv:2508.17760v1 Announce Type: new  Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model's understanding of the concept of interactive \"actions\" more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.",
        "arxiv_id": "2508.17760",
        "ARXIVID": "2508.17760",
        "COMMENT": "Matches criterion 4 (proposes a new diffusion model for text-to-image generation with entity and interaction control, leveraging LLMs for relationship mining). The dual control for entity and interaction is a novel approach.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.17062": {
        "authors": [
            "Peng Hu",
            "Yu Gu",
            "Liang Luo",
            "Fuji Ren"
        ],
        "title": "SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation",
        "abstract": "arXiv:2508.17062v1 Announce Type: new  Abstract: Controllable video generation aims to synthesize video content that aligns precisely with user-provided conditions, such as text descriptions and initial images. However, a significant challenge persists in this domain: existing models often struggle to maintain strong semantic consistency, frequently generating videos that deviate from the nuanced details specified in the prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided Diffusion Transformer), a novel and efficient framework for high-fidelity controllable video generation. Our approach introduces a decoupled two-stage process. The first stage, Spatial Signal Prompting, generates a spatially aware visual prompt by leveraging the rich internal representations of a pre-trained multi-modal model. This prompt, combined with the original text, forms a joint condition that is then injected into a frozen video DiT backbone via our lightweight and parameter-efficient SSG-Adapter. This unique design, featuring a dual-branch attention mechanism, allows the model to simultaneously harness its powerful generative priors while being precisely steered by external spatial signals. Extensive experiments demonstrate that SSG-DiT achieves state-of-the-art performance, outperforming existing models on multiple key metrics in the VBench benchmark, particularly in spatial relationship control and overall consistency.",
        "arxiv_id": "2508.17062",
        "ARXIVID": "2508.17062",
        "COMMENT": "Matches criterion 4 (proposes a new framework for controllable video generation using spatial signals from pre-trained multi-modal models, i.e., vision foundation models). The spatial signal guided prompting is a novel angle for video generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.16884": {
        "authors": [
            "Yi Zhang",
            "Lingxiao Wei",
            "Bowei Zhang",
            "Ziwei Liu",
            "Kai Yi",
            "Shu Hu"
        ],
        "title": "A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism",
        "abstract": "arXiv:2508.16884v1 Announce Type: new  Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its strong long-range dependency modelling ability. However, its large model size with high computational cost and weak local feature modeling ability hinder its application in real scenarios. To balance computation efficiency and performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight ViT based model with convolution blocks, in this paper to achieve efficient downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated Attention (SAA) module that performs adaptive sparse sampling based on image redundancy and recovers the feature map via deconvolution operation, which significantly reduces the computational complexity of attention operations. In addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed to enhance inter-channel information exchange through feature decomposition and redistribution, mitigating redundancy in traditional feed-forward networks (FNN). Finally, a hierarchical pyramid structure with embedded depth-wise separable convolutional blocks (DWSConv) is devised to further strengthen convolutional features. Extensive experiments on mainstream datasets show that SAEViT achieves Top-1 accuracies of 76.3\\% and 79.6\\% on the ImageNet-1K classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively, demonstrating a lightweight solution for various fundamental vision tasks.",
        "arxiv_id": "2508.16884",
        "ARXIVID": "2508.16884",
        "COMMENT": "Matches criterion 4: Proposes a new lightweight vision transformer model (SAEViT) with convolutional integration and multi-scale self-attention, relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.16976": {
        "authors": [
            "Bin Pan",
            "Shiyu Shen",
            "Zongbin Wang",
            "Zhenwei Shi",
            "Xia Xu"
        ],
        "title": "Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection",
        "abstract": "arXiv:2508.16976v1 Announce Type: new  Abstract: Domain generalization seeks to develop models trained on a limited set of source domains that are capable of generalizing effectively to unseen target domains. While the predominant approach leverages large-scale pre-trained vision models as initialization, recent studies have highlighted that full fine-tuning can compromise the intrinsic generalization capabilities of these models. To address this limitation, parameter-efficient adaptation strategies have emerged, wherein only a subset of model parameters is selectively fine-tuned, thereby balancing task adaptation with the preservation of generalization. Motivated by this paradigm, we introduce Joint Parameter Selection (JPS), a novel method that restricts updates to a small, sparse subset of parameters, thereby retaining and harnessing the generalization strength of pre-trained models. Theoretically, we establish a generalization error bound that explicitly accounts for the sparsity of parameter updates, thereby providing a principled justification for selective fine-tuning. Practically, we design a selection mechanism employing dual operators to identify and update parameters exhibiting consistent and significant gradients across all source domains. Extensive benchmark experiments demonstrate that JPS achieves superior performance compared to state-of-the-art domain generalization methods, substantiating both the efficiency and efficacy of the proposed approach.",
        "arxiv_id": "2508.16976",
        "ARXIVID": "2508.16976",
        "COMMENT": "This paper proposes a new method for preserving domain generalization in fine-tuning vision foundation models, which matches criterion 4 (vision foundation models and their applications).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.17537": {
        "authors": [
            "Petr Hruby",
            "Marc Pollefeys"
        ],
        "title": "Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks",
        "abstract": "arXiv:2508.17537v1 Announce Type: new  Abstract: We address the problem of estimating both translational and angular velocity of a camera from asynchronous point tracks, a formulation relevant to rolling shutter and event cameras. Since the original problem is non-polynomial, we propose a polynomial approximation, classify the resulting minimal problems, and determine their algebraic degrees. Furthermore, we develop minimal solvers for several problems with low degrees and evaluate them on synthetic and real datasets. The code will be made publicly available.",
        "arxiv_id": "2508.17537",
        "ARXIVID": "2508.17537",
        "COMMENT": "This paper proposes new minimal solvers for full DoF motion estimation from asynchronous tracks, relevant to spatial understanding in embodied agents (criterion 1). The focus on asynchronous tracks (event/rolling shutter cameras) is a methodological improvement for spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.17750": {
        "authors": [
            "Ryan Ramos",
            "Yusuke Hirota",
            "Yuta Nakashima",
            "Noa Garcia"
        ],
        "title": "From Global to Local: Social Bias Transfer in CLIP",
        "abstract": "arXiv:2508.17750v1 Announce Type: new  Abstract: The recycling of contrastive language-image pre-trained (CLIP) models as backbones for a large number of downstream tasks calls for a thorough analysis of their transferability implications, especially their well-documented reproduction of social biases and human stereotypes. How do such biases, learned during pre-training, propagate to downstream applications like visual question answering or image captioning? Do they transfer at all?   We investigate this phenomenon, referred to as bias transfer in prior literature, through a comprehensive empirical analysis. Firstly, we examine how pre-training bias varies between global and local views of data, finding that bias measurement is highly dependent on the subset of data on which it is computed. Secondly, we analyze correlations between biases in the pre-trained models and the downstream tasks across varying levels of pre-training bias, finding difficulty in discovering consistent trends in bias transfer. Finally, we explore why this inconsistency occurs, showing that under the current paradigm, representation spaces of different pre-trained CLIPs tend to converge when adapted for downstream tasks. We hope this work offers valuable insights into bias behavior and informs future research to promote better bias mitigation practices.",
        "arxiv_id": "2508.17750",
        "ARXIVID": "2508.17750",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications), as it analyzes social bias transfer in CLIP models, which are vision foundation models. Provides empirical insights into bias transfer in downstream tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.17299": {
        "authors": [
            "Zhihao Chen",
            "Qi Gao",
            "Zilong Li",
            "Junping Zhang",
            "Yi Zhang",
            "Jun Zhao",
            "Hongming Shan"
        ],
        "title": "FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising",
        "abstract": "arXiv:2508.17299v1 Announce Type: new  Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff.",
        "arxiv_id": "2508.17299",
        "ARXIVID": "2508.17299",
        "COMMENT": "FoundDiff is a foundational diffusion model for generalizable low-dose CT denoising, using a vision-language pretraining approach. It matches criterion 4 (vision foundation models and applications), though the application is medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.17816": {
        "authors": [
            "Xingyu Ai",
            "Shaoyu Wang",
            "Zhiyuan Jia",
            "Ao Xu",
            "Hongming Shan",
            "Jianhua Ma",
            "Qiegen Liu"
        ],
        "title": "UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization",
        "abstract": "arXiv:2508.17816v1 Announce Type: new  Abstract: During raw-data acquisition in CT imaging, diverse factors can degrade the collected sinograms, with undersampling and noise leading to severe artifacts and noise in reconstructed images and compromising diagnostic accuracy. Conventional correction methods rely on manually designed algorithms or fixed empirical parameters, but these approaches often lack generalizability across heterogeneous artifact types. To address these limitations, we propose UniSino, a foundation model for universal CT sinogram standardization. Unlike existing foundational models that operate in image domain, UniSino directly standardizes data in the projection domain, which enables stronger generalization across diverse undersampling scenarios. Its training framework incorporates the physical characteristics of sinograms, enhancing generalization and enabling robust performance across multiple subtasks spanning four benchmark datasets. Experimental results demonstrate thatUniSino achieves superior reconstruction quality both single and mixed undersampling case, demonstrating exceptional robustness and generalization in sinogram enhancement for CT imaging. The code is available at: https://github.com/yqx7150/UniSino.",
        "arxiv_id": "2508.17816",
        "ARXIVID": "2508.17816",
        "COMMENT": "Matches criterion 4 (proposes a foundation model for CT sinogram standardization, which is a vision foundation model applied to medical imaging). The projection-domain approach is a novel angle.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.16830": {
        "authors": [
            "Alexander Yakovenko",
            "George Chakvetadze",
            "Ilya Khrapov",
            "Maksim Zhelezov",
            "Dmitry Vatolin",
            "Radu Timofte",
            "Youngjin Oh",
            "Junhyeong Kwon",
            "Junyoung Park",
            "Nam Ik Cho",
            "Senyan Xu",
            "Ruixuan Jiang",
            "Long Peng",
            "Xueyang Fu",
            "Zheng-Jun Zha",
            "Xiaoping Peng",
            "Hansen Feng",
            "Zhanyi Tie",
            "Ziming Xia",
            "Lizhi Wang"
        ],
        "title": "AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results",
        "abstract": "arXiv:2508.16830v1 Announce Type: new  Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light RAW Video Denoising Challenge. The task is to develop methods that denoise low-light RAW video by exploiting temporal redundancy while operating under exposure-time limits imposed by frame rate and adapting to sensor-specific, signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences captured with 14 smartphone camera sensors across nine conditions (illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR references obtained via burst averaging. Participants process linear RAW sequences and output the denoised 10th frame while preserving the Bayer pattern. Submissions are evaluated on a private test set using full-reference PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This report describes the dataset, challenge protocol, and submitted approaches.",
        "arxiv_id": "2508.16830",
        "ARXIVID": "2508.16830",
        "COMMENT": "Matches criterion 3 (introduces a new benchmark for low-light RAW video denoising, which is relevant for embodied AI and simulator-related research). The dataset covers multiple sensors and conditions, which is a novel contribution.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.17846": {
        "authors": [
            "Yang Chen",
            "Yanbin Wei",
            "Ke Jin",
            "Yi Kong",
            "James Kwok",
            "Yu Zhang"
        ],
        "title": "Alternating Training-based Label Smoothing Enhances Prompt Generalization",
        "abstract": "arXiv:2508.17846v1 Announce Type: new  Abstract: Recent advances in pre-trained vision-language models have demonstrated remarkable zero-shot generalization capabilities. To further enhance these models' adaptability to various downstream tasks, prompt tuning has emerged as a parameter-efficient fine-tuning method. However, despite its efficiency, the generalization ability of prompt remains limited. In contrast, label smoothing (LS) has been widely recognized as an effective regularization technique that prevents models from becoming over-confident and improves their generalization. This inspires us to explore the integration of LS with prompt tuning. However, we have observed that the vanilla LS even weakens the generalization ability of prompt tuning. To address this issue, we propose the Alternating Training-based Label Smoothing (ATLaS) method, which alternately trains with standard one-hot labels and soft labels generated by LS to supervise the prompt tuning. Moreover, we introduce two types of efficient offline soft labels, including Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide inter-class or instance-class relationships for prompt tuning. The theoretical properties of the proposed ATLaS method are analyzed. Extensive experiments demonstrate that the proposed ATLaS method, combined with CSL and ISL, consistently enhances the generalization performance of prompt tuning. Moreover, the proposed ATLaS method exhibits high compatibility with prevalent prompt tuning methods, enabling seamless integration into existing methods.",
        "arxiv_id": "2508.17846",
        "ARXIVID": "2508.17846",
        "COMMENT": "This paper proposes a new method (ATLaS) to improve prompt tuning for vision-language models by alternating label smoothing strategies. It is relevant to criterion 2 (VLLMs/MLLMs) as it focuses on improving prompt generalization in pre-trained vision-language models, and also touches on clever statistical tricks (label smoothing).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.17434": {
        "authors": [
            "Linwei Dong",
            "Qingnan Fan",
            "Yuhang Yu",
            "Qi Zhang",
            "Jinwei Chen",
            "Yawei Luo",
            "Changqing Zou"
        ],
        "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
        "abstract": "arXiv:2508.17434v1 Announce Type: new  Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.",
        "arxiv_id": "2508.17434",
        "ARXIVID": "2508.17434",
        "COMMENT": "TinySR presents a compact diffusion model for real-world image super-resolution, focusing on pruning and efficiency. It is related to vision foundation models and generative modeling (criterion 4), but does not introduce a new VLLM/MLLM or spatial intelligence method.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2508.18095": {
        "authors": [
            "Zhicong Tang",
            "Tiankai Hang",
            "Shuyang Gu",
            "Dong Chen",
            "Baining Guo"
        ],
        "title": "Incorporating Pre-trained Diffusion Models in Solving the Schr\\\"odinger Bridge Problem",
        "abstract": "arXiv:2508.18095v1 Announce Type: new  Abstract: This paper aims to unify Score-based Generative Models (SGMs), also known as Diffusion models, and the Schr\\\"odinger Bridge (SB) problem through three reparameterization techniques: Iterative Proportional Mean-Matching (IPMM), Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional Flow-Matching (IPFM). These techniques significantly accelerate and stabilize the training of SB-based models. Furthermore, the paper introduces novel initialization strategies that use pre-trained SGMs to effectively train SB-based models. By using SGMs as initialization, we leverage the advantages of both SB-based models and SGMs, ensuring efficient training of SB-based models and further improving the performance of SGMs. Extensive experiments demonstrate the significant effectiveness and improvements of the proposed methods. We believe this work contributes to and paves the way for future research on generative models.",
        "arxiv_id": "2508.18095",
        "ARXIVID": "2508.18095",
        "COMMENT": "This paper unifies score-based generative models (diffusion models) and the Schr\u00f6dinger Bridge problem, introducing new reparameterization techniques and initialization strategies. While it is a generative modeling paper, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Relevant to generative modeling, but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.17661": {
        "authors": [
            "Minhyeong Lee",
            "Suyoung Hwang",
            "Seunghyun Moon",
            "Geonho Nah",
            "Donghyun Koh",
            "Youngjun Cho",
            "Johyun Park",
            "Hojin Yoo",
            "Jiho Park",
            "Haneul Choi",
            "Sungbin Moon",
            "Taehoon Hwang",
            "Seungwon Kim",
            "Jaeyeong Kim",
            "Seongjun Kim",
            "Juneau Jung"
        ],
        "title": "Spacer: Towards Engineered Scientific Inspiration",
        "abstract": "arXiv:2508.17661v1 Announce Type: new  Abstract: Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs.",
        "arxiv_id": "2508.17661",
        "ARXIVID": "2508.17661",
        "COMMENT": "Does not directly match any of the four criteria. While it discusses a system for scientific discovery using LLMs and embeddings, it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18186": {
        "authors": [
            "Le Zhang",
            "Fuping Wu",
            "Arun Thirunavukarasu",
            "Kevin Bronik",
            "Thomas Nichols",
            "Bartlomiej W. Papiez"
        ],
        "title": "Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning",
        "abstract": "arXiv:2508.18186v1 Announce Type: new  Abstract: Large annotated datasets are vital for training segmentation models, but pixel-level labeling is time-consuming, error-prone, and often requires scarce expert annotators, especially in medical imaging. In contrast, coarse annotations are quicker, cheaper, and easier to produce, even by non-experts. In this paper, we propose to use coarse drawings from both positive (target) and negative (background) classes in the image, even with noisy pixels, to train a convolutional neural network (CNN) for semantic segmentation. We present a method for learning the true segmentation label distributions from purely noisy coarse annotations using two coupled CNNs. The separation of the two CNNs is achieved by high fidelity with the characters of the noisy training annotations. We propose to add a complementary label learning that encourages estimating negative label distribution. To illustrate the properties of our method, we first use a toy segmentation dataset based on MNIST. We then present the quantitative results of experiments using publicly available datasets: Cityscapes dataset for multi-class segmentation, and retinal images for medical applications. In all experiments, our method outperforms state-of-the-art methods, particularly in the cases where the ratio of coarse annotations is small compared to the given dense annotations.",
        "arxiv_id": "2508.18186",
        "ARXIVID": "2508.18186",
        "COMMENT": "Presents a method for semantic segmentation using noisy coarse annotations, with a focus on learning from positive and negative coarse labels. While relevant to computer vision, it does not directly address the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.17017": {
        "authors": [
            "Konstantina Nikolaidou",
            "George Retsinas",
            "Giorgos Sfikas",
            "Silvia Cascianelli",
            "Rita Cucchiara",
            "Marcus Liwicki"
        ],
        "title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation",
        "abstract": "arXiv:2508.17017v1 Announce Type: new  Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve impressive results on frequent, in-vocabulary words observed at training time and on regular styles. However, they are prone to memorizing training samples and often struggle with style variability and generation clarity. In particular, standard diffusion models tend to produce artifacts or distortions that negatively affect the readability of the generated text, especially when the style is hard to produce. To tackle these issues, we propose a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an orthogonal projection of a negatively perturbed prompt onto the original positive prompt. This approach helps steer the generation away from artifacts while maintaining the intended content, and encourages more diverse, yet plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which relies on unconditional predictions and produces noise at high guidance scales, DOG introduces a more stable, disentangled direction in the latent space. To control the strength of the guidance across the denoising process, we apply a triangular schedule: weak at the start and end of denoising, when the process is most sensitive, and strongest in the middle steps. Experimental results on the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.",
        "arxiv_id": "2508.17017",
        "ARXIVID": "2508.17017",
        "COMMENT": "Presents a new sampling guidance strategy (Dual Orthogonal Guidance) for diffusion-based handwritten text generation, improving style variability and clarity. While it is a generative modeling paper, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is relevant to generative modeling in vision, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.17356": {
        "authors": [
            "Jiazi Bu",
            "Pengyang Ling",
            "Yujie Zhou",
            "Yibin Wang",
            "Yuhang Zang",
            "Tong Wu",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
        "abstract": "arXiv:2508.17356v1 Announce Type: new  Abstract: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: \"When to cache\" and \"How to use cache\", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.",
        "arxiv_id": "2508.17356",
        "ARXIVID": "2508.17356",
        "COMMENT": "Related to generative modeling (diffusion models) and clever statistical tricks (adaptive caching), but does not directly match any of the four criteria. May be of general interest for generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.17029": {
        "authors": [
            "Mingliang Li",
            "Lin Yuanbo Wu",
            "Changhong Liu",
            "Hanxi Li"
        ],
        "title": "A Novel Local Focusing Mechanism for Deepfake Detection Generalization",
        "abstract": "arXiv:2508.17029v1 Announce Type: new  Abstract: The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in https://github.com/lmlpy/LFM.git",
        "arxiv_id": "2508.17029",
        "ARXIVID": "2508.17029",
        "COMMENT": "Does not match any specific criterion. Proposes a new local focus mechanism for deepfake detection, which is computer vision but not directly related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.17778": {
        "authors": [
            "Maxime Elkael",
            "Salvatore D'Oro",
            "Leonardo Bonati",
            "Michele Polese",
            "Yunseong Lee",
            "Koichiro Furueda",
            "Tommaso Melodia"
        ],
        "title": "AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks",
        "abstract": "arXiv:2508.17778v1 Announce Type: new  Abstract: The Open RAN movement has catalyzed a transformation toward programmable, interoperable cellular infrastructures. Yet, today's deployments still rely heavily on static control and manual operations. To move beyond this limitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic framework that generates and orchestrates a fabric of distributed AI agents based on Natural Language (NL) intents. Unlike traditional approaches that require explicit programming, AgentRAN's LLM-powered agents interpret natural language intents, negotiate strategies through structured conversations, and orchestrate control loops across the network. AgentRAN instantiates a self-organizing hierarchy of agents that decompose complex intents across time scales (from sub-millisecond to minutes), spatial domains (cell to network-wide), and protocol layers (PHY/MAC to RRC). A central innovation is the AI-RAN Factory, an automated synthesis pipeline that observes agent interactions and continuously generates new agents embedding improved control algorithms, effectively transforming the network from a static collection of functions into an adaptive system capable of evolving its own intelligence. We demonstrate AgentRAN through live experiments on 5G testbeds where competing user demands are dynamically balanced through cascading intents. By replacing rigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G networks autonomously interpret, adapt, and optimize their behavior to meet operator goals.",
        "arxiv_id": "2508.17778",
        "ARXIVID": "2508.17778",
        "COMMENT": "Does not match any specific criterion. Focuses on agentic AI for 6G networks using LLMs for network control, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.17843": {
        "authors": [
            "Weiqi Yan",
            "Lvhai Chen",
            "Shengchuan Zhang",
            "Yan Zhang",
            "Liujuan Cao"
        ],
        "title": "SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection",
        "abstract": "arXiv:2508.17843v1 Announce Type: new  Abstract: The difficulty of pixel-level annotation has significantly hindered the development of the Camouflaged Object Detection (COD) field. To save on annotation costs, previous works leverage the semi-supervised COD framework that relies on a small number of labeled data and a large volume of unlabeled data. We argue that there is still significant room for improvement in the effective utilization of unlabeled data. To this end, we introduce a Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection (SCOUT). It includes an Adaptive Data Augment and Selection (ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable data for annotation through an adversarial augment and sampling strategy. The TFM module further leverages the selected valuable data by combining camouflage-related knowledge and text-visual interaction. To adapt to this work, we build a new dataset, namely RefTextCOD. Extensive experiments show that the proposed method surpasses previous semi-supervised methods in the COD field and achieves state-of-the-art performance. Our code will be released at https://github.com/Heartfirey/SCOUT.",
        "arxiv_id": "2508.17843",
        "ARXIVID": "2508.17843",
        "COMMENT": "Somewhat related to computer vision and semi-supervised learning, but does not directly match any of the four criteria. Focuses on camouflaged object detection with text fusion, but not on embodied agents, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16777": {
        "authors": [
            "Mingyang Li",
            "Viktor Schlegel",
            "Tingting Mu",
            "Wuraola Oyewusi",
            "Kai Kang",
            "Goran Nenadic"
        ],
        "title": "Evaluation and LLM-Guided Learning of ICD Coding Rationales",
        "abstract": "arXiv:2508.16777v1 Announce Type: new  Abstract: Automated clinical coding involves mapping unstructured text from Electronic Health Records (EHRs) to standardized code systems such as the International Classification of Diseases (ICD). While recent advances in deep learning have significantly improved the accuracy and efficiency of ICD coding, the lack of explainability in these models remains a major limitation, undermining trust and transparency. Current explorations about explainability largely rely on attention-based techniques and qualitative assessments by physicians, yet lack systematic evaluation using consistent criteria on high-quality rationale datasets, as well as dedicated approaches explicitly trained to generate rationales for further enhancing explanation. In this work, we conduct a comprehensive evaluation of the explainability of the rationales for ICD coding through two key lenses: faithfulness that evaluates how well explanations reflect the model's actual reasoning and plausibility that measures how consistent the explanations are with human expert judgment. To facilitate the evaluation of plausibility, we construct a new rationale-annotated dataset, offering denser annotations with diverse granularity and aligns better with current clinical practice, and conduct evaluation across three types of rationales of ICD coding. Encouraged by the promising plausibility of LLM-generated rationales for ICD coding, we further propose new rationale learning methods to improve the quality of model-generated rationales, where rationales produced by prompting LLMs with/without annotation examples are used as distant supervision signals. We empirically find that LLM-generated rationales align most closely with those of human experts. Moreover, incorporating few-shot human-annotated examples not only further improves rationale generation but also enhances rationale-learning approaches.",
        "arxiv_id": "2508.16777",
        "ARXIVID": "2508.16777",
        "COMMENT": "Related to LLMs and explainability in clinical coding, but does not directly match any of the four criteria. No focus on spatial intelligence, VLLMs/MLLMs for vision, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17635": {
        "authors": [
            "Remi Chierchia",
            "Rodrigo Santa Cruz",
            "L\\'eo Lebrat",
            "Yulia Arzhaeva",
            "Mohammad Ali Armin",
            "Jeremy Oorloff",
            "Chuong Nguyen",
            "Olivier Salvado",
            "Clinton Fookes",
            "David Ahmedt-Aristizabal"
        ],
        "title": "Wound3DAssist: A Practical Framework for 3D Wound Assessment",
        "abstract": "arXiv:2508.17635v1 Announce Type: new  Abstract: Managing chronic wounds remains a major healthcare challenge, with clinical assessment often relying on subjective and time-consuming manual documentation methods. Although 2D digital videometry frameworks aided the measurement process, these approaches struggle with perspective distortion, a limited field of view, and an inability to capture wound depth, especially in anatomically complex or curved regions. To overcome these limitations, we present Wound3DAssist, a practical framework for 3D wound assessment using monocular consumer-grade videos. Our framework generates accurate 3D models from short handheld smartphone video recordings, enabling non-contact, automatic measurements that are view-independent and robust to camera motion. We integrate 3D reconstruction, wound segmentation, tissue classification, and periwound analysis into a modular workflow. We evaluate Wound3DAssist across digital models with known geometry, silicone phantoms, and real patients. Results show that the framework supports high-quality wound bed visualization, millimeter-level accuracy, and reliable tissue composition analysis. Full assessments are completed in under 20 minutes, demonstrating feasibility for real-world clinical use.",
        "arxiv_id": "2508.17635",
        "ARXIVID": "2508.17635",
        "COMMENT": "Related to 3D reconstruction and computer vision, but does not directly match any of the four criteria. Focuses on medical application (wound assessment) rather than embodied agents, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18007": {
        "authors": [
            "Xinyue Liu",
            "Jianyuan Wang",
            "Biao Leng",
            "Shuo Zhang"
        ],
        "title": "Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection",
        "abstract": "arXiv:2508.18007v1 Announce Type: new  Abstract: Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any labels even when the training set may contain anomalous samples. To achieve FUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based on teacher-student framework into the FUAD setting. However, due to the presence of anomalies in the training data, traditional KD methods risk enabling the student to learn the teacher's representation of anomalies under FUAD setting, thereby resulting in poor anomaly detection performance. To address this issue, we propose a novel Cross-Domain Distillation (CDD) framework based on the widely studied reverse distillation (RD) paradigm. Specifically, we design a Domain-Specific Training, which divides the training set into multiple domains with lower anomaly ratios and train a domain-specific student for each. Cross-Domain Knowledge Aggregation is then performed, where pseudo-normal features generated by domain-specific students collaboratively guide a global student to learn generalized normal representations across all samples. Experimental results on noisy versions of the MVTec AD and VisA datasets demonstrate that our method achieves significant performance improvements over the baseline, validating its effectiveness under FUAD setting.",
        "arxiv_id": "2508.18007",
        "ARXIVID": "2508.18007",
        "COMMENT": "Related to anomaly detection and knowledge distillation, but does not match any of the four criteria directly. No focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17186": {
        "authors": [
            "Zhenghui Zhao",
            "Chen Wu",
            "Di Wang",
            "Hongruixuan Chen",
            "Cuiqun Chen",
            "Zhuo Zheng",
            "Bo Du",
            "Liangpei Zhang"
        ],
        "title": "Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting",
        "abstract": "arXiv:2508.17186v1 Announce Type: new  Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP",
        "arxiv_id": "2508.17186",
        "ARXIVID": "2508.17186",
        "COMMENT": "Somewhat related to general computer vision and machine learning, but does not directly match any of the four criteria. Focuses on weakly-supervised change detection in satellite images using adversarial prompting, not on embodied agents, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16934": {
        "authors": [
            "Tim Mach",
            "Daniel Rueckert",
            "Alex Berger",
            "Laurin Lux",
            "Ivan Ezhov"
        ],
        "title": "Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation",
        "abstract": "arXiv:2508.16934v1 Announce Type: new  Abstract: This work presents a novel deep learning framework for segmenting cerebral vasculature in hyperspectral brain images. We address the critical challenge of severe label scarcity, which impedes conventional supervised training. Our approach utilizes a novel unsupervised domain adaptation methodology, using a small, expert-annotated ground truth alongside unlabeled data. Quantitative and qualitative evaluations confirm that our method significantly outperforms existing state-of-the-art approaches, demonstrating the efficacy of domain adaptation for label-scarce biomedical imaging tasks.",
        "arxiv_id": "2508.16934",
        "ARXIVID": "2508.16934",
        "COMMENT": "Does not directly match any criterion. Focuses on unsupervised domain adaptation for hyperspectral brain image segmentation, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17265": {
        "authors": [
            "Zhenyu Liu",
            "Huizhi Liang",
            "Xinrun Li",
            "Vaclav Snasel",
            "Varun Ojha"
        ],
        "title": "AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks",
        "abstract": "arXiv:2508.17265v1 Announce Type: new  Abstract: Adversarial distillation (AD) is a knowledge distillation technique that facilitates the transfer of robustness from teacher deep neural network (DNN) models to lightweight target (student) DNN models, enabling the target models to perform better than only training the student model independently. Some previous works focus on using a small, learnable teacher (guide) model to improve the robustness of a student model. Since a learnable guide model starts learning from scratch, maintaining its optimal state for effective knowledge transfer during co-training is challenging. Therefore, we propose a novel Adaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT, dynamically adjusts the training state of the guide model to install robustness to the target model. Specifically, we develop two separate loss functions as part of the AdaGAT method, allowing the guide model to participate more actively in backpropagation to achieve its optimal state. We evaluated our approach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and TinyImageNet, using the WideResNet-34-10 model as the target model. Our observations reveal that appropriately adjusting the guide model within a certain accuracy range enhances the target model's robustness across various adversarial attacks compared to a variety of baseline models.",
        "arxiv_id": "2508.17265",
        "ARXIVID": "2508.17265",
        "COMMENT": "Does not directly match any criterion. Focuses on adversarial training and knowledge distillation for robustness in DNNs, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17213": {
        "authors": [
            "Qibin Zhang",
            "Xinyu Hao",
            "Qiao Chen",
            "Rui Xu",
            "Fengyu Cong",
            "Cheng Lu",
            "Hongming Xu"
        ],
        "title": "Multi-modal Knowledge Decomposition based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology",
        "abstract": "arXiv:2508.17213v1 Announce Type: new  Abstract: Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data fusion analysis. However, the simultaneous acquisition of multi-modal data, such as genomic and pathological information, is often challenging due to cost or technical limitations. To address this challenge, we propose an online distillation approach based on Multi-modal Knowledge Decomposition (MKD) to enhance IHC biomarker prediction in haematoxylin and eosin (H\\&E) stained histopathology images. This method leverages paired genomic-pathology data during training while enabling inference using either pathology slides alone or both modalities. Two teacher and one student models are developed to extract modality-specific and modality-general features by minimizing the MKD loss. To maintain the internal structural relationships between samples, Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally, Collaborative Learning for Online Distillation (CLOD) facilitates mutual learning between teacher and student models, encouraging diverse and complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU datasets demonstrate that our approach achieves superior performance in IHC biomarker prediction using uni-modal data. Our code is available at https://github.com/qiyuanzz/MICCAI2025_MKD.",
        "arxiv_id": "2508.17213",
        "ARXIVID": "2508.17213",
        "COMMENT": "Does not directly match any criterion. Focuses on multi-modal knowledge distillation for biomarker prediction, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17885": {
        "authors": [
            "Raul Balmez",
            "Alexandru Brateanu",
            "Ciprian Orhei",
            "Codruta Ancuti",
            "Cosmin Ancuti"
        ],
        "title": "ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement",
        "abstract": "arXiv:2508.17885v1 Announce Type: new  Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication.",
        "arxiv_id": "2508.17885",
        "ARXIVID": "2508.17885",
        "COMMENT": "ISALux introduces a transformer-based model for low-light image enhancement with semantic and illumination priors. While it is a vision model, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17282": {
        "authors": [
            "Xin Zhang",
            "Jiaming Chu",
            "Jian Zhao",
            "Yuchu Jiang",
            "Xu Yang",
            "Lei Jin",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection",
        "abstract": "arXiv:2508.17282v1 Announce Type: new  Abstract: Deepfake detection is a critical task in identifying manipulated multimedia content. In real-world scenarios, deepfake content can manifest across multiple modalities, including audio and video. To address this challenge, we present ERF-BA-TFD+, a novel multimodal deepfake detection model that combines enhanced receptive field (ERF) and audio-visual fusion. Our model processes both audio and video features simultaneously, leveraging their complementary information to improve detection accuracy and robustness. The key innovation of ERF-BA-TFD+ lies in its ability to model long-range dependencies within the audio-visual input, allowing it to better capture subtle discrepancies between real and fake content. In our experiments, we evaluate ERF-BA-TFD+ on the DDL-AV dataset, which consists of both segmented and full-length video clips. Unlike previous benchmarks, which focused primarily on isolated segments, the DDL-AV dataset allows us to assess the model's performance in a more comprehensive and realistic setting. Our method achieves state-of-the-art results on this dataset, outperforming existing techniques in terms of both accuracy and processing speed. The ERF-BA-TFD+ model demonstrated its effectiveness in the \"Workshop on Deepfake Detection, Localization, and Interpretability,\" Track 2: Audio-Visual Detection and Localization (DDL-AV), and won first place in this competition.",
        "arxiv_id": "2508.17282",
        "ARXIVID": "2508.17282",
        "COMMENT": "Presents a new multimodal (audio-visual) deepfake detection model. While it is a multi-modal model, it is not a VLLM/MLLM or vision foundation model, nor does it address spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17502": {
        "authors": [
            "Hugo Bohy",
            "Minh Tran",
            "Kevin El Haddad",
            "Thierry Dutoit",
            "Mohammad Soleymani"
        ],
        "title": "Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice",
        "abstract": "arXiv:2508.17502v1 Announce Type: new  Abstract: Human social behaviors are inherently multimodal necessitating the development of powerful audiovisual models for their perception. In this paper, we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE), which is pre-trained on audiovisual social data. Specifically, we modify CAV-MAE to receive a larger number of frames as input and pre-train it on a large dataset of human social interaction (VoxCeleb2) in a self-supervised manner. We demonstrate the effectiveness of this model by finetuning and evaluating the model on different social and affective downstream tasks, namely, emotion recognition, laughter detection and apparent personality estimation. The model achieves state-of-the-art results on multimodal emotion recognition and laughter recognition and competitive results for apparent personality estimation, demonstrating the effectiveness of in-domain self-supervised pre-training. Code and model weight are available here https://github.com/HuBohy/SocialMAE.",
        "arxiv_id": "2508.17502",
        "ARXIVID": "2508.17502",
        "COMMENT": "Presents a multimodal autoencoder for face and voice, which is related to multi-modal learning but does not introduce a new VLLM/MLLM or vision foundation model, nor does it focus on spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17849": {
        "authors": [
            "Jingyi Liao",
            "Xun Xu",
            "Chuan-Sheng Foo",
            "Lile Cai"
        ],
        "title": "Box-Level Class-Balanced Sampling for Active Object Detection",
        "abstract": "arXiv:2508.17849v1 Announce Type: new  Abstract: Training deep object detectors demands expensive bounding box annotation. Active learning (AL) is a promising technique to alleviate the annotation burden. Performing AL at box-level for object detection, i.e., selecting the most informative boxes to label and supplementing the sparsely-labelled image with pseudo labels, has been shown to be more cost-effective than selecting and labelling the entire image. In box-level AL for object detection, we observe that models at early stage can only perform well on majority classes, making the pseudo labels severely class-imbalanced. We propose a class-balanced sampling strategy to select more objects from minority classes for labelling, so as to make the final training data, \\ie, ground truth labels obtained by AL and pseudo labels, more class-balanced to train a better model. We also propose a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo labels. We evaluate our method on public benchmarking datasets and show that our method achieves state-of-the-art performance.",
        "arxiv_id": "2508.17849",
        "ARXIVID": "2508.17849",
        "COMMENT": "Does not directly match any specific criterion. Focuses on active learning for object detection, which is related to computer vision but not to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17007": {
        "authors": [
            "Riad Hassan",
            "M. Rubaiyat Hossain Mondal",
            "Sheikh Iqbal Ahamed",
            "Fahad Mostafa",
            "Md Mostafijur Rahman"
        ],
        "title": "An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation",
        "abstract": "arXiv:2508.17007v1 Announce Type: new  Abstract: Proper segmentation of organs-at-risk is important for radiation therapy, surgical planning, and diagnostic decision-making in medical image analysis. While deep learning-based segmentation architectures have made significant progress, they often fail to balance segmentation accuracy with computational efficiency. Most of the current state-of-the-art methods either prioritize performance at the cost of high computational complexity or compromise accuracy for efficiency. This paper addresses this gap by introducing an efficient dual-line decoder segmentation network (EDLDNet). The proposed method features a noisy decoder, which learns to incorporate structured perturbation at training time for better model robustness, yet at inference time only the noise-free decoder is executed, leading to lower computational cost. Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs), and Up-Convolution Blocks (UCBs) are further utilized to optimize feature representation and boost segmentation performance. By leveraging multi-scale segmentation masks from both decoders, we also utilize a mutation-based loss function to enhance the model's generalization. Our approach outperforms SOTA segmentation architectures on four publicly available medical imaging datasets. EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse dataset, surpassing baseline model like UNet by 13.89% in Dice score while significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice score but also maintains comparable computational efficiency. The outstanding performance across diverse datasets establishes EDLDNet's strong generalization, computational efficiency, and robustness. The source code, pre-processed data, and pre-trained weights will be available at https://github.com/riadhassan/EDLDNet .",
        "arxiv_id": "2508.17007",
        "ARXIVID": "2508.17007",
        "COMMENT": "Does not match any specific criterion. Proposes a new segmentation network for medical images, not directly related to spatial intelligence in embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17261": {
        "authors": [
            "Sankalp Pandey",
            "Xuan Bac Nguyen",
            "Nicholas Borys",
            "Hugh Churchill",
            "Khoa Luu"
        ],
        "title": "CLIFF: Continual Learning for Incremental Flake Features in 2D Material Identification",
        "abstract": "arXiv:2508.17261v1 Announce Type: new  Abstract: Identifying quantum flakes is crucial for scalable quantum hardware; however, automated layer classification from optical microscopy remains challenging due to substantial appearance shifts across different materials. In this paper, we propose a new Continual-Learning Framework for Flake Layer Classification (CLIFF). To our knowledge, this is the first systematic study of continual learning in the domain of two-dimensional (2D) materials. Our method enables the model to differentiate between materials and their physical and optical properties by freezing a backbone and base head trained on a reference material. For each new material, it learns a material-specific prompt, embedding, and a delta head. A prompt pool and a cosine-similarity gate modulate features and compute material-specific corrections. Additionally, we incorporate memory replay with knowledge distillation. CLIFF achieves competitive accuracy with significantly lower forgetting than naive fine-tuning and a prompt-based baseline.",
        "arxiv_id": "2508.17261",
        "ARXIVID": "2508.17261",
        "COMMENT": "Does not match any specific criterion. Focuses on continual learning for 2D material identification, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18190": {
        "authors": [
            "Zirui Tang",
            "Boyu Niu",
            "Xuanhe Zhou",
            "Boxiu Li",
            "Wei Zhou",
            "Jiannan Wang",
            "Guoliang Li",
            "Xinyi Zhang",
            "Fan Wu"
        ],
        "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
        "abstract": "arXiv:2508.18190v1 Announce Type: new  Abstract: Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.",
        "arxiv_id": "2508.18190",
        "ARXIVID": "2508.18190",
        "COMMENT": "Does not match any specific criterion. Focuses on table question answering with LLMs, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17605": {
        "authors": [
            "Jonathan P. Crall",
            "Charles V. Stewart",
            "Tanya Y. Berger-Wolf",
            "Daniel I. Rubenstein",
            "Siva R. Sundaresan"
        ],
        "title": "HotSpotter - Patterned Species Instance Recognition",
        "abstract": "arXiv:2508.17605v1 Announce Type: new  Abstract: We present HotSpotter, a fast, accurate algorithm for identifying individual animals against a labeled database. It is not species specific and has been applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We describe two approaches, both based on extracting and matching keypoints or \"hotspots\". The first tests each new query image sequentially against each database image, generating a score for each database image in isolation, and ranking the results. The second, building on recent techniques for instance recognition, matches the query image against the database using a fast nearest neighbor search. It uses a competitive scoring mechanism derived from the Local Naive Bayes Nearest Neighbor algorithm recently proposed for category recognition. We demonstrate results on databases of more than 1000 images, producing more accurate matches than published methods and matching each query image in just a few seconds.",
        "arxiv_id": "2508.17605",
        "ARXIVID": "2508.17605",
        "COMMENT": "Does not match any specific criterion. Focuses on animal instance recognition using keypoint matching, not on embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}