{
    "2601.16520": {
        "authors": [
            "Daixian Liu",
            "Jiayi Kuang",
            "Yinghui Li",
            "Yangning Li",
            "Di Yin",
            "Haoyu Cao",
            "Xing Sun",
            "Ying Shen",
            "Hai-Tao Zheng",
            "Liang Lin",
            "Philip S. Yu"
        ],
        "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
        "abstract": "arXiv:2601.16520v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.",
        "arxiv_id": "2601.16520",
        "ARXIVID": "2601.16520",
        "COMMENT": "Matches criteria 1 (spatial reasoning in MLLMs for embodied agents), 2 (evaluates MLLMs), and 3 (introduces a new benchmark for compositional spatial reasoning with rigorous geometric grounding). The TangramPuzzle benchmark and TCE framework are novel contributions for evaluating spatial intelligence in MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16449": {
        "authors": [
            "Xiaojiang Peng",
            "Jingyi Chen",
            "Zebang Cheng",
            "Bao Peng",
            "Fengyi Wu",
            "Yifei Dong",
            "Shuyuan Tu",
            "Qiyu Hu",
            "Huiting Huang",
            "Yuxiang Lin",
            "Jun-Yan He",
            "Kai Wang",
            "Zheng Lian",
            "Zhi-Qi Cheng"
        ],
        "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
        "abstract": "arXiv:2601.16449v1 Announce Type: new  Abstract: Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.",
        "arxiv_id": "2601.16449",
        "ARXIVID": "2601.16449",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for multimodal emotion understanding). Presents Emotion-LLaMAv2, a new multimodal LLM for emotion reasoning, and MMEVerse, a large-scale benchmark for multimodal emotion understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16965": {
        "authors": [
            "Riyang Bao",
            "Cheng Yang",
            "Dazhou Yu",
            "Zhexiang Tang",
            "Gengchen Mai",
            "Liang Zhao"
        ],
        "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts",
        "abstract": "arXiv:2601.16965v1 Announce Type: new  Abstract: Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.",
        "arxiv_id": "2601.16965",
        "ARXIVID": "2601.16965",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding and spatial intelligence on embodied agents). The paper introduces Spatial-Agent, a new agent grounded in spatial information science, formalizing geo-analytical QA as concept transformation and outperforming baselines on spatial reasoning tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16973": {
        "authors": [
            "Zirui Wang",
            "Junyi Zhang",
            "Jiaxin Ge",
            "Long Lian",
            "Letian Fu",
            "Lisa Dunlap",
            "Ken Goldberg",
            "XuDong Wang",
            "Ion Stoica",
            "David M. Chan",
            "Sewon Min",
            "Joseph E. Gonzalez"
        ],
        "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
        "abstract": "arXiv:2601.16973v1 Announce Type: new  Abstract: Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
        "arxiv_id": "2601.16973",
        "ARXIVID": "2601.16973",
        "COMMENT": "Matches criterion 3: introduces VisGym, a new suite of diverse, customizable, and scalable environments for evaluating and training multimodal agents, with a focus on VLMs in interactive, embodied settings. Also relevant to criterion 4 as it benchmarks vision foundation models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16964": {
        "authors": [
            "Mohamed Amine Ferrag",
            "Abderrahmane Lakas",
            "Merouane Debbah"
        ],
        "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems",
        "abstract": "arXiv:2601.16964v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive",
        "arxiv_id": "2601.16964",
        "ARXIVID": "2601.16964",
        "COMMENT": "Matches criterion 3: introduces AgentDrive, a new large-scale benchmark dataset and MCQ benchmark for agentic AI reasoning in autonomous systems, with a novel LLM-generated scenario pipeline and structured evaluation. Also relevant to embodied AI and simulator-based benchmarking.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16471": {
        "authors": [
            "Meng Cao",
            "Haoran Tang",
            "Haoze Zhao",
            "Mingfei Han",
            "Ruyang Liu",
            "Qiang Sun",
            "Xiaojun Chang",
            "Ian Reid",
            "Xiaodan Liang"
        ],
        "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
        "abstract": "arXiv:2601.16471v1 Announce Type: new  Abstract: Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence.",
        "arxiv_id": "2601.16471",
        "ARXIVID": "2601.16471",
        "COMMENT": "Matches criteria 2 (MLLMs), 3 (new dataset and benchmark for physical reasoning in embodied AI), and 4 (applies vision foundation models to physical world understanding). The use of gameplay glitches as supervision for physical reasoning is a novel and clever empirical approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.16725": {
        "authors": [
            "Meituan LongCat Team",
            "Anchun Gui",
            "Bei Li",
            "Bingyang Tao",
            "Bole Zhou",
            "Borun Chen",
            "Chao Zhang",
            "Chao Zhang",
            "Chen Gao",
            "Chen Zhang",
            "Chengcheng Han",
            "Chenhui Yang",
            "Chuyu Zhang",
            "Cong Chen",
            "Cunguang Wang",
            "Daoru Pan",
            "Defei Bu",
            "Dengchang Zhao",
            "Di Xiu",
            "Dishan Liu",
            "Dongyu Ru",
            "Dunwei Tu",
            "Fan Wu",
            "Fengcheng Yuan",
            "Fengcun Li",
            "Gang Xu",
            "Guanyu Wu",
            "Guoyuan Lin",
            "Haibin Wang",
            "Hansi Yang",
            "Hao Yang",
            "Haonan Yan",
            "Haoxiang Ma",
            "Haoxing Wen",
            "Hongyan Hao",
            "Hongyin Tang",
            "Hongyu Zang",
            "Hongzhi Ni",
            "Hui Su",
            "Jiacheng Zhang",
            "Jiahong Zhou",
            "Jiahuan Li",
            "Jiaming Wang",
            "Jian Yang",
            "Jianfei Zhang",
            "Jianhao Xu",
            "Jianing Wang",
            "Jiapeng Zhu",
            "Jiaqi Sun",
            "Jiarong Shi",
            "Jiarui Zhao",
            "Jingang Wang",
            "Jinluan Yang",
            "Jinrui Ding",
            "Jinwei Xiao",
            "Jiyuan He",
            "Juncan Xu",
            "Kefeng Zhang",
            "Keheng Wang",
            "Li Wei",
            "Lianhui Ma",
            "Lin Qiu",
            "Lingbing Kong",
            "Lingchuan Liu",
            "Linsen Guo",
            "Mengshen Zhu",
            "Mengxia Shen",
            "Mingyang Zhu",
            "Peiguang Li",
            "Peng Pei",
            "Pengcheng Jia",
            "Pengtao Zhang",
            "Peng Zhao",
            "Qi Gu",
            "Qiong Huang",
            "Qiyuan Duan",
            "Quanchi Weng",
            "Rongxiang Weng",
            "Rongzhi Zhang",
            "Rumei Li",
            "Shanglin Lei",
            "Shengnan An",
            "Shijun Dai",
            "Shuaikang Liu",
            "Shuang Zhou",
            "Shuo Wang",
            "Songyuan Zhao",
            "Tao Liang",
            "Tianhao Hu",
            "Tianze Chen",
            "Wei Liu",
            "Wei Shi",
            "Wei Wang",
            "Weifeng Tang",
            "Wenjie Shi",
            "Wenlong Zhu",
            "Wentao Chen",
            "Wentao Shi",
            "Xi Su",
            "Xiangcheng Liu",
            "Xiandi Ma",
            "Xiangyu Xi",
            "Xiangyuan Liu",
            "Xiangzhou Huang",
            "Xiao Liu",
            "Xiaodong Cai",
            "Xiaolong Chen",
            "Xiaowei Shi",
            "Xiaoyu Li",
            "Xin Chen",
            "Xingchen Liu",
            "Xuan Huang",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yan Chen",
            "Yang Bai",
            "Yang Liu",
            "Yang Yang",
            "Yang Zheng",
            "Yaoming Wang",
            "Yaoming Zhu",
            "Yaqi Huo",
            "Yanyu Chen",
            "Yaorui Shi",
            "Yerui Sun",
            "Yi Zhang",
            "Yihao Chen",
            "Yi-Kai Zhang",
            "Yifan Lu",
            "Yifan Zhao",
            "Yitao Zhai",
            "Yongjing Yin",
            "Yongwei Zhou",
            "Youshao Xiao",
            "Yuchuan Dai",
            "Yuchen Xie",
            "Yuchen Yu",
            "Yufei Zhang",
            "Yuhuai Wei",
            "Yulei Qian",
            "Yunfan Liang",
            "Yunke Zhao",
            "Yuwei Jiang",
            "Yuxin Bian",
            "Yuxin Chen",
            "Yuxin Liu",
            "Yue Xu",
            "Yueqing Sun",
            "Zeyang Yu",
            "Zhao Yang",
            "Zhengsheng Huang",
            "Zhengyu Chen",
            "Zhijian Liu",
            "Zhikang Xia",
            "Zhimin Lin",
            "Zhiyuan Yao",
            "Zhuofan Chen",
            "Zhuowen Han",
            "Zijian Zhang",
            "Ziran Li",
            "Ziwen Wang",
            "Ziyuan Zhuang"
        ],
        "title": "LongCat-Flash-Thinking-2601 Technical Report",
        "abstract": "arXiv:2601.16725v1 Announce Type: new  Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
        "arxiv_id": "2601.16725",
        "ARXIVID": "2601.16725",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs). Introduces LongCat-Flash-Thinking-2601, a 560B-parameter open-source MoE reasoning model with strong agentic reasoning and tool-use capabilities, and novel training/infrastructure for robust agentic behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.16582": {
        "authors": [
            "Yuqian Zheng",
            "Mariana-Iuliana Georgescu"
        ],
        "title": "X-Aligner: Composed Visual Retrieval without the Bells and Whistles",
        "abstract": "arXiv:2601.16582v1 Announce Type: new  Abstract: Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.",
        "arxiv_id": "2601.16582",
        "ARXIVID": "2601.16582",
        "COMMENT": "Matches criterion 2 and 4: proposes a new framework leveraging VLMs (BLIP/BLIP-2) for composed visual retrieval, with a novel cross-attention module and strong empirical results. Directly about vision-language models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.16982": {
        "authors": [
            "Basile Van Hoorick",
            "Dian Chen",
            "Shun Iwase",
            "Pavel Tokmakov",
            "Muhammad Zubair Irshad",
            "Igor Vasiljevic",
            "Swati Gupta",
            "Fangzhou Cheng",
            "Sergey Zakharov",
            "Vitor Campagnolo Guizilini"
        ],
        "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
        "abstract": "arXiv:2601.16982v1 Announce Type: new  Abstract: Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/",
        "arxiv_id": "2601.16982",
        "ARXIVID": "2601.16982",
        "COMMENT": "Matches criteria 4 (vision foundation models and their application to dynamic view synthesis), and partially criteria 3 (introduces AnyViewBench, a new benchmark for dynamic view synthesis in real-world scenarios). The method is a generalist spatiotemporal representation for novel view synthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16649": {
        "authors": [
            "Amin Rakhsha",
            "Thomas Hehn",
            "Pietro Mazzaglia",
            "Fabio Valerio Massoli",
            "Arash Behboodi",
            "Tribhuvanesh Orekondy"
        ],
        "title": "LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents",
        "abstract": "arXiv:2601.16649v1 Announce Type: new  Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.",
        "arxiv_id": "2601.16649",
        "ARXIVID": "2601.16649",
        "COMMENT": "Matches criteria 3 (embodied AI, new benchmark for multi-turn, long-horizon agentic problems with controlled environments and oracle interventions). The paper introduces a new suite of procedurally generated tasks and an oracle counterfactual framework, offering a novel angle on skill isolation in agentic environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16344": {
        "authors": [
            "Fan Nie",
            "Junlin Wang",
            "Harper Hua",
            "Federico Bianchi",
            "Yongchan Kwon",
            "Zhenting Qi",
            "Owen Queen",
            "Shang Zhu",
            "James Zou"
        ],
        "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
        "abstract": "arXiv:2601.16344v1 Announce Type: new  Abstract: Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.",
        "arxiv_id": "2601.16344",
        "ARXIVID": "2601.16344",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks/simulators). Introduces DSGym, a holistic framework and benchmark for evaluating and training data science agents in realistic environments, with a focus on agentic planning and execution.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16836": {
        "authors": [
            "Chenxi Ruan",
            "Yu Xiao",
            "Yihan Hou",
            "Guosheng Hu",
            "Wei Zeng"
        ],
        "title": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models",
        "abstract": "arXiv:2601.16836v1 Announce Type: new  Abstract: While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.",
        "arxiv_id": "2601.16836",
        "ARXIVID": "2601.16836",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a new benchmark (ColorConceptBench) to evaluate text-to-image models' understanding of probabilistic color-concept associations, revealing limitations in current models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16885": {
        "authors": [
            "Yangfan Xu",
            "Lilian Zhang",
            "Xiaofeng He",
            "Pengdong Wu",
            "Wenqi Wu",
            "Jun Mao"
        ],
        "title": "GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss",
        "abstract": "arXiv:2601.16885v1 Announce Type: new  Abstract: Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.",
        "arxiv_id": "2601.16885",
        "ARXIVID": "2601.16885",
        "COMMENT": "Matches criterion 1: proposes a self-supervised, geometry and physics-aware loss for transformer-based visual geometry models (VGGT) for large-scale localization and 3D scene understanding, which is a methodological improvement for spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16296": {
        "authors": [
            "Dohun Lee",
            "Chun-Hao Paul Huang",
            "Xuelin Chen",
            "Jong Chul Ye",
            "Duygu Ceylan",
            "Hyeonho Jeong"
        ],
        "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
        "abstract": "arXiv:2601.16296v1 Announce Type: new  Abstract: Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
        "arxiv_id": "2601.16296",
        "ARXIVID": "2601.16296",
        "COMMENT": "Related to criterion 4: proposes Memory-V2V, a memory-augmented video-to-video diffusion model for multi-turn video editing, with a novel token compressor and empirical improvements. Relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.16286": {
        "authors": [
            "Varun Chillara",
            "Dylan Kline",
            "Christopher Alvares",
            "Evan Wooten",
            "Huan Yang",
            "Shlok Khetan",
            "Cade Bauer",
            "Tr\\'e Guillory",
            "Tanishka Shah",
            "Yashodhara Dhariwal",
            "Volodymyr Pavlov",
            "George Popstefanov"
        ],
        "title": "SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems",
        "abstract": "arXiv:2601.16286v1 Announce Type: new  Abstract: Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.   We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.   The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.",
        "arxiv_id": "2601.16286",
        "ARXIVID": "2601.16286",
        "COMMENT": "Matches criterion 1: proposes a new method for improving agentic systems by caching intermediate reasoning steps, which is a methodological improvement for agentic/spatial intelligence systems.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.16645": {
        "authors": [
            "Minsu Gong",
            "Nuri Ryu",
            "Jungseul Ok",
            "Sunghyun Cho"
        ],
        "title": "Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss",
        "abstract": "arXiv:2601.16645v1 Announce Type: new  Abstract: Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.",
        "arxiv_id": "2601.16645",
        "ARXIVID": "2601.16645",
        "COMMENT": "Related to criterion 4: proposes a novel structure-preservation loss for diffusion-based image editing, improving structural fidelity. Relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.16487": {
        "authors": [
            "Remi Chierchia",
            "L\\'eo Lebrat",
            "David Ahmedt-Aristizabal",
            "Yulia Arzhaeva",
            "Olivier Salvado",
            "Clinton Fookes",
            "Rodrigo Santa Cruz"
        ],
        "title": "Multi-View Consistent Wound Segmentation With Neural Fields",
        "abstract": "arXiv:2601.16487v1 Announce Type: new  Abstract: Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm.",
        "arxiv_id": "2601.16487",
        "ARXIVID": "2601.16487",
        "COMMENT": "Somewhat related to criterion 4: applies neural fields (NeRF) for multi-view consistent wound segmentation, which is a vision foundation model application, but in a specific medical context.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.16515": {
        "authors": [
            "Tongcheng Fang",
            "Hanling Zhang",
            "Ruiqi Xie",
            "Zhuo Han",
            "Xin Tao",
            "Tianchen Zhao",
            "Pengfei Wan",
            "Wenbo Ding",
            "Wanli Ouyang",
            "Xuefei Ning",
            "Yu Wang"
        ],
        "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
        "abstract": "arXiv:2601.16515v1 Announce Type: new  Abstract: Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
        "arxiv_id": "2601.16515",
        "ARXIVID": "2601.16515",
        "COMMENT": "Relevant to video generation and efficient transformer attention, but does not directly match any of the four criteria (no spatial intelligence, MLLMs, embodied AI, or vision foundation model application focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.16773": {
        "authors": [
            "Shuai Huang",
            "Xuhan Lin",
            "Yuwu Lu"
        ],
        "title": "CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts",
        "abstract": "arXiv:2601.16773v1 Announce Type: new  Abstract: Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.",
        "arxiv_id": "2601.16773",
        "ARXIVID": "2601.16773",
        "COMMENT": "Does not match any specific criterion. Focuses on few-shot class-incremental learning with prompt-based methods, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.16933": {
        "authors": [
            "Jingran Zhang",
            "Ning Li",
            "Yuanhao Ban",
            "Andrew Bai",
            "Justin Cui"
        ],
        "title": "Reward-Forcing: Autoregressive Video Generation with Reward Feedback",
        "abstract": "arXiv:2601.16933v1 Announce Type: new  Abstract: While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.",
        "arxiv_id": "2601.16933",
        "ARXIVID": "2601.16933",
        "COMMENT": "Does not match any specific criterion. Focuses on autoregressive video generation with reward feedback, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.16771": {
        "authors": [
            "Jiahao Li",
            "Yunpeng Bai",
            "Yongkang Dai",
            "Hao Guo",
            "Hongping Gan",
            "Yilei Shi"
        ],
        "title": "AutoRegressive Generation with B-rep Holistic Token Sequence Representation",
        "abstract": "arXiv:2601.16771v1 Announce Type: new  Abstract: Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.",
        "arxiv_id": "2601.16771",
        "ARXIVID": "2601.16771",
        "COMMENT": "Does not match any specific criterion. Focuses on B-rep (boundary representation) generation for geometry/topology, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.16685": {
        "authors": [
            "Suzhong Fu",
            "Jingqi Dong",
            "Xuan Ding",
            "Rui Sun",
            "Yiming Yang",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "title": "AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning",
        "abstract": "arXiv:2601.16685v1 Announce Type: new  Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.",
        "arxiv_id": "2601.16685",
        "ARXIVID": "2601.16685",
        "COMMENT": "Somewhat related to criterion 2: uses multi-agent reasoning for evaluating medical imaging reports, but not a new VLLM/MLLM. More focused on evaluation methodology in medical domain.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.16541": {
        "authors": [
            "Erik Wallin",
            "Fredrik Kahl",
            "Lars Hammarstrand"
        ],
        "title": "Semi-Supervised Hierarchical Open-Set Classification",
        "abstract": "arXiv:2601.16541v1 Announce Type: new  Abstract: Hierarchical open-set classification handles previously unseen classes by assigning them to the most appropriate high-level category in a class taxonomy. We extend this paradigm to the semi-supervised setting, enabling the use of large-scale, uncurated datasets containing a mixture of known and unknown classes to improve the hierarchical open-set performance. To this end, we propose a teacher-student framework based on pseudo-labeling. Two key components are introduced: 1) subtree pseudo-labels, which provide reliable supervision in the presence of unknown data, and 2) age-gating, a mechanism that mitigates overconfidence in pseudo-labels. Experiments show that our framework outperforms self-supervised pretraining followed by supervised adaptation, and even matches the fully supervised counterpart when using only 20 labeled samples per class on the iNaturalist19 benchmark. Our code is available at https://github.com/walline/semihoc.",
        "arxiv_id": "2601.16541",
        "ARXIVID": "2601.16541",
        "COMMENT": "Does not match any specific criterion. Focuses on semi-supervised hierarchical open-set classification, which is not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.16954": {
        "authors": [
            "Ba-Thinh Lam",
            "Thanh-Huy Nguyen",
            "Hoang-Thien Nguyen",
            "Quang-Khai Bui-Tran",
            "Nguyen Lan Vi Vu",
            "Phat K. Huynh",
            "Ulas Bagci",
            "Min Xu"
        ],
        "title": "Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment",
        "abstract": "arXiv:2601.16954v1 Announce Type: new  Abstract: Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.",
        "arxiv_id": "2601.16954",
        "ARXIVID": "2601.16954",
        "COMMENT": "Not directly matching any criterion, but relevant to general computer vision and machine learning interests. Focuses on domain-invariant semi-supervised segmentation in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.16967": {
        "authors": [
            "Bernes Lorier Atabonfack",
            "Ahmed Tahiru Issah",
            "Mohammed Hardi Abdul Baaki",
            "Clemence Ingabire",
            "Tolulope Olusuyi",
            "Maruf Adewole",
            "Udunna C. Anazodo",
            "Timothy X Brown"
        ],
        "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
        "abstract": "arXiv:2601.16967v1 Announce Type: new  Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.",
        "arxiv_id": "2601.16967",
        "ARXIVID": "2601.16967",
        "COMMENT": "Does not match any specific criterion. Focuses on AI-powered support for medical equipment maintenance, not spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}