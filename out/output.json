{
    "2510.09606": {
        "authors": [
            "Peiwen Sun",
            "Shiqiang Lang",
            "Dongming Wu",
            "Yi Ding",
            "Kaituo Feng",
            "Huadai Liu",
            "Zhen Ye",
            "Rui Liu",
            "Yun-Hui Liu",
            "Jianan Wang",
            "Xiangyu Yue"
        ],
        "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
        "abstract": "arXiv:2510.09606v1 Announce Type: new  Abstract: With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km .",
        "arxiv_id": "2510.09606",
        "ARXIVID": "2510.09606",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents), criterion 3 (new benchmark and dataset for spatial reasoning), and criterion 2 (MLLMs for spatial intelligence). Introduces a large-scale, multi-scale spatial reasoning dataset and model (SpaceVista-7B), with a new benchmark for all-scale spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.08759": {
        "authors": [
            "Yu Qi",
            "Haibo Zhao",
            "Ziyu Guo",
            "Siyuan Ma",
            "Ziyan Chen",
            "Yaokun Han",
            "Renrui Zhang",
            "Zitiantao Lin",
            "Shiji Xin",
            "Yijian Huang",
            "Kai Cheng",
            "Peiheng Wang",
            "Jiazheng Liu",
            "Jiayi Zhang",
            "Yizhe Zhu",
            "Wenqing Wang",
            "Yiran Qin",
            "Xupeng Zhu",
            "Haojie Huang",
            "Lawson L. S. Wong"
        ],
        "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities",
        "abstract": "arXiv:2510.08759v1 Announce Type: new  Abstract: Embodied capabilities refer to a suite of fundamental abilities for an agent to perceive, comprehend, and interact with the physical world. While multimodal large language models (MLLMs) show promise as embodied agents, a thorough and systematic evaluation of their embodied capabilities remains underexplored, as existing benchmarks primarily focus on specific domains such as planning or spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive and fine-grained benchmark that evaluates MLLMs on atomic embodied capabilities. BEAR comprises 4,469 interleaved image-video-text entries across 14 domains in 6 categories, including tasks from low-level pointing, trajectory understanding, spatial reasoning, to high-level planning. Extensive evaluation results of 20 representative MLLMs reveal their persistent limitations across all domains of embodied capabilities. To tackle the shortfall, we propose BEAR-Agent, a multimodal conversable agent that integrates pretrained vision models to strengthen MLLM perception, 3D understanding, and planning capabilities. It substantially enhances MLLM performance across diverse embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that improving MLLM embodied capabilities can benefit embodied tasks in simulated environments. Project website: https://bear-official66.github.io/",
        "arxiv_id": "2510.08759",
        "ARXIVID": "2510.08759",
        "COMMENT": "Matches criterion 2 and 3. This paper introduces BEAR, a comprehensive benchmark for evaluating atomic embodied capabilities in MLLMs, including spatial reasoning and planning. It also proposes BEAR-Agent, a new method for enhancing MLLM embodied capabilities, and demonstrates improvements in simulated environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.09507": {
        "authors": [
            "Zixin Zhang",
            "Kanghao Chen",
            "Xingwang Lin",
            "Lutao Jiang",
            "Xu Zheng",
            "Yuanhuiyi Lyu",
            "Litao Guo",
            "Yinchuan Li",
            "Ying-Cong Chen"
        ],
        "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
        "abstract": "arXiv:2510.09507v1 Announce Type: new  Abstract: The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.",
        "arxiv_id": "2510.09507",
        "ARXIVID": "2510.09507",
        "COMMENT": "Matches criterion 2 and 3. This paper introduces PhysToolBench, a new benchmark for evaluating MLLMs' understanding of physical tools, with tasks on recognition, understanding, and creation. It provides a comprehensive evaluation and analysis of 32 MLLMs, revealing deficiencies and proposing solutions.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.09361": {
        "authors": [
            "Junyan Ye",
            "Dongzhi Jiang",
            "Jun He",
            "Baichuan Zhou",
            "Zilong Huang",
            "Zhiyuan Yan",
            "Hongsheng Li",
            "Conghui He",
            "Weijia Li"
        ],
        "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception",
        "abstract": "arXiv:2510.09361v1 Announce Type: new  Abstract: Recently, Multimodal Large Language Models (MLLMs) have made rapid progress, particularly in enhancing their reasoning capabilities. However, existing reasoning benchmarks still primarily assess language-based reasoning, often treating visual input as replaceable context. To address this gap, we introduce BLINK-Twice, a vision-centric reasoning benchmark grounded in challenging perceptual tasks. Instead of relying on external knowledge, our tasks require models to reason from visual content alone, shifting the focus from language-based to image-grounded reasoning. Compared to prior perception benchmarks, it moves beyond shallow perception (\"see\") and requires fine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice integrates three core components: seven types of visual challenges for testing visual reasoning, natural adversarial image pairs that enforce reliance on visual content, and annotated reasoning chains for fine-grained evaluation of the reasoning process rather than final answers alone. We evaluate 20 leading MLLMs, including 12 foundation models and 8 reasoning-enhanced models. BLINK-Twice poses a significant challenge to current models. While existing reasoning strategies in the language space-such as chain-of-thought or self-criticism can improve performance, they often result in unstable and redundant reasoning. We observe that repeated image observation improves performance across models, and active visual interaction, as demonstrated by models like o3, highlights the need for a new paradigm for vision reasoning. The dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice",
        "arxiv_id": "2510.09361",
        "ARXIVID": "2510.09361",
        "COMMENT": "Matches criterion 2 and 3. This paper introduces BLINK-Twice, a new vision-centric reasoning benchmark for MLLMs, focusing on image-grounded reasoning and fine-grained visual perception. It evaluates 20 leading MLLMs and highlights their limitations, providing a new dataset and analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.08928": {
        "authors": [
            "Yushuo Zheng",
            "Zicheng Zhang",
            "Xiongkuo Min",
            "Huiyu Duan",
            "Guangtao Zhai"
        ],
        "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition",
        "abstract": "arXiv:2510.08928v1 Announce Type: new  Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture their performance in real-time, adversarial environments. We introduce LM Fight Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by pitting them against each other in the classic fighting game Mortal Kombat II, a task requiring rapid visual understanding and tactical, sequential decision-making. In a controlled tournament, we test six leading open- and closed-source models, where each agent operates controlling the same character to ensure a fair comparison. The models are prompted to interpret game frames and state data to select their next actions. Unlike static evaluations, LM Fight Arena provides a fully automated, reproducible, and objective assessment of an LMM's strategic reasoning capabilities in a dynamic setting. This work introduces a challenging and engaging benchmark that bridges the gap between AI evaluation and interactive entertainment.",
        "arxiv_id": "2510.08928",
        "ARXIVID": "2510.08928",
        "COMMENT": "Matches criterion 3: introduces a new benchmark for evaluating LMMs in a dynamic, interactive game environment, which is a novel angle for embodied AI evaluation. Also relevant to criterion 2 as it evaluates VLLMs/MLLMs in a new way.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.08987": {
        "authors": [
            "Qixiang Yin",
            "Huanjin Yao",
            "Jianghao Chen",
            "Jiaxing Huang",
            "Zhicheng Zhao",
            "Fei Su"
        ],
        "title": "Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging",
        "abstract": "arXiv:2510.08987v1 Announce Type: new  Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, they encounter numerous challenges in terms of reasoning efficiency, such as large model size, overthinking, and compromised accuracy in lightweight scenarios. However, research on the reasoning capabilities of lightweight MLLMs is quite lacking. To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves faster inference and higher accuracy via a two-stage optimization, while unifying multimodal reasoning across multiple tasks and using fewer tokens. In the first stage, Tiny-R1V introduces Length-Informed Relative Policy Optimization (LIPO), a novel reinforcement learning method, to train each reasoning model. The LIPO is designed to dynamically adjusts advantages of responses within groups, that is, by prioritizing concise yet high-quality responses to encourage the generation of shorter and more accurate response. In the second stage, we propose Adaptive Model Merging (AMM), a training-free model merging method that merges multiple specialist models into a unified architecture. Specifically, AMM adaptively adjusts the weights of task vectors and robustly optimizes the merged vectors via a novel gradient projection regularization loss function, thus mitigating redundant conflicts between them. Extensive evaluations on ten widely-used reasoning benchmarks covering mathematics, structured data (charts, tables, documents), OCR, and general capabilities showcase the superior performance of Tiny-R1V, enabling lightweight models to excel in diverse multimodal reasoning tasks.",
        "arxiv_id": "2510.08987",
        "ARXIVID": "2510.08987",
        "COMMENT": "Matches criterion 2 (new MLLMs): Proposes Tiny-R1V, a lightweight unified multimodal reasoning model with novel optimization and model merging methods. Directly relevant to multi-modal large language models and their efficient deployment.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.09285": {
        "authors": [
            "Siyuan Huang",
            "Xiaoye Qu",
            "Yafu Li",
            "Yun Luo",
            "Zefeng He",
            "Daizong Liu",
            "Yu Cheng"
        ],
        "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
        "abstract": "arXiv:2510.09285v1 Announce Type: new  Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs.",
        "arxiv_id": "2510.09285",
        "ARXIVID": "2510.09285",
        "COMMENT": "Matches criterion 2: proposes a new policy optimization method for multimodal RL with LVLMs, focusing on token-level visual perception, which is a novel angle for improving VLLM reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.09358": {
        "authors": [
            "Qihang Ma",
            "Shengyu Li",
            "Jie Tang",
            "Dingkang Yang",
            "Shaodong Chen",
            "Yingyi Zhang",
            "Chao Feng",
            "Jiao Ran"
        ],
        "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models",
        "abstract": "arXiv:2510.09358v1 Announce Type: new  Abstract: Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only methods by incorporating multiple modalities of input information to produce a set of conclusive phrases. Traditional multi-modal approaches have been proven to have significant limitations in handling the challenging absence and unseen scenarios. Additionally, we identify shortcomings in existing benchmarks that overestimate model capability due to significant overlap in training tests. In this work, we propose leveraging vision-language models (VLMs) for the MMKP task. Firstly, we use two widely-used strategies, e.g., zero-shot and supervised fine-tuning (SFT) to assess the lower bound performance of VLMs. Next, to improve the complex reasoning capabilities of VLMs, we adopt Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a teacher model to finetune smaller models. Finally, to address the \"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively injects CoT data during training, allowing the model to flexibly leverage its reasoning capabilities during the inference stage. We evaluate the proposed strategies on various datasets and the experimental results demonstrate the effectiveness of the proposed approaches. The code is available at https://github.com/bytedance/DynamicCoT.",
        "arxiv_id": "2510.09358",
        "ARXIVID": "2510.09358",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes new methods for multi-modal keyphrase prediction using vision-language models, dynamic chain-of-thought, and fine-tuning strategies. Also relevant for surprising empirical results in VLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.09320": {
        "authors": [
            "Wenyao Zhang",
            "Hongsi Liu",
            "Bohan Li",
            "Jiawei He",
            "Zekun Qi",
            "Yunnan Wang",
            "Shengyang Zhao",
            "Xinqiang Yu",
            "Wenjun Zeng",
            "Xin Jin"
        ],
        "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation",
        "abstract": "arXiv:2510.09320v1 Announce Type: new  Abstract: Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, a novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach introduces a coarse-to-fine progressive learning framework: 1) Firstly, we aggregate multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance. A proxy task comparing close-distant image patches is designed to enforce depth-aware feature alignment using text prompts; 2) Next, building on the coarse features, we integrate camera pose information and pixel-wise language alignment to refine depth predictions. This module seamlessly integrates with existing self-supervised MDE pipelines (e.g., Monodepth2, ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth estimation. By aggregating CLIP's semantic context and DINO's spatial details through language guidance, our method effectively addresses feature granularity mismatches. Extensive experiments on the KITTI benchmark demonstrate that our method significantly outperforms SOTA methods across all metrics, which also indeed benefits downstream tasks like BEV perception. Code is available at https://github.com/Zhangwenyao1/Hybrid-depth.",
        "arxiv_id": "2510.09320",
        "ARXIVID": "2510.09320",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Uses CLIP and DINO for semantic-spatial feature aggregation in monocular depth estimation, with language guidance for spatial reasoning. Also relevant for downstream embodied tasks like BEV perception.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08668": {
        "authors": [
            "Songtao Jiang",
            "Yuan Wang",
            "Sibo Song",
            "Tianxiang Hu",
            "Chenyi Zhou",
            "Bin Pu",
            "Yan Zhang",
            "Zhibo Yang",
            "Yang Feng",
            "Joey Tianyi Zhou",
            "Jin Hao",
            "Zijian Chen",
            "Ruijia Wu",
            "Tao Tang",
            "Junhui Lv",
            "Hongxia Xu",
            "Hongwei Wang",
            "Jun Xiao",
            "Bin Feng",
            "Fudong Zhu",
            "Kenli Li",
            "Weidi Xie",
            "Jimeng Sun",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "title": "Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding",
        "abstract": "arXiv:2510.08668v1 Announce Type: new  Abstract: Real-world clinical decision-making grapples with integrating information from diverse data modalities, including medical text, 2D/3D images, and video, leading to inefficiencies and potential diagnostic oversights. While generalist vision-language models (VLMs) offer promise, their medical development faces challenges of opaque pipelines, data scarcity, and architectural inflexibility. Here we present Hulu-Med, a transparent medical VLM that unifies understanding across all these modalities. Built upon a unified patch-based vision encoder and an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M) samples to scale from 2D to 3D and video comprehension. The medical-aware token reduction enables efficient training, requiring only 4,000 to 40,000 GPU hours for 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks exhibits state-of-the-art performance, surpassing leading open-source models and competing with proprietary systems in tasks spanning visual question-answering, medical report generation, and complex reasoning in multilingual and rare disease scenarios. By open-sourcing our complete pipeline, we establish that high-performance medical VLM can be achieved transparently, providing a foundational tool for accessible and impactful clinical AI. Code is released on \\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}.",
        "arxiv_id": "2510.08668",
        "ARXIVID": "2510.08668",
        "COMMENT": "Matches criterion 2: introduces Hulu-Med, a new generalist medical vision-language model (VLM) with multi-modal capabilities and open-source pipeline.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08818": {
        "authors": [
            "Yiyang Huang",
            "Yizhou Wang",
            "Yun Fu"
        ],
        "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition",
        "abstract": "arXiv:2510.08818v1 Announce Type: new  Abstract: Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at https://github.com/hukcc/D-CoDe.",
        "arxiv_id": "2510.08818",
        "ARXIVID": "2510.08818",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a training-free adaptation for scaling image-pretrained VLMs to video, with dynamic compression and question decomposition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08638": {
        "authors": [
            "Thomas Fel",
            "Binxu Wang",
            "Michael A. Lepori",
            "Matthew Kowal",
            "Andrew Lee",
            "Randall Balestriero",
            "Sonia Joseph",
            "Ekdeep S. Lubana",
            "Talia Konkle",
            "Demba Ba",
            "Martin Wattenberg"
        ],
        "title": "Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry",
        "abstract": "arXiv:2510.08638v1 Announce Type: new  Abstract: DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts.   In the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits \"Elsewhere\" concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular depth cues matching visual neuroscience principles.   Following these functional results, we analyze the geometry and statistics of the concepts learned by the SAE. We found that representations are partly dense rather than strictly sparse. The dictionary evolves toward greater coherence and departs from maximally orthogonal ideals (Grassmannian frames). Within an image, tokens occupy a low dimensional, locally connected set persisting after removing position. These signs suggest representations are organized beyond linear sparsity alone.   Synthesizing these observations, we propose a refined view: tokens are formed by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). This structure is grounded in Gardenfors' conceptual spaces and in the model's mechanism as multi-head attention produces sums of convex mixtures, defining regions bounded by archetypes. We introduce the Minkowski Representation Hypothesis (MRH) and examine its empirical signatures and implications for interpreting vision-transformer representations.",
        "arxiv_id": "2510.08638",
        "ARXIVID": "2510.08638",
        "COMMENT": "Matches criterion 4 (vision foundation models and interpretability). Provides a deep interpretability study of DINOv2, proposing a new hypothesis (Minkowski Representation Hypothesis) for vision transformer representations.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2510.08978": {
        "authors": [
            "Zichuan Wang",
            "Bo Peng",
            "Songlin Yang",
            "Zhenchen Tang",
            "Jing Dong"
        ],
        "title": "HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images",
        "abstract": "arXiv:2510.08978v1 Announce Type: new  Abstract: Although recent text-to-image (T2I) models have significantly improved the overall visual quality of generated images, they still struggle in the generation of accurate details in complex local regions, especially human hands. Generated hands often exhibit structural distortions and unrealistic textures, which can be very noticeable even when the rest of the body is well-generated. However, the quality assessment of hand regions remains largely neglected, limiting downstream task performance like human-centric generation quality optimization and AIGC detection. To address this, we propose the first quality assessment task targeting generated hand regions and showcase its abundant downstream applications. We first introduce the HandPair dataset for training hand quality assessment models. It consists of 48k images formed by high- and low-quality hand pairs, enabling low-cost, efficient supervision without manual annotation. Based on it, we develop HandEval, a carefully designed hand-specific quality assessment model. It leverages the powerful visual understanding capability of Multimodal Large Language Model (MLLM) and incorporates prior knowledge of hand keypoints, gaining strong perception of hand quality. We further construct a human-annotated test set with hand images from various state-of-the-art (SOTA) T2I models to validate its quality evaluation capability. Results show that HandEval aligns better with human judgments than existing SOTA methods. Furthermore, we integrate HandEval into image generation and AIGC detection pipelines, prominently enhancing generated hand realism and detection accuracy, respectively, confirming its universal effectiveness in downstream applications. Code and dataset will be available.",
        "arxiv_id": "2510.08978",
        "ARXIVID": "2510.08978",
        "COMMENT": "Matches criterion 4: introduces HandEval, a new hand-specific quality assessment model leveraging MLLMs for evaluating generated images, and provides a new dataset and evaluation method for vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.09182": {
        "authors": [
            "Johann-Friedrich Feiden",
            "Tim K\\\"uchler",
            "Denis Zavadski",
            "Bogdan Savchynskyy",
            "Carsten Rother"
        ],
        "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption",
        "abstract": "arXiv:2510.09182v1 Announce Type: new  Abstract: Depth estimation from monocular video has become a key component of many real-world computer vision systems. Recently, Video Depth Anything (VDA) has demonstrated strong performance on long video sequences. However, it relies on batch-processing which prohibits its use in an online setting. In this work, we overcome this limitation and introduce online VDA (oVDA). The key innovation is to employ techniques from Large Language Models (LLMs), namely, caching latent features during inference and masking frames at training. Our oVDA method outperforms all competing online video depth estimation methods in both accuracy and VRAM usage. Low VRAM usage is particularly important for deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release both, code and compilation scripts, making oVDA easy to deploy on low-power hardware.",
        "arxiv_id": "2510.09182",
        "ARXIVID": "2510.09182",
        "COMMENT": "Matches criterion 1: proposes a new method for temporally-consistent depth prediction in online video, using LLM-inspired techniques for spatial understanding in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.08790": {
        "authors": [
            "Guangya Wan",
            "Mingyang Ling",
            "Xiaoqi Ren",
            "Rujun Han",
            "Sheng Li",
            "Zizhao Zhang"
        ],
        "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context",
        "abstract": "arXiv:2510.08790v1 Announce Type: new  Abstract: Long-horizon tasks that require sustained reasoning and multiple tool interactions remain challenging for LLM agents: small errors compound across steps, and even state-of-the-art models often hallucinate or lose coherence. We identify context management as the central bottleneck -- extended histories cause agents to overlook critical evidence or become distracted by irrelevant information, thus failing to replan or reflect from previous mistakes. To address this, we propose COMPASS (Context-Organized Multi-Agent Planning and Strategy System), a lightweight hierarchical framework that separates tactical execution, strategic oversight, and context organization into three specialized components: (1) a Main Agent that performs reasoning and tool use, (2) a Meta-Thinker that monitors progress and issues strategic interventions, and (3) a Context Manager that maintains concise, relevant progress briefs for different reasoning stages. Across three challenging benchmarks -- GAIA, BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20% relative to both single- and multi-agent baselines. We further introduce a test-time scaling extension that elevates performance to match established DeepResearch agents, and a post-training pipeline that delegates context management to smaller models for enhanced efficiency.",
        "arxiv_id": "2510.08790",
        "ARXIVID": "2510.08790",
        "COMMENT": "Matches criterion 1 (methodological improvements to agent reasoning/context management) and partially criterion 3 (novel agent framework for long-horizon tasks). COMPASS introduces a hierarchical context management system for LLM agents, addressing context bottlenecks in long-horizon reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.09244": {
        "authors": [
            "Victor de Lamo Castrillo",
            "Habtom Kahsay Gidey",
            "Alexander Lenz",
            "Alois Knoll"
        ],
        "title": "Fundamentals of Building Autonomous LLM Agents",
        "abstract": "arXiv:2510.09244v1 Announce Type: new  Abstract: This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.",
        "arxiv_id": "2510.09244",
        "ARXIVID": "2510.09244",
        "COMMENT": "Matches criterion 3 (embodied AI, agentic LLMs): Reviews architecture and implementation of autonomous LLM agents, including perception, reasoning, memory, and execution systems. Focuses on novel integration for agentic behavior, relevant for embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.09173": {
        "authors": [
            "Yuchen Zhang",
            "Yao Lu",
            "Johannes Betz"
        ],
        "title": "TARO: Toward Semantically Rich Open-World Object Detection",
        "abstract": "arXiv:2510.09173v1 Announce Type: new  Abstract: Modern object detectors are largely confined to a \"closed-world\" assumption, limiting them to a predefined set of classes and posing risks when encountering novel objects in real-world scenarios. While open-set detection methods aim to address this by identifying such instances as 'Unknown', this is often insufficient. Rather than treating all unknowns as a single class, assigning them more descriptive subcategories can enhance decision-making in safety-critical contexts. For example, identifying an object as an 'Unknown Animal' (requiring an urgent stop) versus 'Unknown Debris' (requiring a safe lane change) is far more useful than just 'Unknown' in autonomous driving. To bridge this gap, we introduce TARO, a novel detection framework that not only identifies unknown objects but also classifies them into coarse parent categories within a semantic hierarchy. TARO employs a unique architecture with a sparsemax-based head for modeling objectness, a hierarchy-guided relabeling component that provides auxiliary supervision, and a classification module that learns hierarchical relationships. Experiments show TARO can categorize up to 29.9% of unknowns into meaningful coarse classes, significantly reduce confusion between unknown and known classes, and achieve competitive performance in both unknown recall and known mAP. Code will be made available.",
        "arxiv_id": "2510.09173",
        "ARXIVID": "2510.09173",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new open-world object detection framework (TARO) that assigns unknowns to coarse semantic categories, improving practical utility.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.09171": {
        "authors": [
            "Yankun Wu",
            "Zakaria Laskar",
            "Giorgos Kordopatis-Zilos",
            "Noa Garcia",
            "Giorgos Tolias"
        ],
        "title": "Instance-Level Generation for Representation Learning",
        "abstract": "arXiv:2510.09171v1 Announce Type: new  Abstract: Instance-level recognition (ILR) focuses on identifying individual objects rather than broad categories, offering the highest granularity in image classification. However, this fine-grained nature makes creating large-scale annotated datasets challenging, limiting ILR's real-world applicability across domains. To overcome this, we introduce a novel approach that synthetically generates diverse object instances from multiple domains under varied conditions and backgrounds, forming a large-scale training set. Unlike prior work on automatic data synthesis, our method is the first to address ILR-specific challenges without relying on any real images. Fine-tuning foundation vision models on the generated data significantly improves retrieval performance across seven ILR benchmarks spanning multiple domains. Our approach offers a new, efficient, and effective alternative to extensive data collection and curation, introducing a new ILR paradigm where the only input is the names of the target domains, unlocking a wide range of real-world applications.",
        "arxiv_id": "2510.09171",
        "ARXIVID": "2510.09171",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new synthetic data generation paradigm for instance-level recognition, showing significant improvements when fine-tuning foundation vision models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.09230": {
        "authors": [
            "Jindong Hong",
            "Wencheng Zhang",
            "Shiqin Qiao",
            "Jianhai Chen",
            "Jianing Qiu",
            "Chuanyang Zheng",
            "Qian Xu",
            "Yun Ji",
            "Qianyue Wen",
            "Weiwei Sun",
            "Hao Li",
            "Huizhen Li",
            "Huichao Wang",
            "Kai Wu",
            "Meng Li",
            "Yijun He",
            "Lingjie Luo",
            "Jiankai Sun"
        ],
        "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras",
        "abstract": "arXiv:2510.09230v1 Announce Type: new  Abstract: Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis), are common conditions affecting the health of people worldwide, and have a high incidence rate among the elderly and workers engaged in repetitive shoulder tasks. In regions with scarce medical resources, achieving early and accurate diagnosis poses significant challenges, and there is an urgent need for low-cost and easily scalable auxiliary diagnostic solutions. This research introduces videos captured by consumer-grade devices as the basis for diagnosis, reducing the cost for users. We focus on the innovative application of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of shoulder disorders and propose a Hybrid Motion Video Diagnosis framework (HMVDx). This framework divides the two tasks of action understanding and disease diagnosis, which are respectively completed by two MLLMs. In addition to traditional evaluation indicators, this work proposes a novel metric called Usability Index by the logical process of medical decision-making (action recognition, movement diagnosis, and final diagnosis). This index evaluates the effectiveness of MLLMs in the medical field from the perspective of the entire medical diagnostic pathway, revealing the potential value of low-cost MLLMs in medical applications for medical practitioners. In experimental comparisons, the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by 79.6\\% compared with direct video diagnosis, a significant technical contribution to future research on the application of MLLMs for video understanding in the medical field.",
        "arxiv_id": "2510.09230",
        "ARXIVID": "2510.09230",
        "COMMENT": "Matches criterion 2. This paper applies MLLMs to the diagnosis of shoulder disorders using consumer-grade video, introducing a hybrid framework and a new usability metric for medical video understanding. It is a novel application of MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.08771": {
        "authors": [
            "Xiaohui Li",
            "Shaobin Zhuang",
            "Shuo Cao",
            "Yang Yang",
            "Yuandong Pu",
            "Qi Qin",
            "Siqi Luo",
            "Bin Fu",
            "Yihao Liu"
        ],
        "title": "LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution",
        "abstract": "arXiv:2510.08771v1 Announce Type: new  Abstract: Generative models for Image Super-Resolution (SR) are increasingly powerful, yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a major computational bottleneck. Linear Attention offers an O(N) solution, but its promise for photorealistic SR has remained largely untapped, historically hindered by a cascade of interrelated and previously unsolved challenges. This paper introduces LinearSR, a holistic framework that, for the first time, systematically overcomes these critical hurdles. Specifically, we resolve a fundamental, training instability that causes catastrophic model divergence using our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF) strategy. Furthermore, we mitigate the classic perception-distortion trade-off with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we establish an effective and lightweight guidance paradigm, TAG, derived from our \"precision-over-volume\" principle. Our resulting LinearSR model simultaneously delivers state-of-the-art perceptual quality with exceptional efficiency. Its core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its overall multi-step inference time remains highly competitive. This work provides the first robust methodology for applying Linear Attention in the photorealistic SR domain, establishing a foundational paradigm for future research in efficient generative super-resolution.",
        "arxiv_id": "2510.08771",
        "ARXIVID": "2510.08771",
        "COMMENT": "Somewhat relevant to criterion 4: introduces LinearSR, a new efficient generative model for image super-resolution using linear attention, which is a methodological advance in generative vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.09404": {
        "authors": [
            "Christian Bluethgen",
            "Dave Van Veen",
            "Daniel Truhn",
            "Jakob Nikolas Kather",
            "Michael Moor",
            "Malgorzata Polacin",
            "Akshay Chaudhari",
            "Thomas Frauenfelder",
            "Curtis P. Langlotz",
            "Michael Krauthammer",
            "Farhad Nooralahzadeh"
        ],
        "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges",
        "abstract": "arXiv:2510.09404v1 Announce Type: new  Abstract: Building agents, systems that perceive and act upon their environment with a degree of autonomy, has long been a focus of AI research. This pursuit has recently become vastly more practical with the emergence of large language models (LLMs) capable of using natural language to integrate information, follow instructions, and perform forms of \"reasoning\" and planning across a wide range of tasks. With its multimodal data streams and orchestrated workflows spanning multiple systems, radiology is uniquely suited to benefit from agents that can adapt to context and automate repetitive yet complex tasks. In radiology, LLMs and their multimodal variants have already demonstrated promising performance for individual tasks such as information extraction and report summarization. However, using LLMs in isolation underutilizes their potential to support complex, multi-step workflows where decisions depend on evolving context from multiple information sources. Equipping LLMs with external tools and feedback mechanisms enables them to drive systems that exhibit a spectrum of autonomy, ranging from semi-automated workflows to more adaptive agents capable of managing complex processes. This review examines the design of such LLM-driven agentic systems, highlights key applications, discusses evaluation methods for planning and tool use, and outlines challenges such as error cascades, tool-use efficiency, and health IT integration.",
        "arxiv_id": "2510.09404",
        "ARXIVID": "2510.09404",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Reviews agentic systems in radiology using LLMs and multimodal variants, with focus on workflow automation and tool use. Relevant for applications of VLLMs/MLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2510.09205": {
        "authors": [
            "Yue Li",
            "Shida Sun",
            "Yu Hong",
            "Feihu Xu",
            "Zhiwei Xiong"
        ],
        "title": "3D Reconstruction from Transient Measurements with Time-Resolved Transformer",
        "abstract": "arXiv:2510.09205v1 Announce Type: new  Abstract: Transient measurements, captured by the timeresolved systems, are widely employed in photon-efficient reconstruction tasks, including line-of-sight (LOS) and non-line-of-sight (NLOS) imaging. However, challenges persist in their 3D reconstruction due to the low quantum efficiency of sensors and the high noise levels, particularly for long-range or complex scenes. To boost the 3D reconstruction performance in photon-efficient imaging, we propose a generic Time-Resolved Transformer (TRT) architecture. Different from existing transformers designed for high-dimensional data, TRT has two elaborate attention designs tailored for the spatio-temporal transient measurements. Specifically, the spatio-temporal self-attention encoders explore both local and global correlations within transient data by splitting or downsampling input features into different scales. Then, the spatio-temporal cross attention decoders integrate the local and global features in the token space, resulting in deep features with high representation capabilities. Building on TRT, we develop two task-specific embodiments: TRT-LOS for LOS imaging and TRT-NLOS for NLOS imaging. Extensive experiments demonstrate that both embodiments significantly outperform existing methods on synthetic data and real-world data captured by different imaging systems. In addition, we contribute a large-scale, high-resolution synthetic LOS dataset with various noise levels and capture a set of real-world NLOS measurements using a custom-built imaging system, enhancing the data diversity in this field. Code and datasets are available at https://github.com/Depth2World/TRT.",
        "arxiv_id": "2510.09205",
        "ARXIVID": "2510.09205",
        "COMMENT": "Somewhat relevant to criterion 4: proposes a new transformer architecture for 3D reconstruction from transient measurements, which is a methodological advance in vision modeling, but not directly about foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.08976": {
        "authors": [
            "Maoliang Li",
            "Ke Li",
            "Yaoyang Liu",
            "Jiayu Chen",
            "Zihao Zheng",
            "Yinjun Wu",
            "Xiang Chen"
        ],
        "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval",
        "abstract": "arXiv:2510.08976v1 Announce Type: new  Abstract: To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications. However, conventional retrieval approaches often suffer from limited retrieval accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images. They still suffer from sub-optimal accuracy and efficiency, overlooking alignment between the query and varying image objects and redundant fine-grained image segments. In this work, we present an efficient scheduling framework for image retrieval - HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple intermediate granularities for varying image objects to enhance alignment. Second, we minimize redundancy in retrieval by leveraging cross-hierarchy similarity consistency and hierarchy sparsity to minimize unnecessary matching computation. Furthermore, we configure parameters for each dataset automatically for practicality across diverse scenarios. Our empirical study shows that, HiMIR not only achieves substantial accuracy improvements but also reduces computation by up to 3.5 times over the existing MVR system.",
        "arxiv_id": "2510.08976",
        "ARXIVID": "2510.08976",
        "COMMENT": "Somewhat relevant to criterion 4: proposes a new hierarchical scheduling framework for multi-vector image retrieval, which is an application area for vision foundation models, but not a direct foundation model contribution.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.09012": {
        "authors": [
            "Xiaoxiao Ma",
            "Feng Zhao",
            "Pengyang Ling",
            "Haibo Qiu",
            "Zhixiang Wei",
            "Hu Yu",
            "Jie Huang",
            "Zhixiong Zeng",
            "Lin Ma"
        ],
        "title": "Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy",
        "abstract": "arXiv:2510.09012v1 Announce Type: new  Abstract: In this work, we first revisit the sampling issues in current autoregressive (AR) image generation models and identify that image tokens, unlike text tokens, exhibit lower information density and non-uniform spatial distribution. Accordingly, we present an entropy-informed decoding strategy that facilitates higher autoregressive generation quality with faster synthesis speed. Specifically, the proposed method introduces two main innovations: 1) dynamic temperature control guided by spatial entropy of token distributions, enhancing the balance between content diversity, alignment accuracy, and structural coherence in both mask-based and scale-wise models, without extra computational overhead, and 2) entropy-aware acceptance rules in speculative decoding, achieving near-lossless generation at about 85\\% of the inference cost of conventional acceleration methods. Extensive experiments across multiple benchmarks using diverse AR image generation models demonstrate the effectiveness and generalizability of our approach in enhancing both generation quality and sampling speed.",
        "arxiv_id": "2510.09012",
        "ARXIVID": "2510.09012",
        "COMMENT": "Somewhat relevant to criterion 4: proposes a new entropy-informed decoding strategy for autoregressive image generation, which is a methodological improvement in generative modeling, but not directly about vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.09088": {
        "authors": [
            "Weijia Wang",
            "Yuanzhi Su",
            "Pei-Gen Ye",
            "Yuan-Gen Wang",
            "Xuequan Lu"
        ],
        "title": "MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling",
        "abstract": "arXiv:2510.09088v1 Announce Type: new  Abstract: We present MambaH-Fit, a state space modelling framework tailored for hyper-surface fitting-based point cloud normal estimation. Existing normal estimation methods often fall short in modelling fine-grained geometric structures, thereby limiting the accuracy of the predicted normals. Recently, state space models (SSMs), particularly Mamba, have demonstrated strong modelling capability by capturing long-range dependencies with linear complexity and inspired adaptations to point cloud processing. However, existing Mamba-based approaches primarily focus on understanding global shape structures, leaving the modelling of local, fine-grained geometric details largely under-explored. To address the issues above, we first introduce an Attention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fuse multi-scale point cloud patch features, significantly enhancing geometric context learning in local point cloud neighbourhoods. Building upon this, we further propose Patch-wise State Space Model (PSSM) that models point cloud patches as implicit hyper-surfaces via state dynamics, enabling effective fine-grained geometric understanding for normal prediction. Extensive experiments on benchmark datasets show that our method outperforms existing ones in terms of accuracy, robustness, and flexibility. Ablation studies further validate the contribution of the proposed components.",
        "arxiv_id": "2510.09088",
        "ARXIVID": "2510.09088",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1), as it proposes a new method for point cloud normal estimation using state space modeling, with a focus on fine-grained geometric understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.09035": {
        "authors": [
            "Weitong Kong",
            "Zichao Zeng",
            "Di Wen",
            "Jiale Wei",
            "Kunyu Peng",
            "June Moh Goo",
            "Jan Boehm",
            "Rainer Stiefelhagen"
        ],
        "title": "Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels",
        "abstract": "arXiv:2510.09035v1 Announce Type: new  Abstract: Accurate perception is critical for vehicle safety, with LiDAR as a key enabler in autonomous driving. To ensure robust performance across environments, sensor types, and weather conditions without costly re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is essential. However, LiDAR annotations are often noisy due to sensor imperfections, occlusions, and human errors. Such noise degrades segmentation accuracy and is further amplified under domain shifts, threatening system reliability. While noisy-label learning is well-studied in images, its extension to 3D LiDAR segmentation under domain generalization remains largely unexplored, as the sparse and irregular structure of point clouds limits direct use of 2D methods. To address this gap, we introduce the novel task Domain Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL) and establish the first benchmark by adapting three representative noisy-label learning strategies from image classification to 3D segmentation. However, we find that existing noisy-label learning approaches adapt poorly to LiDAR data. We therefore propose DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency and apply cross-entropy loss based on confidence-aware filtering of predictions. Our approach shows state-of-the-art performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric label noise, with an overall Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby demonstrating robust domain generalization in DGLSS-NL tasks. The code is available on our project page.",
        "arxiv_id": "2510.09035",
        "ARXIVID": "2510.09035",
        "COMMENT": "Somewhat relevant to criterion 4: proposes a new benchmark and method for domain generalization in LiDAR-based semantic segmentation under noisy labels, which is an application of vision models but not directly about foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.08635": {
        "authors": [
            "Conor McCarthy",
            "Loes Quirijnen",
            "Jan Peter van Zandwijk",
            "Zeno Geradts",
            "Marcel Worring"
        ],
        "title": "Hi-OSCAR: Hierarchical Open-set Classifier for Human Activity Recognition",
        "abstract": "arXiv:2510.08635v1 Announce Type: new  Abstract: Within Human Activity Recognition (HAR), there is an insurmountable gap between the range of activities performed in life and those that can be captured in an annotated sensor dataset used in training. Failure to properly handle unseen activities seriously undermines any HAR classifier's reliability. Additionally within HAR, not all classes are equally dissimilar, some significantly overlap or encompass other sub-activities. Based on these observations, we arrange activity classes into a structured hierarchy. From there, we propose Hi-OSCAR: a Hierarchical Open-set Classifier for Activity Recognition, that can identify known activities at state-of-the-art accuracy while simultaneously rejecting unknown activities. This not only enables open-set classification, but also allows for unknown classes to be localized to the nearest internal node, providing insight beyond a binary \"known/unknown\" classification. To facilitate this and future open-set HAR research, we collected a new dataset: NFI_FARED. NFI_FARED contains data from multiple subjects performing nineteen activities from a range of contexts, including daily living, commuting, and rapid movements, which is fully public and available for download.",
        "arxiv_id": "2510.08635",
        "ARXIVID": "2510.08635",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications), but focuses on hierarchical open-set classification for human activity recognition using sensor data, not vision or multi-modal models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.09256": {
        "authors": [
            "Patrick Wienholt",
            "Sophie Caselitz",
            "Robert Siepmann",
            "Philipp Bruners",
            "Keno Bressem",
            "Christiane Kuhl",
            "Jakob Nikolas Kather",
            "Sven Nebelung",
            "Daniel Truhn"
        ],
        "title": "Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy",
        "abstract": "arXiv:2510.09256v1 Announce Type: new  Abstract: To determine whether using discrete semantic entropy (DSE) to reject questions likely to generate hallucinations can improve the accuracy of black-box vision-language models (VLMs) in radiologic image based visual question answering (VQA). This retrospective study evaluated DSE using two publicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500 images with clinical questions and short-text answers) and (ii) a diagnostic radiology dataset (206 cases: 60 computed tomography scans, 60 magnetic resonance images, 60 radiographs, 26 angiograms) with corresponding ground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times using a temperature of 1.0. Baseline accuracy was determined using low-temperature answers (temperature 0.1). Meaning-equivalent responses were grouped using bidirectional entailment checks, and DSE was computed from the relative frequencies of the resulting semantic clusters. Accuracy was recalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and 95% confidence intervals were obtained using bootstrap resampling and a Bonferroni-corrected threshold of p  0.3), accuracy on the remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and 63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains were observed across both datasets and largely remained statistically significant after Bonferroni correction. DSE enables reliable hallucination detection in black-box VLMs by quantifying semantic inconsistency. This method significantly improves diagnostic answer accuracy and offers a filtering strategy for clinical VLM applications.",
        "arxiv_id": "2510.09256",
        "ARXIVID": "2510.09256",
        "COMMENT": "This paper proposes a method for hallucination filtering in radiology VLMs using discrete semantic entropy. While it is a vision-language model application, it does not directly match any of the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.09043": {
        "authors": [
            "Sang Hun Kim",
            "Jongmin Lee",
            "Dongkyu Park",
            "So Young Lee",
            "Yosep Chong"
        ],
        "title": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
        "abstract": "arXiv:2510.09043v1 Announce Type: new  Abstract: Human consciousness is still a concept hard to define with current scientific understanding. Although Large Language Models (LLMs) have recently demonstrated significant advancements across various domains including translation and summarization, human consciousness is not something to imitate with current upfront technology owing to so-called hallucination. This study, therefore, proposes a novel approach to address these challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. We developed three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) based on the principles of psychoanalysis. Additionally, we designed 16 characters with different personalities representing the sixteen MBTI types, with several attributes such as needs, status, and memories. To determine if our model's artificial consciousness exhibits human-like cognition, we created ten distinct situations considering seven attributes such as emotional understanding and logical thinking. The decision-making process of artificial consciousness and the final action were evaluated in three ways: survey evaluation, three-tier classification via ChatGPT, and qualitative review. Both quantitative and qualitative analyses indicated a high likelihood of well-simulated consciousness, although the difference in response between different characters and consciousnesses was not very significant. This implies that the developed models incorporating elements of psychoanalysis and personality theory can lead to building a more intuitive and adaptable AI system with humanoid consciousness. Therefore, this study contributes to opening up new avenues for improving AI interactions in complex cognitive contexts.",
        "arxiv_id": "2510.09043",
        "ARXIVID": "2510.09043",
        "COMMENT": "This paper proposes a framework for artificial consciousness in humanoid agents using LLMs and psychoanalysis/personality theory. While interesting, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.08761": {
        "authors": [
            "Jiayang Liu",
            "Daniel Tso",
            "Yiming Bu",
            "Qinru Qiu"
        ],
        "title": "SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense",
        "abstract": "arXiv:2510.08761v1 Announce Type: new  Abstract: Adversarial attacks significantly challenge the safe deployment of deep learning models, particularly in real-world applications. Traditional defenses often rely on computationally intensive optimization (e.g., adversarial training or data augmentation) to improve robustness, whereas the human visual system achieves inherent robustness to adversarial perturbations through evolved biological mechanisms. We hypothesize that attention guided non-homogeneous sparse sampling and predictive coding plays a key role in this robustness. To test this hypothesis, we propose a novel defense framework incorporating three key biological mechanisms: foveal-peripheral processing, saccadic eye movements, and cortical filling-in. Our approach employs reinforcement learning-guided saccades to selectively capture multiple foveal-peripheral glimpses, which are integrated into a reconstructed image before classification. This biologically inspired preprocessing effectively mitigates adversarial noise, preserves semantic integrity, and notably requires no retraining or fine-tuning of downstream classifiers, enabling seamless integration with existing systems. Experiments on the ImageNet dataset demonstrate that our method improves system robustness across diverse classifiers and attack types, while significantly reducing training overhead compared to both biologically and non-biologically inspired defense techniques.",
        "arxiv_id": "2510.08761",
        "ARXIVID": "2510.08761",
        "COMMENT": "This paper introduces a biologically inspired adversarial defense for vision models using saccade-assisted foveal-peripheral vision and reinforcement learning. While it is an interesting vision paper, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.09227": {
        "authors": [
            "Hyundong Jin",
            "Joonghyuk Hahn",
            "Yo-Sub Han"
        ],
        "title": "RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems",
        "abstract": "arXiv:2510.09227v1 Announce Type: new  Abstract: Large language models (LLMs) show strong performance across natural language processing (NLP), mathematical reasoning, and programming, and recent large reasoning models (LRMs) further emphasize explicit reasoning. Yet their computational limits, particularly spatial complexity constrained by finite context windows, remain poorly understood. While recent works often focus on problems within the NP complexity class, we push the boundary by introducing a novel benchmark grounded in two PSPACE-complete regular expression (regex) problems: equivalence decision (RegexEQ) and minimization (RegexMin). PSPACE-complete problems serve as a more rigorous standard for assessing computational capacity, as their solutions require massive search space exploration. We perform a double-exponential space exploration to construct a labeled dataset of over a million regex instances with a sound filtering process to build the benchmark. We conduct extensive evaluations on 6 LLMs and 5 LRMs of varying scales, revealing common failure patterns such as verbosity and repetition. With its well-defined structure and quantitative evaluation metrics, this work presents the first empirical investigation into the spatial computational limitations of LLMs and LRMs, offering a new framework for evaluating their advanced reasoning capabilities. Our code is available at https://github.com/hyundong98/RegexPSPACE .",
        "arxiv_id": "2510.09227",
        "ARXIVID": "2510.09227",
        "COMMENT": "Does not match any specific criterion. Focuses on LLM reasoning for regex problems, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.09329": {
        "authors": [
            "Zenan Lin",
            "Wei Li",
            "Jintao Chen",
            "Zihao Wu",
            "Wenxiong Kang",
            "Changxin Gao",
            "Liansheng Wang",
            "Jin-Gang Yu"
        ],
        "title": "Instance-Aware Robust Consistency Regularization for Semi-Supervised Nuclei Instance Segmentation",
        "abstract": "arXiv:2510.09329v1 Announce Type: new  Abstract: Nuclei instance segmentation in pathological images is crucial for downstream tasks such as tumor microenvironment analysis. However, the high cost and scarcity of annotated data limit the applicability of fully supervised methods, while existing semi-supervised methods fail to adequately regularize consistency at the instance level, lack leverage of the inherent prior knowledge of pathological structures, and are prone to introducing noisy pseudo-labels during training. In this paper, we propose an Instance-Aware Robust Consistency Regularization Network (IRCR-Net) for accurate instance-level nuclei segmentation. Specifically, we introduce the Matching-Driven Instance-Aware Consistency (MIAC) and Prior-Driven Instance-Aware Consistency (PIAC) mechanisms to refine the nuclei instance segmentation result of the teacher and student subnetwork, particularly for densely distributed and overlapping nuclei. We incorporate morphological prior knowledge of nuclei in pathological images and utilize these priors to assess the quality of pseudo-labels generated from unlabeled data. Low-quality pseudo-labels are discarded, while high-quality predictions are enhanced to reduce pseudo-label noise and benefit the network's robust training. Experimental results demonstrate that the proposed method significantly enhances semi-supervised nuclei instance segmentation performance across multiple public datasets compared to existing approaches, even surpassing fully supervised methods in some scenarios.",
        "arxiv_id": "2510.09329",
        "ARXIVID": "2510.09329",
        "COMMENT": "This paper proposes a new semi-supervised method for nuclei instance segmentation using instance-aware consistency regularization. While it is a vision paper with clever statistical tricks, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.08955": {
        "authors": [
            "Nisha Pillai",
            "Aditi Virupakshaiah",
            "Harrison W. Smith",
            "Amanda J. Ashworth",
            "Prasanna Gowda",
            "Phillip R. Owens",
            "Adam R. Rivers",
            "Bindu Nanduri",
            "Mahalingam Ramkumar"
        ],
        "title": "Denoised Diffusion for Object-Focused Image Augmentation",
        "abstract": "arXiv:2510.08955v1 Announce Type: new  Abstract: Modern agricultural operations increasingly rely on integrated monitoring systems that combine multiple data sources for farm optimization. Aerial drone-based animal health monitoring serves as a key component but faces limited data availability, compounded by scene-specific issues such as small, occluded, or partially visible animals. Transfer learning approaches often fail to address this limitation due to the unavailability of large datasets that reflect specific farm conditions, including variations in animal breeds, environments, and behaviors. Therefore, there is a need for developing a problem-specific, animal-focused data augmentation strategy tailored to these unique challenges. To address this gap, we propose an object-focused data augmentation framework designed explicitly for animal health monitoring in constrained data settings. Our approach segments animals from backgrounds and augments them through transformations and diffusion-based synthesis to create realistic, diverse scenes that enhance animal detection and monitoring performance. Our initial experiments demonstrate that our augmented dataset yields superior performance compared to our baseline models on the animal detection task. By generating domain-specific data, our method empowers real-time animal health monitoring solutions even in data-scarce scenarios, bridging the gap between limited data and practical applicability.",
        "arxiv_id": "2510.08955",
        "ARXIVID": "2510.08955",
        "COMMENT": "This paper presents an object-focused data augmentation framework using diffusion models for animal health monitoring. It is relevant to generative modeling and computer vision, but does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.09475": {
        "authors": [
            "Ruben Pascual",
            "Mikel Sesma-Sara",
            "Aranzazu Jurio",
            "Daniel Paternain",
            "Mikel Galar"
        ],
        "title": "Few-shot multi-token DreamBooth with LoRa for style-consistent character generation",
        "abstract": "arXiv:2510.09475v1 Announce Type: new  Abstract: The audiovisual industry is undergoing a profound transformation as it is integrating AI developments not only to automate routine tasks but also to inspire new forms of art. This paper addresses the problem of producing a virtually unlimited number of novel characters that preserve the artistic style and shared visual traits of a small set of human-designed reference characters, thus broadening creative possibilities in animation, gaming, and related domains. Our solution builds upon DreamBooth, a well-established fine-tuning technique for text-to-image diffusion models, and adapts it to tackle two core challenges: capturing intricate character details beyond textual prompts and the few-shot nature of the training data. To achieve this, we propose a multi-token strategy, using clustering to assign separate tokens to individual characters and their collective style, combined with LoRA-based parameter-efficient fine-tuning. By removing the class-specific regularization set and introducing random tokens and embeddings during generation, our approach allows for unlimited character creation while preserving the learned style. We evaluate our method on five small specialized datasets, comparing it to relevant baselines using both quantitative metrics and a human evaluation study. Our results demonstrate that our approach produces high-quality, diverse characters while preserving the distinctive aesthetic features of the reference characters, with human evaluation further reinforcing its effectiveness and highlighting the potential of our method.",
        "arxiv_id": "2510.09475",
        "ARXIVID": "2510.09475",
        "COMMENT": "This paper proposes a new method for style-consistent character generation using DreamBooth with LoRA and a multi-token strategy. While it is relevant to generative modeling in multi-modal learning, it does not directly address any of the four criteria (no new VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model application).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.09450": {
        "authors": [
            "Ruirui Lin",
            "Guoxi Huang",
            "Nantheera Anantrasirichai"
        ],
        "title": "Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement",
        "abstract": "arXiv:2510.09450v1 Announce Type: new  Abstract: Low-light video enhancement (LLVE) is challenging due to noise, low contrast, and color degradations. Learning-based approaches offer fast inference but still struggle with heavy noise in real low-light scenes, primarily due to limitations in effectively leveraging temporal information. In this paper, we address this issue with DWTA-Net, a novel two-stage framework that jointly exploits short- and long-term temporal cues. Stage I employs Visual State-Space blocks for multi-frame alignment, recovering brightness, color, and structure with local consistency. Stage II introduces a recurrent refinement module with dynamic weight-based temporal aggregation guided by optical flow, adaptively balancing static and dynamic regions. A texture-adaptive loss further preserves fine details while promoting smoothness in flat areas. Experiments on real-world low-light videos show that DWTA-Net effectively suppresses noise and artifacts, delivering superior visual quality compared with state-of-the-art methods.",
        "arxiv_id": "2510.09450",
        "ARXIVID": "2510.09450",
        "COMMENT": "General computer vision (low-light video enhancement), but does not match any specific criterion (no spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.09551": {
        "authors": [
            "Gavriel Di Nepi",
            "Federico Siciliano",
            "Fabrizio Silvestri"
        ],
        "title": "Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model",
        "abstract": "arXiv:2510.09551v1 Announce Type: new  Abstract: By the end of 2024, Google researchers introduced Titans: Learning at Test Time, a neural memory model achieving strong empirical results across multiple tasks. However, the lack of publicly available code and ambiguities in the original description hinder reproducibility. In this work, we present a lightweight reimplementation of Titans and conduct a comprehensive evaluation on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks. Our results reveal that Titans does not always outperform established baselines due to chunking. However, its Neural Memory component consistently improves performance compared to attention-only models. These findings confirm the model's innovative potential while highlighting its practical limitations and raising questions for future research.",
        "arxiv_id": "2510.09551",
        "ARXIVID": "2510.09551",
        "COMMENT": "Does not match any specific criterion. Focuses on memory models for test-time learning, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}