{
    "2506.20670": {
        "authors": [
            "Jinming Wu",
            "Zihao Deng",
            "Wei Li",
            "Yiding Liu",
            "Bo You",
            "Bo Li",
            "Zejun Ma",
            "Ziwei Liu"
        ],
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "abstract": "arXiv:2506.20670v1 Announce Type: new  Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",
        "arxiv_id": "2506.20670",
        "ARXIVID": "2506.20670",
        "COMMENT": "Matches criterion 2 as it discusses a multimodal large language model (LMM) framework for search tasks, which is directly relevant to VLLMs/MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.20601": {
        "authors": [
            "Rui Huang",
            "Guangyao Zhai",
            "Zuria Bauer",
            "Marc Pollefeys",
            "Federico Tombari",
            "Leonidas Guibas",
            "Gao Huang",
            "Francis Engelmann"
        ],
        "title": "Video Perception Models for 3D Scene Synthesis",
        "abstract": "arXiv:2506.20601v1 Announce Type: new  Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released.",
        "arxiv_id": "2506.20601",
        "ARXIVID": "2506.20601",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 3D scene synthesis using video perception models, addressing spatial reasoning and consistency.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.20590": {
        "authors": [
            "Chaojun Ni",
            "Jie Li",
            "Haoyun Li",
            "Hengyu Liu",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guosheng Zhao",
            "Boyuan Wang",
            "Chenxin Li",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration",
        "abstract": "arXiv:2506.20590v1 Announce Type: new  Abstract: Interactive 3D scene generation from a single image has gained significant attention due to its potential to create immersive virtual worlds. However, a key challenge in current 3D generation methods is the limited explorability, which cannot render high-quality images during larger maneuvers beyond the original viewpoint, particularly when attempting to move forward into unseen areas. To address this challenge, we propose WonderFree, the first model that enables users to interactively generate 3D worlds with the freedom to explore from arbitrary angles and directions. Specifically, we decouple this challenge into two key subproblems: novel view quality, which addresses visual artifacts and floating issues in novel views, and cross-view consistency, which ensures spatial consistency across different viewpoints. To enhance rendering quality in novel views, we introduce WorldRestorer, a data-driven video restoration model designed to eliminate floaters and artifacts. In addition, a data collection pipeline is presented to automatically gather training data for WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D scene generation. Furthermore, to improve cross-view consistency, we propose ConsistView, a multi-view joint restoration mechanism that simultaneously restores multiple perspectives while maintaining spatiotemporal coherence. Experimental results demonstrate that WonderFree not only enhances rendering quality across diverse viewpoints but also significantly improves global coherence and consistency. These improvements are confirmed by CLIP-based metrics and a user study showing a 77.20% preference for WonderFree over WonderWorld enabling a seamless and immersive 3D exploration experience. The code, model, and data will be publicly available.",
        "arxiv_id": "2506.20590",
        "ARXIVID": "2506.20590",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D scene exploration with enhanced cross-view consistency and novel view quality.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.20222": {
        "authors": [
            "Pujing Yang",
            "Guangyi Zhang",
            "Yunlong Cai",
            "Lei Yu",
            "Guanding Yu"
        ],
        "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission",
        "abstract": "arXiv:2506.20222v1 Announce Type: new  Abstract: Event cameras asynchronously capture pixel-level intensity changes with extremely low latency. They are increasingly used in conjunction with RGB cameras for a wide range of vision-related applications. However, a major challenge in these hybrid systems lies in the transmission of the large volume of triggered events and RGB images. To address this, we propose a transmission scheme that retains efficient reconstruction performance of both sources while accomplishing real-time deblurring in parallel. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs. To address this, we develop a joint event and image (E-I) transmission framework to eliminate redundancy and thereby optimize channel bandwidth utilization. Our approach employs Bayesian modeling and the information bottleneck method to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. Moreover, it adaptively allocates transmission bandwidth based on scene dynamics, i.e., more symbols are allocated to events for dynamic details or to images for static information. Simulation results demonstrate that the proposed scheme not only achieves superior reconstruction quality compared to conventional systems but also delivers enhanced deblurring performance.",
        "arxiv_id": "2506.20222",
        "ARXIVID": "2506.20222",
        "COMMENT": "Matches criterion 3 as it proposes a novel hybrid event-RGB transmission framework with adaptive bandwidth allocation, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20066": {
        "authors": [
            "Hsiang-Wei Huang",
            "Wenhao Chai",
            "Kuang-Ming Chen",
            "Cheng-Yen Yang",
            "Jenq-Neng Hwang"
        ],
        "title": "ToSA: Token Merging with Spatial Awareness",
        "abstract": "arXiv:2506.20066v1 Announce Type: new  Abstract: Token merging has emerged as an effective strategy to accelerate Vision Transformers (ViT) by reducing computational costs. However, existing methods primarily rely on the visual token's feature similarity for token merging, overlooking the potential of integrating spatial information, which can serve as a reliable criterion for token merging in the early layers of ViT, where the visual tokens only possess weak visual information. In this paper, we propose ToSA, a novel token merging method that combines both semantic and spatial awareness to guide the token merging process. ToSA leverages the depth image as input to generate pseudo spatial tokens, which serve as auxiliary spatial information for the visual token merging process. With the introduced spatial awareness, ToSA achieves a more informed merging strategy that better preserves critical scene structure. Experimental results demonstrate that ToSA outperforms previous token merging methods across multiple benchmarks on visual and embodied question answering while largely reducing the runtime of the ViT, making it an efficient solution for ViT acceleration. The code will be available at: https://github.com/hsiangwei0903/ToSA",
        "arxiv_id": "2506.20066",
        "ARXIVID": "2506.20066",
        "COMMENT": "Matches criterion 1 as it proposes a novel token merging method with spatial awareness for Vision Transformers, which improves spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20388": {
        "authors": [
            "Shen Tan",
            "Xin Zhang",
            "Liangxiu Han",
            "Huaguo Huang",
            "Han Wang"
        ],
        "title": "A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management",
        "abstract": "arXiv:2506.20388v1 Announce Type: new  Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB) is crucial for supporting local livelihoods and carbon sequestration initiatives like the China Certified Emission Reduction (CCER) program. High-resolution canopy height maps (CHMs) are essential for this, but standard lidar-based methods are expensive. While deep learning with RGB imagery offers an alternative, accurately extracting canopy height features remains challenging. To address this, we developed a novel model for high-resolution CHM generation using a Large Vision Foundation Model (LVFM). Our model integrates a feature extractor, a self-supervised feature enhancement module to preserve spatial details, and a height estimator. Tested in Beijing's Fangshan District using 1-meter Google Earth imagery, our model outperformed existing methods, including conventional CNNs. It achieved a mean absolute error of 0.09 m, a root mean square error of 0.24 m, and a correlation of 0.78 against lidar-based CHMs. The resulting CHMs enabled over 90% success in individual tree detection, high accuracy in AGB estimation, and effective tracking of plantation growth, demonstrating strong generalization to non-training areas. This approach presents a promising, scalable tool for evaluating carbon sequestration in both plantations and natural forests.",
        "arxiv_id": "2506.20388",
        "ARXIVID": "2506.20388",
        "COMMENT": "Matches criterion 4 as it focuses on a novel application of a Large Vision Foundation Model (LVFM) for generating high-resolution canopy height maps.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20381": {
        "authors": [
            "Ben Kang",
            "Xin Chen",
            "Jie Zhao",
            "Chunjuan Bo",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking",
        "abstract": "arXiv:2506.20381v1 Announce Type: new  Abstract: Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers.Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT.",
        "arxiv_id": "2506.20381",
        "ARXIVID": "2506.20381",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in visual tracking with lightweight transformers and dynamic frameworks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20616": {
        "authors": [
            "Quoc-Duy Tran",
            "Anh-Tuan Vo",
            "Dinh-Khoi Vo",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "Shape2Animal: Creative Animal Generation from Natural Silhouettes",
        "abstract": "arXiv:2506.20616v1 Announce Type: new  Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous stimuli, a cognitive phenomenon known as pareidolia. This paper introduces Shape2Animal framework to mimics this imaginative capacity by reinterpreting natural object silhouettes, such as clouds, stones, or flames, as plausible animal forms. Our automated framework first performs open-vocabulary segmentation to extract object silhouette and interprets semantically appropriate animal concepts using vision-language models. It then synthesizes an animal image that conforms to the input shape, leveraging text-to-image diffusion model and seamlessly blends it into the original scene to generate visually coherent and spatially consistent compositions. We evaluated Shape2Animal on a diverse set of real-world inputs, demonstrating its robustness and creative potential. Our Shape2Animal can offer new opportunities for visual storytelling, educational content, digital art, and interactive media design. Our project page is here: https://shape2image.github.io",
        "arxiv_id": "2506.20616",
        "ARXIVID": "2506.20616",
        "COMMENT": "Does not match any specific criteria but discusses a creative application of vision-language models for generating animal images from silhouettes.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.20486": {
        "authors": [
            "Salvatore Milite",
            "Giulio Caravagna",
            "Andrea Sottoriva"
        ],
        "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization",
        "abstract": "arXiv:2506.20486v1 Announce Type: new  Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model self-organizing processes, with potential applications in life science. However, their deterministic nature limits their ability to capture the stochasticity of real-world biological and physical systems.   We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework incorporating the idea of mixture models into the NCA paradigm. By combining probabilistic rule assignments with intrinsic noise, MNCAs can model diverse local behaviors and reproduce the stochastic dynamics observed in biological processes.   We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic simulations of tissue growth and differentiation, (2) image morphogenesis robustness, and (3) microscopy image segmentation. Results show that MNCAs achieve superior robustness to perturbations, better recapitulate real biological growth patterns, and provide interpretable rule segmentation. These findings position MNCAs as a promising tool for modeling stochastic dynamical systems and studying self-growth processes.",
        "arxiv_id": "2506.20486",
        "ARXIVID": "2506.20486",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and self-organization using neural cellular automata.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.20151": {
        "authors": [
            "Haipeng Fan",
            "Shiyuan Zhang",
            "Baohunesitu",
            "Zihang Guo",
            "Huaiwen Zhang"
        ],
        "title": "EAR: Erasing Concepts from Unified Autoregressive Models",
        "abstract": "arXiv:2506.20151v1 Announce Type: new  Abstract: Autoregressive (AR) models have achieved unified and strong performance across both visual understanding and image generation tasks. However, removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge. In this paper, we propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning. Furthermore, we propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models. Specifically, we first employ structured templates across diverse large language models (LLMs) to pre-generate a large-scale corpus of target-replacement concept prompt pairs. Subsequently, we generate images from these prompts and subject them to rigorous filtering via a visual classifier to ensure concept fidelity and alignment. Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation. Code is available at: https://github.com/immc-lab/ear/",
        "arxiv_id": "2506.20151",
        "ARXIVID": "2506.20151",
        "COMMENT": "Does not match any specific criteria but discusses concept erasure in autoregressive models, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20583": {
        "authors": [
            "Zhiwang Zhang",
            "Dong Xu",
            "Wanli Ouyang",
            "Luping Zhou"
        ],
        "title": "Dense Video Captioning using Graph-based Sentence Summarization",
        "abstract": "arXiv:2506.20583v1 Announce Type: new  Abstract: Recently, dense video captioning has made attractive progress in detecting and captioning all events in a long untrimmed video. Despite promising results were achieved, most existing methods do not sufficiently explore the scene evolution within an event temporal proposal for captioning, and therefore perform less satisfactorily when the scenes and objects change over a relatively long proposal. To address this problem, we propose a graph-based partition-and-summarization (GPaS) framework for dense video captioning within two stages. For the ``partition\" stage, a whole event proposal is split into short video segments for captioning at a finer level. For the ``summarization\" stage, the generated sentences carrying rich description information for each segment are summarized into one sentence to describe the whole event. We particularly focus on the ``summarization\" stage, and propose a framework that effectively exploits the relationship between semantic words for summarization. We achieve this goal by treating semantic words as nodes in a graph and learning their interactions by coupling Graph Convolutional Network (GCN) and Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN and LSTM. The effectiveness of our approach is demonstrated via an extensive comparison with the state-of-the-arts methods on the two benchmarks ActivityNet Captions dataset and YouCook II dataset.",
        "arxiv_id": "2506.20583",
        "ARXIVID": "2506.20583",
        "COMMENT": "Does not match any specific criteria but discusses dense video captioning, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20342": {
        "authors": [
            "Lei Wang",
            "Piotr Koniusz"
        ],
        "title": "Feature Hallucination for Self-supervised Action Recognition",
        "abstract": "arXiv:2506.20342v1 Announce Type: new  Abstract: Understanding human actions in videos requires more than raw pixel analysis; it relies on high-level semantic reasoning and effective integration of multimodal features. We propose a deep translational action recognition framework that enhances recognition accuracy by jointly predicting action concepts and auxiliary features from RGB video frames. At test time, hallucination streams infer missing cues, enriching feature representations without increasing computational overhead. To focus on action-relevant regions beyond raw pixels, we introduce two novel domain-specific descriptors. Object Detection Features (ODF) aggregate outputs from multiple object detectors to capture contextual cues, while Saliency Detection Features (SDF) highlight spatial and intensity patterns crucial for action recognition. Our framework seamlessly integrates these descriptors with auxiliary modalities such as optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It remains compatible with state-of-the-art architectures, including I3D, AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE V2 and InternVideo2. To handle uncertainty in auxiliary features, we incorporate aleatoric uncertainty modeling in the hallucination step and introduce a robust loss function to mitigate feature noise. Our multimodal self-supervised action recognition framework achieves state-of-the-art performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and Something-Something V2, demonstrating its effectiveness in capturing fine-grained action dynamics.",
        "arxiv_id": "2506.20342",
        "ARXIVID": "2506.20342",
        "COMMENT": "Does not match any specific criteria but discusses self-supervised action recognition, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20274": {
        "authors": [
            "Liya Wang",
            "David Yi",
            "Damien Jose",
            "John Passarelli",
            "James Gao",
            "Jordan Leventis",
            "Kang Li"
        ],
        "title": "Enterprise Large Language Model Evaluation Benchmark",
        "abstract": "arXiv:2506.20274v1 Announce Type: new  Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking. Our benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.",
        "arxiv_id": "2506.20274",
        "ARXIVID": "2506.20274",
        "COMMENT": "Does not match any specific criteria but discusses evaluation benchmarks for LLMs, which is tangentially relevant to your friend's interest in language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20599": {
        "authors": [
            "Ji Qi",
            "Xinchang Zhang",
            "Dingqi Ye",
            "Yongjia Ruan",
            "Xin Guo",
            "Shaowen Wang",
            "Haifeng Li"
        ],
        "title": "SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection",
        "abstract": "arXiv:2506.20599v1 Announce Type: new  Abstract: The rapid advancement of generative artificial intelligence is producing fake remote sensing imagery (RSI) that is increasingly difficult to detect, potentially leading to erroneous intelligence, fake news, and even conspiracy theories. Existing forgery detection methods typically rely on single visual features to capture predefined artifacts, such as spatial-domain cues to detect forged objects like roads or buildings in RSI, or frequency-domain features to identify artifacts from up-sampling operations in adversarial generative networks (GANs). However, the nature of artifacts can significantly differ depending on geographic terrain, land cover types, or specific features within the RSI. Moreover, these complex artifacts evolve as generative models become more sophisticated. In short, over-reliance on a single visual cue makes existing forgery detectors struggle to generalize across diverse remote sensing data. This paper proposed a novel forgery detection framework called SFNet, designed to identify fake images in diverse remote sensing data by leveraging spatial and frequency domain features. Specifically, to obtain rich and comprehensive visual information, SFNet employs two independent feature extractors to capture spatial and frequency domain features from input RSIs. To fully utilize the complementary domain features, the domain feature mapping module and the hybrid domain feature refinement module(CBAM attention) of SFNet are designed to successively align and fuse the multi-domain features while suppressing redundant information. Experiments on three datasets show that SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art RS forgery detection methods and exhibits robust generalization capabilities. The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.",
        "arxiv_id": "2506.20599",
        "ARXIVID": "2506.20599",
        "COMMENT": "Does not match any specific criterion but is relevant to remote sensing and forgery detection using spatial and frequency-domain features.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20563": {
        "authors": [
            "Lei Zhu",
            "Jun Zhou",
            "Rick Siow Mong Goh",
            "Yong Liu"
        ],
        "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation",
        "abstract": "arXiv:2506.20563v1 Announce Type: new  Abstract: Vision Transformer has recently gained tremendous popularity in medical image segmentation task due to its superior capability in capturing long-range dependencies. However, transformer requires a large amount of labeled data to be effective, which hinders its applicability in annotation scarce semi-supervised learning scenario where only limited labeled data is available. State-of-the-art semi-supervised learning methods propose combinatorial CNN-Transformer learning to cross teach a transformer with a convolutional neural network, which achieves promising results. However, it remains a challenging task to effectively train the transformer with limited labeled data. In this paper, we propose an adversarial masked image modeling method to fully unleash the potential of transformer for semi-supervised medical image segmentation. The key challenge in semi-supervised learning with transformer lies in the lack of sufficient supervision signal. To this end, we propose to construct an auxiliary masked domain from original domain with masked image modeling and train the transformer to predict the entire segmentation mask with masked inputs to increase supervision signal. We leverage the original labels from labeled data and pseudo-labels from unlabeled data to learn the masked domain. To further benefit the original domain from masked domain, we provide a theoretical analysis of our method from a multi-domain learning perspective and devise a novel adversarial training loss to reduce the domain gap between the original and masked domain, which boosts semi-supervised learning performance. We also extend adversarial masked image modeling to CNN network. Extensive experiments on three public medical image segmentation datasets demonstrate the effectiveness of our method, where our method outperforms existing methods significantly. Our code is publicly available at https://github.com/zlheui/AdvMIM.",
        "arxiv_id": "2506.20563",
        "ARXIVID": "2506.20563",
        "COMMENT": "Does not match any specific criterion but is relevant to vision transformers and semi-supervised learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.19977": {
        "authors": [
            "Deng Pan",
            "Keerthiram Murugesan",
            "Nuno Moniz",
            "Nitesh Chawla"
        ],
        "title": "Context Attribution with Multi-Armed Bandit Optimization",
        "abstract": "arXiv:2506.19977v1 Announce Type: new  Abstract: Understanding which parts of the retrieved context contribute to a large language model's generated answer is essential for building interpretable and trustworthy generative QA systems. We propose a novel framework that formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each context segment is treated as a bandit arm, and we employ Combinatorial Thompson Sampling (CTS) to efficiently explore the exponentially large space of context subsets under a limited query budget. Our method defines a reward function based on normalized token likelihoods, capturing how well a subset of segments supports the original model response. Unlike traditional perturbation-based attribution methods such as SHAP, which sample subsets uniformly and incur high computational costs, our approach adaptively balances exploration and exploitation by leveraging posterior estimates of segment relevance. This leads to substantially improved query efficiency while maintaining high attribution fidelity. Extensive experiments on diverse datasets and LLMs demonstrate that our method achieves competitive attribution quality with fewer model queries.",
        "arxiv_id": "2506.19977",
        "ARXIVID": "2506.19977",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20302": {
        "authors": [
            "Abbas Anwar",
            "Mohammad Shullar",
            "Ali Arshad Nasir",
            "Mudassir Masood",
            "Saeed Anwar"
        ],
        "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks",
        "abstract": "arXiv:2506.20302v1 Announce Type: new  Abstract: Images captured in challenging environments often experience various forms of degradation, including noise, color cast, blur, and light scattering. These effects significantly reduce image quality, hindering their applicability in downstream tasks such as object detection, mapping, and classification. Our transformer-based diffusion model was developed to address image restoration tasks, aiming to improve the quality of degraded images. This model was evaluated against existing deep learning methodologies across multiple quality metrics for underwater image enhancement, denoising, and deraining on publicly available datasets. Our findings demonstrate that the diffusion model, combined with transformers, surpasses current methods in performance. The results of our model highlight the efficacy of diffusion models and transformers in improving the quality of degraded images, consequently expanding their utility in downstream tasks that require high-fidelity visual data.",
        "arxiv_id": "2506.20302",
        "ARXIVID": "2506.20302",
        "COMMENT": "Does not match any specific criteria but focuses on image restoration using transformers and diffusion models, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.20263": {
        "authors": [
            "Ning Luo",
            "Meiyin Hu",
            "Huan Wan",
            "Yanyan Yang",
            "Zhuohang Jiang",
            "Xin Wei"
        ],
        "title": "Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification",
        "abstract": "arXiv:2506.20263v1 Announce Type: new  Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant challenge, requiring models to distinguish visually similar subclasses with limited labeled examples. Existing methods have critical limitations: metric-based methods lose spatial information and misalign local features, while reconstruction-based methods fail to utilize hierarchical feature information and lack mechanisms to focus on discriminative regions. We propose the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which integrates dual-layer feature reconstruction with mask-enhanced feature processing to improve fine-grained classification. HMDRN incorporates a dual-layer feature reconstruction and fusion module that leverages complementary visual information from different network hierarchies. Through learnable fusion weights, the model balances high-level semantic representations from the last layer with mid-level structural details from the penultimate layer. Additionally, we design a spatial binary mask-enhanced transformer self-reconstruction module that processes query features through adaptive thresholding while maintaining complete support features, enhancing focus on discriminative regions while filtering background noise. Extensive experiments on three challenging fine-grained datasets demonstrate that HMDRN consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12 backbone architectures. Comprehensive ablation studies validate the effectiveness of each proposed component, revealing that dual-layer reconstruction enhances inter-class discrimination while mask-enhanced transformation reduces intra-class variations. Visualization results provide evidence of HMDRN's superior feature reconstruction capabilities.",
        "arxiv_id": "2506.20263",
        "ARXIVID": "2506.20263",
        "COMMENT": "Does not match any specific criteria but is related to fine-grained image classification, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.20464": {
        "authors": [
            "Dibyayan Patra",
            "Pasindu Ranasinghe",
            "Bikram Banerjee",
            "Simit Raval"
        ],
        "title": "A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners",
        "abstract": "arXiv:2506.20464v1 Announce Type: new  Abstract: Rock bolts are crucial components of the subterranean support systems in underground mines that provide adequate structural reinforcement to the rock mass to prevent unforeseen hazards like rockfalls. This makes frequent assessments of such bolts critical for maintaining rock mass stability and minimising risks in underground mining operations. Where manual surveying of rock bolts is challenging due to the low light conditions in the underground mines and the time-intensive nature of the process, automated detection of rock bolts serves as a plausible solution. To that end, this study focuses on the automatic identification of rock bolts within medium to large-scale 3D point clouds obtained from underground mines using mobile laser scanners. Existing techniques for automated rock bolt identification primarily rely on feature engineering and traditional machine learning approaches. However, such techniques lack robustness as these point clouds present several challenges due to data noise, varying environments, and complex surrounding structures. Moreover, the target rock bolts are extremely small objects within large-scale point clouds and are often partially obscured due to the application of reinforcement shotcrete. Addressing these challenges, this paper proposes an approach termed DeepBolt, which employs a novel two-stage deep learning architecture specifically designed for handling severe class imbalance for the automatic and efficient identification of rock bolts in complex 3D point clouds. The proposed method surpasses state-of-the-art semantic segmentation models by up to 42.5% in Intersection over Union (IoU) for rock bolt points. Additionally, it outperforms existing rock bolt identification techniques, achieving a 96.41% precision and 96.96% recall in classifying rock bolts, demonstrating its robustness and effectiveness in complex underground environments.",
        "arxiv_id": "2506.20464",
        "ARXIVID": "2506.20464",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D point cloud analysis and object detection in underground environments.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.20567": {
        "authors": [
            "Zhiwang Zhang",
            "Dong Xu",
            "Wanli Ouyang",
            "Chuanqi Tan"
        ],
        "title": "Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization",
        "abstract": "arXiv:2506.20567v1 Announce Type: new  Abstract: In this work, we propose a division-and-summarization (DaS) framework for dense video captioning. After partitioning each untrimmed long video as multiple event proposals, where each event proposal consists of a set of short video segments, we extract visual feature (e.g., C3D feature) from each segment and use the existing image/video captioning approach to generate one sentence description for this segment. Considering that the generated sentences contain rich semantic descriptions about the whole event proposal, we formulate the dense video captioning task as a visual cue aided sentence summarization problem and propose a new two stage Long Short Term Memory (LSTM) approach equipped with a new hierarchical attention mechanism to summarize all generated sentences as one descriptive sentence with the aid of visual features. Specifically, the first-stage LSTM network takes all semantic words from the generated sentences and the visual features from all segments within one event proposal as the input, and acts as the encoder to effectively summarize both semantic and visual information related to this event proposal. The second-stage LSTM network takes the output from the first-stage LSTM network and the visual features from all video segments within one event proposal as the input, and acts as the decoder to generate one descriptive sentence for this event proposal. Our comprehensive experiments on the ActivityNet Captions dataset demonstrate the effectiveness of our newly proposed DaS framework for dense video captioning.",
        "arxiv_id": "2506.20567",
        "ARXIVID": "2506.20567",
        "COMMENT": "Does not match any specific criterion but is relevant to video captioning and dense video understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}