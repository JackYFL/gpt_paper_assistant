{
    "2510.08316": {
        "authors": [
            "Yu Huang",
            "Zelin Peng",
            "Changsong Wen",
            "Xiaokang Yang",
            "Wei Shen"
        ],
        "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge",
        "abstract": "arXiv:2510.08316v1 Announce Type: new  Abstract: Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.",
        "arxiv_id": "2510.08316",
        "ARXIVID": "2510.08316",
        "COMMENT": "Matches criteria 1 and 4: Introduces a method to transfer 2D semantic knowledge from vision foundation models to 3D affordance segmentation, improving spatial understanding for embodied AI and leveraging vision foundation models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.07791": {
        "authors": [
            "Qinghongbing Xie",
            "Zhaoyuan Xia",
            "Feng Zhu",
            "Lijun Gong",
            "Ziyue Li",
            "Rui Zhao",
            "Long Zeng"
        ],
        "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models",
        "abstract": "arXiv:2510.07791v1 Announce Type: new  Abstract: Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has attracted much attention due to its importance for Autonomous Driving, Embodied AI and General Artificial Intelligence. Existing spatial-temporal benchmarks mainly focus on egocentric perspective reasoning with images/video context, or geographic perspective reasoning with graphics context (eg. a map), thus fail to assess VLMs' geographic spatial-temporal intelligence with both images/video and graphics context, which is important for areas like traffic management and emergency response. To address the gaps, we introduce Geo-Temporal Reasoning benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of moving targets in a large-scale camera network. GTR-Bench is more challenging as it requires multiple perspective switches between maps and videos, joint reasoning across multiple videos with non-overlapping fields of view, and inference over spatial-temporal regions that are unobserved by any video context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags behind human performance (78.61%) on geo-temporal reasoning. Moreover, our comprehensive analysis on GTR-Bench reveals three primary deficiencies of current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in temporal forecasting, which leads to worse performance on temporal-emphasized tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to comprehend or align the map data with multi-view video inputs. We believe GTR-Bench offers valuable insights and opens up new opportunities for research and applications in spatial-temporal intelligence. Benchmark and code will be released at https://github.com/X-Luffy/GTR-Bench.",
        "arxiv_id": "2510.07791",
        "ARXIVID": "2510.07791",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks with novel angles). Introduces GTR-Bench, a new benchmark for geo-temporal reasoning in VLMs, focusing on multi-perspective, map-video joint reasoning, and identifies deficiencies in current models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.08531": {
        "authors": [
            "Hongxing Li",
            "Dingming Li",
            "Zixuan Wang",
            "Yuchen Yan",
            "Hang Wu",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
        "abstract": "arXiv:2510.08531v1 Announce Type: new  Abstract: Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.",
        "arxiv_id": "2510.08531",
        "ARXIVID": "2510.08531",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 2 (shows a new VLLM for spatial reasoning). The paper introduces a progressive training framework and a new dataset for spatial reasoning in VLMs, achieving SOTA and showing strong generalization.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.08510": {
        "authors": [
            "Jiayun Luo",
            "Wan-Cyuan Fan",
            "Lyuyang Wang",
            "Xiangteng He",
            "Tanzila Rahman",
            "Purang Abolmaesumi",
            "Leonid Sigal"
        ],
        "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
        "abstract": "arXiv:2510.08510v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.",
        "arxiv_id": "2510.08510",
        "ARXIVID": "2510.08510",
        "COMMENT": "Matches criterion 2 (insightful analysis and improvement of VLLMs). Investigates the role of ViT attention sinks in LVLMs, proposes new methods to leverage them, and demonstrates improvements in visual reasoning tasks. Offers surprising empirical insights into VLLM internals.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.08442": {
        "authors": [
            "Andrew Lee",
            "Ian Chuang",
            "Dechen Gao",
            "Kai Fukazawa",
            "Iman Soltani"
        ],
        "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning",
        "abstract": "arXiv:2510.08442v1 Announce Type: new  Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.",
        "arxiv_id": "2510.08442",
        "ARXIVID": "2510.08442",
        "COMMENT": "This paper introduces a novel visual attention mechanism for visual RL agents, inspired by human foveation, and demonstrates significant sample efficiency improvements on the ManiSkill3 benchmark. This matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (embodied AI, new methods, and benchmarks).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.08564": {
        "authors": [
            "Zhen Zhu",
            "Yiming Gong",
            "Yao Xiao",
            "Yaoyao Liu",
            "Derek Hoiem"
        ],
        "title": "How to Teach Large Multimodal Models New Skills",
        "abstract": "arXiv:2510.08564v1 Announce Type: new  Abstract: How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL",
        "arxiv_id": "2510.08564",
        "ARXIVID": "2510.08564",
        "COMMENT": "This paper directly addresses criterion 2 (new MLLMs/VLLMs) by studying how to teach large multimodal models new skills without catastrophic forgetting, and proposes robust tuning recipes. It is highly relevant for anyone interested in multi-modal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.08559": {
        "authors": [
            "Andong Deng",
            "Taojiannan Yang",
            "Shoubin Yu",
            "Lincoln Spencer",
            "Mohit Bansal",
            "Chen Chen",
            "Serena Yeung-Levy",
            "Xiaohan Wang"
        ],
        "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
        "abstract": "arXiv:2510.08559v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
        "arxiv_id": "2510.08559",
        "ARXIVID": "2510.08559",
        "COMMENT": "Matches criterion 3: Introduces SciVideoBench, a new benchmark for scientific video reasoning in LMMs, focusing on complex, domain-specific, spatiotemporal, and logical reasoning tasks that are underexplored in current benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.08431": {
        "authors": [
            "Kaiwen Zheng",
            "Yuji Wang",
            "Qianli Ma",
            "Huayu Chen",
            "Jintao Zhang",
            "Yogesh Balaji",
            "Jianfei Chen",
            "Ming-Yu Liu",
            "Jun Zhu",
            "Qinsheng Zhang"
        ],
        "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
        "abstract": "arXiv:2510.08431v1 Announce Type: new  Abstract: This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
        "arxiv_id": "2510.08431",
        "ARXIVID": "2510.08431",
        "COMMENT": "Matches criterion 4: Presents a new, scalable method for distilling large-scale diffusion models, with applications to image and video generation. The work is highly relevant to vision foundation models and their efficient deployment.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.08565": {
        "authors": [
            "Changyao Tian",
            "Hao Li",
            "Gen Luo",
            "Xizhou Zhu",
            "Weijie Su",
            "Hanming Deng",
            "Jinguo Zhu",
            "Jie Shao",
            "Ziran Zhu",
            "Yunpeng Liu",
            "Lewei Lu",
            "Wenhai Wang",
            "Hongsheng Li",
            "Jifeng Dai"
        ],
        "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
        "abstract": "arXiv:2510.08565v1 Announce Type: new  Abstract: Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
        "arxiv_id": "2510.08565",
        "ARXIVID": "2510.08565",
        "COMMENT": "Matches criterion 2: Systematic study of native (end-to-end) MLLM scaling under data constraints, introduces NaViL, and provides insights into architecture and scaling. Directly relevant to new MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.07852": {
        "authors": [
            "Shuangyan Deng",
            "Haizhou Peng",
            "Jiachen Xu",
            "Rui Mao",
            "Ciprian Doru Giurc\\u{a}neanu",
            "Jiamou Liu"
        ],
        "title": "FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning",
        "abstract": "arXiv:2510.07852v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in recent years. However, their rigorous evaluation within specialized domains like finance is hindered by the absence of datasets characterized by professional-level knowledge intensity, detailed annotations, and advanced reasoning complexity. To address this critical gap, we introduce FinMR, a high-quality, knowledge-intensive multimodal dataset explicitly designed to evaluate expert-level financial reasoning capabilities at a professional analyst's standard. FinMR comprises over 3,200 meticulously curated and expertly annotated question-answer pairs across 15 diverse financial topics, ensuring broad domain diversity and integrating sophisticated mathematical reasoning, advanced financial knowledge, and nuanced visual interpretation tasks across multiple image types. Through comprehensive benchmarking with leading closed-source and open-source MLLMs, we highlight significant performance disparities between these models and professional financial analysts, uncovering key areas for model advancement, such as precise image analysis, accurate application of complex financial formulas, and deeper contextual financial understanding. By providing richly varied visual content and thorough explanatory annotations, FinMR establishes itself as an essential benchmark tool for assessing and advancing multimodal financial reasoning toward professional analyst-level competence.",
        "arxiv_id": "2510.07852",
        "ARXIVID": "2510.07852",
        "COMMENT": "Matches criterion 2: Presents FinMR, a new knowledge-intensive multimodal benchmark for financial reasoning, directly evaluating and comparing MLLMs in a specialized domain.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08352": {
        "authors": [
            "Nikos Theodoridis",
            "Tim Brophy",
            "Reenu Mohandas",
            "Ganesh Sistu",
            "Fiachra Collins",
            "Anthony Scanlan",
            "Ciaran Eising"
        ],
        "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
        "abstract": "arXiv:2510.08352v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not \"shortsighted\", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.",
        "arxiv_id": "2510.08352",
        "ARXIVID": "2510.08352",
        "COMMENT": "Matches criterion 2: Evaluates small vision-language models (VLMs) on a new benchmark for distance-dependent traffic perception, providing empirical insights into their limitations in perception tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08551": {
        "authors": [
            "Guanghao Li",
            "Kerui Ren",
            "Linning Xu",
            "Zhewen Zheng",
            "Changjian Jiang",
            "Xin Gao",
            "Bo Dai",
            "Jian Pu",
            "Mulin Yu",
            "Jiangmiao Pang"
        ],
        "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation",
        "abstract": "arXiv:2510.08551v1 Announce Type: new  Abstract: On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/.",
        "arxiv_id": "2510.08551",
        "ARXIVID": "2510.08551",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes ARTDECO, a unified framework for efficient and high-fidelity 3D reconstruction using 3D foundation models, with a novel hierarchical Gaussian representation and LoD-aware rendering.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.07636": {
        "authors": [
            "Shashank Gupta",
            "Gregoire Phillips",
            "Alan C. Bovik"
        ],
        "title": "PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment",
        "abstract": "arXiv:2510.07636v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have recently enabled considerable advances in the realm of image and video quality assessment, but this progress has yet to be fully explored in the domain of 3D assets. We are interested in using these models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where the aim is to automatically evaluate the perceptual quality of a point cloud in absence of a reference. We begin with the observation that different modalities of data - text descriptions, 2D projections, and 3D point cloud views - provide complementary information about point cloud quality. We then construct PIT-QMM, a novel LMM for NR-PCQA that is capable of consuming text, images and point clouds end-to-end to predict quality scores. Extensive experimentation shows that our proposed method outperforms the state-of-the-art by significant margins on popular benchmarks with fewer training iterations. We also demonstrate that our framework enables distortion localization and identification, which paves a new way forward for model explainability and interactivity. Code and datasets are available at https://www.github.com/shngt/pit-qmm.",
        "arxiv_id": "2510.07636",
        "ARXIVID": "2510.07636",
        "COMMENT": "Matches criterion 2 (shows a new large multimodal model for point cloud quality assessment) and criterion 4 (vision foundation models and applications). Proposes PIT-QMM, a large multimodal model for no-reference point cloud quality assessment, using text, images, and 3D data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.07729": {
        "authors": [
            "Jian Gao",
            "Mengqi Yuan",
            "Yifei Zeng",
            "Chang Zeng",
            "Zhihao Li",
            "Zhenyu Chen",
            "Weichao Qiu",
            "Xiao-Xiao Long",
            "Hao Zhu",
            "Xun Cao",
            "Yao Yao"
        ],
        "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes",
        "abstract": "arXiv:2510.07729v1 Announce Type: new  Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.",
        "arxiv_id": "2510.07729",
        "ARXIVID": "2510.07729",
        "COMMENT": "This paper introduces a new method for efficient 3D object-scene composition, with a focus on relightable object reconstruction and scene lighting estimation. It proposes Surface Octahedral Probes for efficient 3D querying and a novel approach to lighting estimation. This matches criterion 1 (spatial understanding/methodological improvements) and criterion 4 (vision foundation models and applications, as it builds on Gaussian Splatting and diffusion models).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.07915": {
        "authors": [
            "Peiran Wu",
            "Zhuorui Yu",
            "Yunze Liu",
            "Chi-Hao Wu",
            "Enmin Zhou",
            "Junxiao Shen"
        ],
        "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding",
        "abstract": "arXiv:2510.07915v1 Announce Type: new  Abstract: The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.",
        "arxiv_id": "2510.07915",
        "ARXIVID": "2510.07915",
        "COMMENT": "Matches criterion 2: Proposes MARC, a memory-augmented RL token compression method for efficient video understanding in VLMs, addressing computational bottlenecks in video-based VLMs. Also relevant to criterion 4 as it enables scalable vision foundation model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08540": {
        "authors": [
            "Xiangyu Zhao",
            "Junming Lin",
            "Tianhao Liang",
            "Yifan Zhou",
            "Wenhao Chai",
            "Yuzhe Gu",
            "Weiyun Wang",
            "Kai Chen",
            "Gen Luo",
            "Wenwei Zhang",
            "Junchi Yan",
            "Hua Yang",
            "Haodong Duan",
            "Xue Yang"
        ],
        "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization",
        "abstract": "arXiv:2510.08540v1 Announce Type: new  Abstract: While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.",
        "arxiv_id": "2510.08540",
        "ARXIVID": "2510.08540",
        "COMMENT": "Matches criterion 2: Proposes MM-HELIX, a new benchmark and training strategy for long-chain reflective reasoning in MLLMs, and introduces Adaptive Hybrid Policy Optimization for better learning. Strong focus on improving MLLM reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.08480": {
        "authors": [
            "Zhenlong Yuan",
            "Xiangyan Qu",
            "Chengxuan Qian",
            "Rui Chen",
            "Jing Tang",
            "Lei Sun",
            "Xiangxiang Chu",
            "Dapeng Zhang",
            "Yiwei Wang",
            "Yujun Cai",
            "Shuo Li"
        ],
        "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools",
        "abstract": "arXiv:2510.08480v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization.",
        "arxiv_id": "2510.08480",
        "ARXIVID": "2510.08480",
        "COMMENT": "Matches criterion 2: Proposes Video-STAR, a new MLLM framework for open-vocabulary action recognition, using tool-augmented reinforcement learning and sub-motion decomposition for fine-grained, visually grounded reasoning. Also relevant to criterion 4 as it advances vision-language model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.07978": {
        "authors": [
            "Dhruv Jain",
            "Harshit Shukla",
            "Gautam Rajeev",
            "Ashish Kulkarni",
            "Chandra Khatri",
            "Shubham Agarwal"
        ],
        "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?",
        "abstract": "arXiv:2510.07978v1 Announce Type: new  Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.",
        "arxiv_id": "2510.07978",
        "ARXIVID": "2510.07978",
        "COMMENT": "Matches criterion 3: Introduces VoiceAgentBench, a new benchmark for evaluating speech language models in agentic tasks, including multi-tool workflows and adversarial robustness, which is relevant to embodied AI benchmarking.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.07990": {
        "authors": [
            "Gaurvi Goyal",
            "Pham Cong Thuong",
            "Arren Glover",
            "Masayoshi Mizuno",
            "Chiara Bartolozzi"
        ],
        "title": "GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network",
        "abstract": "arXiv:2510.07990v1 Announce Type: new  Abstract: Human Pose Estimation is a crucial module in human-machine interaction applications and, especially since the rise in deep learning technology, robust methods are available to consumers using RGB cameras and commercial GPUs. On the other hand, event-based cameras have gained popularity in the vision research community for their low latency and low energy advantages that make them ideal for applications where those resources are constrained like portable electronics and mobile robots. In this work we propose a Graph Neural Network, GraphEnet, that leverages the sparse nature of event camera output, with an intermediate line based event representation, to estimate 2D Human Pose of a single person at a high frequency. The architecture incorporates a novel offset vector learning paradigm with confidence based pooling to estimate the human pose. This is the first work that applies Graph Neural Networks to event data for Human Pose Estimation. The code is open-source at https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.",
        "arxiv_id": "2510.07990",
        "ARXIVID": "2510.07990",
        "COMMENT": "Matches criterion 1: Proposes a novel method for human pose estimation using event-driven data and graph neural networks, which is a methodological improvement for spatial understanding in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.07575": {
        "authors": [
            "Zerui Cheng",
            "Stella Wohnig",
            "Ruchika Gupta",
            "Samiul Alam",
            "Tassallah Abdullahi",
            "Jo\\~ao Alves Ribeiro",
            "Christian Nielsen-Garcia",
            "Saif Mir",
            "Siran Li",
            "Jason Orender",
            "Seyed Ali Bahrainian",
            "Daniel Kirste",
            "Aaron Gokaslan",
            "Miko{\\l}aj Glinka",
            "Carsten Eickhoff",
            "Ruben Wolff"
        ],
        "title": "Benchmarking is Broken - Don't Let AI be its Own Judge",
        "abstract": "arXiv:2510.07575v1 Announce Type: new  Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding market capitalization, presents both transformative opportunities and critical challenges. Chief among these is the urgent need for a new, unified paradigm for trustworthy evaluation, as current benchmarks increasingly reveal critical vulnerabilities. Issues like data contamination and selective reporting by model developers fuel hype, while inadequate data quality control can lead to biased evaluations that, even if unintentionally, may favor specific approaches. As a flood of participants enters the AI space, this \"Wild West\" of assessment makes distinguishing genuine progress from exaggerated claims exceptionally difficult. Such ambiguity blurs scientific signals and erodes public confidence, much as unchecked claims would destabilize financial markets reliant on credible oversight from agencies like Moody's.   In high-stakes human examinations (e.g., SAT, GRE), substantial effort is devoted to ensuring fairness and credibility; why settle for less in evaluating AI, especially given its profound societal impact? This position paper argues that the current laissez-faire approach is unsustainable. We contend that true, sustainable AI advancement demands a paradigm shift: a unified, live, and quality-controlled benchmarking framework robust by construction, not by mere courtesy and goodwill. To this end, we dissect the systemic flaws undermining today's AI evaluation, distill the essential requirements for a new generation of assessments, and introduce PeerBench, a community-governed, proctored evaluation blueprint that embodies this paradigm through sealed execution, item banking with rolling renewal, and delayed transparency. Our goal is to pave the way for evaluations that can restore integrity and deliver genuinely trustworthy measures of AI progress.",
        "arxiv_id": "2510.07575",
        "ARXIVID": "2510.07575",
        "COMMENT": "Matches criterion 3: This position paper discusses the flaws in current AI benchmarking and proposes PeerBench, a new, community-governed, proctored evaluation framework. The focus on benchmarking and evaluation frameworks is directly relevant to embodied AI and simulator-related benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.08143": {
        "authors": [
            "Shian Du",
            "Menghan Xia",
            "Chang Liu",
            "Quande Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Xiangyang Ji"
        ],
        "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution",
        "abstract": "arXiv:2510.08143v1 Announce Type: new  Abstract: Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.",
        "arxiv_id": "2510.08143",
        "ARXIVID": "2510.08143",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes UniMMVSR, a unified multi-modal video super-resolution framework using latent video diffusion models and hybrid-modal conditions (text, images, videos).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.08508": {
        "authors": [
            "Lu Liu",
            "Chunlei Cai",
            "Shaocheng Shen",
            "Jianfeng Liang",
            "Weimin Ouyang",
            "Tianxiao Ye",
            "Jian Mao",
            "Huiyu Duan",
            "Jiangchao Yao",
            "Xiaoyun Zhang",
            "Qiang Hu",
            "Guangtao Zhai"
        ],
        "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
        "abstract": "arXiv:2510.08508v1 Announce Type: new  Abstract: Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo \\underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \\underline{Res}tored \\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.",
        "arxiv_id": "2510.08508",
        "ARXIVID": "2510.08508",
        "COMMENT": "Partially matches criterion 2 (uses VLMs and LLMs in a modular video restoration system) and criterion 4 (applies vision-language models to video restoration). The integration of VLMs/LLMs for degradation identification and routing is a novel application, but not a new VLLM/MLLM itself.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.08385": {
        "authors": [
            "Sofia Kirsanova",
            "Yao-Yi Chiang",
            "Weiwei Duan"
        ],
        "title": "Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning",
        "abstract": "arXiv:2510.08385v1 Announce Type: new  Abstract: Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles.",
        "arxiv_id": "2510.08385",
        "ARXIVID": "2510.08385",
        "COMMENT": "Matches criterion 2: Uses GPT-4o (a VLLM) with in-context learning for spatially-aware legend item detection and linking on historical maps. Also leverages LayoutLMv3 for layout detection, combining vision and language models in a novel way for spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.07670": {
        "authors": [
            "Haoyi Duan",
            "Yunzhi Zhang",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "title": "Controllable Video Synthesis via Variational Inference",
        "abstract": "arXiv:2510.07670v1 Announce Type: new  Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.",
        "arxiv_id": "2510.07670",
        "ARXIVID": "2510.07670",
        "COMMENT": "This paper introduces a variational inference-based method for controllable video synthesis, allowing for user controls at various granularities and leveraging multiple video generation backbones. It is relevant to generative modeling and vision, and the method for compositional control is novel, but it does not directly address spatial intelligence on embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.07961": {
        "authors": [
            "Yidi Liu",
            "Xueyang Fu",
            "Jie Huang",
            "Jie Xiao",
            "Dong Li",
            "Wenlong Zhang",
            "Lei Bai",
            "Zheng-Jun Zha"
        ],
        "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement",
        "abstract": "arXiv:2510.07961v1 Announce Type: new  Abstract: Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
        "arxiv_id": "2510.07961",
        "ARXIVID": "2510.07961",
        "COMMENT": "This paper proposes a new VAE-based framework for UHD image restoration with latent space regularization and controllable refinement. It is relevant to vision foundation models and generative modeling (criterion 4), though it is more focused on restoration than general-purpose vision models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.08189": {
        "authors": [
            "Yi Lu",
            "Jianing Wang",
            "Linsen Guo",
            "Wei He",
            "Hongyin Tang",
            "Tao Gui",
            "Xuanjing Huang",
            "Xuezhi Cao",
            "Wei Wang",
            "Xunliang Cai"
        ],
        "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?",
        "abstract": "arXiv:2510.08189v1 Announce Type: new  Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
        "arxiv_id": "2510.08189",
        "ARXIVID": "2510.08189",
        "COMMENT": "Somewhat related to criterion 3: Proposes R-HORIZON, a new benchmark for evaluating long-horizon reasoning in large reasoning models, which is tangentially related to embodied AI benchmarking but not focused on vision or embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.08512": {
        "authors": [
            "Nikolaos Stathoulopoulos",
            "Christoforos Kanellakis",
            "George Nikolakopoulos"
        ],
        "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression",
        "abstract": "arXiv:2510.08512v1 Announce Type: new  Abstract: Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans.",
        "arxiv_id": "2510.08512",
        "ARXIVID": "2510.08512",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications). Proposes a deep compression framework for point clouds using semantic scene graphs, with applications in multi-agent robotics. The semantic-aware approach is interesting for downstream vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.07721": {
        "authors": [
            "Zipeng Guo",
            "Lichen Ma",
            "Xiaolong Fu",
            "Gaojing Zhou",
            "Lan Yang",
            "Yuchen Zhou",
            "Linkai Liu",
            "Yu He",
            "Ximan Liu",
            "Shiping Dong",
            "Jingling Fu",
            "Zhen Chen",
            "Yu Shi",
            "Junshi Huang",
            "Jason Li",
            "Chao Gou"
        ],
        "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning",
        "abstract": "arXiv:2510.07721v1 Announce Type: new  Abstract: In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.",
        "arxiv_id": "2510.07721",
        "ARXIVID": "2510.07721",
        "COMMENT": "Some relevance to criterion 4: Applies reinforcement learning and spatial-matting to object removal in e-commerce images, with a new dataset and benchmark. Focus is more on application than foundation model development.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.08263": {
        "authors": [
            "Shunyu An",
            "Miao Wang",
            "Yongchao Li",
            "Dong Wan",
            "Lina Wang",
            "Ling Qin",
            "Liqin Gao",
            "Congyao Fan",
            "Zhiyong Mao",
            "Jiange Pu",
            "Wenji Xia",
            "Dong Zhao",
            "Rui Hu",
            "Ji Lu",
            "Guiyue Zhou",
            "Baoyu Tang",
            "Yanqin Gao",
            "Yongsheng Du",
            "Daigang Xu",
            "Lingjun Huang",
            "Baoli Wang",
            "Xiwen Zhang",
            "Luyao Wang",
            "Shilong Liu"
        ],
        "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report",
        "abstract": "arXiv:2510.08263v1 Announce Type: new  Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications.",
        "arxiv_id": "2510.08263",
        "ARXIVID": "2510.08263",
        "COMMENT": "Somewhat related to criterion 3: Proposes a new protocol for multi-agent systems, focusing on interoperability and knowledge sharing, which is tangentially related to embodied AI benchmarks but not directly about simulators or novel embodied AI methods.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.07652": {
        "authors": [
            "Harshala Gammulle",
            "Clinton Fookes",
            "Sridha Sridharan",
            "Simon Denman"
        ],
        "title": "Dual-Stream Alignment for Action Segmentation",
        "abstract": "arXiv:2510.07652v1 Announce Type: new  Abstract: Action segmentation is a challenging yet active research area that involves identifying when and where specific actions occur in continuous video streams. Most existing work has focused on single-stream approaches that model the spatio- temporal aspects of frame sequences. However, recent research has shifted toward two-stream methods that learn action-wise features to enhance action segmentation performance. In this work, we propose the Dual-Stream Alignment Network (DSA Net) and investigate the impact of incorporating a second stream of learned action features to guide segmentation by capturing both action and action-transition cues. Communication between the two streams is facilitated by a Temporal Context (TC) block, which fuses complementary information using cross- attention and Quantum-based Action-Guided Modulation (Q- ActGM), enhancing the expressive power of the fused features. To the best of our knowledge, this is the first study to introduce a hybrid quantum-classical machine learning framework for action segmentation. Our primary objective is for the two streams (frame-wise and action-wise) to learn a shared feature space through feature alignment. This is encouraged by the proposed Dual-Stream Alignment Loss, which comprises three components: relational consistency, cross-level contrastive, and cycle-consistency reconstruction losses. Following prior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA, Breakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of each component through extensive ablation studies. Notably, DSA Net achieves state-of-the-art performance, significantly outperforming existing",
        "arxiv_id": "2510.07652",
        "ARXIVID": "2510.07652",
        "COMMENT": "This paper proposes a dual-stream alignment network for action segmentation in video, introducing a hybrid quantum-classical framework. While it is relevant to video understanding and action segmentation, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.07546": {
        "authors": [
            "Soroush Mehraban",
            "Vida Adeli",
            "Jacob Rommann",
            "Babak Taati",
            "Kyryl Truskovskyi"
        ],
        "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
        "abstract": "arXiv:2510.07546v1 Announce Type: new  Abstract: We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.",
        "arxiv_id": "2510.07546",
        "ARXIVID": "2510.07546",
        "COMMENT": "This paper proposes a new video-to-video style transfer framework using diffusion models and context-style adapters. While it is relevant to generative modeling and vision, it does not directly address spatial intelligence, embodied AI, VLLMs/MLLMs, or vision foundation models in the sense of large-scale pretraining or general-purpose models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.07631": {
        "authors": [
            "Shreshth Saini",
            "Shashank Gupta",
            "Alan C. Bovik"
        ],
        "title": "Rectified-CFG++ for Flow Based Models",
        "abstract": "arXiv:2510.07631v1 Announce Type: new  Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: https://rectified-cfgpp.github.io/",
        "arxiv_id": "2510.07631",
        "ARXIVID": "2510.07631",
        "COMMENT": "This paper proposes a new guidance method (Rectified-CFG++) for flow-based diffusion models, improving text-to-image generation. It is relevant to generative modeling in multi-modal learning, but does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.07953": {
        "authors": [
            "Yifang Yin",
            "Shengkai Chen",
            "Yiyao Li",
            "Lu Wang",
            "Ruibing Jin",
            "Wei Cui",
            "Shili Xiang"
        ],
        "title": "SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation",
        "abstract": "arXiv:2510.07953v1 Announce Type: new  Abstract: Precipitation nowcasting predicts future radar sequences based on current observations, which is a highly challenging task driven by the inherent complexity of the Earth system. Accurate nowcasting is of utmost importance for addressing various societal needs, including disaster management, agriculture, transportation, and energy optimization. As a complementary to existing non-autoregressive nowcasting approaches, we investigate the impact of prediction horizons on nowcasting models and propose SimCast, a novel training pipeline featuring a short-to-long term knowledge distillation technique coupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved nowcasting predictions can be obtained without introducing additional overhead during inference. As SimCast generates deterministic predictions, we further integrate it into a diffusion-based framework named CasCast, leveraging the strengths from probabilistic models to overcome limitations such as blurriness and distribution shift in deterministic outputs. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed framework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and 0.361 on MeteoNet, which outperforms existing approaches by a significant margin.",
        "arxiv_id": "2510.07953",
        "ARXIVID": "2510.07953",
        "COMMENT": "This paper presents a new knowledge distillation technique for precipitation nowcasting, with a diffusion-based framework for probabilistic predictions. While it is relevant to generative modeling and vision, it does not directly address the specific criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.08383": {
        "authors": [
            "Yi Jiang",
            "Lei Shen",
            "Lujie Niu",
            "Sendong Zhao",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
        "abstract": "arXiv:2510.08383v1 Announce Type: new  Abstract: Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment.",
        "arxiv_id": "2510.08383",
        "ARXIVID": "2510.08383",
        "COMMENT": "Some relevance to criterion 2: Proposes QAgent, a modular RAG framework with RL for query understanding, but not specifically a VLLM or MLLM; more about retrieval-augmented LLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.08207": {
        "authors": [
            "Matteo Gregorini",
            "Chiara Boldrini",
            "Lorenzo Valerio"
        ],
        "title": "DODO: Causal Structure Learning with Budgeted Interventions",
        "abstract": "arXiv:2510.08207v1 Announce Type: new  Abstract: Artificial Intelligence has achieved remarkable advancements in recent years, yet much of its progress relies on identifying increasingly complex correlations. Enabling causality awareness in AI has the potential to enhance its performance by enabling a deeper understanding of the underlying mechanisms of the environment. In this paper, we introduce DODO, an algorithm defining how an Agent can autonomously learn the causal structure of its environment through repeated interventions. We assume a scenario where an Agent interacts with a world governed by a causal Directed Acyclic Graph (DAG), which dictates the system's dynamics but remains hidden from the Agent. The Agent's task is to accurately infer the causal DAG, even in the presence of noise. To achieve this, the Agent performs interventions, leveraging causal inference techniques to analyze the statistical significance of observed changes. Results show better performance for DODO, compared to observational approaches, in all but the most limited resource conditions. DODO is often able to reconstruct with as low as zero errors the structure of the causal graph. In the most challenging configuration, DODO outperforms the best baseline by +0.25 F1 points.",
        "arxiv_id": "2510.08207",
        "ARXIVID": "2510.08207",
        "COMMENT": "Some relevance to embodied AI (criterion 3): Proposes DODO, an agent that learns causal structure via interventions, but not focused on spatial intelligence or vision; more about causal inference in agent-environment interaction.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.08517": {
        "authors": [
            "Grace Liu",
            "Yuxiao Qu",
            "Jeff Schneider",
            "Aarti Singh",
            "Aviral Kumar"
        ],
        "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
        "abstract": "arXiv:2510.08517v1 Announce Type: new  Abstract: Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.",
        "arxiv_id": "2510.08517",
        "ARXIVID": "2510.08517",
        "COMMENT": "Some relevance to criterion 2: Proposes CaRT, a method for teaching LLM agents when to stop gathering information, with applications in interactive domains. Focus is on LLMs, not specifically VLLMs or MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.07790": {
        "authors": [
            "Hao Wu",
            "Wei Liu"
        ],
        "title": "GCPO: When Contrast Fails, Go Gold",
        "abstract": "arXiv:2510.07790v1 Announce Type: new  Abstract: Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.",
        "arxiv_id": "2510.07790",
        "ARXIVID": "2510.07790",
        "COMMENT": "Does not match any specific criterion. Focuses on reinforcement learning for language models, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.07858": {
        "authors": [
            "Zhiqing Cui",
            "Binwu Wang",
            "Qingxiang Liu",
            "Yeqiang Wang",
            "Zhengyang Zhou",
            "Yuxuan Liang",
            "Yang Wang"
        ],
        "title": "Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models",
        "abstract": "arXiv:2510.07858v1 Announce Type: new  Abstract: Large language models (LLM) have emerged as a promising avenue for time series forecasting, offering the potential to integrate multimodal data. However, existing LLM-based approaches face notable limitations-such as marginalized role in model architectures, reliance on coarse statistical text prompts, and lack of interpretability. In this work, we introduce Augur, a fully LLM driven time series forecasting framework that exploits LLM causal reasoning to discover and use directed causal associations among covariates. Augur uses a two stage teacher student architecture where a powerful teacher LLM infers a directed causal graph from time series using heuristic search together with pairwise causality testing. A lightweight student agent then refines the graph and fine tune on high confidence causal associations that are encoded as rich textual prompts to perform forecasting. This design improves predictive accuracy while yielding transparent, traceable reasoning about variable interactions. Extensive experiments on real-world datasets with 25 baselines demonstrate that Augur achieves competitive performance and robust zero-shot generalization.",
        "arxiv_id": "2510.07858",
        "ARXIVID": "2510.07858",
        "COMMENT": "This paper uses LLMs for time series forecasting with causal reasoning, but does not focus on spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models. It is more about time series and causality.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.08308": {
        "authors": [
            "Liwei Kang",
            "Yue Deng",
            "Yao Xiao",
            "Zhanfeng Mo",
            "Wee Sun Lee",
            "Lidong Bing"
        ],
        "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
        "abstract": "arXiv:2510.08308v1 Announce Type: new  Abstract: Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
        "arxiv_id": "2510.08308",
        "ARXIVID": "2510.08308",
        "COMMENT": "This paper analyzes reflection in reasoning models (LLMs) but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about language model reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}