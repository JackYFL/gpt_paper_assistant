{
    "2602.18941": {
        "authors": [
            "Kaiming Jin",
            "Yuefan Wu",
            "Shengqiong Wu",
            "Bobo Li",
            "Shuicheng Yan",
            "Tat-Seng Chua"
        ],
        "title": "Global Commander and Local Operative: A Dual-Agent Framework for Scene Navigation",
        "abstract": "arXiv:2602.18941v1 Announce Type: new  Abstract: Vision-and-Language Scene navigation is a fundamental capability for embodied human-AI collaboration, requiring agents to follow natural language instructions to execute coherent action sequences in complex environments. Existing approaches either rely on multiple agents, incurring high coordination and resource costs, or adopt a single-agent paradigm, which overloads the agent with both global planning and local perception, often leading to degraded reasoning and instruction drift in long-horizon settings. To address these issues, we introduce DACo, a planning-grounding decoupled architecture that disentangles global deliberation from local grounding. Concretely, it employs a Global Commander for high-level strategic planning and a Local Operative for egocentric observing and fine-grained execution. By disentangling global reasoning from local action, DACo alleviates cognitive overload and improves long-horizon stability. The framework further integrates dynamic subgoal planning and adaptive replanning to enable structured and resilient navigation. Extensive evaluations on R2R, REVERIE, and R4R demonstrate that DACo achieves 4.9%, 6.5%, 5.4% absolute improvements over the best-performing baselines in zero-shot settings, and generalizes effectively across both closed-source (e.g., GPT-4o) and open-source (e.g., Qwen-VL Series) backbones. DACo provides a principled and extensible paradigm for robust long-horizon navigation. Project page: https://github.com/ChocoWu/DACo",
        "arxiv_id": "2602.18941",
        "ARXIVID": "2602.18941",
        "COMMENT": "Matches criterion 1 and 3: New methodological improvements to spatial intelligence on embodied agents and new methods for embodied AI. Proposes a dual-agent framework for scene navigation with significant improvements over baselines.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.19710": {
        "authors": [
            "Haitao Lin",
            "Hanyang Yu",
            "Jingshun Huang",
            "He Zhang",
            "Yonggen Ling",
            "Ping Tan",
            "Xiangyang Xue",
            "Yanwei Fu"
        ],
        "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
        "abstract": "arXiv:2602.19710v1 Announce Type: new  Abstract: Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.   To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.   Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
        "arxiv_id": "2602.19710",
        "ARXIVID": "2602.19710",
        "COMMENT": "Strongly matches criterion 1 (methodological improvements to spatial intelligence on embodied agents) and criterion 3 (embodied AI, new methods for spatial grounding and generalization). Proposes a decoupled paradigm for vision-language-action models with universal pose pretraining, improving spatial grounding and generalization in embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.18817": {
        "authors": [
            "Chongyang Xu",
            "Shen Cheng",
            "Haipeng Li",
            "Haoqiang Fan",
            "Ziliang Feng",
            "Shuaicheng Liu"
        ],
        "title": "HeRO: Hierarchical 3D Semantic Representation for Pose-aware Object Manipulation",
        "abstract": "arXiv:2602.18817v1 Announce Type: new  Abstract: Imitation learning for robotic manipulation has progressed from 2D image policies to 3D representations that explicitly encode geometry. Yet purely geometric policies often lack explicit part-level semantics, which are critical for pose-aware manipulation (e.g., distinguishing a shoe's toe from heel). In this paper, we present HeRO, a diffusion-based policy that couples geometry and semantics via hierarchical semantic fields. HeRO employs dense semantics lifting to fuse discriminative, geometry-sensitive features from DINOv2 with the smooth, globally coherent correspondences from Stable Diffusion, yielding dense features that are both fine-grained and spatially consistent. These features are processed and partitioned to construct a global field and a set of local fields. A hierarchical conditioning module conditions the generative denoiser on global and local fields using permutation-invariant network architecture, thereby avoiding order-sensitive bias and producing a coherent control policy for pose-aware manipulation. In various tests, HeRO establishes a new state-of-the-art, improving success on Place Dual Shoes by 12.3% and averaging 6.5% gains across six challenging pose-aware tasks. Code is available at https://github.com/Chongyang-99/HeRO.",
        "arxiv_id": "2602.18817",
        "ARXIVID": "2602.18817",
        "COMMENT": "Matches criterion 1: New methodological improvements to spatial understanding on embodied agents. Proposes a hierarchical 3D semantic representation for pose-aware object manipulation using diffusion-based policy.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.20159": {
        "authors": [
            "Maijunxian Wang",
            "Ruisi Wang",
            "Juyi Lin",
            "Ran Ji",
            "Thadd\\\"aus Wiedemer",
            "Qingying Gao",
            "Dezhi Luo",
            "Yaoyao Qian",
            "Lianyu Huang",
            "Zelong Hong",
            "Jiahui Ge",
            "Qianli Ma",
            "Hang He",
            "Yifan Zhou",
            "Lingzi Guo",
            "Lantao Mei",
            "Jiachen Li",
            "Hanwen Xing",
            "Tianqi Zhao",
            "Fengyuan Yu",
            "Weihang Xiao",
            "Yizheng Jiao",
            "Jianheng Hou",
            "Danyang Zhang",
            "Pengcheng Xu",
            "Boyang Zhong",
            "Zehong Zhao",
            "Gaoyun Fang",
            "John Kitaoka",
            "Yile Xu",
            "Hua Xu",
            "Kenton Blacutt",
            "Tin Nguyen",
            "Siyuan Song",
            "Haoran Sun",
            "Shaoyue Wen",
            "Linyang He",
            "Runming Wang",
            "Yanzhi Wang",
            "Mengyue Yang",
            "Ziqiao Ma",
            "Rapha\\\"el Milli\\`ere",
            "Freda Shi",
            "Nuno Vasconcelos",
            "Daniel Khashabi",
            "Alan Yuille",
            "Yilun Du",
            "Ziming Liu",
            "Bo Li",
            "Dahua Lin",
            "Ziwei Liu",
            "Vikash Kumar",
            "Yijiang Li",
            "Lei Yang",
            "Zhongang Cai",
            "Hokin Deng"
        ],
        "title": "A Very Big Video Reasoning Suite",
        "abstract": "arXiv:2602.20159v1 Announce Type: new  Abstract: Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
        "arxiv_id": "2602.20159",
        "ARXIVID": "2602.20159",
        "COMMENT": "Matches criterion 3: Embodied AI papers on building new benchmarks (simulator related) or new methods. Introduces a large-scale video reasoning dataset and benchmark, focusing on spatiotemporal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.19571": {
        "authors": [
            "Chang Liu",
            "Yunfan Ye",
            "Qingyang Zhou",
            "Xichen Tan",
            "Mengxuan Luo",
            "Zhenyu Qiu",
            "Wei Peng",
            "Zhiping Cai"
        ],
        "title": "HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies",
        "abstract": "arXiv:2602.19571v1 Announce Type: new  Abstract: Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.",
        "arxiv_id": "2602.19571",
        "ARXIVID": "2602.19571",
        "COMMENT": "Strongly matches criterion 3 (embodied AI, new benchmark for predictive world modeling in Video-LLMs). Introduces HOCA-Bench, a benchmark for evaluating physical reasoning and anomaly detection in video-language models, highlighting gaps in current models' physical understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.18845": {
        "authors": [
            "Chengwei Xia",
            "Fan Ma",
            "Ruijie Quan",
            "Yunqiu Xu",
            "Kun Zhan",
            "Yi Yang"
        ],
        "title": "Echoes of Ownership: Adversarial-Guided Dual Injection for Copyright Protection in MLLMs",
        "abstract": "arXiv:2602.18845v1 Announce Type: new  Abstract: With the rapid deployment and widespread adoption of multimodal large language models (MLLMs), disputes regarding model version attribution and ownership have become increasingly frequent, raising significant concerns about intellectual property protection. In this paper, we propose a framework for generating copyright triggers for MLLMs, enabling model publishers to embed verifiable ownership information into the model. The goal is to construct trigger images that elicit ownership-related textual responses exclusively in fine-tuned derivatives of the original model, while remaining inert in other non-derivative models. Our method constructs a tracking trigger image by treating the image as a learnable tensor, performing adversarial optimization with dual-injection of ownership-relevant semantic information. The first injection is achieved by enforcing textual consistency between the output of an auxiliary MLLM and a predefined ownership-relevant target text; the consistency loss is backpropagated to inject this ownership-related information into the image. The second injection is performed at the semantic-level by minimizing the distance between the CLIP features of the image and those of the target text. Furthermore, we introduce an additional adversarial training stage involving the auxiliary model derived from the original model itself. This auxiliary model is specifically trained to resist generating ownership-relevant target text, thereby enhancing robustness in heavily fine-tuned derivative models. Extensive experiments demonstrate the effectiveness of our dual-injection approach in tracking model lineage under various fine-tuning and domain-shift scenarios.",
        "arxiv_id": "2602.18845",
        "ARXIVID": "2602.18845",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multi-modal large language models). Proposes a framework for copyright protection in MLLMs using adversarial triggers.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18757": {
        "authors": [
            "Xiaoru Dong",
            "Ruiqin Li",
            "Xiao Han",
            "Zhenxuan Wu",
            "Jiamin Wang",
            "Jian Chen",
            "Qi Jiang",
            "SM Yiu",
            "Xinge Zhu",
            "Yuexin Ma"
        ],
        "title": "Driving with A Thousand Faces: A Benchmark for Closed-Loop Personalized End-to-End Autonomous Driving",
        "abstract": "arXiv:2602.18757v1 Announce Type: new  Abstract: Human driving behavior is inherently diverse, yet most end-to-end autonomous driving (E2E-AD) systems learn a single average driving style, neglecting individual differences. Achieving personalized E2E-AD faces challenges across three levels: limited real-world datasets with individual-level annotations, a lack of quantitative metrics for evaluating personal driving styles, and the absence of algorithms that can learn stylized representations from users' trajectories. To address these gaps, we propose Person2Drive, a comprehensive personalized E2E-AD platform and benchmark. It includes an open-source, flexible data collection system that simulates realistic scenarios to generate scalable and diverse personalized driving datasets; style vector-based evaluation metrics with Maximum Mean Discrepancy and KL divergence to comprehensively quantify individual driving behaviors; and a personalized E2E-AD framework with a style reward model that efficiently adapts E2E models for safe and individualized driving. Extensive experiments demonstrate that Person2Drive enables fine-grained analysis, reproducible evaluation, and effective personalization in end-to-end autonomous driving. Our dataset and code will be released after acceptance.",
        "arxiv_id": "2602.18757",
        "ARXIVID": "2602.18757",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator for personalized autonomous driving). Introduces Person2Drive, a benchmark and framework for personalized end-to-end autonomous driving with new evaluation metrics and data collection system.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.19575": {
        "authors": [
            "Minseo Kim",
            "Minchan Kwon",
            "Dongyeun Lee",
            "Yunho Jeon",
            "Junmo Kim"
        ],
        "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization",
        "abstract": "arXiv:2602.19575v1 Announce Type: new  Abstract: Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.",
        "arxiv_id": "2602.19575",
        "ARXIVID": "2602.19575",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. Proposes a new framework for concept disentanglement in personalized diffusion models for text-to-image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.18936": {
        "authors": [
            "Yu Li",
            "Yujun Cai",
            "Chi Zhang"
        ],
        "title": "CRAFT-LoRA: Content-Style Personalization via Rank-Constrained Adaptation and Training-Free Fusion",
        "abstract": "arXiv:2602.18936v1 Announce Type: new  Abstract: Personalized image generation requires effectively balancing content fidelity with stylistic consistency when synthesizing images based on text and reference examples. Low-Rank Adaptation (LoRA) offers an efficient personalization approach, with potential for precise control through combining LoRA weights on different concepts. However, existing combination techniques face persistent challenges: entanglement between content and style representations, insufficient guidance for controlling elements' influence, and unstable weight fusion that often require additional training. We address these limitations through CRAFT-LoRA, with complementary components: (1) rank-constrained backbone fine-tuning that injects low-rank projection residuals to encourage learning decoupled content and style subspaces; (2) a prompt-guided approach featuring an expert encoder with specialized branches that enables semantic extension and precise control through selective adapter aggregation; and (3) a training-free, timestep-dependent classifier-free guidance scheme that enhances generation stability by strategically adjusting noise predictions across diffusion steps. Our method significantly improves content-style disentanglement, enables flexible semantic control over LoRA module combinations, and achieves high-fidelity generation without additional retraining overhead.",
        "arxiv_id": "2602.18936",
        "ARXIVID": "2602.18936",
        "COMMENT": "Matches criterion 4: Vision foundation models and applications. Proposes a new method for content-style disentanglement in personalized image generation using LoRA and diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.19180": {
        "authors": [
            "Wenhao Shen",
            "Hao Wang",
            "Wanqi Yin",
            "Fayao Liu",
            "Xulei Yang",
            "Chao Liang",
            "Zhongang Cai",
            "Guosheng Lin"
        ],
        "title": "VLM-Guided Group Preference Alignment for Diffusion-based Human Mesh Recovery",
        "abstract": "arXiv:2602.19180v1 Announce Type: new  Abstract: Human mesh recovery (HMR) from a single RGB image is inherently ambiguous, as multiple 3D poses can correspond to the same 2D observation. Recent diffusion-based methods tackle this by generating various hypotheses, but often sacrifice accuracy. They yield predictions that are either physically implausible or drift from the input image, especially under occlusion or in cluttered, in-the-wild scenes. To address this, we introduce a dual-memory augmented HMR critique agent with self-reflection to produce context-aware quality scores for predicted meshes. These scores distill fine-grained cues about 3D human motion structure, physical feasibility, and alignment with the input image. We use these scores to build a group-wise HMR preference dataset. Leveraging this dataset, we propose a group preference alignment framework for finetuning diffusion-based HMR models. This process injects the rich preference signals into the model, guiding it to generate more physically plausible and image-consistent human meshes. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches.",
        "arxiv_id": "2602.19180",
        "ARXIVID": "2602.19180",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications). Proposes a VLM-guided group preference alignment for diffusion-based human mesh recovery, improving physical plausibility and image consistency in 3D human mesh predictions.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.18765": {
        "authors": [
            "Lubin Bai",
            "Sheng Xiao",
            "Ziyu Yin",
            "Haoyu Wang",
            "Siyang Wu",
            "Xiuyuan Zhang",
            "Shihong Du"
        ],
        "title": "A high-resolution nationwide urban village mapping product for 342 Chinese cities based on foundation models",
        "abstract": "arXiv:2602.18765v1 Announce Type: new  Abstract: Urban Villages (UVs) represent a distinctive form of high-density informal settlement embedded within China's rapidly urbanizing cities. Accurate identification of UVs is critical for urban governance, renewal, and sustainable development. But due to the pronounced heterogeneity and diversity of UVs across China's vast territory, a consistent and reliable nationwide dataset has been lacking. In this work, we present GeoLink-UV, a high-resolution nationwide UV mapping product that clearly delineates the locations and boundaries of UVs in 342 Chinese cities. The dataset is derived from multisource geospatial data, including optical remote sensing images and geo-vector data, and is generated through a foundation model-driven mapping framework designed to address the generalization issues and improve the product quality. A geographically stratified accuracy assessment based on independent samples from 28 cities confirms the reliability and scientific credibility of the nationwide dataset across heterogeneous urban contexts. Based on this nationwide product, we reveal substantial interregional disparities in UV prevalence and spatial configuration. On average, UV areas account for 8 % of built-up land, with marked clustering in central and south China. Building-level analysis further confirms a consistent low-rise, high-density development pattern of UVs nationwide, while highlighting regionally differentiated morphological characteristics. The GeoLink-UV dataset provides an open and systematically validated geospatial foundation for urban studies, informal settlement monitoring, and evidence-based urban renewal planning, and contributes directly to large-scale assessments aligned with Sustainable Development Goal 11. The GeoLink-UV dataset introduced in this article is freely available at https://doi.org/10.5281/zenodo.18688062.",
        "arxiv_id": "2602.18765",
        "ARXIVID": "2602.18765",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. Uses a foundation model-driven mapping framework for high-resolution urban village mapping.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.20051": {
        "authors": [
            "Yeonsung Kim",
            "Junggeun Do",
            "Seunguk Do",
            "Sangmin Kim",
            "Jaesik Park",
            "Jay-Yoon Lee"
        ],
        "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency",
        "abstract": "arXiv:2602.20051v1 Announce Type: new  Abstract: 3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.",
        "arxiv_id": "2602.20051",
        "ARXIVID": "2602.20051",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding). Introduces a learnable loss-net for structural consistency in 3D human pose estimation, replacing hand-crafted priors with a data-driven approach.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.19723": {
        "authors": [
            "Yue Zhang",
            "Zhizheng Zhuo",
            "Siyao Xu",
            "Shan Lv",
            "Zhaoxi Liu",
            "Jun Qiu",
            "Qiuli Wang",
            "Yaou Liu",
            "S. Kevin Zhou"
        ],
        "title": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets",
        "abstract": "arXiv:2602.19723v1 Announce Type: new  Abstract: Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.",
        "arxiv_id": "2602.19723",
        "ARXIVID": "2602.19723",
        "COMMENT": "Relevant to multi-modal learning and generative modeling, but focused on medical imaging. Proposes a personalized MRI synthesis framework for heterogeneous datasets.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.18968": {
        "authors": [
            "Tao Zhe",
            "Haoyu Wang",
            "Bo Luo",
            "Min Wu",
            "Wei Fan",
            "Xiao Luo",
            "Zijun Yao",
            "Haifeng Chen",
            "Dongjie Wang"
        ],
        "title": "Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction",
        "abstract": "arXiv:2602.18968v1 Announce Type: new  Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.",
        "arxiv_id": "2602.18968",
        "ARXIVID": "2602.18968",
        "COMMENT": "Does not match any specific criterion. Focuses on tool orchestration in agentic systems, not specifically spatial intelligence or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.19536": {
        "authors": [
            "Zhiwei Ning",
            "Xuanang Gao",
            "Jiaxi Cao",
            "Runze Yang",
            "Huiying Xu",
            "Xinzhong Zhu",
            "Jie Yang",
            "Wei Liu"
        ],
        "title": "Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection",
        "abstract": "arXiv:2602.19536v1 Announce Type: new  Abstract: Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.",
        "arxiv_id": "2602.19536",
        "ARXIVID": "2602.19536",
        "COMMENT": "Relevant to 3D object detection and vision foundation models, but does not directly match any specific criterion. Proposes a Mamba-based backbone for foreground-enhanced encoding in 3D object detection.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.20021": {
        "authors": [
            "Natalie Shapira",
            "Chris Wendler",
            "Avery Yen",
            "Gabriele Sarti",
            "Koyena Pal",
            "Olivia Floody",
            "Adam Belfki",
            "Alex Loftus",
            "Aditya Ratan Jannali",
            "Nikhil Prakash",
            "Jasmine Cui",
            "Giordano Rogers",
            "Jannik Brinkmann",
            "Can Rager",
            "Amir Zur",
            "Michael Ripa",
            "Aruna Sankaranarayanan",
            "David Atkinson",
            "Rohit Gandikota",
            "Jaden Fiotto-Kaufman",
            "EunJeong Hwang",
            "Hadas Orgad",
            "P Sam Sahil",
            "Negev Taglicht",
            "Tomer Shabtay",
            "Atai Ambus",
            "Nitay Alon",
            "Shiri Oron",
            "Ayelet Gordon-Tapiero",
            "Yotam Kaplan",
            "Vered Shwartz",
            "Tamar Rott Shaham",
            "Christoph Riedl",
            "Reuth Mirsky",
            "Maarten Sap",
            "David Manheim",
            "Tomer Ullman",
            "David Bau"
        ],
        "title": "Agents of Chaos",
        "abstract": "arXiv:2602.20021v1 Announce Type: new  Abstract: We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
        "arxiv_id": "2602.20021",
        "ARXIVID": "2602.20021",
        "COMMENT": "Interesting empirical study on autonomous agents, but does not directly match any specific criterion. Focuses on security and governance vulnerabilities in language-model-powered agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.19323": {
        "authors": [
            "Yiran Qiao",
            "Yiren Lu",
            "Yunlai Zhou",
            "Rui Yang",
            "Linlin Hou",
            "Yu Yin",
            "Jing Ma"
        ],
        "title": "DefenseSplat: Enhancing the Robustness of 3D Gaussian Splatting via Frequency-Aware Filtering",
        "abstract": "arXiv:2602.19323v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for real-time and high-fidelity 3D reconstruction from posed images. However, recent studies reveal its vulnerability to adversarial corruptions in input views, where imperceptible yet consistent perturbations can drastically degrade rendering quality, increase training and rendering time, and inflate memory usage, even leading to server denial-of-service. In our work, to mitigate this issue, we begin by analyzing the distinct behaviors of adversarial perturbations in the low- and high-frequency components of input images using wavelet transforms. Based on this observation, we design a simple yet effective frequency-aware defense strategy that reconstructs training views by filtering high-frequency noise while preserving low-frequency content. This approach effectively suppresses adversarial artifacts while maintaining the authenticity of the original scene. Notably, it does not significantly impair training on clean data, achieving a desirable trade-off between robustness and performance on clean inputs. Through extensive experiments under a wide range of attack intensities on multiple benchmarks, we demonstrate that our method substantially enhances the robustness of 3DGS without access to clean ground-truth supervision. By highlighting and addressing the overlooked vulnerabilities of 3D Gaussian Splatting, our work paves the way for more robust and secure 3D reconstructions.",
        "arxiv_id": "2602.19323",
        "ARXIVID": "2602.19323",
        "COMMENT": "Relevant to vision foundation models and robustness, but does not directly match any specific criterion. Focuses on robustness in 3D Gaussian Splatting for 3D reconstruction.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.20104": {
        "authors": [
            "Hasan Amin",
            "Ming Yin",
            "Rajiv Khanna"
        ],
        "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
        "abstract": "arXiv:2602.20104v1 Announce Type: new  Abstract: In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
        "arxiv_id": "2602.20104",
        "ARXIVID": "2602.20104",
        "COMMENT": "Does not match any specific criterion. Focuses on human-AI collaboration in decision making, not spatial intelligence or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.19458": {
        "authors": [
            "Ziyang Guo",
            "Yifan Wu",
            "Jason Hartline",
            "Kenneth Holstein",
            "Jessica Hullman"
        ],
        "title": "ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making",
        "abstract": "arXiv:2602.19458v1 Announce Type: new  Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.",
        "arxiv_id": "2602.19458",
        "ARXIVID": "2602.19458",
        "COMMENT": "Does not match any specific criterion. Focuses on decision-making with LLMs, not vision or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.19611": {
        "authors": [
            "Mingxiu Cai",
            "Zhe Zhang",
            "Gaochang Wu",
            "Tianyou Chai",
            "Xiatian Zhu"
        ],
        "title": "RAID: Retrieval-Augmented Anomaly Detection",
        "abstract": "arXiv:2602.19611v1 Announce Type: new  Abstract: Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.",
        "arxiv_id": "2602.19611",
        "ARXIVID": "2602.19611",
        "COMMENT": "Relevant to computer vision and anomaly detection, but does not directly match any specific criterion. Proposes a retrieval-augmented anomaly detection framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}