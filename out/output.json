{
    "2511.00062": {
        "authors": [
            "NVIDIA",
            ":",
            "Arslan Ali",
            "Junjie Bai",
            "Maciej Bala",
            "Yogesh Balaji",
            "Aaron Blakeman",
            "Tiffany Cai",
            "Jiaxin Cao",
            "Tianshi Cao",
            "Elizabeth Cha",
            "Yu-Wei Chao",
            "Prithvijit Chattopadhyay",
            "Mike Chen",
            "Yongxin Chen",
            "Yu Chen",
            "Shuai Cheng",
            "Yin Cui",
            "Jenna Diamond",
            "Yifan Ding",
            "Jiaojiao Fan",
            "Linxi Fan",
            "Liang Feng",
            "Francesco Ferroni",
            "Sanja Fidler",
            "Xiao Fu",
            "Ruiyuan Gao",
            "Yunhao Ge",
            "Jinwei Gu",
            "Aryaman Gupta",
            "Siddharth Gururani",
            "Imad El Hanafi",
            "Ali Hassani",
            "Zekun Hao",
            "Jacob Huffman",
            "Joel Jang",
            "Pooya Jannaty",
            "Jan Kautz",
            "Grace Lam",
            "Xuan Li",
            "Zhaoshuo Li",
            "Maosheng Liao",
            "Chen-Hsuan Lin",
            "Tsung-Yi Lin",
            "Yen-Chen Lin",
            "Huan Ling",
            "Ming-Yu Liu",
            "Xian Liu",
            "Yifan Lu",
            "Alice Luo",
            "Qianli Ma",
            "Hanzi Mao",
            "Kaichun Mo",
            "Seungjun Nah",
            "Yashraj Narang",
            "Abhijeet Panaskar",
            "Lindsey Pavao",
            "Trung Pham",
            "Morteza Ramezanali",
            "Fitsum Reda",
            "Scott Reed",
            "Xuanchi Ren",
            "Haonan Shao",
            "Yue Shen",
            "Stella Shi",
            "Shuran Song",
            "Bartosz Stefaniak",
            "Shangkun Sun",
            "Shitao Tang",
            "Sameena Tasmeen",
            "Lyne Tchapmi",
            "Wei-Cheng Tseng",
            "Jibin Varghese",
            "Andrew Z. Wang",
            "Hao Wang",
            "Haoxiang Wang",
            "Heng Wang",
            "Ting-Chun Wang",
            "Fangyin Wei",
            "Jiashu Xu",
            "Dinghao Yang",
            "Xiaodong Yang",
            "Haotian Ye",
            "Seonghyeon Ye",
            "Xiaohui Zeng",
            "Jing Zhang",
            "Qinsheng Zhang",
            "Kaiwen Zheng",
            "Andrew Zhu",
            "Yuke Zhu"
        ],
        "title": "World Simulation with Video Foundation Models for Physical AI",
        "abstract": "arXiv:2511.00062v1 Announce Type: new  Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.",
        "arxiv_id": "2511.00062",
        "ARXIVID": "2511.00062",
        "COMMENT": "Matches criteria 2, 3, and 4: introduces Cosmos-Predict2.5, a new generation of video foundation models for world simulation, unifying text, image, and video generation for Physical AI. Provides new benchmarks and open-source models for embodied intelligence, with applications in robotics and simulation.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2511.00389": {
        "authors": [
            "Fan Zhang",
            "Haoxuan Li",
            "Shengju Qian",
            "Xin Wang",
            "Zheng Lian",
            "Hao Wu",
            "Zhihong Zhu",
            "Yuan Gao",
            "Qiankun Li",
            "Yefeng Zheng",
            "Zhouchen Lin",
            "Pheng-Ann Heng"
        ],
        "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond",
        "abstract": "arXiv:2511.00389v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous research fields, including computer vision and affective computing. As a pivotal challenge in this interdisciplinary domain, facial expression recognition (FER) has evolved from separate, domain-specific models to more unified approaches. One promising avenue to unify FER tasks is converting conventional FER datasets into visual question-answering (VQA) formats, enabling the direct application of powerful generalist MLLMs for inference. However, despite the success of cutting-edge MLLMs in various tasks, their performance on FER tasks remains largely unexplored. To address this gap, we provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art MLLMs across four widely used FER datasets. Our results reveal that, while MLLMs exhibit good classification performance, they still face significant limitations in reasoning and interpretability. To this end, we introduce post-training strategies aimed at enhancing the facial expression reasoning capabilities of MLLMs. Specifically, we curate two high-quality and large-scale datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards (RLVR), respectively. Building upon them, we develop a unified and interpretable FER foundation model termed UniFER-7B, which outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).",
        "arxiv_id": "2511.00389",
        "ARXIVID": "2511.00389",
        "COMMENT": "Matches criteria 2 (new MLLMs/VLLMs) and 4 (vision foundation models and applications). Introduces a new FER benchmark for MLLMs, analyzes their limitations, and proposes a new interpretable FER foundation model (UniFER-7B) that outperforms other MLLMs. Also curates new large-scale datasets for post-training. This is a strong fit for your interest in vision-language models, benchmarks, and surprising empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.01618": {
        "authors": [
            "Xiaoyu Zhan",
            "Wenxuan Huang",
            "Hao Sun",
            "Xinyu Fu",
            "Changfeng Ma",
            "Shaosheng Cao",
            "Bohan Jia",
            "Shaohui Lin",
            "Zhenfei Yin",
            "Lei Bai",
            "Wanli Ouyang",
            "Yuanqi Li",
            "Jie Guo",
            "Yanwen Guo"
        ],
        "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models",
        "abstract": "arXiv:2511.01618v1 Announce Type: new  Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.",
        "arxiv_id": "2511.01618",
        "ARXIVID": "2511.01618",
        "COMMENT": "Matches criterion 1 and 2. This paper directly addresses new methodological improvements to spatial reasoning in MLLMs (criterion 1), introducing a new dataset (Viewpoint-100K) and a two-stage fine-tuning strategy to improve spatial reasoning in MLLMs (criterion 2). It also discusses applications to robotics and 3D scene understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.00613": {
        "authors": [
            "Yating Yu",
            "Congqi Cao",
            "Zhaoying Wang",
            "Weihua Meng",
            "Jie Li",
            "Yuxin Li",
            "Zihao Wei",
            "Zhongpei Shen",
            "Jiajun Zhang"
        ],
        "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World",
        "abstract": "arXiv:2511.00613v1 Announce Type: new  Abstract: How far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.",
        "arxiv_id": "2511.00613",
        "ARXIVID": "2511.00613",
        "COMMENT": "Matches criterion 3: introduces CueBench, a new benchmark for context-aware video anomaly understanding, with a unified evaluation framework and tasks for vision-language models. Also proposes a new method (Cue-R1) for reinforcement fine-tuning. Strong focus on embodied AI evaluation and novel benchmarking.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.00916": {
        "authors": [
            "Yan Shu",
            "Chi Liu",
            "Robin Chen",
            "Derek Li",
            "Bryan Dai"
        ],
        "title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs",
        "abstract": "arXiv:2511.00916v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature -- encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI.",
        "arxiv_id": "2511.00916",
        "ARXIVID": "2511.00916",
        "COMMENT": "Matches criterion 2: presents Fleming-VL, a new multimodal large language model (MLLM) for universal medical visual reasoning across heterogeneous modalities, with new benchmarks and unified framework.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.00801": {
        "authors": [
            "Zhihui Chen",
            "Mengling Feng"
        ],
        "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing",
        "abstract": "arXiv:2511.00801v1 Announce Type: new  Abstract: Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [https://github.com/richardChenzhihui/med-banana-50k].",
        "arxiv_id": "2511.00801",
        "ARXIVID": "2511.00801",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multi-modal large language models) and criterion 4: Vision foundation models and its applications. Introduces a large-scale, cross-modality dataset for text-guided medical image editing, leveraging Gemini-2.5-Flash-Image and LLM-as-Judge for quality control, which is highly relevant for multi-modal generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.27623": {
        "authors": [
            "Qiusi Zhan",
            "Hyeonjeong Ha",
            "Rui Yang",
            "Sirui Xu",
            "Hanyang Chen",
            "Liang-Yan Gui",
            "Yu-Xiong Wang",
            "Huan Zhang",
            "Heng Ji",
            "Daniel Kang"
        ],
        "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning",
        "abstract": "arXiv:2510.27623v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.",
        "arxiv_id": "2510.27623",
        "ARXIVID": "2510.27623",
        "COMMENT": "Matches criterion 3: introduces BEAT, a framework for visual backdoor attacks on MLLM-based embodied agents, and proposes a novel training scheme (Contrastive Trigger Learning). Focuses on a novel security angle in embodied AI that is underexplored.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.27363": {
        "authors": [
            "Mengjie Deng",
            "Guanting Dong",
            "Zhicheng Dou"
        ],
        "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use",
        "abstract": "arXiv:2510.27363v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.",
        "arxiv_id": "2510.27363",
        "ARXIVID": "2510.27363",
        "COMMENT": "Matches criterion 2: introduces ToolScope, an agentic framework for vision-guided, long-horizon tool use, leveraging MLLMs and external tools for multimodal reasoning. Focuses on improving MLLM capabilities in embodied agent settings.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.01390": {
        "authors": [
            "Xinyu Mao",
            "Junsi Li",
            "Haoji Zhang",
            "Yu Liang",
            "Ming Sun"
        ],
        "title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment",
        "abstract": "arXiv:2511.01390v1 Announce Type: new  Abstract: Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at https://github.com/Sweet4tars/seps.git.",
        "arxiv_id": "2511.01390",
        "ARXIVID": "2511.01390",
        "COMMENT": "Matches criteria 2 (new MLLMs/VLLMs) and 4 (vision foundation models and applications). Proposes a semantic-enhanced patch slimming framework for fine-grained cross-modal alignment, leveraging MLLMs for improved patch-word correspondence. Shows strong empirical improvements and addresses a core challenge in vision-language alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.00540": {
        "authors": [
            "Wenbing Zhu",
            "Chengjie Wang",
            "Bin-Bin Gao",
            "Jiangning Zhang",
            "Guannan Jiang",
            "Jie Hu",
            "Zhenye Gan",
            "Lidong Wang",
            "Ziqing Zhou",
            "Linjie Cheng",
            "Yurui Pan",
            "Bo Peng",
            "Mingmin Chi",
            "Lizhuang Ma"
        ],
        "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era",
        "abstract": "arXiv:2511.00540v1 Announce Type: new  Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark's substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-/few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.",
        "arxiv_id": "2511.00540",
        "ARXIVID": "2511.00540",
        "COMMENT": "Matches criterion 4: introduces Real-IAD Variety, a large and diverse benchmark for industrial anomaly detection, and demonstrates the robustness of vision-language models (foundation models) in this context. The scale and diversity of the benchmark is a significant contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.00962": {
        "authors": [
            "Dongheng Lin",
            "Mengxue Qu",
            "Kunyang Han",
            "Jianbo Jiao",
            "Xiaojie Jin",
            "Yunchao Wei"
        ],
        "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis",
        "abstract": "arXiv:2511.00962v1 Announce Type: new  Abstract: Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: https://rathgrith.github.io/Unified_Frame_VAA/.",
        "arxiv_id": "2511.00962",
        "ARXIVID": "2511.00962",
        "COMMENT": "Matches criterion 4. This paper proposes a unified reasoning framework for zero-shot video anomaly analysis using foundation models, enabling temporal detection, spatial localization, and textual explanation in a chained, zero-shot manner. It demonstrates the power of vision foundation models for holistic video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.00446": {
        "authors": [
            "Xin Yao",
            "Haiyang Zhao",
            "Yimin Chen",
            "Jiawei Guo",
            "Kecheng Huang",
            "Ming Zhao"
        ],
        "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training",
        "abstract": "arXiv:2511.00446v1 Announce Type: new  Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP's training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via https://github.com/xinyaocse/ToxicTextCLIP/.",
        "arxiv_id": "2511.00446",
        "ARXIVID": "2511.00446",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. ToxicTextCLIP explores text-based poisoning and backdoor attacks on CLIP pre-training, which is a novel and important security angle for vision-language foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00103": {
        "authors": [
            "Rotem Ezra",
            "Hedi Zisling",
            "Nimrod Berman",
            "Ilan Naiman",
            "Alexey Gorkor",
            "Liran Nochumsohn",
            "Eliya Nachmani",
            "Omri Azencot"
        ],
        "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video",
        "abstract": "arXiv:2511.00103v1 Announce Type: new  Abstract: Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: https://azencot-group.github.io/FreeSliders/",
        "arxiv_id": "2511.00103",
        "ARXIVID": "2511.00103",
        "COMMENT": "Related to criterion 4: introduces FreeSliders, a training-free, modality-agnostic method for fine-grained diffusion control across images, audio, and video. Extends concept sliders to multiple modalities and proposes new evaluation metrics. Relevant for generative modeling and vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2511.00510": {
        "authors": [
            "Kai Luo",
            "Hao Shi",
            "Kunyu Peng",
            "Fei Teng",
            "Sheng Wu",
            "Kaiwei Wang",
            "Kailun Yang"
        ],
        "title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback",
        "abstract": "arXiv:2511.00510v1 Announce Type: new  Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack.",
        "arxiv_id": "2511.00510",
        "ARXIVID": "2511.00510",
        "COMMENT": "Matches criteria 3 (embodied AI, new benchmark/simulator, novel methods). Proposes a new method for panoramic multi-object tracking with a feedback-driven framework and introduces the EmboTrack benchmark, including data from quadruped and bipedal robots. The focus on panoramic perception and new datasets is a novel contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00908": {
        "authors": [
            "Heng Zheng",
            "Yuling Shi",
            "Xiaodong Gu",
            "Haochen You",
            "Zijian Zhang",
            "Lubin Gan",
            "Hao Zhang",
            "Wenjun Huang",
            "Jin Huang"
        ],
        "title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks",
        "abstract": "arXiv:2511.00908v1 Announce Type: new  Abstract: Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.",
        "arxiv_id": "2511.00908",
        "ARXIVID": "2511.00908",
        "COMMENT": "Matches criteria 1 (methodological improvements to spatial understanding on embodied agents) and 4 (vision foundation models and applications). Proposes a novel multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization, with a dual-level debate mechanism and cross-level topology refinement. The structured debate for cognitive conflict resolution is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00427": {
        "authors": [
            "Daichi Zhang",
            "Tong Zhang",
            "Jianmin Bao",
            "Shiming Ge",
            "Sabine S\\\"usstrunk"
        ],
        "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection",
        "abstract": "arXiv:2511.00427v1 Announce Type: new  Abstract: With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.",
        "arxiv_id": "2511.00427",
        "ARXIVID": "2511.00427",
        "COMMENT": "Matches criterion 4: proposes a new method for fake image detection leveraging hierarchical image-text misalignment in CLIP's joint space, demonstrating strong generalization and robustness, and is directly related to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.01357": {
        "authors": [
            "Qiangguo Jin",
            "Xianyao Zheng",
            "Hui Cui",
            "Changming Sun",
            "Yuqi Fang",
            "Cong Cong",
            "Ran Su",
            "Leyi Wei",
            "Ping Xuan",
            "Junbo Wang"
        ],
        "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering",
        "abstract": "arXiv:2511.01357v1 Announce Type: new  Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model's capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at https://github.com/BioMedIA-repo/CMI-MTL.",
        "arxiv_id": "2511.01357",
        "ARXIVID": "2511.01357",
        "COMMENT": "Matches criterion 2: introduces CMI-MTL, a new multi-task learning framework for medical visual question answering, with novel cross-modal feature alignment and reasoning modules, advancing MLLM capabilities in the medical domain.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00171": {
        "authors": [
            "Rahul Ghosh",
            "Baishali Chaudhury",
            "Hari Prasanna Das",
            "Meghana Ashok",
            "Ryan Razkenari",
            "Sungmin Hong",
            "Chun-Hao Liu"
        ],
        "title": "CompAgent: An Agentic Framework for Visual Compliance Verification",
        "abstract": "arXiv:2511.00171v1 Announce Type: new  Abstract: Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.",
        "arxiv_id": "2511.00171",
        "ARXIVID": "2511.00171",
        "COMMENT": "Matches criterion 2: proposes CompAgent, an agentic framework for visual compliance verification that augments MLLMs with visual tools and agentic planning, showing improved performance on public benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27383": {
        "authors": [
            "Yueyang Wang",
            "Mehmet Dogar",
            "Gustav Markkula"
        ],
        "title": "Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints",
        "abstract": "arXiv:2510.27383v1 Announce Type: new  Abstract: Modelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.",
        "arxiv_id": "2510.27383",
        "ARXIVID": "2510.27383",
        "COMMENT": "Matches criterion 1: proposes a multi-agent RL framework for modeling pedestrian-driver interactions with explicit human perceptual-motor constraints, improving spatial understanding in embodied agents. Also introduces a novel angle by modeling population-level distributions of constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00114": {
        "authors": [
            "Hanae Elmekki",
            "Amanda Spilkin",
            "Ehsan Zakeri",
            "Antonela Mariel Zanuttini",
            "Ahmed Alagha",
            "Hani Sami",
            "Jamal Bentahar",
            "Lyes Kadem",
            "Wen-Fang Xie",
            "Philippe Pibarot",
            "Rabeb Mizouni",
            "Hadi Otrok",
            "Azzam Mourad",
            "Sami Muhaidat"
        ],
        "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning",
        "abstract": "arXiv:2511.00114v1 Announce Type: new  Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.",
        "arxiv_id": "2511.00114",
        "ARXIVID": "2511.00114",
        "COMMENT": "Matches criterion 3. This paper presents a new end-to-end framework for autonomous ultrasound scanning, integrating generative AI and deep reinforcement learning, and introduces a generative simulator for realistic action-conditioned images. It also releases a public dataset and focuses on reproducibility, which is a novel angle in embodied AI and simulator-based benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2511.00141": {
        "authors": [
            "Janghoon Cho",
            "Jungsoo Lee",
            "Munawar Hayat",
            "Kyuwoong Hwang",
            "Fatih Porikli",
            "Sungha Choi"
        ],
        "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding",
        "abstract": "arXiv:2511.00141v1 Announce Type: new  Abstract: Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.",
        "arxiv_id": "2511.00141",
        "ARXIVID": "2511.00141",
        "COMMENT": "Matches criterion 4. This paper introduces FLoC, a visual token compression framework for long video understanding, which is designed to work with video-LMMs (a type of vision foundation model). The method is training-free and model-agnostic, and is evaluated on large-scale benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00981": {
        "authors": [
            "Suzhong Fu",
            "Rui Sun",
            "Xuan Ding",
            "Jingqi Dong",
            "Yiming Yang",
            "Yao Zhu",
            "Min Chang Jordan Ren",
            "Delin Deng",
            "Angelica Aviles-Rivero",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel",
        "abstract": "arXiv:2511.00981v1 Announce Type: new  Abstract: Accurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.",
        "arxiv_id": "2511.00981",
        "ARXIVID": "2511.00981",
        "COMMENT": "Matches criterion 4. This paper presents VesSAM, a framework that adapts the Segment Anything Model (SAM), a vision foundation model, for vessel segmentation. It introduces a new multi-prompting approach and a benchmark dataset, focusing on improving and applying vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.01419": {
        "authors": [
            "Yongqi Yang",
            "Huayang Huang",
            "Xu Peng",
            "Xiaobin Hu",
            "Donghao Luo",
            "Jiangning Zhang",
            "Chengjie Wang",
            "Yu Wu"
        ],
        "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation",
        "abstract": "arXiv:2511.01419v1 Announce Type: new  Abstract: Recent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model's n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.",
        "arxiv_id": "2511.01419",
        "ARXIVID": "2511.01419",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. Proposes a distillation-based framework for efficient causal video generation, enabling high-quality synthesis with very few denoising steps, which is a novel method for video generation using vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.01200": {
        "authors": [
            "Mengyuan Liu",
            "Sheng Yan",
            "Yong Wang",
            "Yingjie Li",
            "Gui-Bin Bian",
            "Hong Liu"
        ],
        "title": "MoSa: Motion Generation with Scalable Autoregressive Modeling",
        "abstract": "arXiv:2511.01200v1 Announce Type: new  Abstract: We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at https://mosa-web.github.io/MoSa-web",
        "arxiv_id": "2511.01200",
        "ARXIVID": "2511.01200",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. MoSa introduces a hierarchical, scalable autoregressive modeling framework for text-driven 3D human motion generation, improving efficiency and fidelity, and generalizing to motion editing.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.00293": {
        "authors": [
            "Hengjia Li",
            "Jianjin Xu",
            "Keli Cheng",
            "Lei Wang",
            "Ning Bi",
            "Boxi Wu",
            "Fernando De la Torre",
            "Deng Cai"
        ],
        "title": "Multi-View Consistent Human Image Customization via In-Context Learning",
        "abstract": "arXiv:2511.00293v1 Announce Type: new  Abstract: Recent advances in personalized generative models demonstrate impressive results in creating identity-consistent images of the same person under diverse settings. Yet, we note that most methods cannot control the viewpoint of the generated image, nor generate consistent multiple views of the person. To address this problem, we propose a lightweight adaptation method, PersonalView, capable of enabling an existing model to acquire multi-view generation capability with as few as 100 training samples. PersonalView consists of two key components: First, we design a conditioning architecture to take advantage of the in-context learning ability of the pre-trained diffusion transformer. Second, we preserve the original generative ability of the pretrained model with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view consistency, text alignment, identity similarity, and visual quality of PersonalView and compare it to recent baselines with potential capability of multi-view customization. PersonalView significantly outperforms baselines trained on a large corpus of multi-view data with only 100 training samples.",
        "arxiv_id": "2511.00293",
        "ARXIVID": "2511.00293",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. Proposes PersonalView, a lightweight adaptation method for multi-view consistent human image customization using pre-trained diffusion transformers, which is a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.00345": {
        "authors": [
            "Amir Ziashahabi",
            "Narges Ghasemi",
            "Sajjad Shahabi",
            "John Krumm",
            "Salman Avestimehr",
            "Cyrus Shahabi"
        ],
        "title": "OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data",
        "abstract": "arXiv:2511.00345v1 Announce Type: new  Abstract: Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at https://github.com/amir-zsh/OSMGen.",
        "arxiv_id": "2511.00345",
        "ARXIVID": "2511.00345",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. OSMGen is a generative framework for satellite image synthesis using OpenStreetMap data, enabling highly controllable and paired (JSON, image) data generation, which is a novel application of generative vision models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.00362": {
        "authors": [
            "Momen Khandoker Ope",
            "Akif Islam",
            "Mohd Ruhul Ameen",
            "Abu Saleh Musa Miah",
            "Md Rashedul Islam",
            "Jungpil Shin"
        ],
        "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery",
        "abstract": "arXiv:2511.00362v1 Announce Type: new  Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.",
        "arxiv_id": "2511.00362",
        "ARXIVID": "2511.00362",
        "COMMENT": "Related to criterion 4: proposes a generative AI framework for 3D reconstruction from street view imagery, leveraging multimodal visual reasoning and neural image-to-3D generation. Applies vision foundation models for cultural heritage reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.00060": {
        "authors": [
            "Zhiqi Qi",
            "Runxin Zhao",
            "Hanyang Zhuang",
            "Chunxiang Wang",
            "Ming Yang"
        ],
        "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?",
        "abstract": "arXiv:2511.00060v1 Announce Type: new  Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical/solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the \"InfraLiDARs' Benchmark,\" a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR's limited perception range, it's a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the \"InfraLiDARs' Benchmark\" dataset to foster further research.",
        "arxiv_id": "2511.00060",
        "ARXIVID": "2511.00060",
        "COMMENT": "Matches criteria 3 (embodied AI, new benchmark/simulator, novel methods). Introduces a new benchmark (InfraLiDARs' Benchmark) for roadside LiDAR perception in simulation, and provides a comprehensive analysis of LiDAR scanning patterns for infrastructure-based perception. The focus on scanning pattern impact and dataset release is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.26852": {
        "authors": [
            "Lingyue Fu",
            "Xin Ding",
            "Yaoming Zhu",
            "Shao Zhang",
            "Lin Qiu",
            "Weiwen Liu",
            "Weinan Zhang",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Jiaxin Ding",
            "Yong Yu"
        ],
        "title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions",
        "abstract": "arXiv:2510.26852v1 Announce Type: new  Abstract: Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.",
        "arxiv_id": "2510.26852",
        "ARXIVID": "2510.26852",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (CATArena) for evaluating LLM agents' learning ability via iterative tournament competitions, focusing on agent self-improvement and peer-learning, which is a novel evaluation angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.01775": {
        "authors": [
            "Zhen Chen",
            "Qing Xu",
            "Jinlin Wu",
            "Biao Yang",
            "Yuhao Zhai",
            "Geng Guo",
            "Jing Zhang",
            "Yinlu Ding",
            "Nassir Navab",
            "Jiebo Luo"
        ],
        "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment",
        "abstract": "arXiv:2511.01775v1 Announce Type: new  Abstract: Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.",
        "arxiv_id": "2511.01775",
        "ARXIVID": "2511.01775",
        "COMMENT": "Matches criterion 4. This paper introduces SurgVeo, an expert-curated benchmark for evaluating video generation models in surgery, and proposes a new evaluation framework (SPP). It provides insights into the limitations of current video foundation models in specialized domains.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.00260": {
        "authors": [
            "Linzhe Jiang",
            "Jiayuan Huang",
            "Sophia Bano",
            "Matthew J. Clarkson",
            "Zhehua Mao",
            "Mobarak I. Hoque"
        ],
        "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba",
        "abstract": "arXiv:2511.00260v1 Announce Type: new  Abstract: Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.",
        "arxiv_id": "2511.00260",
        "ARXIVID": "2511.00260",
        "COMMENT": "Somewhat related to criterion 1: proposes a new method for 3D point cloud registration in endoscopic navigation, which is a spatial understanding problem for embodied agents. Introduces a new large-scale clinical dataset for benchmarking. However, the focus is on medical imaging rather than general embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.01450": {
        "authors": [
            "Jie Du",
            "Xinyu Gong",
            "Qingshan Tan",
            "Wen Li",
            "Yangming Cheng",
            "Weitao Wang",
            "Chenlu Zhan",
            "Suhui Wu",
            "Hao Zhang",
            "Jun Zhang"
        ],
        "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation",
        "abstract": "arXiv:2511.01450v1 Announce Type: new  Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.",
        "arxiv_id": "2511.01450",
        "ARXIVID": "2511.01450",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. Proposes Reg-DPO, a new method for improving video generation using preference optimization and memory-efficient training, with empirical results on multiple datasets.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.26905": {
        "authors": [
            "Pedro Antonio Alarc\\'on Granadeno",
            "Arturo Miguel Bernal Russell",
            "Sofia Nelson",
            "Demetrius Hernandez",
            "Maureen Petterson",
            "Michael Murphy",
            "Walter J. Scheirer",
            "Jane Cleland-Huang"
        ],
        "title": "Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations",
        "abstract": "arXiv:2510.26905v1 Announce Type: new  Abstract: Cyber-physical systems increasingly rely on Foundational Models such as Large Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy through enhanced perception, inference, and planning. However, these models also introduce new types of errors, such as hallucinations, overgeneralizations, and context misalignments, resulting in incorrect and flawed decisions. To address this, we introduce the concept of Cognition Envelopes, designed to establish reasoning boundaries that constrain AI-generated decisions while complementing the use of meta-cognition and traditional safety envelopes. As with safety envelopes, Cognition Envelopes require practical guidelines and systematic processes for their definition, validation, and assurance.",
        "arxiv_id": "2510.26905",
        "ARXIVID": "2510.26905",
        "COMMENT": "Matches criterion 4: discusses the use of Vision-Language Models (VLMs) in autonomous UAS operations and introduces the concept of Cognition Envelopes to bound AI reasoning, which is a novel safety and assurance framework for vision foundation models in embodied systems.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.01109": {
        "authors": [
            "Alexander Thorley",
            "Agis Chartsias",
            "Jordan Strom",
            "Jeremy Slivnick",
            "Dipak Kotecha",
            "Alberto Gomez",
            "Jinming Duan"
        ],
        "title": "Anatomically Constrained Transformers for Echocardiogram Analysis",
        "abstract": "arXiv:2511.01109v1 Announce Type: new  Abstract: Video transformers have recently demonstrated strong potential for echocardiogram (echo) analysis, leveraging self-supervised pre-training and flexible adaptation across diverse tasks. However, like other models operating on videos, they are prone to learning spurious correlations from non-diagnostic regions such as image backgrounds. To overcome this limitation, we propose the Video Anatomically Constrained Transformer (ViACT), a novel framework that integrates anatomical priors directly into the transformer architecture. ViACT represents a deforming anatomical structure as a point set and encodes both its spatial geometry and corresponding image patches into transformer tokens. During pre-training, ViACT follows a masked autoencoding strategy that masks and reconstructs only anatomical patches, enforcing that representation learning is focused on the anatomical region. The pre-trained model can then be fine-tuned for tasks localized to this region. In this work we focus on the myocardium, demonstrating the framework on echo analysis tasks such as left ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA) detection. The anatomical constraint focuses transformer attention within the myocardium, yielding interpretable attention maps aligned with regions of known CA pathology. Moreover, ViACT generalizes to myocardium point tracking without requiring task-specific components such as correlation volumes used in specialized tracking networks.",
        "arxiv_id": "2511.01109",
        "ARXIVID": "2511.01109",
        "COMMENT": "Somewhat related to criterion 1: proposes ViACT, a transformer architecture for echocardiogram analysis that integrates anatomical priors for improved spatial understanding in medical video. Focus is on medical imaging, not general embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.01517": {
        "authors": [
            "Serkan Ozturk",
            "Samet Hicsonmez",
            "Pinar Duygulu"
        ],
        "title": "NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation",
        "abstract": "arXiv:2511.01517v1 Announce Type: new  Abstract: Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at https://github.com/giddyyupp/NSYNC.",
        "arxiv_id": "2511.01517",
        "ARXIVID": "2511.01517",
        "COMMENT": "Related to generative modeling in computer vision, but does not directly match any of the four criteria. Proposes a novel contrastive training scheme for stylized text-to-image translation using negative synthetic images, which is clever but not about VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2511.00573": {
        "authors": [
            "Wei Feng",
            "Zongyuan Ge"
        ],
        "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective",
        "abstract": "arXiv:2511.00573v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from known categories to cluster unlabeled data that may include both known and unknown categories. While existing methods have achieved impressive results under standard conditions, their performance often deteriorates in the presence of distribution shifts. In this paper, we explore a more realistic task: Domain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled data includes not only unknown categories but also samples from unknown domains. To tackle this challenge, we propose a \\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized Cat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE) that enhances the model's ability to discover categories under distributional shift by leveraging frequency-domain information. Specifically, we first propose a frequency-based domain separation strategy that partitions samples into known and unknown domains by measuring their amplitude differences. We then propose two types of frequency-domain perturbation strategies: a cross-domain strategy, which adapts to new distributions by exchanging amplitude components across domains, and an intra-domain strategy, which enhances robustness to intra-domain variations within the unknown domain. Furthermore, we extend the self-supervised contrastive objective and semantic clustering loss to better guide the training process. Finally, we introduce a clustering-difficulty-aware resampling technique to adaptively focus on harder-to-cluster categories, further enhancing model performance. Extensive experiments demonstrate that our method effectively mitigates the impact of distributional shifts across various benchmark datasets and achieves superior performance in discovering both known and unknown categories.",
        "arxiv_id": "2511.00573",
        "ARXIVID": "2511.00573",
        "COMMENT": "Somewhat related to criteria 4 (vision foundation models and applications) as it proposes a frequency-domain approach for category discovery under domain shift, but does not directly address vision-language models or embodied AI. The frequency-based domain separation and perturbation strategies are interesting, but not a direct match to the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.27410": {
        "authors": [
            "Jianwen Sun",
            "Yukang Feng",
            "Yifan Chang",
            "Chuanhao Li",
            "Zizhen Li",
            "Jiaxin Ai",
            "Fanrui Zhang",
            "Yu Dai",
            "Kaipeng Zhang"
        ],
        "title": "Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry",
        "abstract": "arXiv:2510.27410v1 Announce Type: new  Abstract: A fundamental bottleneck in human-AI collaboration is the \"intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.",
        "arxiv_id": "2510.27410",
        "ARXIVID": "2510.27410",
        "COMMENT": "Does not match any specific criterion. Focuses on dialogue systems and human-AI collaboration, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.26887": {
        "authors": [
            "Francisco Villaescusa-Navarro",
            "Boris Bolliet",
            "Pablo Villanueva-Domingo",
            "Adrian E. Bayer",
            "Aidan Acquah",
            "Chetana Amancharla",
            "Almog Barzilay-Siegal",
            "Pablo Bermejo",
            "Camille Bilodeau",
            "Pablo C\\'ardenas Ram\\'irez",
            "Miles Cranmer",
            "Urbano L. Fran\\c{c}a",
            "ChangHoon Hahn",
            "Yan-Fei Jiang",
            "Raul Jimenez",
            "Jun-Young Lee",
            "Antonio Lerario",
            "Osman Mamun",
            "Thomas Meier",
            "Anupam A. Ojha",
            "Pavlos Protopapas",
            "Shimanto Roy",
            "David N. Spergel",
            "Pedro Taranc\\'on-\\'Alvarez",
            "Ujjwal Tiwari",
            "Matteo Viel",
            "Digvijay Wadekar",
            "Chi Wang",
            "Bonny Y. Wang",
            "Licong Xu",
            "Yossi Yovel",
            "Shuwen Yue",
            "Wen-Han Zhou",
            "Qiyao Zhu",
            "Jiajun Zou",
            "\\'I\\~nigo Zubeldia"
        ],
        "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
        "abstract": "arXiv:2510.26887v1 Announce Type: new  Abstract: We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
        "arxiv_id": "2510.26887",
        "ARXIVID": "2510.26887",
        "COMMENT": "Does not match any specific criterion. While it discusses multi-agent AI systems for scientific discovery, it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.01175": {
        "authors": [
            "Peng Du",
            "Hui Li",
            "Han Xu",
            "Paul Barom Jeon",
            "Dongwook Lee",
            "Daehyun Ji",
            "Ran Yang",
            "Feng Zhu"
        ],
        "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution",
        "abstract": "arXiv:2511.01175v1 Announce Type: new  Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.",
        "arxiv_id": "2511.01175",
        "ARXIVID": "2511.01175",
        "COMMENT": "Related to generative modeling and computer vision, but does not directly match any of the four criteria. Proposes a diffusion transformer with wavelet spectrum for single image super-resolution, which is a technical advance but not in the area of VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.01724": {
        "authors": [
            "Yi Zhang",
            "Zheng Wang",
            "Chen Zhen",
            "Wenjie Ruan",
            "Qing Guo",
            "Siddartha Khastgir",
            "Carsten Maple",
            "Xingyu Zhao"
        ],
        "title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark",
        "abstract": "arXiv:2511.01724v1 Announce Type: new  Abstract: Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at https://tmpspace.github.io/PRBenchLeaderboard/.",
        "arxiv_id": "2511.01724",
        "ARXIVID": "2511.01724",
        "COMMENT": "This paper introduces PRBench, a benchmark for probabilistic robustness in deep learning. While it is a new benchmark, it is not focused on embodied AI, spatial intelligence, VLLMs/MLLMs, or vision foundation models. It is more about robustness evaluation in general deep learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.00429": {
        "authors": [
            "Daichi Zhang",
            "Tong Zhang",
            "Shiming Ge",
            "Sabine S\\\"usstrunk"
        ],
        "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection",
        "abstract": "arXiv:2511.00429v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.",
        "arxiv_id": "2511.00429",
        "ARXIVID": "2511.00429",
        "COMMENT": "This paper proposes a frequency-based method for detecting diffusion-generated images. While it is relevant to generative modeling and computer vision, it does not match the specific criteria (spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27419": {
        "authors": [
            "Tian Liang",
            "Wenxiang Jiao",
            "Zhiwei He",
            "Jiahao Xu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains",
        "abstract": "arXiv:2510.27419v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.",
        "arxiv_id": "2510.27419",
        "ARXIVID": "2510.27419",
        "COMMENT": "Does not directly match any specific criterion. Focuses on reasoning efficiency in large reasoning models, which is more NLP-centric and not directly about spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00231": {
        "authors": [
            "Fuming Yang",
            "Yicong Li",
            "Hanspeter Pfister",
            "Jeff W. Lichtman",
            "Yaron Meirovitch"
        ],
        "title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior",
        "abstract": "arXiv:2511.00231v1 Announce Type: new  Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and downstream analysis toward their current limits. We present a vector-quantized variational autoencoder-based (VQ-VAE) compression framework for EM that spans 16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme compression, with an optional Transformer prior that predicts bottom tokens (without changing the compression ratio) to restore texture via feature-wise linear modulation (FiLM) and concatenation; we further introduce an ROI-driven workflow that performs selective high-resolution reconstruction from 1024x-compressed latents only where needed.",
        "arxiv_id": "2511.00231",
        "ARXIVID": "2511.00231",
        "COMMENT": "Related to generative modeling and computer vision, but does not match any of the four criteria directly. Focuses on VQ-VAE and Transformer priors for electron microscopy image compression, which is an application in a specific domain.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01730": {
        "authors": [
            "Yefeng Wu",
            "Yucheng Song",
            "Ling Wu",
            "Shan Wan",
            "Yecheng Zhao"
        ],
        "title": "CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays",
        "abstract": "arXiv:2511.01730v1 Announce Type: new  Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide, necessitating accurate and efficient automated detection systems. While recent transformer-based detectors like RT-DETR have shown promise in object detection tasks, their application to medical imaging, particularly pneumonia detection in chest X-rays, remains underexplored. This paper presents CGF-DETR, an enhanced real-time detection transformer specifically designed for pneumonia detection. We introduce XFABlock in the backbone to improve multi-scale feature extraction through convolutional attention mechanisms integrated with CSP architecture. To achieve efficient feature aggregation, we propose SPGA module that replaces standard multi-head attention with dynamic gating mechanisms and single-head self-attention. Additionally, GCFC3 is designed for the neck to enhance feature representation through multi-path convolution fusion while maintaining real-time performance via structural re-parameterization. Extensive experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR achieves 82.2\\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\\% while maintaining comparable inference speed at 48.1 FPS. Our ablation studies confirm that each proposed module contributes meaningfully to the overall performance improvement, with the complete model achieving 50.4\\% mAP@[0.5:0.95]",
        "arxiv_id": "2511.01730",
        "ARXIVID": "2511.01730",
        "COMMENT": "Does not directly match any specific criterion. Presents a new transformer-based model for pneumonia detection in X-rays, which is an application of vision models but not a foundation model, VLLM/MLLM, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01274": {
        "authors": [
            "Tan Tang",
            "Yanhong Wu",
            "Junming Gao",
            "Yingcai Wu"
        ],
        "title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers",
        "abstract": "arXiv:2511.01274v1 Announce Type: new  Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.",
        "arxiv_id": "2511.01274",
        "ARXIVID": "2511.01274",
        "COMMENT": "Does not directly match any specific criterion. Focuses on restoration of ancient paintings using color transformers, which is an application of generative modeling but not in spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00143": {
        "authors": [
            "Jinsu Kim",
            "Yunhun Nam",
            "Minseon Kim",
            "Sangpil Kim",
            "Jongheon Jeong"
        ],
        "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing",
        "abstract": "arXiv:2511.00143v1 Announce Type: new  Abstract: Recent advances in text-to-image models have increased the exposure of powerful image editing techniques as a tool, raising concerns about their potential for malicious use. An emerging line of research to address such threats focuses on implanting \"protective\" adversarial noise into images before their public release, so future attempts to edit them using text-to-image models can be impeded. However, subsequent works have shown that these adversarial noises are often easily \"reversed,\" e.g., with techniques as simple as JPEG compression, casting doubt on the practicality of the approach. In this paper, we argue that adversarial noise for image protection should not only be imperceptible, as has been a primary focus of prior work, but also irreversible, viz., it should be difficult to detect as noise provided that the original image is hidden. We propose a surprisingly simple method to enhance the robustness of image protection methods against noise reversal techniques. Specifically, it applies an adaptive per-region Gaussian blur on the noise to adjust the overall frequency spectrum. Through extensive experiments, we show that our method consistently improves the per-sample worst-case protection performance of existing methods against a wide range of reversal techniques on diverse image editing scenarios, while also reducing quality degradation due to noise in terms of perceptual metrics. Code is available at https://github.com/jsu-kim/BlurGuard.",
        "arxiv_id": "2511.00143",
        "ARXIVID": "2511.00143",
        "COMMENT": "This paper proposes a method for robustifying image protection against AI-powered editing. While it is relevant to computer vision and generative models, it does not match the specific criteria (spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00211": {
        "authors": [
            "Wenxuan Zhang",
            "Peng Hu"
        ],
        "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals",
        "abstract": "arXiv:2511.00211v1 Announce Type: new  Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet's essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.",
        "arxiv_id": "2511.00211",
        "ARXIVID": "2511.00211",
        "COMMENT": "Does not directly match any specific criterion. Presents a transfer learning method for weather condition detection on ground terminals, which is an application of computer vision but not a new vision foundation model or embodied AI benchmark.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.27051": {
        "authors": [
            "Aaditya Shukla",
            "Sidney Knowles",
            "Meenakshi Madugula",
            "Dave Farris",
            "Ryan Angilly",
            "Santiago Pombo",
            "Anbang Xu",
            "Lu An",
            "Abhinav Balasubramanian",
            "Tan Yu",
            "Jiaxiang Ren",
            "Rama Akkiraju"
        ],
        "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
        "abstract": "arXiv:2510.27051v1 Announce Type: new  Abstract: Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\\%) and query rephrasal errors (3.2\\%). Using NVIDIA NeMo microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\\% accuracy, a 10x reduction in model size, and 70\\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\\% gain in accuracy and a 40\\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.",
        "arxiv_id": "2510.27051",
        "ARXIVID": "2510.27051",
        "COMMENT": "This paper describes a practical closed-loop system for enterprise AI agents using a MAPE-driven data flywheel and human-in-the-loop feedback. While it discusses agent improvement and adaptation, it does not focus on spatial understanding, embodied agents, VLLMs/MLLMs, or vision foundation models. It is more about enterprise agent robustness and feedback loops.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}