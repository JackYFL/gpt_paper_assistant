{
    "2602.01756": {
        "authors": [
            "Jun He",
            "Junyan Ye",
            "Zilong Huang",
            "Dongzhi Jiang",
            "Chenjue Zhang",
            "Leqi Zhu",
            "Renrui Zhang",
            "Xiang Zhang",
            "Weijia Li"
        ],
        "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
        "abstract": "arXiv:2602.01756v1 Announce Type: new  Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.",
        "arxiv_id": "2602.01756",
        "ARXIVID": "2602.01756",
        "COMMENT": "Matches criterion 2 (new VLLM/MLLM) and criterion 4 (vision foundation models and applications). Proposes a unified agentic framework for image generation with cognitive search and reasoning, and introduces a new benchmark for evaluating these capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2602.01780": {
        "authors": [
            "Shicheng Yin",
            "Kaixuan Yin",
            "Weixing Chen",
            "Yang Liu",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
        "abstract": "arXiv:2602.01780v1 Announce Type: new  Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.",
        "arxiv_id": "2602.01780",
        "ARXIVID": "2602.01780",
        "COMMENT": "Matches criterion 3: proposes a new efficient world model (DDP-WM) for embodied AI, with a novel disentangled dynamics prediction approach and strong empirical results in robotics tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.00463": {
        "authors": [
            "Xin Zhang",
            "Shen Chen",
            "Jiale Zhou",
            "Lei Li"
        ],
        "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting",
        "abstract": "arXiv:2602.00463v1 Announce Type: new  Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.",
        "arxiv_id": "2602.00463",
        "ARXIVID": "2602.00463",
        "COMMENT": "Matches criterion 2 and 4: introduces a text-driven 3D scene generation framework using MLLMs and Gaussian Splatting, with a novel layout reasoning layer for spatial understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.01418": {
        "authors": [
            "Christoffer Koo {\\O}hrstr{\\o}m",
            "Rafael I. Cabral Muchacho",
            "Yifei Dong",
            "Filippos Moumtzidellis",
            "Ronja G\\\"uldenring",
            "Florian T. Pokorny",
            "Lazaros Nalpantidis"
        ],
        "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
        "abstract": "arXiv:2602.01418v1 Announce Type: new  Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.",
        "arxiv_id": "2602.01418",
        "ARXIVID": "2602.01418",
        "COMMENT": "Matches criterion 4: introduces a new, principled position encoding (PaPE) for vision foundation models, with strong empirical results across multiple modalities.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.02437": {
        "authors": [
            "Dianyi Wang",
            "Chaofan Ma",
            "Feng Han",
            "Size Wu",
            "Wei Song",
            "Yibin Wang",
            "Zhixiong Zhang",
            "Tianhang Wang",
            "Siyuan Wang",
            "Zhongyu Wei",
            "Jiaqi Wang"
        ],
        "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
        "abstract": "arXiv:2602.02437v1 Announce Type: new  Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
        "arxiv_id": "2602.02437",
        "ARXIVID": "2602.02437",
        "COMMENT": "Matches criterion 2 (new unified multimodal reasoning framework, likely a new MLLM/VLLM) and criterion 4 (vision foundation models and their application to reasoning-centric image generation/editing). Also introduces a new large-scale reasoning dataset.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.00971": {
        "authors": [
            "Meng Luo",
            "Bobo Li",
            "Shanqing Xu",
            "Shize Zhang",
            "Qiuchan Chen",
            "Menglu Han",
            "Wenhao Chen",
            "Yanxiang Huang",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
        "abstract": "arXiv:2602.00971v1 Announce Type: new  Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.",
        "arxiv_id": "2602.00971",
        "ARXIVID": "2602.00971",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for multi-modal emotion reasoning with ToM). Introduces a new benchmark and method for theory-of-mind-guided multi-modal emotion reasoning, directly relevant to vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.01844": {
        "authors": [
            "Yuliang Zhan",
            "Jian Li",
            "Wenbing Huang",
            "Wenbing Huang",
            "Yang Liu",
            "Hao Sun"
        ],
        "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
        "abstract": "arXiv:2602.01844v1 Announce Type: new  Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.",
        "arxiv_id": "2602.01844",
        "ARXIVID": "2602.01844",
        "COMMENT": "Matches criterion 1 (new methodological improvement for spatial understanding in embodied agents) by proposing a visual-only, unsupervised method for learning cloth dynamics from multi-view observations, which is highly relevant for spatial intelligence in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.00574": {
        "authors": [
            "Yifei Shao",
            "Kun Zhou",
            "Ziming Xu",
            "Mohammad Atif Quamar",
            "Shibo Hao",
            "Zhen Wang",
            "Zhiting Hu",
            "Biwei Huang"
        ],
        "title": "Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings",
        "abstract": "arXiv:2602.00574v1 Announce Type: new  Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.",
        "arxiv_id": "2602.00574",
        "ARXIVID": "2602.00574",
        "COMMENT": "This paper proposes modal-mixed chain-of-thought reasoning with latent embeddings, extending CoT to multimodal reasoning and using VLMs as encoders. It is a strong match for criterion 2 (new VLLMs/MLLMs) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.00904": {
        "authors": [
            "Kunal Mahatha",
            "Ali Bahri",
            "Pierre Marza",
            "Sahar Dastani",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis",
            "Jose Dolz",
            "Christian Desrosiers"
        ],
        "title": "OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection",
        "abstract": "arXiv:2602.00904v1 Announce Type: new  Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.",
        "arxiv_id": "2602.00904",
        "ARXIVID": "2602.00904",
        "COMMENT": "This paper introduces OCTOPUS, a novel vision state space model architecture that enhances spatial awareness by multi-directional recurrence, directly addressing spatial understanding in vision models. This is a strong match for criterion 1 (new methodological improvements to spatial understanding on embodied agents) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.00414": {
        "authors": [
            "Trishna Chakraborty",
            "Udita Ghosh",
            "Aldair Ernesto Gongora",
            "Ruben Glatt",
            "Yue Dong",
            "Jiachen Li",
            "Amit K. Roy-Chowdhury",
            "Chengyu Song"
        ],
        "title": "Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure",
        "abstract": "arXiv:2602.00414v1 Announce Type: new  Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.",
        "arxiv_id": "2602.00414",
        "ARXIVID": "2602.00414",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models and applications). Proposes a new benchmark and method for VLMs in laboratory safety monitoring, with a novel scene-graph-guided alignment approach for hazard detection.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.00249": {
        "authors": [
            "Rishav Pramanik",
            "Ian E. Nielsen",
            "Jeff Smith",
            "Saurav Pandit",
            "Ravi P. Ramachandran",
            "Zhaozheng Yin"
        ],
        "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis",
        "abstract": "arXiv:2602.00249v1 Announce Type: new  Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.",
        "arxiv_id": "2602.00249",
        "ARXIVID": "2602.00249",
        "COMMENT": "This paper presents SANEval, a new open-vocabulary compositional benchmark for text-to-image models, with a focus on spatial, attribute, and numeracy evaluation. It is a strong match for criterion 3 (embodied AI papers on building new benchmarks, especially with a novel angle on compositionality and spatial relations).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.00110": {
        "authors": [
            "Yu Li",
            "Guilherme N. DeSouza",
            "Praveen Rao",
            "Chi-Ren Shyu"
        ],
        "title": "Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer",
        "abstract": "arXiv:2602.00110v1 Announce Type: new  Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.",
        "arxiv_id": "2602.00110",
        "ARXIVID": "2602.00110",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a geospatially guided visual transformer for remote sensing imagery, outperforming existing geospatial foundation models in disease prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.00190": {
        "authors": [
            "Mohit Jiwatode",
            "Alexander Dockhorn",
            "Bodo Rosenhahn"
        ],
        "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models",
        "abstract": "arXiv:2602.00190v1 Announce Type: new  Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.",
        "arxiv_id": "2602.00190",
        "ARXIVID": "2602.00190",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for understanding game mechanics from gameplay traces, with a novel causal induction approach using LLMs). Also relevant to multi-modal learning and interpretable agents.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2602.00810": {
        "authors": [
            "Ze Huang",
            "Zhongyang Xiao",
            "Mingliang Song",
            "Longan Yang",
            "Hongyuan Yuan",
            "Li Sun"
        ],
        "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
        "abstract": "arXiv:2602.00810v1 Announce Type: new  Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.",
        "arxiv_id": "2602.00810",
        "ARXIVID": "2602.00810",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents) and criterion 4 (vision foundation models and applications). Proposes a unified neural network pipeline for vehicle localization using multi-camera input, with confidence estimation and efficient training. Strong spatial intelligence and vision model application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.02341": {
        "authors": [
            "Zhenpeng Huang",
            "Jiaqi Li",
            "Zihan Jia",
            "Xinhao Li",
            "Desen Meng",
            "Lingxue Song",
            "Xi Chen",
            "Liang Li",
            "Limin Wang"
        ],
        "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
        "abstract": "arXiv:2602.02341v1 Announce Type: new  Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.",
        "arxiv_id": "2602.02341",
        "ARXIVID": "2602.02341",
        "COMMENT": "Matches criterion 2 and 4: introduces LongVPO, a new preference optimization framework for long-form video understanding using vision-language models, with a novel synthetic data generation and reasoning pipeline.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.01127": {
        "authors": [
            "Matej Suchanek",
            "Klara Janouskova",
            "Ondrej Vasatko",
            "Jiri Matas"
        ],
        "title": "Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis",
        "abstract": "arXiv:2602.01127v1 Announce Type: new  Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.   Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.",
        "arxiv_id": "2602.01127",
        "ARXIVID": "2602.01127",
        "COMMENT": "Matches criterion 4: presents a new closed-form adaptation method for vision-language models (CLIP) using Fukunaga-Koontz Linear Discriminant Analysis, improving class separability and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.00485": {
        "authors": [
            "Shule Lu",
            "Yujing Wang",
            "Hainan Zhang",
            "Xiaoshan Yang",
            "Hongwei Zheng",
            "Yongxin Tong",
            "Changsheng Xu",
            "Zhiming Zheng"
        ],
        "title": "Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models",
        "abstract": "arXiv:2602.00485v1 Announce Type: new  Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.",
        "arxiv_id": "2602.00485",
        "ARXIVID": "2602.00485",
        "COMMENT": "Matches criterion 2 and 4: proposes a federated alignment framework for heterogeneous vision-language models (VLMs), using a novel preference-based approach for privacy-preserving training.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.00211": {
        "authors": [
            "Zafar Iqbal",
            "Anwar Ul Haq",
            "Srimannarayana Grandhi"
        ],
        "title": "Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning",
        "abstract": "arXiv:2602.00211v1 Announce Type: new  Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.",
        "arxiv_id": "2602.00211",
        "ARXIVID": "2602.00211",
        "COMMENT": "Matches criterion 1: introduces a new interpretable, unsupervised deformable image registration method with a multi-hop visual reasoning framework for spatial understanding in medical images.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.00570": {
        "authors": [
            "Xingyu Luo",
            "Yidong Cai",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Limin Wang"
        ],
        "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates",
        "abstract": "arXiv:2602.00570v1 Announce Type: new  Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD",
        "arxiv_id": "2602.00570",
        "ARXIVID": "2602.00570",
        "COMMENT": "Matches criterion 2 (vision-language tracking with generative multi-modal fusion using diffusion models) and criterion 4 (applies generative models to vision-language tracking).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.01610": {
        "authors": [
            "Zitao Guo",
            "Changyang Jiang",
            "Tianhong Zhao",
            "Jinzhou Cao",
            "Genan Dai",
            "Bowen Zhang"
        ],
        "title": "ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning",
        "abstract": "arXiv:2602.01610v1 Announce Type: new  Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.",
        "arxiv_id": "2602.01610",
        "ARXIVID": "2602.01610",
        "COMMENT": "Matches criterion 1 (spatial understanding on urban region representation) and criterion 2 (uses a frozen MLLM for task-aware prompting). Introduces spatial priors into prompt-based region embedding and aligns with task semantics using MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.01666": {
        "authors": [
            "Yan Wang",
            "Partho Hassan",
            "Samiha Sadeka",
            "Nada Soliman",
            "M M Sayeef Abdullah",
            "Sabit Hassan"
        ],
        "title": "Moonworks Lunara Aesthetic II: An Image Variation Dataset",
        "abstract": "arXiv:2602.01666v1 Announce Type: new  Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.",
        "arxiv_id": "2602.01666",
        "ARXIVID": "2602.01666",
        "COMMENT": "Matches criterion 3 (new benchmark for image generation/editing systems with contextual variation and identity preservation). Also relevant to criterion 4 (vision foundation models and applications) as it supports benchmarking and fine-tuning of generative models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.02156": {
        "authors": [
            "Wen-Jie Shu",
            "Xuerui Qiu",
            "Rui-Jie Zhu",
            "Harold Haodong Chen",
            "Yexin Liu",
            "Harry Yang"
        ],
        "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
        "abstract": "arXiv:2602.02156v1 Announce Type: new  Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
        "arxiv_id": "2602.02156",
        "ARXIVID": "2602.02156",
        "COMMENT": "This paper introduces LoopViT, a recursive transformer architecture for visual reasoning, decoupling reasoning depth from model capacity and introducing a dynamic exit mechanism. It is relevant to vision foundation models (criterion 4) and introduces a novel architectural approach for visual reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2602.01649": {
        "authors": [
            "Yinchao Ma",
            "Qiang Zhou",
            "Zhibin Wang",
            "Xianing Chen",
            "Hanqing Yang",
            "Jun Song",
            "Bo Zheng"
        ],
        "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning",
        "abstract": "arXiv:2602.01649v1 Announce Type: new  Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \\textbf{C}ontribution-\\textbf{a}ware token \\textbf{Co}mpression algorithm for \\textbf{VID}eo understanding (\\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.",
        "arxiv_id": "2602.01649",
        "ARXIVID": "2602.01649",
        "COMMENT": "This paper proposes a reinforcement learning-based token compression method for efficient video understanding in video large language models. It is a methodological improvement for spatial intelligence in video models and uses RL for token selection, which is a clever statistical trick. Closely matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and is also relevant to criterion 2 (VLLMs/MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.01034": {
        "authors": [
            "Xiangwei Wang",
            "Wei Wang",
            "Ken Chen",
            "Nanduni Nimalsiri",
            "Saman Halgamuge"
        ],
        "title": "Discovering Process-Outcome Credit in Multi-Step LLM Reasoning",
        "abstract": "arXiv:2602.01034v1 Announce Type: new  Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.",
        "arxiv_id": "2602.01034",
        "ARXIVID": "2602.01034",
        "COMMENT": "Matches criterion 2 (shows new methods for multi-modal LLMs, with a focus on reasoning and credit assignment in multi-step LLM reasoning, including multi-modal benchmarks).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.02222": {
        "authors": [
            "Ruiqi Liu",
            "Manni Cui",
            "Ziheng Qin",
            "Zhiyuan Yan",
            "Ruoxin Chen",
            "Yi Han",
            "Zhiheng Li",
            "Junkai Chen",
            "ZhiJin Chen",
            "Kaiqing Lin",
            "Jialiang Shen",
            "Lubin Weng",
            "Jing Dong",
            "Yan Wang",
            "Shu Wu"
        ],
        "title": "MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection",
        "abstract": "arXiv:2602.02222v1 Announce Type: new  Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the \"superhuman crossover\" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR",
        "arxiv_id": "2602.02222",
        "ARXIVID": "2602.02222",
        "COMMENT": "This paper introduces MIRROR, a manifold-based reference reconstruction method for detecting AI-generated images, and proposes a new human-imperceptible benchmark. It is relevant to vision foundation models and their applications (criterion 4), especially in the context of generative model forensics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.01077": {
        "authors": [
            "Haopeng Li",
            "Shitong Shao",
            "Wenliang Zhong",
            "Zikai Zhou",
            "Lichen Bai",
            "Hui Xiong",
            "Zeke Xie"
        ],
        "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
        "abstract": "arXiv:2602.01077v1 Announce Type: new  Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.",
        "arxiv_id": "2602.01077",
        "ARXIVID": "2602.01077",
        "COMMENT": "This paper introduces PISA, a piecewise sparse attention mechanism for efficient diffusion transformers, improving speed and quality trade-offs in generative vision models. It is relevant to vision foundation models (criterion 4) and generative modeling, with a novel attention mechanism.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.01345": {
        "authors": [
            "Yu Zhang",
            "Jingyi Liu",
            "Feng Liu",
            "Duoqian Miao",
            "Qi Zhang",
            "Kexue Fu",
            "Changwei Wang",
            "Longbing Cao"
        ],
        "title": "Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis",
        "abstract": "arXiv:2602.01345v1 Announce Type: new  Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.",
        "arxiv_id": "2602.01345",
        "ARXIVID": "2602.01345",
        "COMMENT": "Somewhat related to criterion 4: proposes a new acceleration framework for visual autoregressive models using entropy analysis, which is a clever statistical trick for vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.00350": {
        "authors": [
            "Ignacy Kolton",
            "Kacper Marzol",
            "Pawe{\\l} Batorski",
            "Marcin Mazur",
            "Paul Swoboda",
            "Przemys{\\l}aw Spurek"
        ],
        "title": "ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models",
        "abstract": "arXiv:2602.00350v1 Announce Type: new  Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe",
        "arxiv_id": "2602.00350",
        "ARXIVID": "2602.00350",
        "COMMENT": "This paper introduces a reinforcement learning-based adversarial prompt search for erased concepts in unlearned diffusion models. It is a new methodological improvement for red-teaming diffusion models, but does not directly address spatial understanding or embodied agents (criterion 1), nor is it a new VLLM/MLLM (criterion 2), nor a new benchmark for embodied AI (criterion 3), nor a vision foundation model application (criterion 4). However, it is relevant to generative modeling and adversarial evaluation.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2602.02419": {
        "authors": [
            "Qingni Wang",
            "Yue Fan",
            "Xin Eric Wang"
        ],
        "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
        "abstract": "arXiv:2602.02419v1 Announce Type: new  Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
        "arxiv_id": "2602.02419",
        "ARXIVID": "2602.02419",
        "COMMENT": "Somewhat related to spatial intelligence (criterion 1) as it deals with spatial grounding in GUIs and introduces uncertainty calibration for spatial predictions, but not embodied agents or spatial reasoning in physical space.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.01864": {
        "authors": [
            "Yuan Wang",
            "Yuhao Wan",
            "Siming Zheng",
            "Bo Li",
            "Qibin Hou",
            "Peng-Tao Jiang"
        ],
        "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling",
        "abstract": "arXiv:2602.01864v1 Announce Type: new  Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a \"Trust but Verify\" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.",
        "arxiv_id": "2602.01864",
        "ARXIVID": "2602.01864",
        "COMMENT": "This paper proposes Ada-RefSR, an adaptive reference-based diffusion super-resolution method with implicit correlation modeling. While it is a novel generative modeling method, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models in the sense of general-purpose models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.00579": {
        "authors": [
            "JiaKui Hu",
            "Zhengjian Yao",
            "Lujia Jin",
            "Yanye Lu"
        ],
        "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration",
        "abstract": "arXiv:2602.00579v1 Announce Type: new  Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.",
        "arxiv_id": "2602.00579",
        "ARXIVID": "2602.00579",
        "COMMENT": "Does not match any specific criterion. Focuses on universal image restoration with a new degradation discrimination and generation approach, but not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.01012": {
        "authors": [
            "Yiyang Su",
            "Minchul Kim",
            "Jie Zhu",
            "Christopher Perry",
            "Feng Liu",
            "Anil Jain",
            "Xiaoming Liu"
        ],
        "title": "LocalScore: Local Density-Aware Similarity Scoring for Biometrics",
        "abstract": "arXiv:2602.01012v1 Announce Type: new  Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.",
        "arxiv_id": "2602.01012",
        "ARXIVID": "2602.01012",
        "COMMENT": "Does not match any specific criterion. Proposes a new scoring method for biometrics, but not related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.00618": {
        "authors": [
            "Yian Zhao",
            "Rushi Ye",
            "Ruochong Zheng",
            "Zesen Cheng",
            "Chaoran Feng",
            "Jiashu Yang",
            "Pengchong Qiao",
            "Chang Liu",
            "Jie Chen"
        ],
        "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting",
        "abstract": "arXiv:2602.00618v1 Announce Type: new  Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \\textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.",
        "arxiv_id": "2602.00618",
        "ARXIVID": "2602.00618",
        "COMMENT": "Related to 3D style transfer and generative modeling, but not directly to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.02334": {
        "authors": [
            "Fatemeh Zargarbashi",
            "Dhruv Agrawal",
            "Jakob Buhmann",
            "Martin Guay",
            "Stelian Coros",
            "Robert W. Sumner"
        ],
        "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
        "abstract": "arXiv:2602.02334v1 Announce Type: new  Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.",
        "arxiv_id": "2602.02334",
        "ARXIVID": "2602.02334",
        "COMMENT": "Related to generative modeling and disentanglement in motion, but does not directly match any of the four criteria (no spatial intelligence on embodied agents, no VLLM/MLLM, no new embodied AI benchmark, not vision foundation model focused).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.01207": {
        "authors": [
            "Hui Wu",
            "Hengyi Cai",
            "Jinman Zhao",
            "Xinran Chen",
            "Ziheng Li",
            "Zhejun Zhao",
            "Shuaiqiang Wang",
            "Yuchen Li",
            "Dawei Yin"
        ],
        "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
        "abstract": "arXiv:2602.01207v1 Announce Type: new  Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.",
        "arxiv_id": "2602.01207",
        "ARXIVID": "2602.01207",
        "COMMENT": "This paper proposes a stability-aware, gradient-efficient alignment method for reasoning models, focusing on preference-based alignment. While it introduces a clever optimization trick, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.00585": {
        "authors": [
            "Guochen Yan",
            "Jialong Wu",
            "Zhengwei Tao",
            "Bo Li",
            "Qintong Zhang",
            "Jiahao Xu",
            "Haitao Mi",
            "Yuejian Fang",
            "Qingni Shen",
            "Wentao Zhang",
            "Zhonghai Wu"
        ],
        "title": "Exploring Information Seeking Agent Consolidation",
        "abstract": "arXiv:2602.00585v1 Announce Type: new  Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.",
        "arxiv_id": "2602.00585",
        "ARXIVID": "2602.00585",
        "COMMENT": "Does not match any specific criterion. Focuses on consolidation of information-seeking agents, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.00710": {
        "authors": [
            "Yueqi Zhang",
            "Jin Hu",
            "Shaoxiong Feng",
            "Peiwen Yuan",
            "Xinglin Wang",
            "Yiwei Li",
            "Jiayi Shi",
            "Chuyi Tan",
            "Ji Zhang",
            "Boyuan Pan",
            "Yao Hu",
            "Kan Li"
        ],
        "title": "Learning More from Less: Unlocking Internal Representations for Benchmark Compression",
        "abstract": "arXiv:2602.00710v1 Announce Type: new  Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.",
        "arxiv_id": "2602.00710",
        "ARXIVID": "2602.00710",
        "COMMENT": "Related to benchmarking and model evaluation, but not specifically about embodied AI benchmarks or simulators (criterion 3), nor about vision foundation models or spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.02223": {
        "authors": [
            "Junchi Feng",
            "Nikhil Ballem",
            "Mahya Beheshti",
            "Giles Hamilton-Fletcher",
            "Todd Hudson",
            "Maurizio Porfiri",
            "William H. Seiple",
            "John-Ross Rizzo"
        ],
        "title": "Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type",
        "abstract": "arXiv:2602.02223v1 Announce Type: new  Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.",
        "arxiv_id": "2602.02223",
        "ARXIVID": "2602.02223",
        "COMMENT": "Does not match any specific criterion. Focuses on OCR performance in assistive technology, not on new methods for spatial intelligence, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}