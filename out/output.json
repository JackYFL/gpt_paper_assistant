{
    "2512.02458": {
        "authors": [
            "Zhongyi Cai",
            "Yi Du",
            "Chen Wang",
            "Yu Kong"
        ],
        "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration",
        "abstract": "arXiv:2512.02458v1 Announce Type: new  Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.",
        "arxiv_id": "2512.02458",
        "ARXIVID": "2512.02458",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents), criterion 3 (new embodied AI benchmark and method), and criterion 2 (MLLMs): Introduces SEER-Bench for sequential embodied exploration and reasoning, and proposes a 3D spatial memory reasoning approach that augments MLLMs with geometric information for spatial reasoning. This is a novel angle for embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.02395": {
        "authors": [
            "Yifan Zhang",
            "Liang Hu",
            "Haofeng Sun",
            "Peiyu Wang",
            "Yichen Wei",
            "Shukang Yin",
            "Jiangbo Pei",
            "Wei Shen",
            "Peng Xia",
            "Yi Peng",
            "Tianyidan Xie",
            "Eric Li",
            "Yang Liu",
            "Xuchen Song",
            "Yahui Zhou"
        ],
        "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
        "abstract": "arXiv:2512.02395v1 Announce Type: new  Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
        "arxiv_id": "2512.02395",
        "ARXIVID": "2512.02395",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Introduces Skywork-R1V4, a large multimodal agentic model with interleaved visual and knowledge reasoning, achieving SOTA on several benchmarks and showing emergent long-horizon reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.03004": {
        "authors": [
            "Xiaoxue Chen",
            "Ziyi Xiong",
            "Yuantao Chen",
            "Gen Li",
            "Nan Wang",
            "Hongcheng Luo",
            "Long Chen",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Hongyang Li",
            "Ya-Qin Zhang",
            "Hao Zhao"
        ],
        "title": "DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images",
        "abstract": "arXiv:2512.03004v1 Announce Type: new  Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \\textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.",
        "arxiv_id": "2512.03004",
        "ARXIVID": "2512.03004",
        "COMMENT": "Matches criterion 3. DGGT proposes a feedforward, pose-free 4D reconstruction framework for dynamic driving scenes using unposed images. It introduces a new method for scalable, fast 4D scene reconstruction, which is highly relevant to embodied AI, simulation, and novel methods for world modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.02340": {
        "authors": [
            "Qiyao Xue",
            "Weichen Liu",
            "Shiqi Wang",
            "Haoming Wang",
            "Yuyang Wu",
            "Wei Gao"
        ],
        "title": "Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective",
        "abstract": "arXiv:2512.02340v1 Announce Type: new  Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.",
        "arxiv_id": "2512.02340",
        "ARXIVID": "2512.02340",
        "COMMENT": "Matches criterion 1 and 3. This paper introduces ReMindView-Bench, a cognitively grounded benchmark for evaluating spatial reasoning in vision-language models (VLMs) across multiple views. It analyzes how VLMs construct and maintain spatial mental models, and provides a new benchmark and analysis tools for spatial reasoning, which is a novel angle in spatial intelligence and embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.02425": {
        "authors": [
            "Woongyeong Yeo",
            "Kangsan Kim",
            "Jaehong Yoon",
            "Sung Ju Hwang"
        ],
        "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
        "abstract": "arXiv:2512.02425v1 Announce Type: new  Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
        "arxiv_id": "2512.02425",
        "ARXIVID": "2512.02425",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 1 (methodological improvements for long video reasoning in multimodal agents). Proposes WorldMM, a dynamic multimodal memory agent for long video reasoning, with significant improvements over baselines.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.03000": {
        "authors": [
            "Kairun Wen",
            "Yuzhi Huang",
            "Runyu Chen",
            "Hui Zheng",
            "Yunlong Lin",
            "Panwang Pan",
            "Chenxin Li",
            "Wenyan Cong",
            "Jian Zhang",
            "Junbin Lu",
            "Chenguo Lin",
            "Dilin Wang",
            "Zhicheng Yan",
            "Hongyu Xu",
            "Justin Theiss",
            "Yue Huang",
            "Xinghao Ding",
            "Rakesh Ranjan",
            "Zhiwen Fan"
        ],
        "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
        "abstract": "arXiv:2512.03000v1 Announce Type: new  Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
        "arxiv_id": "2512.03000",
        "ARXIVID": "2512.03000",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/framework for 4D world modeling). Introduces DynamicVerse, a large-scale, physically-aware multimodal 4D world modeling framework and dataset for dynamic real-world video.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02895": {
        "authors": [
            "Wei Chen",
            "Chaoqun Du",
            "Feng Gu",
            "Wei He",
            "Qizhen Li",
            "Zide Liu",
            "Xuhao Pan",
            "Chang Ren",
            "Xudong Rao",
            "Chenfeng Wang",
            "Tao Wei",
            "Chengjun Yu",
            "Pengfei Yu",
            "Yufei Zheng",
            "Chunpeng Zhou",
            "Pan Zhou",
            "Xuhan Zhu"
        ],
        "title": "MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm",
        "abstract": "arXiv:2512.02895v1 Announce Type: new  Abstract: We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.",
        "arxiv_id": "2512.02895",
        "ARXIVID": "2512.02895",
        "COMMENT": "Matches criterion 2 (MLLMs): Presents MindGPT-4ov, a new MLLM with a multi-stage post-training paradigm, including innovations in data generation, curriculum learning, and reinforcement learning for multimodal reasoning. The model achieves SOTA on multiple benchmarks and will be open-sourced.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02566": {
        "authors": [
            "Kun Yuan",
            "Min Woo Sun",
            "Zhen Chen",
            "Alejandro Lozano",
            "Xiangteng He",
            "Shi Li",
            "Nassir Navab",
            "Xiaoxiao Sun",
            "Nicolas Padoy",
            "Serena Yeung-Levy"
        ],
        "title": "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature",
        "abstract": "arXiv:2512.02566v1 Announce Type: new  Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.",
        "arxiv_id": "2512.02566",
        "ARXIVID": "2512.02566",
        "COMMENT": "Matches criterion 2 and 4: Introduces a new vision-language pretraining pipeline (Panel2Patch) for biomedical literature, with a novel hierarchical data mining and granularity-aware pretraining strategy, improving vision-language model performance in biomedical applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.03043": {
        "authors": [
            "Kaituo Feng",
            "Manyuan Zhang",
            "Hongyu Li",
            "Kaixuan Fan",
            "Shuang Chen",
            "Yilei Jiang",
            "Dian Zheng",
            "Peiwen Sun",
            "Yiyuan Zhang",
            "Haoze Sun",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Xiangyu Yue"
        ],
        "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
        "abstract": "arXiv:2512.03043v1 Announce Type: new  Abstract: Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
        "arxiv_id": "2512.03043",
        "ARXIVID": "2512.03043",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes OneThinker, a unified model for image and video reasoning across multiple tasks, with a new training corpus and a novel RL optimization for multi-task learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02450": {
        "authors": [
            "Valentin Bieri",
            "Marie-Julie Rakotosaona",
            "Keisuke Tateno",
            "Francis Engelmann",
            "Leonidas Guibas"
        ],
        "title": "HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild",
        "abstract": "arXiv:2512.02450v1 Announce Type: new  Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.",
        "arxiv_id": "2512.02450",
        "ARXIVID": "2512.02450",
        "COMMENT": "Matches criterion 3 (embodied AI/new benchmark/simulator). Introduces HouseLayout3D, a new real-world benchmark for 3D layout estimation at building scale, and a training-free baseline. Focuses on spatial understanding at a scale and complexity not previously addressed.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02942": {
        "authors": [
            "Lanxiang Hu",
            "Abhilash Shankarampeta",
            "Yixin Huang",
            "Zilin Dai",
            "Haoyang Yu",
            "Yujie Zhao",
            "Haoqiang Kang",
            "Daniel Zhao",
            "Tajana Rosing",
            "Hao Zhang"
        ],
        "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
        "abstract": "arXiv:2512.02942v1 Announce Type: new  Abstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.",
        "arxiv_id": "2512.02942",
        "ARXIVID": "2512.02942",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks, simulator related, or new methods). Introduces a new benchmark (VideoScience-Bench) for evaluating scientific reasoning in video generation models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.02835": {
        "authors": [
            "Yifan Li",
            "Yingda Yin",
            "Lingting Zhu",
            "Weikai Chen",
            "Shengju Qian",
            "Xin Wang",
            "Yanwei Fu"
        ],
        "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
        "abstract": "arXiv:2512.02835v1 Announce Type: new  Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
        "arxiv_id": "2512.02835",
        "ARXIVID": "2512.02835",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Introduces ReVSeg, a video segmentation method that explicitly decomposes reasoning into sequential steps using pretrained VLMs and reinforcement learning, yielding interpretable reasoning chains and SOTA results. The approach is novel in its explicit reasoning chain and RL optimization.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.02161": {
        "authors": [
            "Kevin David Hayes",
            "Micah Goldblum",
            "Vikash Sehwag",
            "Gowthami Somepalli",
            "Ashwinee Panda",
            "Tom Goldstein"
        ],
        "title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges",
        "abstract": "arXiv:2512.02161v1 Announce Type: new  Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.",
        "arxiv_id": "2512.02161",
        "ARXIVID": "2512.02161",
        "COMMENT": "Matches criterion 2 and 4: Proposes a new evaluation framework and dataset for failure modes in text-to-image models using VLMs as judges, providing new insights into VLM and generative model evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.02421": {
        "authors": [
            "Xinyao Li",
            "Yinjie Min",
            "Hongbo Chen",
            "Zhekai Du",
            "Fengling Li",
            "Jingjing Li"
        ],
        "title": "Generalizing Vision-Language Models with Dedicated Prompt Guidance",
        "abstract": "arXiv:2512.02421v1 Announce Type: new  Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.",
        "arxiv_id": "2512.02421",
        "ARXIVID": "2512.02421",
        "COMMENT": "Matches criterion 2: Proposes a new method (GuiDG) for generalizing vision-language models using prompt-guided domain experts and cross-modal attention, with new benchmarks and theoretical insights.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.02361": {
        "authors": [
            "Zhengzhuo Xu",
            "Chong Sun",
            "SiNan Du",
            "Chen Li",
            "Jing Lyu",
            "Chun Yuan"
        ],
        "title": "VACoT: Rethinking Visual Data Augmentation with VLMs",
        "abstract": "arXiv:2512.02361v1 Announce Type: new  Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.",
        "arxiv_id": "2512.02361",
        "ARXIVID": "2512.02361",
        "COMMENT": "Matches criterion 2 and 4. VACoT introduces a framework for visual data augmentation in VLMs, improving robustness and generalization, especially in adversarial scenarios. It leverages agentic reinforcement learning and structured augmentations, which is a novel application of vision foundation models and VLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.02624": {
        "authors": [
            "Zheng Huang",
            "Xukai Liu",
            "Tianyu Hu",
            "Kai Zhang",
            "Ye Liu"
        ],
        "title": "PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding",
        "abstract": "arXiv:2512.02624v1 Announce Type: new  Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.",
        "arxiv_id": "2512.02624",
        "ARXIVID": "2512.02624",
        "COMMENT": "Matches criterion 2. PPTBench is a new benchmark for evaluating large language models (LLMs) and MLLMs on PowerPoint layout and design understanding, focusing on multimodal reasoning and layout-centric challenges. It reveals gaps in current MLLMs' spatial and visual-structural reasoning, which is highly relevant to VLLMs/MLLMs and their evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.02982": {
        "authors": [
            "Xiang Xu",
            "Ao Liang",
            "Youquan Liu",
            "Linfeng Li",
            "Lingdong Kong",
            "Ziwei Liu",
            "Qingshan Liu"
        ],
        "title": "U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences",
        "abstract": "arXiv:2512.02982v1 Announce Type: new  Abstract: Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a \"hard-to-easy\" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.",
        "arxiv_id": "2512.02982",
        "ARXIVID": "2512.02982",
        "COMMENT": "Matches criterion 3. U4D is an uncertainty-aware 4D world modeling framework for LiDAR sequences, relevant to embodied AI and simulation. It introduces a novel uncertainty-driven approach for generative modeling of dynamic 3D environments, which is important for simulation and autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.02423": {
        "authors": [
            "Haolong Yan",
            "Yeqing Shen",
            "Xin Huang",
            "Jia Wang",
            "Kaijun Tan",
            "Zhixuan Liang",
            "Hongxin Li",
            "Zheng Ge",
            "Osamu Yoshie",
            "Si Li",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
        "abstract": "arXiv:2512.02423v1 Announce Type: new  Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
        "arxiv_id": "2512.02423",
        "ARXIVID": "2512.02423",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator for GUI agent navigation). Introduces GUI Exploration Lab, a simulation environment for GUI navigation research, and investigates RL-based methods for screen navigation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02473": {
        "authors": [
            "Yuta Oshima",
            "Yusuke Iwasawa",
            "Masahiro Suzuki",
            "Yutaka Matsuo",
            "Hiroki Furuta"
        ],
        "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling",
        "abstract": "arXiv:2512.02473v1 Announce Type: new  Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.",
        "arxiv_id": "2512.02473",
        "ARXIVID": "2512.02473",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial intelligence in video world modeling for embodied agents). Proposes compressed memory for improved spatial consistency in long-term video world modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02685": {
        "authors": [
            "Huankun Sheng",
            "Ming Li",
            "Yixiang Wei",
            "Yeying Fan",
            "Yu-Hui Wen",
            "Tieliang Gong",
            "Yong-Jin Liu"
        ],
        "title": "Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance",
        "abstract": "arXiv:2512.02685v1 Announce Type: new  Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.",
        "arxiv_id": "2512.02685",
        "ARXIVID": "2512.02685",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding/object-centric scene decomposition). Proposes a novel foreground-aware slot attention mechanism with pseudo-mask guidance for unsupervised scene decomposition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02339": {
        "authors": [
            "Chenshuang Zhang",
            "Kang Zhang",
            "Joon Son Chung",
            "In So Kweon",
            "Junmo Kim",
            "Chengzhi Mao"
        ],
        "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
        "abstract": "arXiv:2512.02339v1 Announce Type: new  Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.",
        "arxiv_id": "2512.02339",
        "ARXIVID": "2512.02339",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in tracking similar-looking objects using video diffusion models). Also relevant to criterion 4 (vision foundation models and their applications) as it leverages pre-trained video diffusion models for tracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02341": {
        "authors": [
            "Fengyi Zhang",
            "Tianjun Zhang",
            "Kasra Khosoussi",
            "Zheng Zhang",
            "Zi Huang",
            "Yadan Luo"
        ],
        "title": "TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction",
        "abstract": "arXiv:2512.02341v1 Announce Type: new  Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \\href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.",
        "arxiv_id": "2512.02341",
        "ARXIVID": "2512.02341",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a new framework for improving global consistency in 3D vision foundation models for online reconstruction, with a novel alignment method robust to noisy geometry. The method is plug-and-play for various 3D foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02790": {
        "authors": [
            "Keming Ye",
            "Zhipeng Huang",
            "Canmiao Fu",
            "Qingyang Liu",
            "Jiani Cai",
            "Zheqi Lv",
            "Chen Li",
            "Jing Lyu",
            "Zhou Zhao",
            "Shengyu Zhang"
        ],
        "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
        "abstract": "arXiv:2512.02790v1 Announce Type: new  Abstract: With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \\textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \\textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \\textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \\textit{Non-edit Consistency} and \\textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
        "arxiv_id": "2512.02790",
        "ARXIVID": "2512.02790",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (benchmark for vision foundation models): Introduces a large-scale dataset and benchmark for reasoning-enriched image editing, with new metrics for spatial and knowledge-driven reasoning. The work is relevant for evaluating and improving VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02715": {
        "authors": [
            "Peirong Zhang",
            "Yidan Zhang",
            "Luxiao Xu",
            "Jinliang Lin",
            "Zonghao Guo",
            "Fengxiang Wang",
            "Xue Yang",
            "Kaiwen Wei",
            "Lei Wang"
        ],
        "title": "GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding",
        "abstract": "arXiv:2512.02715v1 Announce Type: new  Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.",
        "arxiv_id": "2512.02715",
        "ARXIVID": "2512.02715",
        "COMMENT": "Matches criterion 1 (spatial understanding) and criterion 2 (MLLMs): Proposes a geospatially rewarded visual search framework for remote sensing visual grounding, focusing on spatial reasoning and multimodal perception. The method is designed for fine-grained spatial understanding in challenging remote sensing scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02700": {
        "authors": [
            "Zhenkai Wu",
            "Xiaowen Ma",
            "Zhenliang Ni",
            "Dengming Zhang",
            "Han Shu",
            "Xin Jiang",
            "Xinghao Chen"
        ],
        "title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm",
        "abstract": "arXiv:2512.02700v1 Announce Type: new  Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup.",
        "arxiv_id": "2512.02700",
        "ARXIVID": "2512.02700",
        "COMMENT": "Matches criterion 2 and 4: Proposes VLM-Pruner, a new token pruning method for efficient vision-language models, with a novel spatial sparsity-aware approach, improving VLM deployment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02867": {
        "authors": [
            "Yaqi Wang",
            "Zhi Li",
            "Chengyu Wu",
            "Jun Liu",
            "Yifan Zhang",
            "Jialuo Chen",
            "Jiaxue Ni",
            "Qian Luo",
            "Jin Liu",
            "Can Han",
            "Changkai Ji",
            "Zhi Qin Tan",
            "Ajo Babu George",
            "Liangyu Chen",
            "Qianni Zhang",
            "Dahong Qian",
            "Shuai Wang",
            "Huiyu Zhou"
        ],
        "title": "MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration",
        "abstract": "arXiv:2512.02867v1 Announce Type: new  Abstract: Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.",
        "arxiv_id": "2512.02867",
        "ARXIVID": "2512.02867",
        "COMMENT": "Matches criterion 3: Presents a new benchmark challenge (MICCAI STSR 2025) for semi-supervised segmentation and registration in dental imaging, with novel tasks and open-source solutions, relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02496": {
        "authors": [
            "Mizuki Kikkawa",
            "Tatsuya Yatagawa",
            "Yutaka Ohtake",
            "Hiromasa Suzuki"
        ],
        "title": "Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration",
        "abstract": "arXiv:2512.02496v1 Announce Type: new  Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.",
        "arxiv_id": "2512.02496",
        "ARXIVID": "2512.02496",
        "COMMENT": "Matches criterion 1: Proposes a new attention-based method (ARPS layer) for spatial understanding in point set registration, improving transformation invariance and performance in deep learning-based registration methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02906": {
        "authors": [
            "Fan Yang",
            "Kaihao Zhang"
        ],
        "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
        "abstract": "arXiv:2512.02906v1 Announce Type: new  Abstract: Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.",
        "arxiv_id": "2512.02906",
        "ARXIVID": "2512.02906",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Proposes a multi-resolution retrieval-detection fusion framework for high-resolution image understanding in MLLMs, addressing semantic fragmentation and improving localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02622": {
        "authors": [
            "Xuming He",
            "Zehao Fan",
            "Hengjia Li",
            "Fan Zhuo",
            "Hankun Xu",
            "Senlin Cheng",
            "Di Weng",
            "Haifeng Liu",
            "Can Ye",
            "Boxi Wu"
        ],
        "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
        "abstract": "arXiv:2512.02622v1 Announce Type: new  Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
        "arxiv_id": "2512.02622",
        "ARXIVID": "2512.02622",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 3 (benchmark for reasoning in video generation). Introduces RULER-Bench, a new benchmark for rule-based reasoning in video generation models, highlighting a novel evaluation angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02536": {
        "authors": [
            "Jian Yang",
            "Dacheng Yin",
            "Xiaoxuan He",
            "Yong Li",
            "Fengyun Rao",
            "Jing Lyu",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens",
        "abstract": "arXiv:2512.02536v1 Announce Type: new  Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.",
        "arxiv_id": "2512.02536",
        "ARXIVID": "2512.02536",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications). Proposes a new method to bridge VLMs and diffusion models using noisy query tokens, with continual learning and a VAE branch for fine-grained details.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02781": {
        "authors": [
            "Xu Han",
            "Biao Zhang",
            "Xiangjun Tang",
            "Xianzhi Li",
            "Peter Wonka"
        ],
        "title": "LumiX: Structured and Coherent Text-to-Intrinsic Generation",
        "abstract": "arXiv:2512.02781v1 Announce Type: new  Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.",
        "arxiv_id": "2512.02781",
        "ARXIVID": "2512.02781",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a structured diffusion framework for text-to-intrinsic generation, with new attention and adaptation mechanisms for physically consistent scene modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.02643": {
        "authors": [
            "Yongchuan Cui",
            "Peng Liu",
            "Yi Zeng"
        ],
        "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
        "abstract": "arXiv:2512.02643v1 Announce Type: new  Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
        "arxiv_id": "2512.02643",
        "ARXIVID": "2512.02643",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a pretraining strategy leveraging large-scale simulated datasets to learn spatial-spectral priors for remote sensing image fusion, improving generalization and establishing a new benchmark for cross-domain pansharpening.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.02897": {
        "authors": [
            "Pierpaolo Serio",
            "Giulio Pisaneschi",
            "Andrea Dan Ryals",
            "Vincenzo Infantino",
            "Lorenzo Gentilini",
            "Valentina Donzella",
            "Lorenzo Pollini"
        ],
        "title": "Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models",
        "abstract": "arXiv:2512.02897v1 Announce Type: new  Abstract: This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.",
        "arxiv_id": "2512.02897",
        "ARXIVID": "2512.02897",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Systematically evaluates LiDAR-to-image projections for place recognition using vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.02268": {
        "authors": [
            "Jeremy Andrew Irvin",
            "Jiaqi Han",
            "Zikui Wang",
            "Abdulaziz Alharbi",
            "Yufei Zhao",
            "Nomin-Erdene Bayarsaikhan",
            "Daniele Visioni",
            "Andrew Y. Ng",
            "Duncan Watson-Parris"
        ],
        "title": "Spatiotemporal Pyramid Flow Matching for Climate Emulation",
        "abstract": "arXiv:2512.02268v1 Announce Type: new  Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .",
        "arxiv_id": "2512.02268",
        "ARXIVID": "2512.02268",
        "COMMENT": "Somewhat related to criterion 1: Proposes a new generative model (Spatiotemporal Pyramid Flows) for climate emulation, with hierarchical spatial-temporal modeling, but not directly about embodied agents or spatial intelligence in the robotics sense.",
        "RELEVANCE": 4,
        "NOVELTY": 8
    },
    "2512.02358": {
        "authors": [
            "Ran Zhang",
            "Kun Ouyang",
            "Tiancheng Ma",
            "Yida Yang",
            "Dong Fang"
        ],
        "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games",
        "abstract": "arXiv:2512.02358v1 Announce Type: new  Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.",
        "arxiv_id": "2512.02358",
        "ARXIVID": "2512.02358",
        "COMMENT": "Somewhat related to criterion 3: Proposes a generative agent-based simulation system for MMO games using LLMs, which is a novel simulation/benchmarking approach, though not strictly embodied AI in robotics or physical environments.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.02870": {
        "authors": [
            "Zhaoqing Wang",
            "Xiaobo Xia",
            "Zhuolin Bie",
            "Jinlin Liu",
            "Dongdong Yu",
            "Jia-Wang Bian",
            "Changhu Wang"
        ],
        "title": "Taming Camera-Controlled Video Generation with Verifiable Geometry Reward",
        "abstract": "arXiv:2512.02870v1 Announce Type: new  Abstract: Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.",
        "arxiv_id": "2512.02870",
        "ARXIVID": "2512.02870",
        "COMMENT": "This paper introduces an RL-based framework for camera-controlled video generation with a novel geometry-based reward. While it is a methodological improvement in generative modeling and uses reinforcement learning, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2512.02650": {
        "authors": [
            "Junwon Lee",
            "Juhan Nam",
            "Jiyoung Lee"
        ],
        "title": "Hear What Matters! Text-conditioned Selective Video-to-Audio Generation",
        "abstract": "arXiv:2512.02650v1 Announce Type: new  Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.",
        "arxiv_id": "2512.02650",
        "ARXIVID": "2512.02650",
        "COMMENT": "Somewhat related to criterion 2: Proposes a new text-conditioned selective video-to-audio generation model (SelVA), which is multi-modal but not a VLLM/MLLM in the vision-language sense.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2512.02530": {
        "authors": [
            "Yuxiang He",
            "Jian Zhao",
            "Yuchen Yuan",
            "Tianle Zhang",
            "Wei Cai",
            "Haojie Cheng",
            "Ziyan Shi",
            "Ming Zhu",
            "Haichuan Tang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration",
        "abstract": "arXiv:2512.02530v1 Announce Type: new  Abstract: The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",
        "arxiv_id": "2512.02530",
        "ARXIVID": "2512.02530",
        "COMMENT": "Partially matches criterion 2 (MLLMs) as it proposes a multimodal content safety framework using multi-agent debate, but the focus is on content moderation rather than core vision-language modeling or spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.02699": {
        "authors": [
            "Hyeongseop Rha",
            "Jeong Hun Yeo",
            "Junil Won",
            "Se Jin Park",
            "Yong Man Ro"
        ],
        "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding",
        "abstract": "arXiv:2512.02699v1 Announce Type: new  Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.",
        "arxiv_id": "2512.02699",
        "ARXIVID": "2512.02699",
        "COMMENT": "This paper presents a modality-importance-guided reasoning framework for multimodal emotion understanding in MLLMs. While it is relevant to MLLMs, its focus is on emotion understanding rather than spatial intelligence, embodied AI, or vision foundation models. It is a methodological improvement in reasoning, but not a direct match to the criteria.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.02453": {
        "authors": [
            "Kerui Chen",
            "Jianrong Zhang",
            "Ming Li",
            "Zhonglong Zheng",
            "Hehe Fan"
        ],
        "title": "ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation",
        "abstract": "arXiv:2512.02453v1 Announce Type: new  Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.",
        "arxiv_id": "2512.02453",
        "ARXIVID": "2512.02453",
        "COMMENT": "Somewhat relevant to criterion 1 (methodological improvements in modeling intra-style diversity for stylized motion generation), but less directly related to spatial understanding or embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.02737": {
        "authors": [
            "Tristan Amadei",
            "Enric Meinhardt-Llopis",
            "Benedicte Bascle",
            "Corentin Abgrall",
            "Gabriele Facciolo"
        ],
        "title": "Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone",
        "abstract": "arXiv:2512.02737v1 Announce Type: new  Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.",
        "arxiv_id": "2512.02737",
        "ARXIVID": "2512.02737",
        "COMMENT": "This paper proposes a self-supervised method for UAV geo-localization using only reference imagery, which is a methodological improvement in remote sensing and localization. However, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.02931": {
        "authors": [
            "Ying Yang",
            "Zhengyao Lv",
            "Tianlin Pan",
            "Haofan Wang",
            "Binxin Yang",
            "Hubery Yin",
            "Chen Li",
            "Chenyang Si"
        ],
        "title": "DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation",
        "abstract": "arXiv:2512.02931v1 Announce Type: new  Abstract: In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.",
        "arxiv_id": "2512.02931",
        "ARXIVID": "2512.02931",
        "COMMENT": "Relevant to generative modeling in vision (friend's general interest), but does not directly match any specific criterion. Proposes a method to improve diversity in bitwise autoregressive image generation.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.02469": {
        "authors": [
            "Fengli Ran",
            "Xiao Pu",
            "Bo Liu",
            "Xiuli Bi",
            "Bin Xiao"
        ],
        "title": "TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution",
        "abstract": "arXiv:2512.02469v1 Announce Type: new  Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.",
        "arxiv_id": "2512.02469",
        "ARXIVID": "2512.02469",
        "COMMENT": "This paper proposes a new method for dataset distillation using trajectory-guided distribution matching, which is a methodological improvement in data efficiency for vision models. However, it does not directly address spatial understanding, embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.02447": {
        "authors": [
            "Fan Luo",
            "Zeyu Gao",
            "Xinhao Luo",
            "Kai Zhao",
            "Yanfeng Lu"
        ],
        "title": "Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors",
        "abstract": "arXiv:2512.02447v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.",
        "arxiv_id": "2512.02447",
        "ARXIVID": "2512.02447",
        "COMMENT": "Relevant to spatiotemporal modeling and object detection, but does not directly match any specific criterion. Focuses on spiking neural networks for object detection with improved temporal dynamics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02482": {
        "authors": [
            "Vishwesh Nath",
            "Javier G. Tejero",
            "Ruilong Li",
            "Filippo Filicori",
            "Mahdi Azizian",
            "Sean D. Huver"
        ],
        "title": "G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline",
        "abstract": "arXiv:2512.02482v1 Announce Type: new  Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.",
        "arxiv_id": "2512.02482",
        "ARXIVID": "2512.02482",
        "COMMENT": "This paper presents G-SHARP, a real-time surgical scene reconstruction framework using Gaussian splatting. While it is an application of 3D modeling and real-time reconstruction, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more of a domain-specific application.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02681": {
        "authors": [
            "Zhongbao Yang",
            "Jiangxin Dong",
            "Yazhou Yao",
            "Jinhui Tang",
            "Jinshan Pan"
        ],
        "title": "PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution",
        "abstract": "arXiv:2512.02681v1 Announce Type: new  Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.",
        "arxiv_id": "2512.02681",
        "ARXIVID": "2512.02681",
        "COMMENT": "This paper proposes a lightweight diffusion-based image super-resolution method with phase-guided pruning. While it is relevant to generative modeling and computer vision, it does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02369": {
        "authors": [
            "Qingmei Li",
            "Yang Zhang",
            "Peifeng Zhang",
            "Haohuan Fu",
            "Juepeng Zheng"
        ],
        "title": "SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains",
        "abstract": "arXiv:2512.02369v1 Announce Type: new  Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \\textbf{S}tyle-\\textbf{A}daptive \\textbf{GE}neralization framework (\\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.",
        "arxiv_id": "2512.02369",
        "ARXIVID": "2512.02369",
        "COMMENT": "This paper introduces SAGE, a style-adaptive generalization framework for privacy-constrained semantic segmentation. It focuses on domain generalization and style transfer for segmentation, but does not address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is relevant to computer vision and generalization, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02280": {
        "authors": [
            "Noorbakhsh Amiri Golilarz",
            "Sindhuja Penchala",
            "Shahram Rahimi"
        ],
        "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence",
        "abstract": "arXiv:2512.02280v1 Announce Type: new  Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.",
        "arxiv_id": "2512.02280",
        "ARXIVID": "2512.02280",
        "COMMENT": "This paper discusses the limitations of current AI systems in achieving cognitive autonomy and proposes a shift toward architectures inspired by neurocognitive principles. It touches on embodied feedback and self-directed adaptation, which are tangentially related to spatial intelligence and embodied AI (criteria 1 and 3), but does not present new methods, benchmarks, or models. The focus is more on conceptual analysis and future directions rather than concrete methodological advances.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.02751": {
        "authors": [
            "Rakib Ahsan",
            "MD Sadik Hossain Shanto",
            "Md Sultanul Arifin",
            "Tanzima Hashem"
        ],
        "title": "AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery",
        "abstract": "arXiv:2512.02751v1 Announce Type: new  Abstract: Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.",
        "arxiv_id": "2512.02751",
        "ARXIVID": "2512.02751",
        "COMMENT": "This paper proposes an attention-enhanced deep neural network for methane plume detection in satellite imagery. While it uses attention mechanisms and deep learning, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more of an application paper in remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}