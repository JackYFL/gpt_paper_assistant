{
    "2509.05714": {
        "authors": [
            "Zhaoyu Fan",
            "Kaihang Pan",
            "Mingze Zhou",
            "Bosheng Qin",
            "Juncheng Li",
            "Shengyu Zhang",
            "Wenqiao Zhang",
            "Siliang Tang",
            "Fei Wu",
            "Yueting Zhuang"
        ],
        "title": "Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs",
        "abstract": "arXiv:2509.05714v1 Announce Type: new  Abstract: Knowledge editing enables multimodal large language models (MLLMs) to efficiently update outdated or incorrect information. However, existing benchmarks primarily emphasize cognitive-level modifications while lacking a focus on deeper meta-cognitive processes. To bridge this gap, we introduce CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge editing abilities across three levels: (1) Counterfactual-Driven Editing, assessing self-awareness of knowledge correctness changes; (2) Boundary Constraint Editing, ensuring appropriate generalization without unintended interference; and (3) Noise-Robust Editing, promoting reflective evaluation of uncertain information. To advance meta-cognitive editing, we propose MIND (Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that constructs a meta-knowledge memory for self-awareness, employs game-theoretic interactions to monitor knowledge activation, and incorporates label refinement for noise-robust updates. Extensive experiments show that MIND significantly outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks.",
        "arxiv_id": "2509.05714",
        "ARXIVID": "2509.05714",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multimodal large language models) and introduces a new benchmark (CogEdit) for meta-cognitive knowledge editing. Also proposes a new framework (MIND) for meta-cognitive editing in MLLMs. This is a direct and strong match for your interest in new MLLMs and benchmarks for their capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.06461": {
        "authors": [
            "Yuyao Ge",
            "Shenghua Liu",
            "Yiwei Wang",
            "Lingrui Mei",
            "Baolong Bi",
            "Xuanshan Zhou",
            "Jiayu Yao",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
        "abstract": "arXiv:2509.06461v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.",
        "arxiv_id": "2509.06461",
        "ARXIVID": "2509.06461",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models, applications). Proposes a new, training-free method (CARVE) for enhancing VLMs' visual reasoning via contrastive attention, with strong empirical results and theoretical insights.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.06830": {
        "authors": [
            "Corentin Dancette",
            "Julien Khlaut",
            "Antoine Saporta",
            "Helene Philippe",
            "Elodie Ferreres",
            "Baptiste Callard",
            "Th\\'eo Danielou",
            "L\\'eo Alberge",
            "L\\'eo Machado",
            "Daniel Tordjman",
            "Julie Dupuis",
            "Korentin Le Floch",
            "Jean Du Terrail",
            "Mariam Moshiri",
            "Laurent Dercle",
            "Tom Boeken",
            "Jules Gregory",
            "Maxime Ronot",
            "Fran\\c{c}ois Legou",
            "Pascal Roux",
            "Marc Sapoval",
            "Pierre Manceron",
            "Paul H\\'erent"
        ],
        "title": "Curia: A Multi-Modal Foundation Model for Radiology",
        "abstract": "arXiv:2509.06830v1 Announce Type: new  Abstract: AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.",
        "arxiv_id": "2509.06830",
        "ARXIVID": "2509.06830",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and criterion 2 (multi-modal large language models for radiology). Introduces Curia, a large-scale multi-modal foundation model for radiology, trained on a massive real-world dataset, with open weights.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.06907": {
        "authors": [
            "Bing Han",
            "Chen Zhu",
            "Dong Han",
            "Rui Yu",
            "Songliang Cao",
            "Jianhui Wu",
            "Scott Chapman",
            "Zijian Wang",
            "Bangyou Zheng",
            "Wei Guo",
            "Marie Weiss",
            "Benoit de Solan",
            "Andreas Hund",
            "Lukas Roth",
            "Kirchgessner Norbert",
            "Andrea Visioni",
            "Yufeng Ge",
            "Wenjuan Li",
            "Alexis Comar",
            "Dong Jiang",
            "Dejun Han",
            "Fred Baret",
            "Yanfeng Ding",
            "Hao Lu",
            "Shouyang Liu"
        ],
        "title": "FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data",
        "abstract": "arXiv:2509.06907v1 Announce Type: new  Abstract: Vision-driven field monitoring is central to digital agriculture, yet models built on general-domain pretrained backbones often fail to generalize across tasks, owing to the interaction of fine, variable canopy structures with fluctuating field conditions. We present FoMo4Wheat, one of the first crop-domain vision foundation model pretrained with self-supervision on ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5 million high-resolution images collected over a decade at 30 global sites, spanning >2,000 genotypes and >500 environmental conditions). This wheat-specific pretraining yields representations that are robust for wheat and transferable to other crops and weeds. Across ten in-field vision tasks at canopy and organ levels, FoMo4Wheat models consistently outperform state-of-the-art models pretrained on general-domain dataset. These results demonstrate the value of crop-specific foundation models for reliable in-field perception and chart a path toward a universal crop foundation model with cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is: https://fomo4wheat.phenix-lab.com/.",
        "arxiv_id": "2509.06907",
        "ARXIVID": "2509.06907",
        "COMMENT": "This paper introduces FoMo4Wheat, a crop-domain vision foundation model pretrained on a massive, diverse wheat image dataset. It demonstrates the value of crop-specific foundation models for in-field perception and generalization across tasks and species. This directly matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.06945": {
        "authors": [
            "Wenxuan Huang",
            "Shuang Chen",
            "Zheyong Xie",
            "Shaosheng Cao",
            "Shixiang Tang",
            "Yufan Shen",
            "Qingyu Yin",
            "Wenbo Hu",
            "Xiaoman Wang",
            "Yuntian Tang",
            "Junbo Qiao",
            "Yue Guo",
            "Yao Hu",
            "Zhenfei Yin",
            "Philip Torr",
            "Yu Cheng",
            "Wanli Ouyang",
            "Shaohui Lin"
        ],
        "title": "Interleaving Reasoning for Better Text-to-Image Generation",
        "abstract": "arXiv:2509.06945v1 Announce Type: new  Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
        "arxiv_id": "2509.06945",
        "ARXIVID": "2509.06945",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and 4 (vision foundation models and applications). Proposes a new interleaving reasoning framework for text-to-image generation, with a new dataset and training method, and demonstrates significant empirical improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.06023": {
        "authors": [
            "Mengmeng Liu",
            "Michael Ying Yang",
            "Jiuming Liu",
            "Yunpeng Zhang",
            "Jiangtao Li",
            "Sander Oude Elberink",
            "George Vosselman",
            "Hao Cheng"
        ],
        "title": "DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion",
        "abstract": "arXiv:2509.06023v1 Announce Type: new  Abstract: Visual-LiDAR odometry is a critical component for autonomous system localization, yet achieving high accuracy and strong robustness remains a challenge. Traditional approaches commonly struggle with sensor misalignment, fail to fully leverage temporal information, and require extensive manual tuning to handle diverse sensor configurations. To address these problems, we introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse spatial-temporal fusion to enhance accuracy and robustness. Our approach proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction and Update module that integrates temporally-predicted positions with current frame data, providing better initialization values for pose estimation and enhancing model's robustness against accumulative errors; and (3) a Temporal Clip Training strategy combined with a Collective Average Loss mechanism that aggregates losses across multiple frames, enabling global optimization and reducing the scale drift over long sequences. Extensive experiments on the KITTI and Argoverse Odometry dataset demonstrate the superiority of our proposed DVLO4D, which achieves state-of-the-art performance in terms of both pose accuracy and robustness. Additionally, our method has high efficiency, with an inference time of 82 ms, possessing the potential for the real-time deployment.",
        "arxiv_id": "2509.06023",
        "ARXIVID": "2509.06023",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and 3 (embodied AI, new methods for odometry). Proposes a new visual-LiDAR odometry framework with novel spatial-temporal fusion and training strategies, showing state-of-the-art results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.06942": {
        "authors": [
            "Xiangwei Shen",
            "Zhimin Li",
            "Zhantao Yang",
            "Shiyi Zhang",
            "Yingfang Zhang",
            "Donghao Li",
            "Chunyu Wang",
            "Qinglin Lu",
            "Yansong Tang"
        ],
        "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
        "abstract": "arXiv:2509.06942v1 Announce Type: new  Abstract: Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.",
        "arxiv_id": "2509.06942",
        "ARXIVID": "2509.06942",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and generative modeling. Proposes a new method for aligning diffusion models with human preference, with a novel optimization and reward adjustment strategy, and demonstrates large empirical gains.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.06306": {
        "authors": [
            "Zhang Jing",
            "Pu Nan",
            "Xie Yu Xiang",
            "Guo Yanming",
            "Lu Qianqi",
            "Zou Shiwei",
            "Yan Jie",
            "Chen Yan"
        ],
        "title": "Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning",
        "abstract": "arXiv:2509.06306v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) is an emerging and challenging open-world problem that has garnered increasing attention in recent years. Most existing GCD methods focus on discovering categories in static images. However, relying solely on static visual content is often insufficient to reliably discover novel categories. To bridge this gap, we extend the GCD problem to the video domain and introduce a new setting, termed Video-GCD. Thus, effectively integrating multi-perspective information across time is crucial for accurate Video-GCD. To tackle this challenge, we propose a novel Memory-guided Consistency-aware Contrastive Learning (MCCL) framework, which explicitly captures temporal-spatial cues and incorporates them into contrastive learning through a consistency-guided voting mechanism. MCCL consists of two core components: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided Representation Enhancement (MGRE). CACL exploits multiperspective temporal features to estimate consistency scores between unlabeled instances, which are then used to weight the contrastive loss accordingly. MGRE introduces a dual-level memory buffer that maintains both feature-level and logit-level representations, providing global context to enhance intra-class compactness and inter-class separability. This in turn refines the consistency estimation in CACL, forming a mutually reinforcing feedback loop between representation learning and consistency modeling. To facilitate a comprehensive evaluation, we construct a new and challenging Video-GCD benchmark, which includes action recognition and bird classification video datasets. Extensive experiments demonstrate that our method significantly outperforms competitive GCD approaches adapted from image-based settings, highlighting the importance of temporal information for discovering novel categories in videos. The code will be publicly available.",
        "arxiv_id": "2509.06306",
        "ARXIVID": "2509.06306",
        "COMMENT": "This paper extends Generalized Category Discovery (GCD) to the video domain (Video-GCD), proposes a new memory-guided consistency-aware contrastive learning framework, and introduces a new benchmark for video-based GCD. This matches criterion 3 (embodied AI papers on new benchmarks or methods, especially with novel angles) and criterion 4 (vision foundation models and applications), as it focuses on temporal-spatial understanding and unsupervised category discovery in videos.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.06767": {
        "authors": [
            "Zijie Ning",
            "Enmin Lin",
            "Sudarshan R. Iyengar",
            "Patrick Vandewalle"
        ],
        "title": "Raw2Event: Converting Raw Frame Camera into Event Camera",
        "abstract": "arXiv:2509.06767v1 Announce Type: new  Abstract: Event cameras offer unique advantages such as high temporal resolution, low latency, and high dynamic range, making them more and more popular for vision tasks under challenging light conditions. However, their high cost, limited resolution, and lack of features such as autofocus hinder their broad adoption, particularly for early-stage development and prototyping. In this work, we present Raw2Event, a complete hardware-software system that enables real-time event generation from low-cost raw frame-based cameras. By leveraging direct access to raw Bayer data and bypassing traditional image signal processors (ISP), our system is able to utilize the full potential of camera hardware, delivering higher dynamic range, higher resolution, and more faithful output than RGB-based frame-to-event converters.   Built upon the DVS-Voltmeter model, Raw2Event features a configurable simulation framework optimized for deployment on embedded platforms. We further design a data acquisition pipeline that supports synchronized recording of raw, RGB, and event streams, facilitating downstream evaluation and dataset creation. Experimental results show that Raw2Event can generate event streams closely resembling those from real event cameras, while benefiting from higher resolution and autofocus capabilities. The system also supports user-intuitive parameter tuning, enabling flexible adaptation to various application requirements. Finally, we deploy the system on a Raspberry Pi for real-time operation, providing a scalable and cost-effective solution for event-based vision research and early-stage system development.   The codes are available online: https://anonymous.4open.science/r/raw2event-BFF2/README.md.",
        "arxiv_id": "2509.06767",
        "ARXIVID": "2509.06767",
        "COMMENT": "Matches criterion 3 (embodied AI, new simulator/hardware for event-based vision). The paper presents a hardware-software system to convert raw frame cameras into event cameras, enabling new research and dataset creation for event-based vision, which is a novel angle for embodied vision systems.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.06409": {
        "authors": [
            "Yihong Luo",
            "Wenwu He",
            "Zhuo-Xu Cui",
            "Dong Liang"
        ],
        "title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning",
        "abstract": "arXiv:2509.06409v1 Announce Type: new  Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised fine-tuning to general-purpose vision-language models (VLMs) to emulate radiologists' stepwise diagnostic reasoning using only free-text reports. DiagCoT combines contrastive image-report tuning for domain alignment, chain-of-thought supervision to capture inferential logic, and reinforcement tuning with clinical reward signals to enhance factual accuracy and fluency. On the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08 to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33 (absolute gain of 0.22). It outperformed state-of-the-art models including LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By converting unstructured clinical narratives into structured supervision, DiagCoT offers a scalable approach for developing interpretable and diagnostically competent AI systems for radiology.",
        "arxiv_id": "2509.06409",
        "ARXIVID": "2509.06409",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models, applications). Presents DiagCoT, a framework for stepwise diagnostic reasoning in VLMs for radiology, with strong empirical improvements and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.05614": {
        "authors": [
            "Hanzhen Wang",
            "Jiaming Xu",
            "Jiayi Pan",
            "Yongkang Zhou",
            "Guohao Dai"
        ],
        "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
        "abstract": "arXiv:2509.05614v1 Announce Type: new  Abstract: Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.",
        "arxiv_id": "2509.05614",
        "ARXIVID": "2509.05614",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (embodied AI, new methods for VLA models, novel pruning using action-aware context). Proposes a new, training-free, action-aware pruning method for Vision-Language-Action models, with empirical results on LIBERO.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.06456": {
        "authors": [
            "Zongyi Xu",
            "Zhongpeng Lang",
            "Yilong Chen",
            "Shanshan Zhao",
            "Xiaoshui Huang",
            "Yifan Zuo",
            "Yan Zhang",
            "Qianni Zhang",
            "Xinbo Gao"
        ],
        "title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark",
        "abstract": "arXiv:2509.06456v1 Announce Type: new  Abstract: Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by the sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical lidar and a hybrid semi-solid-state lidar, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas. Then, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration. Extensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$, respectively, and improves the registration recall (RR) by $5.4\\%$, which validates its effectiveness in achieving accurate cross-source registration.",
        "arxiv_id": "2509.06456",
        "ARXIVID": "2509.06456",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks or methods, especially with novel angles). Introduces a large-scale real-world cross-source point cloud registration benchmark and a new registration method fusing visual and geometric information.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05949": {
        "authors": [
            "Qiqi Zhan",
            "Shiwei Li",
            "Qingjie Liu",
            "Yunhong Wang"
        ],
        "title": "AttriPrompt: Dynamic Prompt Composition Learning for CLIP",
        "abstract": "arXiv:2509.05949v1 Announce Type: new  Abstract: The evolution of prompt learning methodologies has driven exploration of deeper prompt designs to enhance model performance. However, current deep text prompting approaches suffer from two critical limitations: Over-reliance on constrastive learning objectives that prioritize high-level semantic alignment, neglecting fine-grained feature optimization; Static prompts across all input categories, preventing content-aware adaptation. To address these limitations, we propose AttriPrompt-a novel framework that enhances and refines textual semantic representations by leveraging the intermediate-layer features of CLIP's vision encoder. We designed an Attribute Retrieval module that first clusters visual features from each layer. The aggregated visual features retrieve semantically similar prompts from a prompt pool, which are then concatenated to the input of every layer in the text encoder. Leveraging hierarchical visual information embedded in prompted text features, we introduce Dual-stream Contrastive Learning to realize fine-grained alignment. Furthermore, we introduce a Self-Regularization mechanism by applying explicit regularization constraints between the prompted and non-prompted text features to prevent overfitting on limited training data. Extensive experiments across three benchmarks demonstrate AttriPrompt's superiority over state-of-the-art methods, achieving up to 7.37\\% improvement in the base-to-novel setting. The observed strength of our method in cross-domain knowledge transfer positions vision-language pre-trained models as more viable solutions for real-world implementation.",
        "arxiv_id": "2509.05949",
        "ARXIVID": "2509.05949",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new prompt learning method for CLIP, with dynamic prompt composition and fine-grained alignment, showing strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06685": {
        "authors": [
            "Shengkai Zhang",
            "Yuhe Liu",
            "Guanjun Wu",
            "Jianhua He",
            "Xinggang Wang",
            "Mozi Chen",
            "Kezhong Liu"
        ],
        "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes",
        "abstract": "arXiv:2509.06685v1 Announce Type: new  Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.",
        "arxiv_id": "2509.06685",
        "ARXIVID": "2509.06685",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and 4 (vision foundation models). Proposes a new method for monocular Gaussian Splatting in large scenes, leveraging foundation models for depth estimation and object-level guidance, with novel depth propagation and refinement.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06467": {
        "authors": [
            "Che Liu",
            "Yinda Chen",
            "Haoyuan Shi",
            "Jinpeng Lu",
            "Bailiang Jian",
            "Jiazhen Pan",
            "Linghan Cai",
            "Jiayi Wang",
            "Yundi Zhang",
            "Jun Li",
            "Cosmin I. Bercea",
            "Cheng Ouyang",
            "Chen Chen",
            "Zhiwei Xiong",
            "Benedikt Wiestler",
            "Christian Wachinger",
            "Daniel Rueckert",
            "Wenjia Bai",
            "Rossella Arcucci"
        ],
        "title": "Does DINOv3 Set a New Medical Vision Standard?",
        "abstract": "arXiv:2509.06467v1 Announce Type: new  Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.",
        "arxiv_id": "2509.06467",
        "ARXIVID": "2509.06467",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Systematic benchmarking of DINOv3 (a vision foundation model) in medical imaging, with surprising empirical findings about scaling and transfer.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05785": {
        "authors": [
            "In-Jae Lee",
            "Sihwan Hwang",
            "Youngseok Kim",
            "Wonjune Kim",
            "Sanmin Kim",
            "Dongsuk Kum"
        ],
        "title": "CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation",
        "abstract": "arXiv:2509.05785v1 Announce Type: new  Abstract: Recently, camera-radar fusion-based 3D object detection methods in bird's eye view (BEV) have gained attention due to the complementary characteristics and cost-effectiveness of these sensors. Previous approaches using forward projection struggle with sparse BEV feature generation, while those employing backward projection overlook depth ambiguity, leading to false positives. In this paper, to address the aforementioned limitations, we propose a novel camera-radar fusion-based 3D object detection and segmentation model named CRAB (Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based view transformation), using a backward projection that leverages radar to mitigate depth ambiguity. During the view transformation, CRAB aggregates perspective view image context features into BEV queries. It improves depth distinction among queries along the same ray by combining the dense but unreliable depth distribution from images with the sparse yet precise depth information from radar occupancy. We further introduce spatial cross-attention with a feature map containing radar context information to enhance the comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our proposed approach achieves a state-of-the-art performance among backward projection-based camera-radar fusion methods with 62.4\\% NDS and 54.0\\% mAP in 3D object detection.",
        "arxiv_id": "2509.05785",
        "ARXIVID": "2509.05785",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a novel camera-radar fusion method for 3D object detection, addressing depth ambiguity with new attention mechanisms and achieving SOTA results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05661": {
        "authors": [
            "Xiaomeng Zhu",
            "Changwei Wang",
            "Haozhe Wang",
            "Xinyu Liu",
            "Fangzhen Lin"
        ],
        "title": "OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation",
        "abstract": "arXiv:2509.05661v1 Announce Type: new  Abstract: A scene graph is a structured represention of objects and their relationships in a scene. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications as intelligent surveillance and human-machine collaboration. Existing SGA approaches primarily leverage visual cues, often struggling to integrate valuable commonsense knowledge, thereby limiting long-term prediction robustness. To explicitly leverage such commonsense knowledge, we propose a new approach to better understand the objects, concepts, and relationships in a scene graph. Our approach decouples the SGA task in two steps: first a scene graph capturing model is used to convert a video clip into a sequence of scene graphs, then a pure text-based model is used to predict scene graphs in future frames. Our focus in this work is on the second step, and we call it Linguistic Scene Graph Anticipation (LSGA) and believes it should have independent interest beyond the use in SGA discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method (OOTSM) where an Large Language Model (LLM) first forecasts object appearances and disappearances before generating detailed human-object relations. We conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o, GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome annotations. For SGA, we combine our OOTSM with STTran++ from, and our experiments demonstrate effective state-of-the-art performance: short-term mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.",
        "arxiv_id": "2509.05661",
        "ARXIVID": "2509.05661",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a decoupled linguistic framework for scene graph anticipation, leveraging LLMs for future scene graph prediction, and demonstrates significant improvements on benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05747": {
        "authors": [
            "Leo Ho",
            "Yinghao Huang",
            "Dafei Qin",
            "Mingyi Shi",
            "Wangpok Tse",
            "Wei Liu",
            "Junichi Yamagishi",
            "Taku Komura"
        ],
        "title": "InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios",
        "abstract": "arXiv:2509.05747v1 Announce Type: new  Abstract: We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ .",
        "arxiv_id": "2509.05747",
        "ARXIVID": "2509.05747",
        "COMMENT": "This paper introduces a large-scale multi-modal dataset (InterAct) for dynamic, expressive, and interactive activities between two people, including body, face, and audio. It also proposes a diffusion-based method for generating interactive motions and expressions. This is relevant to embodied AI (criterion 3), especially for new benchmarks and methods for modeling social interactions, which is a novel angle. The dataset and method are both new contributions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06818": {
        "authors": [
            "Yufeng Cheng",
            "Wenxu Wu",
            "Shaojin Wu",
            "Mengqi Huang",
            "Fei Ding",
            "Qian He"
        ],
        "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward",
        "abstract": "arXiv:2509.06818v1 Announce Type: new  Abstract: Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO",
        "arxiv_id": "2509.06818",
        "ARXIVID": "2509.06818",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). The paper proposes a new framework for multi-identity image customization using diffusion models and reinforcement learning, with a new dataset and metric for identity confusion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05604": {
        "authors": [
            "Jungin Park",
            "Jiyoung Lee",
            "Kwanghoon Sohn"
        ],
        "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization",
        "abstract": "arXiv:2509.05604v1 Announce Type: new  Abstract: Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at https://github.com/park-jungin/videograph.",
        "arxiv_id": "2509.05604",
        "ARXIVID": "2509.05604",
        "COMMENT": "Matches criterion 1 (spatial understanding via spatiotemporal graph modeling) and criterion 2 (language-guided video summarization with multi-modal models). The recursive spatiotemporal graph approach for language-guided video summarization is a novel method for spatial and temporal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06485": {
        "authors": [
            "Andrea Marelli",
            "Alberto Foresti",
            "Leonardo Pesce",
            "Giacomo Boracchi",
            "Mario Grosso"
        ],
        "title": "WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting",
        "abstract": "arXiv:2509.06485v1 Announce Type: new  Abstract: In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \\textit{before} and \\textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including \"before\" and \"after\" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.",
        "arxiv_id": "2509.06485",
        "ARXIVID": "2509.06485",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 3 (new benchmark/dataset for segmentation in an industrial setting). The paper introduces a new weakly supervised segmentation method using before-after supervision and a new dataset for waste sorting, which is a novel angle for spatial intelligence in industrial embodied systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06096": {
        "authors": [
            "Yiwen Ye",
            "Yicheng Wu",
            "Xiangde Luo",
            "He Zhang",
            "Ziyang Chen",
            "Ting Dang",
            "Yanning Zhang",
            "Yong Xia"
        ],
        "title": "MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation",
        "abstract": "arXiv:2509.06096v1 Announce Type: new  Abstract: Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released.",
        "arxiv_id": "2509.06096",
        "ARXIVID": "2509.06096",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes MedSeqFT, a sequential fine-tuning framework for adapting foundation models to new 3D medical segmentation tasks, with knowledge retention and transferability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06477": {
        "authors": [
            "Pengxiang Zhao",
            "Guangyi Liu",
            "Yaozhen Liang",
            "Weiqing He",
            "Zhengxi Lu",
            "Yuehao Huang",
            "Yaxuan Guo",
            "Kexin Zhang",
            "Hao Wang",
            "Liang Liu",
            "Yong Liu"
        ],
        "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents",
        "abstract": "arXiv:2509.06477v1 Announce Type: new  Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.",
        "arxiv_id": "2509.06477",
        "ARXIVID": "2509.06477",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark for GUI agents with hybrid shortcut-augmented actions). Introduces MAS-Bench, a new benchmark for evaluating hybrid GUI agents, focusing on shortcut generation and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05685": {
        "authors": [
            "Jian Yang",
            "Jiahui Wu",
            "Li Fang",
            "Hongchao Fan",
            "Bianying Zhang",
            "Huijie Zhao",
            "Guangyi Yang",
            "Rui Xin",
            "Xiong You"
        ],
        "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions",
        "abstract": "arXiv:2509.05685v1 Announce Type: new  Abstract: Transforming road network data into vector representations using deep learning has proven effective for road network analysis. However, urban road networks' heterogeneous and hierarchical nature poses challenges for accurate representation learning. Graph neural networks, which aggregate features from neighboring nodes, often struggle due to their homogeneity assumption and focus on a single structural scale. To address these issues, this paper presents MSRFormer, a novel road network representation learning framework that integrates multi-scale spatial interactions by addressing their flow heterogeneity and long-distance dependencies. It uses spatial flow convolution to extract small-scale features from large trajectory datasets, and identifies scale-dependent spatial interaction regions to capture the spatial structure of road networks and flow heterogeneity. By employing a graph transformer, MSRFormer effectively captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections, which are fed to a contrastive learning algorithm to derive the final road network representation. Validation on two real-world datasets demonstrates that MSRFormer outperforms baseline methods in two road network analysis tasks. The performance gains of MSRFormer suggest the traffic-related task benefits more from incorporating trajectory data, also resulting in greater improvements in complex road network structures with up to 16% improvements compared to the most competitive baseline method. This research provides a practical framework for developing task-agnostic road network representation models and highlights distinct association patterns of the interplay between scale effects and flow heterogeneity of spatial interactions.",
        "arxiv_id": "2509.05685",
        "ARXIVID": "2509.05685",
        "COMMENT": "MSRFormer is a new framework for road network representation learning using multi-scale feature fusion of heterogeneous spatial interactions, employing spatial flow convolution and graph transformers. This is a methodological improvement for spatial understanding and representation, matching criterion 1 (spatial understanding on embodied agents).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06291": {
        "authors": [
            "Jiangnan Xie",
            "Xiaolong Zheng",
            "Liang Zheng"
        ],
        "title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding",
        "abstract": "arXiv:2509.06291v1 Announce Type: new  Abstract: Visual Grounding (VG) aims to utilize given natural language queries to locate specific target objects within images. While current transformer-based approaches demonstrate strong localization performance in standard scene (i.e, scenarios without any novel objects), they exhibit notable limitations in open-vocabulary scene (i.e, both familiar and novel object categories during testing). These limitations primarily stem from three key factors: (1) imperfect alignment between visual and linguistic modalities, (2) insufficient cross-modal feature fusion, and (3) ineffective utilization of semantic prototype information. To overcome these challenges, we present Prototype-Aware Multimodal Learning (PAML), an innovative framework that systematically addresses these issues through several key components: First, we leverage ALBEF to establish robust cross-modal alignment during initial feature encoding. Subsequently, our Visual Discriminative Feature Encoder selectively enhances salient object representations while suppressing irrelevant visual context. The framework then incorporates a novel prototype discovering and inheriting mechanism that extracts and aggregates multi-neighbor semantic prototypes to facilitate open-vocabulary recognition. These enriched features undergo comprehensive multimodal integration through our Multi-stage Decoder before final bounding box regression. Extensive experiments across five benchmark datasets validate our approach, showing competitive performance in standard scene while achieving state-of-the-art results in open-vocabulary scene. Our code is available at https://github.com/plankXie/PAML.",
        "arxiv_id": "2509.06291",
        "ARXIVID": "2509.06291",
        "COMMENT": "This paper proposes Prototype-Aware Multimodal Learning (PAML) for open-vocabulary visual grounding, addressing cross-modal alignment and semantic prototype utilization. It leverages ALBEF and introduces a prototype discovering mechanism, achieving SOTA in open-vocabulary settings. This matches criterion 2 (new MLLMs/VLLMs) and is relevant for vision-language modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05342": {
        "authors": [
            "Gaspard Beaudouin",
            "Minghan Li",
            "Jaeyeon Kim",
            "Sunghoon Yoon",
            "Mengyu Wang"
        ],
        "title": "Delta Velocity Rectified Flow for Text-to-Image Editing",
        "abstract": "arXiv:2509.05342v1 Announce Type: new  Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.",
        "arxiv_id": "2509.05342",
        "ARXIVID": "2509.05342",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes Delta Velocity Rectified Flow, a new method for text-to-image editing in rectified flow models, bridging score-based diffusion and velocity-based optimization, with strong empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.05970": {
        "authors": [
            "Ye Wang",
            "Zili Yi",
            "Yibo Zhang",
            "Peng Zheng",
            "Xuping Xie",
            "Jiang Lin",
            "Yilin Wang",
            "Rui Ma"
        ],
        "title": "OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization",
        "abstract": "arXiv:2509.05970v1 Announce Type: new  Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by reframing it as a data problem. Our key insight is destylization, reversing style transfer by removing stylistic elements from artworks to recover natural, style-free counterparts. This yields DST-100K, a large-scale dataset that provides authentic supervision signals by aligning real artistic styles with their underlying content. To build DST-100K, we develop (1) DST, a text-guided destylization model that reconstructs stylefree content, and (2) DST-Filter, a multi-stage evaluation model that employs Chain-of-Thought reasoning to automatically discard low-quality pairs while ensuring content fidelity and style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently surpasses state-of-the-art methods across both qualitative and quantitative benchmarks. Our results demonstrate that scalable data generation via destylization provides a reliable supervision paradigm, overcoming the fundamental challenge posed by the lack of ground-truth data in artistic style transfer.",
        "arxiv_id": "2509.05970",
        "ARXIVID": "2509.05970",
        "COMMENT": "This paper introduces OmniStyle2, a new approach to artistic style transfer using a large-scale destylization dataset (DST-100K) and a simple feed-forward model. The work is relevant to vision foundation models and their applications (criterion 4), especially in generative modeling and data-centric approaches. The destylization and data filtering pipeline is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.06040": {
        "authors": [
            "Yuming Li",
            "Yikai Wang",
            "Yuying Zhu",
            "Zhongyu Zhao",
            "Ming Lu",
            "Qi She",
            "Shanghang Zhang"
        ],
        "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models",
        "abstract": "arXiv:2509.06040v1 Announce Type: new  Abstract: Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.",
        "arxiv_id": "2509.06040",
        "ARXIVID": "2509.06040",
        "COMMENT": "This paper proposes BranchGRPO, a new method for efficient and stable preference alignment in diffusion models for image and video generation. It introduces a branch sampling scheme, tree-based advantage estimator, and pruning strategies. This is relevant to vision foundation models and generative modeling (criterion 4), with a clever statistical trick for efficiency and stability in training.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.05321": {
        "authors": [
            "Yunfei Guo",
            "Tao Zhang",
            "Wu Huang",
            "Yao Song"
        ],
        "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD",
        "abstract": "arXiv:2509.05321v1 Announce Type: new  Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.",
        "arxiv_id": "2509.05321",
        "ARXIVID": "2509.05321",
        "COMMENT": "Matches criterion 2 (multi-modal large models) and criterion 3 (new dataset generation scheme for multi-modal learning with EEG and video). The paper introduces a framework for generating EEG signals conditioned on video, enabling new multi-modal research directions.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.05469": {
        "authors": [
            "Chenguang Wang",
            "Xiang Yan",
            "Yilong Dai",
            "Ziyi Wang",
            "Susu Xu"
        ],
        "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation",
        "abstract": "arXiv:2509.05469v1 Announce Type: new  Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.",
        "arxiv_id": "2509.05469",
        "ARXIVID": "2509.05469",
        "COMMENT": "This paper introduces a multi-agent pipeline for generating street design scenarios from real-world imagery, integrating lane localization, prompt optimization, and automated evaluation. It is a novel application of multi-agent systems and generative models for spatial design, and touches on spatial understanding and generative modeling for infrastructure. This matches criterion 1 (spatial understanding on embodied agents) and is relevant for generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.06678": {
        "authors": [
            "Cailei Liang",
            "Adrian Bodenmann",
            "Sam Fenton",
            "Blair Thornton"
        ],
        "title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations",
        "abstract": "arXiv:2509.06678v1 Announce Type: new  Abstract: As long-endurance and seafloor-resident AUVs become more capable, there is an increasing need for extended, real-time interpretation of seafloor imagery to enable adaptive missions and optimise communication efficiency. Although offline image analysis methods are well established, they rely on access to complete datasets and human-labelled examples to manage the strong influence of environmental and operational conditions on seafloor image appearance-requirements that cannot be met in real-time settings. To address this, we introduce an online clustering framework (OCF) capable of interpreting seafloor imagery without supervision, which is designed to operate in real-time on continuous data streams in a scalable, adaptive, and self-consistent manner. The method enables the efficient review and consolidation of common patterns across the entire data history in constant time by identifying and maintaining a set of representative samples that capture the evolving feature distribution, supporting dynamic cluster merging and splitting without reprocessing the full image history. We evaluate the framework on three diverse seafloor image datasets, analysing the impact of different representative sampling strategies on both clustering accuracy and computational cost. The OCF achieves the highest average F1 score of 0.68 across the three datasets among all comparative online clustering approaches, with a standard deviation of 3% across three distinct survey trajectories, demonstrating its superior clustering capability and robustness to trajectory variation. In addition, it maintains consistently lower and bounded computational time as the data volume increases. These properties are beneficial for generating survey data summaries and supporting informative path planning in long-term, persistent autonomous marine exploration.",
        "arxiv_id": "2509.06678",
        "ARXIVID": "2509.06678",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents), as it introduces an online clustering framework for real-time seafloor imagery interpretation during AUV operations, supporting adaptive missions and path planning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.05992": {
        "authors": [
            "Zekun Zhou",
            "Yanru Gong",
            "Liu Shi",
            "Qiegen Liu"
        ],
        "title": "Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction",
        "abstract": "arXiv:2509.05992v1 Announce Type: new  Abstract: Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37\\% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression.",
        "arxiv_id": "2509.05992",
        "ARXIVID": "2509.05992",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and generative modeling. Proposes a new guided diffusion model for sparse-view CT reconstruction with several novel statistical tricks for guidance and correction.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.06336": {
        "authors": [
            "Jeongmin Yu",
            "Susang Kim",
            "Kisu Lee",
            "Taekyoung Kwon",
            "Won-Yong Shin",
            "Ha Young Kim"
        ],
        "title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing",
        "abstract": "arXiv:2509.06336v1 Announce Type: new  Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: https://github.com/Elune001/MVP-FAS.",
        "arxiv_id": "2509.06336",
        "ARXIVID": "2509.06336",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new framework for face anti-spoofing using CLIP-based multi-view slot attention and multi-text patch alignment, leveraging paraphrased texts for better generalization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.06413": {
        "authors": [
            "Yixiao Li",
            "Xin Li",
            "Chris Wei Zhou",
            "Shuo Xing",
            "Hadi Amirpour",
            "Xiaoshuai Hao",
            "Guanghui Yue",
            "Baoquan Zhao",
            "Weide Liu",
            "Xiaoyuan Yang",
            "Zhengzhong Tu",
            "Xinyu Li",
            "Chuanbiao Song",
            "Chenqi Zhang",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Xiaoyan Sun",
            "Shishun Tian",
            "Dongyang Yan",
            "Weixia Zhang",
            "Junlin Chen",
            "Wei Sun",
            "Zhihua Wang",
            "Zhuohang Shi",
            "Zhizun Luo",
            "Hang Ouyang",
            "Tianxin Xiao",
            "Fan Yang",
            "Zhaowang Wu",
            "Kaixin Deng"
        ],
        "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results",
        "abstract": "arXiv:2509.06413v1 Announce Type: new  Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.",
        "arxiv_id": "2509.06413",
        "ARXIVID": "2509.06413",
        "COMMENT": "Matches criterion 3 (embodied AI papers on new benchmarks/simulators) and criterion 4 (vision foundation models and applications). Presents a new challenge and dataset for super-resolution image quality assessment, focusing on generative models (GANs, diffusion).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.05913": {
        "authors": [
            "Md. Abdur Rahman",
            "Mohaimenul Azam Khan Raiaan",
            "Tamanna Shermin",
            "Md Rafiqul Islam",
            "Mukhtar Hussain",
            "Sami Azam"
        ],
        "title": "A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features",
        "abstract": "arXiv:2509.05913v1 Announce Type: new  Abstract: Musculoskeletal disorders pose significant risks to athletes, and assessing risk early is important for prevention. However, most existing methods are designed for controlled settings and fail to reliably assess risk in complex environments due to their reliance on a single type of data. This research proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel multimodal deep learning framework designed to classify musculoskeletal risk using visual and skeletal coordinate-based features. In addition, a custom multimodal dataset is constructed by combining visual data and skeletal coordinates for risk assessment. Each sample is labeled into eight risk categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines a Residual Block with a Lightweight Transformer Block to learn spatial and temporal dependencies jointly. It incorporates two novel modules: the Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature refinement through cross-attention between visual and skeletal inputs, and the Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal coherence by aligning image features with coordinate-based representations. ViSK-GAT achieved strong performance with validation and test accuracies of 93.55\\% and 93.89\\%, respectively; a precision of 93.86\\%; an F1 score of 93.85\\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\\%. The regression results also indicated a low Root Mean Square Error of the predicted probability distribution of 0.1205 and a corresponding Mean Absolute Error of 0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT consistently outperformed previous methods. The ViSK-GAT model advances artificial intelligence implementation and application, transforming musculoskeletal risk classification and enabling impactful early interventions in sports.",
        "arxiv_id": "2509.05913",
        "ARXIVID": "2509.05913",
        "COMMENT": "Matches criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications). Proposes a novel multimodal deep learning framework (ViSK-GAT) for musculoskeletal risk classification using visual and skeletal features, with new attention and geometric correspondence modules.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.06536": {
        "authors": [
            "Senem Aktas",
            "Charles Markham",
            "John McDonald",
            "Rozenn Dahyot"
        ],
        "title": "Benchmarking EfficientTAM on FMO datasets",
        "abstract": "arXiv:2509.06536v1 Announce Type: new  Abstract: Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.",
        "arxiv_id": "2509.06536",
        "ARXIVID": "2509.06536",
        "COMMENT": "This paper benchmarks EfficientTAM, a foundational model for tracking, on fast moving object datasets and introduces new metadata and evaluation protocols. This is relevant to vision foundation models and their applications (criterion 4), especially in object tracking, and provides new evaluation resources.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.06033": {
        "authors": [
            "Nadia Bakhsheshi",
            "Hamid Beigy"
        ],
        "title": "Analysis of Blood Report Images Using General Purpose Vision-Language Models",
        "abstract": "arXiv:2509.06033v1 Announce Type: new  Abstract: The reliable analysis of blood reports is important for health knowledge, but individuals often struggle with interpretation, leading to anxiety and overlooked issues. We explore the potential of general-purpose Vision-Language Models (VLMs) to address this challenge by automatically analyzing blood report images. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini 2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of 100 diverse blood report images. Each model was prompted with clinically relevant questions adapted to each blood report. The answers were then processed using Sentence-BERT to compare and evaluate how closely the models responded. The findings suggest that general-purpose VLMs are a practical and promising technology for developing patient-facing tools for preliminary blood report analysis. Their ability to provide clear interpretations directly from images can improve health literacy and reduce the limitations to understanding complex medical information. This work establishes a foundation for the future development of reliable and accessible AI-assisted healthcare applications. While results are encouraging, they should be interpreted cautiously given the limited dataset size.",
        "arxiv_id": "2509.06033",
        "ARXIVID": "2509.06033",
        "COMMENT": "Matches criterion 2 (evaluation of new VLLMs for medical image understanding). The paper compares three recent VLLMs on blood report image analysis, which is a direct application of vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2509.06733": {
        "authors": [
            "Wenjun Li",
            "Zhi Chen",
            "Jingru Lin",
            "Hannan Cao",
            "Wei Han",
            "Sheng Liang",
            "Zhi Zhang",
            "Kuicai Dong",
            "Dexun Li",
            "Chen Zhang",
            "Yong Liu"
        ],
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
        "abstract": "arXiv:2509.06733v1 Announce Type: new  Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
        "arxiv_id": "2509.06733",
        "ARXIVID": "2509.06733",
        "COMMENT": "Touches on criterion 2 (VLLMs/MLLMs) and criterion 3 (embodied AI benchmarks/methods) by surveying RL foundations for agentic research systems, including multimodal integration and evaluation benchmarks such as VQA and tool-interaction tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.06482": {
        "authors": [
            "Zhongxiang Xie",
            "Shuangxi Miao",
            "Yuhan Jiang",
            "Zhewei Zhang",
            "Jing Yao",
            "Xuecao Li",
            "Jianxi Huang",
            "Pedram Ghamisi"
        ],
        "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection",
        "abstract": "arXiv:2509.06482v1 Announce Type: new  Abstract: Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.",
        "arxiv_id": "2509.06482",
        "ARXIVID": "2509.06482",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding) and 4 (vision foundation models), as it proposes a new method for spatial change detection in remote sensing using frequency-spatial synergy, but not directly about embodied agents or foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.06160": {
        "authors": [
            "Haozhe Wang",
            "Haoran Que",
            "Qixin Xu",
            "Minghao Liu",
            "Wangchunshu Zhou",
            "Jiazhan Feng",
            "Wanjun Zhong",
            "Wei Ye",
            "Tong Yang",
            "Wenhao Huang",
            "Ge Zhang",
            "Fangzhen Lin"
        ],
        "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
        "abstract": "arXiv:2509.06160v1 Announce Type: new  Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.",
        "arxiv_id": "2509.06160",
        "ARXIVID": "2509.06160",
        "COMMENT": "This paper introduces REverse-Engineered Reasoning (REER), a new paradigm for open-ended generation by working backwards from solutions to discover reasoning steps, and releases a large dataset (DeepWriting-20K). While it is a novel approach to reasoning in LLMs, it does not focus on vision, multi-modal, or embodied AI, so it is not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 8
    },
    "2509.05512": {
        "authors": [
            "Bryce Grant",
            "Peng Wang"
        ],
        "title": "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection",
        "abstract": "arXiv:2509.05512v1 Announce Type: new  Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep learning framework that leverages quaternion algebra for rotation equivariant image classification and object detection. Unlike conventional quaternion neural networks attempting to operate entirely in the quaternion domain, QUAN approximates quaternion convolution through Hamilton product decomposition using real-valued operations. This approach preserves geometric properties while enabling efficient implementation with custom CUDA kernels. We introduce Independent Quaternion Batch Normalization (IQBN) for training stability and extend quaternion operations to spatial attention mechanisms. QUAN is evaluated on image classification (CIFAR-10/100, ImageNet), object detection (COCO, DOTA), and robotic perception tasks. In classification tasks, QUAN achieves higher accuracy with fewer parameters and faster convergence compared to existing convolution and quaternion-based models. For objection detection, QUAN demonstrates improved parameter efficiency and rotation handling over standard Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion CNNs in this downstream task. These results highlight its potential for deployment in resource-constrained robotic systems requiring rotation-aware perception and application in other domains.",
        "arxiv_id": "2509.05512",
        "ARXIVID": "2509.05512",
        "COMMENT": "This paper introduces Quaternion Approximate Networks (QUAN), a deep learning framework leveraging quaternion algebra for rotation-equivariant image classification and object detection, with applications in robotic perception. While it is a novel method for vision tasks and rotation handling, it does not directly address spatial intelligence on embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models, but is relevant for vision and robotics.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.05592": {
        "authors": [
            "Changtao Miao",
            "Yi Zhang",
            "Man Luo",
            "Weiwei Feng",
            "Kaiyuan Zheng",
            "Qi Chu",
            "Tao Gong",
            "Jianshu Li",
            "Yunfeng Diao",
            "Wei Zhou",
            "Joey Tianyi Zhou",
            "Xiaoshuai Hao"
        ],
        "title": "MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios",
        "abstract": "arXiv:2509.05592v1 Announce Type: new  Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have enabled increasingly sophisticated face forgeries, posing a significant threat to social security. However, current Deepfake detection methods are limited by constraints in existing datasets, which lack the diversity necessary in real-world scenarios. Specifically, these data sets fall short in four key areas: unknown of advanced forgery techniques, variability of facial scenes, richness of real data, and degradation of real-world propagation. To address these challenges, we propose the Multi-dimensional Face Forgery Image (\\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation Operations. MFFI integrates $50$ different forgery methods and contains $1024K$ image samples. Benchmark evaluations show that MFFI outperforms existing public datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. These results validate the technical advance and practical utility of MFFI in simulating real-world conditions. The dataset and additional details are publicly available at {https://github.com/inclusionConf/MFFI}.",
        "arxiv_id": "2509.05592",
        "ARXIVID": "2509.05592",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications). Introduces a new large-scale, diverse dataset for face forgery detection, which is important for real-world applications but not a new method or model.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.06660": {
        "authors": [
            "Cailei Liang",
            "Adrian Bodenmann",
            "Emma J Curtis",
            "Samuel Simmons",
            "Kazunori Nagano",
            "Stan Brown",
            "Adam Riese",
            "Blair Thornton"
        ],
        "title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery",
        "abstract": "arXiv:2509.06660v1 Announce Type: new  Abstract: High-throughput interpretation of robotically gathered seafloor visual imagery can increase the efficiency of marine monitoring and exploration. Although recent research has suggested that location metadata can enhance self-supervised feature learning (SSL), its benefits across different SSL strategies, models and seafloor image datasets are underexplored. This study evaluates the impact of location-based regularisation on six state-of-the-art SSL frameworks, which include Convolutional Neural Network (CNN) and Vision Transformer (ViT) models with varying latent-space dimensionality. Evaluation across three diverse seafloor image datasets finds that location-regularisation consistently improves downstream classification performance over standard SSL, with average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for ViTs, respectively. While CNNs pretrained on generic datasets benefit from high-dimensional latent representations, dataset-optimised SSL achieves similar performance across the high (512) and low (128) dimensional latent representations. Location-regularised SSL improves CNN performance over pre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and low-dimensional latent representations, respectively. For ViTs, high-dimensionality benefits both pre-trained and dataset-optimised SSL. Although location-regularisation improves SSL performance compared to standard SSL methods, pre-trained ViTs show strong generalisation, matching the best-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$ and $0.795 \\pm 0.077$, respectively. The findings highlight the value of location metadata for SSL regularisation, particularly when using low-dimensional latent representations, and demonstrate strong generalisation of high-dimensional ViTs for seafloor image analysis.",
        "arxiv_id": "2509.06660",
        "ARXIVID": "2509.06660",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Investigates location-regularized self-supervised learning for seafloor imagery, showing empirical improvements and insights into SSL strategies and model dimensionality.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.06442": {
        "authors": [
            "Yixiao Li",
            "Xiaoyuan Yang",
            "Guanghui Yue",
            "Jun Fu",
            "Qiuping Jiang",
            "Xu Jia",
            "Paul L. Rosin",
            "Hantao Liu",
            "Wei Zhou"
        ],
        "title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment",
        "abstract": "arXiv:2509.06442v1 Announce Type: new  Abstract: Many super-resolution (SR) algorithms have been proposed to increase image resolution. However, full-reference (FR) image quality assessment (IQA) metrics for comparing and evaluating different SR algorithms are limited. In this work, we propose the Perception-oriented Bidirectional Attention Network (PBAN) for image SR FR-IQA, which is composed of three modules: an image encoder module, a perception-oriented bidirectional attention (PBA) module, and a quality prediction module. First, we encode the input images for feature representations. Inspired by the characteristics of the human visual system, we then construct the perception-oriented PBA module. Specifically, different from existing attention-based SR IQA methods, we conceive a Bidirectional Attention to bidirectionally construct visual attention to distortion, which is consistent with the generation and evaluation processes of SR images. To further guide the quality assessment towards the perception of distorted information, we propose Grouped Multi-scale Deformable Convolution, enabling the proposed method to adaptively perceive distortion. Moreover, we design Sub-information Excitation Convolution to direct visual perception to both sub-pixel and sub-channel attention. Finally, the quality prediction module is exploited to integrate quality-aware features and regress quality scores. Extensive experiments demonstrate that our proposed PBAN outperforms state-of-the-art quality assessment methods.",
        "arxiv_id": "2509.06442",
        "ARXIVID": "2509.06442",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new perception-oriented bidirectional attention network for image super-resolution quality assessment, with novel attention and convolution modules inspired by human visual system.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.05954": {
        "authors": [
            "Weichao Wang",
            "Wendong Mao",
            "Zhongfeng Wang"
        ],
        "title": "StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud",
        "abstract": "arXiv:2509.05954v1 Announce Type: new  Abstract: The deployment of high-accuracy 3D object detection models from point cloud remains a significant challenge due to their substantial computational and memory requirements. To address this, we introduce StripDet, a novel lightweight framework designed for on-device efficiency. First, we propose the novel Strip Attention Block (SAB), a highly efficient module designed to capture long-range spatial dependencies. By decomposing standard 2D convolutions into asymmetric strip convolutions, SAB efficiently extracts directional features while reducing computational complexity from quadratic to linear. Second, we design a hardware-friendly hierarchical backbone that integrates SAB with depthwise separable convolutions and a simple multiscale fusion strategy, achieving end-to-end efficiency. Extensive experiments on the KITTI dataset validate StripDet's superiority. With only 0.65M parameters, our model achieves a 79.97% mAP for car detection, surpassing the baseline PointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms recent lightweight and knowledge distillation-based methods, achieving a superior accuracy-efficiency trade-off while establishing itself as a practical solution for real-world 3D detection on edge devices.",
        "arxiv_id": "2509.05954",
        "ARXIVID": "2509.05954",
        "COMMENT": "Somewhat relevant to vision foundation models (criterion 4) as it proposes a new lightweight 3D object detection model, but does not directly address foundation models or multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.06917": {
        "authors": [
            "Jiacheng Miao",
            "Joe R. Davis",
            "Jonathan K. Pritchard",
            "James Zou"
        ],
        "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents",
        "abstract": "arXiv:2509.06917v1 Announce Type: new  Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.",
        "arxiv_id": "2509.06917",
        "ARXIVID": "2509.06917",
        "COMMENT": "Paper2Agent presents a framework to convert research papers into interactive AI agents, automating the process of turning code and methods into usable agents. While innovative, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.06740": {
        "authors": [
            "Qing Xu",
            "Wenting Duan",
            "Zhen Chen"
        ],
        "title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation",
        "abstract": "arXiv:2509.06740v1 Announce Type: new  Abstract: Histopathology image analysis is critical yet challenged by the demand of segmenting tissue regions and nuclei instances for tumor microenvironment and cellular morphology analysis. Existing studies focused on tissue semantic segmentation or nuclei instance segmentation separately, but ignored the inherent relationship between these two tasks, resulting in insufficient histopathology understanding. To address this issue, we propose a Co-Seg framework for collaborative tissue and nuclei segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing tissue and nuclei segmentation tasks to mutually enhance each other. To this end, we first devise a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and instance region prompts as prior constraints. Moreover, we design a mutual prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, collaboratively computing semantic and instance segmentation masks. Extensive experiments on the PUMA dataset demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the semantic, instance and panoptic segmentation of tumor tissues and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg.",
        "arxiv_id": "2509.06740",
        "ARXIVID": "2509.06740",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it proposes a new segmentation framework with prompt-based modules, but does not directly address vision foundation models or multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.05892": {
        "authors": [
            "Phongsakon Mark Konrad",
            "Andrei-Alexandru Popa",
            "Yaser Sabzehmeidani",
            "Liang Zhong",
            "Elisa A. Liehn",
            "Serkan Ayvaz"
        ],
        "title": "Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets",
        "abstract": "arXiv:2509.05892v1 Announce Type: new  Abstract: Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.",
        "arxiv_id": "2509.05892",
        "ARXIVID": "2509.05892",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications), as it benchmarks foundation models (SAM, MedSAM) for small organ segmentation, but focus is on benchmarking in low-data settings rather than new methods.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2509.06341": {
        "authors": [
            "Issue Yishu Wang",
            "Kakam Chong",
            "Xiaofeng Wang",
            "Xu Yan",
            "DeXin Kong",
            "Chen Ju",
            "Ming Chen",
            "Shuai Xiao",
            "Shuguang Han",
            "jufeng chen"
        ],
        "title": "Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent",
        "abstract": "arXiv:2509.06341v1 Announce Type: new  Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part of seller-buyer interactions. Large Language Models (LLMs) can act as seller agents, negotiating with buyers on behalf of sellers under given business constraints. A critical ability for such agents is to track and accurately interpret cumulative buyer intents across long negotiations, which directly impacts bargaining effectiveness. We introduce a multi-turn evaluation framework for measuring the bargaining ability of seller agents in e-commerce dialogues. The framework tests whether an agent can extract and track buyer intents. Our contributions are: (1) a large-scale e-commerce bargaining benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated buyer intents, moving beyond outcome-only metrics; and (3) an automated pipeline that extracts reliable intent from massive dialogue data.",
        "arxiv_id": "2509.06341",
        "ARXIVID": "2509.06341",
        "COMMENT": "This paper introduces a multi-turn evaluation framework for LLM-based seller agents in e-commerce bargaining, with a new benchmark and intent extraction pipeline. While it is a new benchmark for LLMs, it is not multi-modal or vision-based, so it does not match the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.05324": {
        "authors": [
            "Rongqian Chen",
            "Shu Hong",
            "Rifatul Islam",
            "Mahdi Imani",
            "G. Gary Tan",
            "Tian Lan"
        ],
        "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality",
        "abstract": "arXiv:2509.05324v1 Announce Type: new  Abstract: Augmented reality (AR) systems are increasingly deployed in tactical environments, but their reliance on seamless human-computer interaction makes them vulnerable to cognitive attacks that manipulate a user's perception and severely compromise user decision-making. To address this challenge, we introduce the Perception Graph, a novel model designed to reason about human perception within these systems. Our model operates by first mimicking the human process of interpreting key information from an MR environment and then representing the outcomes using a semantically meaningful structure. We demonstrate how the model can compute a quantitative score that reflects the level of perception distortion, providing a robust and measurable method for detecting and analyzing the effects of such cognitive attacks.",
        "arxiv_id": "2509.05324",
        "ARXIVID": "2509.05324",
        "COMMENT": "Somewhat relevant to spatial understanding (criterion 1) as it models human perception in AR, but the focus is on cognitive attack reasoning rather than spatial intelligence or embodied AI benchmarks.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.06082": {
        "authors": [
            "Anuraag Mishra",
            "Andrea Gilch",
            "Benjamin Apeleo Zubiri",
            "Jan Rolfes",
            "Frauke Liers"
        ],
        "title": "High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization",
        "abstract": "arXiv:2509.06082v1 Announce Type: new  Abstract: In this work, we develop a novel technique for reconstructing images from projection-based nano- and microtomography. Our contribution focuses on enhancing reconstruction quality, particularly for specimen composed of homogeneous material phases connected by sharp edges. This is accomplished by training a neural network to identify edges within subpictures. The trained network is then integrated into a mathematical optimization model, to reduce artifacts from previous reconstructions. To this end, the optimization approach favors solutions according to the learned predictions, however may also determine alternative solutions if these are strongly supported by the raw data. Hence, our technique successfully incorporates knowledge about the homogeneity and presence of sharp edges in the sample and thereby eliminates blurriness. Our results on experimental datasets show significant enhancements in interface sharpness and material homogeneity compared to benchmark algorithms. Thus, our technique produces high-quality reconstructions, showcasing its potential for advancing tomographic imaging techniques.",
        "arxiv_id": "2509.06082",
        "ARXIVID": "2509.06082",
        "COMMENT": "Somewhat related to vision foundation models (criterion 4) via neural networks for tomographic image reconstruction, but not directly about foundation models or multi-modal learning. Focuses on integrating neural networks with optimization for image reconstruction.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.05550": {
        "authors": [
            "Zixi Li"
        ],
        "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation",
        "abstract": "arXiv:2509.05550v1 Announce Type: new  Abstract: We introduce TreeGPT, a novel neural architecture that combines transformer-based attention mechanisms with global parent-child aggregation for processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks. Unlike traditional approaches that rely solely on sequential processing or graph neural networks, TreeGPT employs a hybrid design that leverages both self-attention for capturing local dependencies and a specialized Tree Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures through iterative message passing.   The core innovation lies in our Global Parent-Child Aggregation mechanism, formalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in E_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This formulation enables each node to progressively aggregate information from the entire tree structure through $T$ iterations.   Our architecture integrates optional enhancements including gated aggregation with learnable edge weights, residual connections for gradient stability, and bidirectional propagation for capturing both bottom-up and top-down dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging visual reasoning benchmark requiring abstract pattern recognition and rule inference. Experimental results demonstrate that TreeGPT achieves 96\\% accuracy, significantly outperforming transformer baselines (1.3\\%), large-scale models like Grok-4 (15.9\\%), and specialized program synthesis methods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive ablation study reveals that edge projection is the most critical component, with the combination of edge projection and gating achieving optimal performance.",
        "arxiv_id": "2509.05550",
        "ARXIVID": "2509.05550",
        "COMMENT": "Does not match any specific criterion. Focuses on program synthesis and abstract syntax tree processing, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.06444": {
        "authors": [
            "Cheng Qian",
            "Hainan Zhang",
            "Yongxin Tong",
            "Hong-Wei Zheng",
            "Zhiming Zheng"
        ],
        "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data",
        "abstract": "arXiv:2509.06444v1 Announce Type: new  Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive data, especially in distributed healthcare settings where patient data spans SQL, knowledge graphs, and clinical notes. Clinicians face difficulties retrieving rare disease cases due to privacy constraints and the limitations of traditional cloud-based RAG systems in handling diverse formats and edge devices. To address this, we introduce HyFedRAG, a unified and efficient Federated RAG framework tailored for Hybrid data modalities. By leveraging an edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across diverse data sources while preserving data privacy. Our key contributions are: (1) We design an edge-cloud collaborative RAG framework built on Flower, which supports querying structured SQL data, semi-structured knowledge graphs, and unstructured documents. The edge-side LLMs convert diverse data into standardized privacy-preserving representations, and the server-side LLMs integrates them for global reasoning and generation. (2) We integrate lightweight local retrievers with privacy-aware LLMs and provide three anonymization tools that enable each client to produce semantically rich, de-identified summaries for global inference across devices. (3) To optimize response latency and reduce redundant computation, we design a three-tier caching strategy consisting of local cache, intermediate representation cache, and cloud inference cache. Experimental results on PMC-Patients demonstrate that HyFedRAG outperforms existing baselines in terms of retrieval quality, generation consistency, and system efficiency. Our framework offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments.",
        "arxiv_id": "2509.06444",
        "ARXIVID": "2509.06444",
        "COMMENT": "This paper introduces a federated retrieval-augmented generation (RAG) framework for heterogeneous and privacy-sensitive data, with edge-cloud LLMs and privacy-aware mechanisms. While it is a new method for LLMs and RAG, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.05630": {
        "authors": [
            "Moqsadur Rahman",
            "Saurav Kumar",
            "Santosh S. Palmate",
            "M. Shahriar Hossain"
        ],
        "title": "Self-supervised Learning for Hyperspectral Images of Trees",
        "abstract": "arXiv:2509.05630v1 Announce Type: new  Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.",
        "arxiv_id": "2509.05630",
        "ARXIVID": "2509.05630",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it discusses self-supervised learning for hyperspectral images, but is more focused on remote sensing and agriculture applications.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.06283": {
        "authors": [
            "Xuan-Phi Nguyen",
            "Shrey Pandit",
            "Revanth Gangi Reddy",
            "Austin Xu",
            "Silvio Savarese",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents",
        "abstract": "arXiv:2509.06283v1 Announce Type: new  Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (``thinking'') models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies.",
        "arxiv_id": "2509.06283",
        "ARXIVID": "2509.06283",
        "COMMENT": "Somewhat related to criterion 2 (VLLMs/MLLMs) as it discusses reinforcement learning for reasoning-optimized LLM agents, but not specifically vision or multi-modal. More focused on language and agentic reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.05582": {
        "authors": [
            "Zhiling Ye",
            "Cong Zhou",
            "Xiubao Zhang",
            "Haifeng Shen",
            "Weihong Deng",
            "Quan Lu"
        ],
        "title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian Head",
        "abstract": "arXiv:2509.05582v1 Announce Type: new  Abstract: In this paper, we explore a reconstruction and reenactment separated framework for 3D Gaussians head, which requires only a single portrait image as input to generate controllable avatar. Specifically, we developed a large-scale one-shot gaussian head generator built upon WebSSL and employed a two-stage training approach that significantly enhances the capabilities of generalization and high-frequency texture reconstruction. During inference, an ultra-lightweight gaussian avatar driven by control signals enables high frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further demonstrate that the proposed framework follows the scaling law, whereby increasing the parameter scale of the reconstruction module leads to improved performance. Moreover, thanks to the separation design, driving efficiency remains unaffected. Finally, extensive quantitative and qualitative experiments validate that our approach outperforms current state-of-the-art methods.",
        "arxiv_id": "2509.05582",
        "ARXIVID": "2509.05582",
        "COMMENT": "This paper presents a framework for 3D Gaussian head reconstruction and reenactment from a single image, with a focus on avatar generation. While it is a vision and generative modeling paper, it does not directly address the listed criteria (spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.06015": {
        "authors": [
            "Zhiwen Shao",
            "Yifan Cheng",
            "Fan Zhang",
            "Xuehuai Shi",
            "Canlin Li",
            "Lizhuang Ma",
            "Dit-yan Yeung"
        ],
        "title": "Micro-Expression Recognition via Fine-Grained Dynamic Perception",
        "abstract": "arXiv:2509.06015v1 Announce Type: new  Abstract: Facial micro-expression recognition (MER) is a challenging task, due to the transience, subtlety, and dynamics of micro-expressions (MEs). Most existing methods resort to hand-crafted features or deep networks, in which the former often additionally requires key frames, and the latter suffers from small-scale and low-diversity training data. In this paper, we develop a novel fine-grained dynamic perception (FDP) framework for MER. We propose to rank frame-level features of a sequence of raw frames in chronological order, in which the rank process encodes the dynamic information of both ME appearances and motions. Specifically, a novel local-global feature-aware transformer is proposed for frame representation learning. A rank scorer is further adopted to calculate rank scores of each frame-level feature. Afterwards, the rank features from rank scorer are pooled in temporal dimension to capture dynamic representation. Finally, the dynamic representation is shared by a MER module and a dynamic image construction module, in which the former predicts the ME category, and the latter uses an encoder-decoder structure to construct the dynamic image. The design of dynamic image construction task is beneficial for capturing facial subtle actions associated with MEs and alleviating the data scarcity issue. Extensive experiments show that our method (i) significantly outperforms the state-of-the-art MER methods, and (ii) works well for dynamic image construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11% over the previous best results in terms of F1-score on the CASME II, SAMM, CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at https://github.com/CYF-cuber/FDP.",
        "arxiv_id": "2509.06015",
        "ARXIVID": "2509.06015",
        "COMMENT": "This paper proposes a novel fine-grained dynamic perception framework for micro-expression recognition, focusing on temporal dynamics and transformer-based modeling. While it is a vision paper with interesting statistical tricks (ranking, temporal pooling), it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about facial expression recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.05662": {
        "authors": [
            "Wasikul Islam"
        ],
        "title": "WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising",
        "abstract": "arXiv:2509.05662v1 Announce Type: new  Abstract: In high-energy particle physics, collider measurements are contaminated by \"pileup\", overlapping soft interactions that obscure the hard-scatter signal of interest. Dedicated subtraction strategies exploit physical priors such as conservation, locality, and isolation. Inspired by this analogy, we investigate how such principles can inform image denoising by embedding physics-guided inductive biases into neural architectures. This paper is a proof of concept: rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether physics-inspired priors improve robustness under strong corruption.   We introduce a hierarchy of PU-inspired denoisers: a residual CNN with conservation constraints, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at $\\sigma\\in\\{15,25,50,75,100\\}$, PU-inspired CNNs are competitive with standard baselines, while WIPUNet shows a \\emph{widening margin} at higher noise. Complementary BSD500 experiments show the same trend, suggesting physics-inspired priors provide stability where purely data-driven models degrade. Our contributions are: (i) translating pileup-mitigation principles into modular inductive biases; (ii) integrating them into UNet; and (iii) demonstrating robustness gains at high noise without relying on heavy SOTA machinery.",
        "arxiv_id": "2509.05662",
        "ARXIVID": "2509.05662",
        "COMMENT": "Does not match any specific criterion. Focuses on physics-inspired inductive biases for image denoising, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.05343": {
        "authors": [
            "Zahid Ullah",
            "Minki Hong",
            "Tahir Mahmood",
            "Jihie Kim"
        ],
        "title": "Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis",
        "abstract": "arXiv:2509.05343v1 Announce Type: new  Abstract: Deep learning has become a powerful tool for medical image analysis; however, conventional Convolutional Neural Networks (CNNs) often fail to capture the fine-grained and complex features critical for accurate diagnosis. To address this limitation, we systematically integrate attention mechanisms into five widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3, DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient regions and improve discriminative performance. Specifically, each baseline model is augmented with either a Squeeze and Excitation block or a hybrid Convolutional Block Attention Module, allowing adaptive recalibration of channel and spatial feature representations. The proposed models are evaluated on two distinct medical imaging datasets, a brain tumor MRI dataset comprising multiple tumor subtypes, and a Products of Conception histopathological dataset containing four tissue categories. Experimental results demonstrate that attention augmented CNNs consistently outperform baseline architectures across all metrics. In particular, EfficientNetB5 with hybrid attention achieves the highest overall performance, delivering substantial gains on both datasets. Beyond improved classification accuracy, attention mechanisms enhance feature localization, leading to better generalization across heterogeneous imaging modalities. This work contributes a systematic comparative framework for embedding attention modules in diverse CNN architectures and rigorously assesses their impact across multiple medical imaging tasks. The findings provide practical insights for the development of robust, interpretable, and clinically applicable deep learning based decision support systems.",
        "arxiv_id": "2509.05343",
        "ARXIVID": "2509.05343",
        "COMMENT": "Somewhat related to vision foundation models (criterion 4) via attention modules in CNNs, but not directly about foundation models or their applications. Focuses on medical image diagnosis with attention-augmented CNNs.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.05378": {
        "authors": [
            "Andreas Motzfeldt",
            "Joakim Edin",
            "Casper L. Christensen",
            "Christian Hardmeier",
            "Lars Maal{\\o}e",
            "Anna Rogers"
        ],
        "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding",
        "abstract": "arXiv:2509.05378v1 Announce Type: new  Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).",
        "arxiv_id": "2509.05378",
        "ARXIVID": "2509.05378",
        "COMMENT": "This paper presents a multi-agent LLM-based framework for medical coding, supporting the full ICD-10 system. While it is a multi-agent system and uses LLMs, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.06174": {
        "authors": [
            "Wei Han",
            "Geng Zhan",
            "Sicheng Yu",
            "Chenyu Wang",
            "Bryan Hooi"
        ],
        "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains",
        "abstract": "arXiv:2509.06174v1 Announce Type: new  Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward over conventional instruction-following LLMs. By applying test-time scaling to generate extended reasoning paths, they establish many SOTAs across a wide range of complex reasoning tasks. However, recent studies show that LRMs are prone to suffer from overthinking -- the tendency to overcomplicate simple problems, leading to excessive strategy switching and long, convoluted reasoning traces that hinder their interpretability. To mitigate this issue, we conduct a systematic investigation into the reasoning efficiency of a broad set of LRMs and uncover a common dilemma: the difficulty in balancing multiple generation objectives such as correctness and brevity. Based on this discovery, we propose a test-time scaling method, EDIT (Efficient Dynamic Inference Trimming), which efficiently guides LRMs to identify the shortest correct reasoning paths at test time. EDIT employs constraint-guided generation while jointly tracking length and answer distributions under varying constraints, allowing it to select responses that strike an optimal balance between conciseness and correctness. Extensive experiments across diverse models and datasets show that EDIT substantially enhance the reasoning efficiency, producing compact yet informative outputs that improve readability and user experience.",
        "arxiv_id": "2509.06174",
        "ARXIVID": "2509.06174",
        "COMMENT": "This paper proposes a new test-time scaling method (EDIT) for improving reasoning efficiency in large reasoning models (LLMs). While it is a methodological improvement for LLMs, it does not focus on spatial understanding, VLLMs/MLLMs, embodied agents, or vision foundation models. It is more about language model reasoning chains.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.06793": {
        "authors": [
            "George Ciubotariu",
            "Florin-Alexandru Vasluianu",
            "Zhuyun Zhou",
            "Nancy Mehta",
            "Radu Timofte",
            "Ke Wu",
            "Long Sun",
            "Lingshun Kong",
            "Zhongbao Yang",
            "Jinshan Pan",
            "Jiangxin Dong",
            "Jinhui Tang",
            "Hao Chen",
            "Yinghui Fang",
            "Dafeng Zhang",
            "Yongqi Song",
            "Jiangbo Guo",
            "Shuhua Jin",
            "Zeyu Xiao",
            "Rui Zhao",
            "Zhuoyuan Li",
            "Cong Zhang",
            "Yufeng Peng",
            "Xin Lu",
            "Zhijing Sun",
            "Chengjie Ge",
            "Zihao Li",
            "Zishun Liao",
            "Ziang Zhou",
            "Qiyu Kang",
            "Xueyang Fu",
            "Zheng-Jun Zha",
            "Yuqian Zhang",
            "Shuai Liu",
            "Jie Liu",
            "Zhuhao Zhang",
            "Lishen Qu",
            "Zhihao Liu",
            "Shihao Zhou",
            "Yaqi Luo",
            "Juncheng Zhou",
            "Jufeng Yang",
            "Qianfeng Yang",
            "Qiyuan Guan",
            "Xiang Chen",
            "Guiyue Jin",
            "Jiyu Jin"
        ],
        "title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results",
        "abstract": "arXiv:2509.06793v1 Announce Type: new  Abstract: This paper presents a comprehensive review of the AIM 2025 High FPS Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions and final results. The objective of this challenge is to identify effective networks capable of producing clearer and visually compelling images in diverse and challenging conditions, by learning representative visual cues for complex aggregations of motion types. A total of 68 participants registered for the competition, and 9 teams ultimately submitted valid entries. This paper thoroughly evaluates the state-of-the-art advances in high-FPS single image motion deblurring, showcasing the significant progress in the field, while leveraging samples of the novel dataset, MIORe, that introduces challenging examples of movement patterns.",
        "arxiv_id": "2509.06793",
        "ARXIVID": "2509.06793",
        "COMMENT": "Relevant to computer vision and benchmarks, but does not match any specific criterion. The paper is a challenge report on motion deblurring, not about spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.05887": {
        "authors": [
            "Caleb Gates",
            "Patrick Moorhead",
            "Jayden Ferguson",
            "Omar Darwish",
            "Conner Stallman",
            "Pablo Rivas",
            "Paapa Quansah"
        ],
        "title": "Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data",
        "abstract": "arXiv:2509.05887v1 Announce Type: new  Abstract: Dust storms harm health and reduce visibility; quick detection from satellites is needed. We present a near real-time system that flags dust at the pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D convolutional network learns patterns across all 36 bands, plus split thermal bands, to separate dust from clouds and surface features. Simple normalization and local filling handle missing data. An improved version raises training speed by 21x and supports fast processing of full scenes. On 17 independent MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error of 0.014. Maps show strong agreement in plume cores, with most misses along edges. These results show that joint band-and-space learning can provide timely dust alerts at global scale; using wider input windows or attention-based models may further sharpen edges.",
        "arxiv_id": "2509.05887",
        "ARXIVID": "2509.05887",
        "COMMENT": "This paper presents a 3D convolutional neural network for near real-time dust aerosol detection from MODIS satellite data. While it is a vision application, it does not focus on spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}