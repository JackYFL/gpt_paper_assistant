{
    "2509.25654": {
        "authors": [
            "Kaiyu Li",
            "Zixuan Jiang",
            "Xiangyong Cao",
            "Jiayu Wang",
            "Yuchen Xiao",
            "Deyu Meng",
            "Zhi Wang"
        ],
        "title": "DescribeEarth: Describe Anything for Remote Sensing Images",
        "abstract": "arXiv:2509.25654v1 Announce Type: new  Abstract: Automated textual description of remote sensing images is crucial for unlocking their full potential in diverse applications, from environmental monitoring to urban planning and disaster management. However, existing studies in remote sensing image captioning primarily focus on the image level, lacking object-level fine-grained interpretation, which prevents the full utilization and transformation of the rich semantic and structural information contained in remote sensing images. To address this limitation, we propose Geo-DLC, a novel task of object-level fine-grained image captioning for remote sensing. To support this task, we construct DE-Dataset, a large-scale dataset contains 25 categories and 261,806 annotated instances with detailed descriptions of object attributes, relationships, and contexts. Furthermore, we introduce DE-Benchmark, a LLM-assisted question-answering based evaluation suite designed to systematically measure model capabilities on the Geo-DLC task. We also present DescribeEarth, a Multi-modal Large Language Model (MLLM) architecture explicitly designed for Geo-DLC, which integrates a scale-adaptive focal strategy and a domain-guided fusion module leveraging remote sensing vision-language model features to encode high-resolution details and remote sensing category priors while maintaining global context. Our DescribeEarth model consistently outperforms state-of-the-art general MLLMs on DE-Benchmark, demonstrating superior factual accuracy, descriptive richness, and grammatical soundness, particularly in capturing intrinsic object features and surrounding environmental attributes across simple, complex, and even out-of-distribution remote sensing scenarios. All data, code and weights are released at https://github.com/earth-insights/DescribeEarth.",
        "arxiv_id": "2509.25654",
        "ARXIVID": "2509.25654",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new MLLM (DescribeEarth) for remote sensing, a new dataset, and a benchmark for fine-grained object-level captioning in remote sensing images.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25370": {
        "authors": [
            "Kunlun Zhu",
            "Zijia Liu",
            "Bingxuan Li",
            "Muxin Tian",
            "Yingxuan Yang",
            "Jiaxun Zhang",
            "Pengrui Han",
            "Qipeng Xie",
            "Fuyang Cui",
            "Weijia Zhang",
            "Xiaoteng Ma",
            "Xiaodong Yu",
            "Gowtham Ramesh",
            "Jialian Wu",
            "Zicheng Liu",
            "Pan Lu",
            "James Zou",
            "Jiaxuan You"
        ],
        "title": "Where LLM Agents Fail and How They can Learn From Failures",
        "abstract": "arXiv:2509.25370v1 Announce Type: new  Abstract: Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug",
        "arxiv_id": "2509.25370",
        "ARXIVID": "2509.25370",
        "COMMENT": "This paper introduces a new benchmark and debugging framework for LLM agents in embodied AI environments (ALFWorld, GAIA, WebShop). Directly matches criterion 3: new embodied AI benchmark and method for error analysis and recovery, with a novel angle on agent failure taxonomy and debugging.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25773": {
        "authors": [
            "Zhengpeng Shi",
            "Hengli Li",
            "Yanpeng Zhao",
            "Jianqun Zhou",
            "Yuxuan Wang",
            "Qinrong Cui",
            "Wei Bi",
            "Songchun Zhu",
            "Bo Zhao",
            "Zilong Zheng"
        ],
        "title": "V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs",
        "abstract": "arXiv:2509.25773v1 Announce Type: new  Abstract: AI models capable of comprehending humor hold real-world promise -- for example, enhancing engagement in human-machine interactions. To gauge and diagnose the capacity of multimodal large language models (MLLMs) for humor understanding, we introduce v-HUB, a novel visual-centric video humor understanding benchmark. v-HUB comprises a curated collection of minimally verbal short videos, sourced from classic silent films and online resources, and reflecting real-world scenarios where humor can be appreciated purely through visual cues. Each video clip is paired with rich annotations, including captions, descriptions, and explanations, supporting evaluation tasks like caption matching and humor explanation. To broaden its applicability, we further construct an open-ended video QA task, making it readily integrable into existing video understanding benchmarks. We evaluate a diverse set of MLLMs, from specialized Video-LLMs to versatile OmniLLMs that can process audio, covering both open-source and proprietary domains. The experimental results expose the difficulties MLLMs face in comprehending humor from visual cues alone. For example, all models exhibit a marked performance drop on caption matching when moving from text-based to video-based evaluation (without audio). Our findings also demonstrate that incorporating audio helps with video humor understanding, highlighting the informativeness of sound and the promise of integrating richer modalities for complex video understanding tasks.",
        "arxiv_id": "2509.25773",
        "ARXIVID": "2509.25773",
        "COMMENT": "Directly matches criterion 2: introduces v-HUB, a new benchmark for evaluating MLLMs (video-LLMs and OmniLLMs) on visual humor understanding. Also provides surprising empirical results about the limitations of current MLLMs in humor comprehension.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25885": {
        "authors": [
            "Ruolin Chen",
            "Yinqian Sun",
            "Jihang Wang",
            "Mingyang Lv",
            "Qian Zhang",
            "Yi Zeng"
        ],
        "title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents",
        "abstract": "arXiv:2509.25885v1 Announce Type: new  Abstract: Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.",
        "arxiv_id": "2509.25885",
        "ARXIVID": "2509.25885",
        "COMMENT": "Matches criterion 3: Introduces SafeMindBench, a new benchmark for safety in embodied LLM agents, and a new modular agent architecture for safety. Focuses on a novel angle (safety risks and constraints) in embodied AI that previous work has largely ignored.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25970": {
        "authors": [
            "Bojun Zhang",
            "Hangjian Ye",
            "Hao Zheng",
            "Jianzheng Huang",
            "Zhengyu Lin",
            "Zhenhong Guo",
            "Feng Zheng"
        ],
        "title": "PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks",
        "abstract": "arXiv:2509.25970v1 Announce Type: new  Abstract: Fine-grained 3D part segmentation is crucial for enabling embodied AI systems to perform complex manipulation tasks, such as interacting with specific functional components of an object. However, existing interactive segmentation methods are largely confined to coarse, instance-level targets, while non-interactive approaches struggle with sparse, real-world scans and suffer from a severe lack of annotated data. To address these limitations, we introduce PinPoint3D, a novel interactive framework for fine-grained, multi-granularity 3D segmentation, capable of generating precise part-level masks from only a few user point clicks. A key component of our work is a new 3D data synthesis pipeline that we developed to create a large-scale, scene-level dataset with dense part annotations, overcoming a critical bottleneck that has hindered progress in this field. Through comprehensive experiments and user studies, we demonstrate that our method significantly outperforms existing approaches, achieving an average IoU of around 55.8% on each object part under first-click settings and surpassing 71.3% IoU with only a few additional clicks. Compared to current state-of-the-art baselines, PinPoint3D yields up to a 16% improvement in IoU and precision, highlighting its effectiveness on challenging, sparse point clouds with high efficiency. Our work represents a significant step towards more nuanced and precise machine perception and interaction in complex 3D environments.",
        "arxiv_id": "2509.25970",
        "ARXIVID": "2509.25970",
        "COMMENT": "PinPoint3D introduces a novel interactive framework for fine-grained 3D part segmentation from a few clicks, with a new 3D data synthesis pipeline and large-scale dataset. This is highly relevant to criterion 1 (spatial understanding for embodied agents) and criterion 3 (new benchmark/method for embodied AI, simulator-related).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.26489": {
        "authors": [
            "Sattwik Basu",
            "Chaitanya Amballa",
            "Zhongweiyang Xu",
            "Jorge Van\\v{c}o Sampedro",
            "Srihari Nelakuditi",
            "Romit Roy Choudhury"
        ],
        "title": "Contrastive Diffusion Guidance for Spatial Inverse Problems",
        "abstract": "arXiv:2509.26489v1 Announce Type: new  Abstract: We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.",
        "arxiv_id": "2509.26489",
        "ARXIVID": "2509.26489",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel methods for spatial inverse problems). Proposes a contrastive diffusion guidance method for reconstructing spatial layouts from movement trajectories, with a new likelihood formulation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.25787": {
        "authors": [
            "Wen Wen",
            "Tianwu Zhi",
            "Kanglong Fan",
            "Yang Li",
            "Xinge Peng",
            "Yabin Zhang",
            "Yiting Liao",
            "Junlin Li",
            "Li Zhang"
        ],
        "title": "Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking",
        "abstract": "arXiv:2509.25787v1 Announce Type: new  Abstract: Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques such as self-consistency have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM's own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model's iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM's perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM's zero-shot performance by 31.8\\% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks.",
        "arxiv_id": "2509.25787",
        "ARXIVID": "2509.25787",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a self-evolving VLM for image quality assessment using a novel self-supervised voting and ranking approach, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.25848": {
        "authors": [
            "Xinyu Tian",
            "Shu Zou",
            "Zhaoyuan Yang",
            "Mengqi He",
            "Fabian Waschkowski",
            "Lukas Wesemann",
            "Peter Tu",
            "Jing Zhang"
        ],
        "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models",
        "abstract": "arXiv:2509.25848v1 Announce Type: new  Abstract: Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/",
        "arxiv_id": "2509.25848",
        "ARXIVID": "2509.25848",
        "COMMENT": "Matches criterion 2 (new VLLMs) and criterion 4 (vision foundation models and applications) by analyzing reasoning in VLMs and introducing a new method (VAPO) to improve visual grounding, with new empirical insights.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.26004": {
        "authors": [
            "Nicola Messina",
            "Rosario Leonardi",
            "Luca Ciampi",
            "Fabio Carrara",
            "Giovanni Maria Farinella",
            "Fabrizio Falchi",
            "Antonino Furnari"
        ],
        "title": "Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations",
        "abstract": "arXiv:2509.26004v1 Announce Type: new  Abstract: Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., \"I am pouring vegetables from the chopping board to the pan\"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations.",
        "arxiv_id": "2509.26004",
        "ARXIVID": "2509.26004",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel method for egocentric in-hand object segmentation using weak supervision from human narrations, with new task and model).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.25866": {
        "authors": [
            "Chi Zhang",
            "Haibo Qiu",
            "Qiming Zhang",
            "Zhixiong Zeng",
            "Lin Ma",
            "Jing Zhang"
        ],
        "title": "DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning",
        "abstract": "arXiv:2509.25866v1 Announce Type: new  Abstract: The \"thinking with images\" paradigm represents a pivotal shift in the reasoning of Vision Language Models (VLMs), moving from text-dominant chain-of-thought to image-interactive reasoning. By invoking visual tools or generating intermediate visual representations, VLMs can iteratively attend to fine-grained regions, enabling deeper image understanding and more faithful multimodal reasoning. As an emerging paradigm, however, it still leaves substantial room for exploration in data construction accuracy, structural design, and broader application scenarios, which offer rich opportunities for advancing multimodal reasoning. To further advance this line of work, we present DeepSketcher, a comprehensive suite comprising both an image-text interleaved dataset and a self-contained model. The dataset contains 31k chain-of-thought (CoT) reasoning trajectories with diverse tool calls and resulting edited images, covering a wide range of data types and manipulation instructions with high annotation accuracy. Building on this resource, we design a model that performs interleaved image-text reasoning and natively generates \"visual thoughts\" by operating directly in the visual embedding space, rather than invoking external tools and repeatedly re-encoding generated images. This design enables tool-free and more flexible \"thinking with images\". Extensive experiments on multimodal reasoning benchmarks demonstrate strong performance, validating both the utility of the dataset and the effectiveness of the model design.",
        "arxiv_id": "2509.25866",
        "ARXIVID": "2509.25866",
        "COMMENT": "Matches criterion 2: Proposes DeepSketcher, a new VLM paradigm for 'thinking with images' and interleaved image-text reasoning, with a new dataset and model for multimodal reasoning. Strong VLLM/MLLM and generative modeling angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.25805": {
        "authors": [
            "Xintong Jiang",
            "Yixue Liu",
            "Mohamed Debbagh",
            "Yu Tian",
            "Valerio Hoyos-Villegas",
            "Viacheslav Adamchuk",
            "Shangpeng Sun"
        ],
        "title": "Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions",
        "abstract": "arXiv:2509.25805v1 Announce Type: new  Abstract: Parameter-Efficient Fine-Tuning (PEFT) of foundation models for agricultural computer vision tasks remains challenging due to limited training data and complex field conditions. This study introduces a Dynamic Similarity-based Graph Adaptation (DSGA) module to adapt the Segment Anything Model (SAM) under extreme data constraints for precise foreground and instance segmentation of small dense objects in complex agricultural environments. Through dynamic similarity graph construction with a learnable polynomial decay-initialized weight ranking mechanism and adaptive local feature aggregation, DSGA establishes robust spatial and dynamic similarity representation with only 4.00M trainable parameters, which is 4.26% of the original SAM. Integrating this graph-based feature adaptation with Low-Rank Adaptation (LoRA) creates a complementary optimization framework that effectively captures both local and global dependencies in image embeddings while preserving model stability and parameter efficiency. Experimental results on a challenging chickpea pod dataset demonstrated that DSGA with LoRA achieved superior performance across multiple metrics evaluated under 2, 4, 8 and 10 shots, with progressive performance gains as shot count increased. Quantitative metrics showed a 17.31% improvement in Structure-measure and a 62.36% gain in adaptive F-measure compared to the baseline SAM fine-tuning. Comprehensive ablation studies and visualization analyses through Grad-CAM and t-SNE validated the framework's effectiveness in feature discrimination. The proposed adaptation demonstrated practical utility for automated agricultural monitoring applications, achieving accurate pod-counting with an adjusted R-squared of 0.8987 for images with 10 to 120 pods under challenging field conditions.",
        "arxiv_id": "2509.25805",
        "ARXIVID": "2509.25805",
        "COMMENT": "This paper adapts the Segment Anything Model (SAM) for few-shot, parameter-efficient small dense object detection in agriculture using a dynamic similarity graph. Directly matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation model adaptation/application).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.25723": {
        "authors": [
            "Shunpeng Chen",
            "Changwei Wang",
            "Rongtao Xu",
            "Xingtian Pei",
            "Yukun Song",
            "Jinzhou Lin",
            "Wenhao Xu",
            "Jingyi Zhang",
            "Li Guo",
            "Shibiao Xu"
        ],
        "title": "SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition",
        "abstract": "arXiv:2509.25723v1 Announce Type: new  Abstract: Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. It attains 98.9%, 95.8%, 94.5%, and 96.0% Recall@1 on SPED, Pitts30k-test, MSLS-val, and Nordland, respectively. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. Code and model will be available at: https://github.com/chenshunpeng/SAGE.",
        "arxiv_id": "2509.25723",
        "ARXIVID": "2509.25723",
        "COMMENT": "Directly matches criterion 1: introduces SAGE, a new method for spatial-visual adaptive graph exploration for visual place recognition, improving spatial understanding in embodied agents. Also relevant to criterion 4 as it uses a vision foundation model (DINOv2) as backbone.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.26006": {
        "authors": [
            "Hanwei Zhu",
            "Yu Tian",
            "Keyan Ding",
            "Baoliang Chen",
            "Bolin Chen",
            "Shiqi Wang",
            "Weisi Lin"
        ],
        "title": "AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment",
        "abstract": "arXiv:2509.26006v1 Announce Type: new  Abstract: Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks -- distortion detection, distortion analysis, tool selection, and tool execution -- coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.",
        "arxiv_id": "2509.26006",
        "ARXIVID": "2509.26006",
        "COMMENT": "Matches criterion 2 and 4: Proposes AgenticIQA, a modular agentic framework integrating vision-language models (VLMs) for adaptive and interpretable image quality assessment, with a new large-scale instruction dataset and benchmark. Strongly relevant to VLLMs and vision foundation model applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.26127": {
        "authors": [
            "Ruixiao Dong",
            "Zhendong Wang",
            "Keli Liu",
            "Li Li",
            "Ying Chen",
            "Kai Li",
            "Daowen Li",
            "Houqiang Li"
        ],
        "title": "EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model",
        "abstract": "arXiv:2509.26127v1 Announce Type: new  Abstract: Subject-driven generation is a critical task in creative AI; yet current state-of-the-art methods present a stark trade-off. They either rely on computationally expensive, per-subject fine-tuning, sacrificing efficiency and zero-shot capability, or employ feed-forward architectures built on diffusion models, which are inherently plagued by slow inference speeds. Visual Auto-Regressive (VAR) models are renowned for their rapid sampling speeds and strong generative quality, making them an ideal yet underexplored foundation for resolving this tension. To bridge this gap, we introduce EchoGen, a pioneering framework that empowers VAR models with subject-driven generation capabilities. The core design of EchoGen is an effective dual-path injection strategy that disentangles a subject's high-level semantic identity from its low-level fine-grained details, enabling enhanced controllability and fidelity. We employ a semantic encoder to extract the subject's abstract identity, which is injected through decoupled cross-attention to guide the overall composition. Concurrently, a content encoder captures intricate visual details, which are integrated via a multi-modal attention mechanism to ensure high-fidelity texture and structural preservation. To the best of our knowledge, EchoGen is the first feed-forward subject-driven framework built upon VAR models. Both quantitative and qualitative results substantiate our design, demonstrating that EchoGen achieves subject fidelity and image quality comparable to state-of-the-art diffusion-based methods with significantly lower sampling latency. Code and models will be released soon.",
        "arxiv_id": "2509.26127",
        "ARXIVID": "2509.26127",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it introduces a new subject-driven generative framework (EchoGen) for visual auto-regressive models, with a novel dual-path injection strategy for controllable image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25771": {
        "authors": [
            "Jia Jun Cheng Xian",
            "Muchen Li",
            "Haotian Yang",
            "Xin Tao",
            "Pengfei Wan",
            "Leonid Sigal",
            "Renjie Liao"
        ],
        "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs",
        "abstract": "arXiv:2509.25771v1 Announce Type: new  Abstract: Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
        "arxiv_id": "2509.25771",
        "ARXIVID": "2509.25771",
        "COMMENT": "Matches criterion 4: Proposes a new method for aligning text-to-image diffusion models without preference image pairs, using LLMs to generate mismatched prompts. This is a clever generative modeling and vision foundation model trick.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.26165": {
        "authors": [
            "Yuansen Liu",
            "Haiming Tang",
            "Jinlong Peng",
            "Jiangning Zhang",
            "Xiaozhong Ji",
            "Qingdong He",
            "Donghao Luo",
            "Zhenye Gan",
            "Junwei Zhu",
            "Yunhang Shen",
            "Chaoyou Fu",
            "Chengjie Wang",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "title": "Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models",
        "abstract": "arXiv:2509.26165v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks. However, their capacity to comprehend human-centric scenes has rarely been explored, primarily due to the absence of comprehensive evaluation benchmarks that take into account both the human-oriented granular level and higher-dimensional causal reasoning ability. Such high-quality evaluation benchmarks face tough obstacles, given the physical complexity of the human body and the difficulty of annotating granular structures. In this paper, we propose Human-MME, a curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric scene understanding. Compared with other existing benchmarks, our work provides three key features: 1. Diversity in human scene, spanning 4 primary visual domains with 15 secondary domains and 43 sub-fields to ensure broad scenario coverage. 2. Progressive and diverse evaluation dimensions, evaluating the human-based activities progressively from the human-oriented granular perception to the higher-dimensional reasoning, consisting of eight dimensions with 19,945 real-world image question pairs and an evaluation suite. 3. High-quality annotations with rich data paradigms, constructing the automated annotation pipeline and human-annotation platform, supporting rigorous manual labeling to facilitate precise and reliable model assessment. Our benchmark extends the single-target understanding to the multi-person and multi-image mutual understanding by constructing the choice, short-answer, grounding, ranking and judgment question components, and complex questions of their combination. The extensive experiments on 17 state-of-the-art MLLMs effectively expose the limitations and guide future MLLMs research toward better human-centric image understanding. All data and code are available at https://github.com/Yuan-Hou/Human-MME.",
        "arxiv_id": "2509.26165",
        "ARXIVID": "2509.26165",
        "COMMENT": "Matches criterion 2: Proposes a new benchmark (Human-MME) for evaluating MLLMs on human-centric scene understanding, with diverse and granular evaluation. This is directly about MLLMs and benchmarking their visual reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.25603": {
        "authors": [
            "Yijia Weng",
            "Zhicheng Wang",
            "Songyou Peng",
            "Saining Xie",
            "Howard Zhou",
            "Leonidas J. Guibas"
        ],
        "title": "GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification",
        "abstract": "arXiv:2509.25603v1 Announce Type: new  Abstract: We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\\times1024$ resolution.",
        "arxiv_id": "2509.25603",
        "ARXIVID": "2509.25603",
        "COMMENT": "Matches criterion 4: Proposes GaussianLens, a new method for localized high-resolution 3D scene reconstruction using on-demand Gaussian densification, leveraging multi-modal information. Relevant for vision foundation models and spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25851": {
        "authors": [
            "Jundong Xu",
            "Hao Fei",
            "Yuhui Zhang",
            "Liangming Pan",
            "Qijun Huang",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan",
            "William Yang Wang",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "title": "MuSLR: Multimodal Symbolic Logical Reasoning",
        "abstract": "arXiv:2509.25851v1 Announce Type: new  Abstract: Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1's Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements. All data and code are publicly available at https://llm-symbol.github.io/MuSLR.",
        "arxiv_id": "2509.25851",
        "ARXIVID": "2509.25851",
        "COMMENT": "This paper introduces MuSLR, a new benchmark for multimodal symbolic logical reasoning, and evaluates VLMs on it. It also proposes LogiCAM, a modular framework for formal logic on multimodal inputs. Closely matches criterion 2 (new VLLMs/MLLMs) and criterion 3 (new benchmark for vision-language reasoning).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25520": {
        "authors": [
            "Tu-Hoa Pham",
            "Philip Bailey",
            "Daniel Posada",
            "Georgios Georgakis",
            "Jorge Enriquez",
            "Surya Suresh",
            "Marco Dolci",
            "Philip Twu"
        ],
        "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
        "abstract": "arXiv:2509.25520v1 Announce Type: new  Abstract: We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware. We propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs. Extensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy, in turn enabling new possibilities for cheap and reliable localization on general-purpose hardware.",
        "arxiv_id": "2509.25520",
        "ARXIVID": "2509.25520",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI). Proposes a new localization algorithm for compute-constrained environments, with strong empirical results in real-world and simulated Mars scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.26272": {
        "authors": [
            "Tuan Nguyen",
            "Naseem Khan",
            "Khang Tran",
            "NhatHai Phan",
            "Issa Khalil"
        ],
        "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection",
        "abstract": "arXiv:2509.26272v1 Announce Type: new  Abstract: The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.",
        "arxiv_id": "2509.26272",
        "ARXIVID": "2509.26272",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new RL-based method for aligning multimodal LLM reasoning with visual evidence for deepfake detection, and introduces a new annotated dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25856": {
        "authors": [
            "Po-Han Huang",
            "Jeng-Lin Li",
            "Po-Hsuan Huang",
            "Ming-Ching Chang",
            "Wei-Chao Chen"
        ],
        "title": "PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection",
        "abstract": "arXiv:2509.25856v1 Announce Type: new  Abstract: Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.",
        "arxiv_id": "2509.25856",
        "ARXIVID": "2509.25856",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a unified patch-based anomaly detection framework compatible with diverse vision foundation models, focusing on visual prompting without textual features.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25564": {
        "authors": [
            "Faizan Farooq Khan",
            "Yousef Radwan",
            "Eslam Abdelrahman",
            "Abdulwahab Felemban",
            "Aymen Mir",
            "Nico K. Michiels",
            "Andrew J. Temple",
            "Michael L. Berumen",
            "Mohamed Elhoseiny"
        ],
        "title": "FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology",
        "abstract": "arXiv:2509.25564v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated impressive cross-domain capabilities, yet their proficiency in specialized scientific fields like marine biology remains underexplored. In this work, we systematically evaluate state-of-the-art MLLMs and reveal significant limitations in their ability to perform fine-grained recognition of fish species, with the best open-source models achieving less than 10\\% accuracy. This task is critical for monitoring marine ecosystems under anthropogenic pressure. To address this gap and investigate whether these failures stem from a lack of domain knowledge, we introduce FishNet++, a large-scale, multimodal benchmark. FishNet++ significantly extends existing resources with 35,133 textual descriptions for multimodal learning, 706,426 key-point annotations for morphological studies, and 119,399 bounding boxes for detection. By providing this comprehensive suite of annotations, our work facilitates the development and evaluation of specialized vision-language models capable of advancing aquatic science.",
        "arxiv_id": "2509.25564",
        "ARXIVID": "2509.25564",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications) by evaluating and benchmarking MLLMs in marine biology, introducing a new large-scale multimodal benchmark (FishNet++).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25863": {
        "authors": [
            "Junjie Zhou",
            "Wei Shao",
            "Yagao Yue",
            "Wei Mu",
            "Peng Wan",
            "Qi Zhu",
            "Daoqiang Zhang"
        ],
        "title": "MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification",
        "abstract": "arXiv:2509.25863v1 Announce Type: new  Abstract: Prompt learning has emerged as a promising paradigm for adapting pre-trained vision-language models (VLMs) to few-shot whole slide image (WSI) classification by aligning visual features with textual representations, thereby reducing annotation cost and enhancing model generalization. Nevertheless, existing methods typically rely on slide-level prompts and fail to capture the subtype-specific phenotypic variations of histological entities (\\emph{e.g.,} nuclei, glands) that are critical for cancer diagnosis. To address this gap, we propose Multi-scale Attribute-enhanced Prompt Learning (\\textbf{MAPLE}), a hierarchical framework for few-shot WSI classification that jointly integrates multi-scale visual semantics and performs prediction at both the entity and slide levels. Specifically, we first leverage large language models (LLMs) to generate entity-level prompts that can help identify multi-scale histological entities and their phenotypic attributes, as well as slide-level prompts to capture global visual descriptions. Then, an entity-guided cross-attention module is proposed to generate entity-level features, followed by aligning with their corresponding subtype-specific attributes for fine-grained entity-level prediction. To enrich entity representations, we further develop a cross-scale entity graph learning module that can update these representations by capturing their semantic correlations within and across scales. The refined representations are then aggregated into a slide-level representation and aligned with the corresponding prompts for slide-level prediction. Finally, we combine both entity-level and slide-level outputs to produce the final prediction results. Results on three cancer cohorts confirm the effectiveness of our approach in addressing few-shot pathology diagnosis tasks.",
        "arxiv_id": "2509.25863",
        "ARXIVID": "2509.25863",
        "COMMENT": "Matches criterion 4: Proposes MAPLE, a prompt learning framework for few-shot whole slide image classification using VLMs and LLMs for multi-scale attribute prompts. Strong vision foundation model application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.26621": {
        "authors": [
            "Xiyi Chen",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Taewon Kang",
            "Sergey Prokudin",
            "Ming Lin"
        ],
        "title": "HART: Human Aligned Reconstruction Transformer",
        "abstract": "arXiv:2509.26621v1 Announce Type: new  Abstract: We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
        "arxiv_id": "2509.26621",
        "ARXIVID": "2509.26621",
        "COMMENT": "Matches criterion 4: Introduces a transformer-based model for sparse-view human reconstruction, integrating SMPL-X body models and Gaussian splats for photorealistic rendering. Strong vision foundation model and generative modeling angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25940": {
        "authors": [
            "Debottam Dutta",
            "Jianchong Chen",
            "Rajalaxmi Rajagopalan",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "title": "CO3: Contrasting Concepts Compose Better",
        "abstract": "arXiv:2509.25940v1 Announce Type: new  Abstract: We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like \"a cat and a dog\" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards \"pure\" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.",
        "arxiv_id": "2509.25940",
        "ARXIVID": "2509.25940",
        "COMMENT": "Matches criterion 4: Proposes a new plug-and-play corrective sampling strategy for text-to-image diffusion models to improve multi-concept compositionality, addressing a known weakness in generative vision models. Relevant for generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25570": {
        "authors": [
            "Hakan Emre Gedik",
            "Andrew Martin",
            "Mustafa Munir",
            "Oguzhan Baser",
            "Radu Marculescu",
            "Sandeep P. Chinchali",
            "Alan C. Bovik"
        ],
        "title": "AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs",
        "abstract": "arXiv:2509.25570v1 Announce Type: new  Abstract: Vision Graph Neural Networks (ViGs) have demonstrated promising performance in image recognition tasks against Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). An essential part of the ViG framework is the node-neighbor feature aggregation method. Although various graph convolution methods, such as Max-Relative, EdgeConv, GIN, and GraphSAGE, have been explored, a versatile aggregation method that effectively captures complex node-neighbor relationships without requiring architecture-specific refinements is needed. To address this gap, we propose a cross-attention-based aggregation method in which the query projections come from the node, while the key projections come from its neighbors. Additionally, we introduce a novel architecture called AttentionViG that uses the proposed cross-attention aggregation scheme to conduct non-local message passing. We evaluated the image recognition performance of AttentionViG on the ImageNet-1K benchmark, where it achieved SOTA performance. Additionally, we assessed its transferability to downstream tasks, including object detection and instance segmentation on MS COCO 2017, as well as semantic segmentation on ADE20K. Our results demonstrate that the proposed method not only achieves strong performance, but also maintains efficiency, delivering competitive accuracy with comparable FLOPs to prior vision GNN architectures.",
        "arxiv_id": "2509.25570",
        "ARXIVID": "2509.25570",
        "COMMENT": "Matches criterion 4: Introduces a new cross-attention-based aggregation method for Vision Graph Neural Networks (ViGs), achieving SOTA on ImageNet and strong transfer to downstream vision tasks. This is a methodological improvement in vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.26457": {
        "authors": [
            "Artur Barros",
            "Carlos Caetano",
            "Jo\\~ao Macedo",
            "Jefersson A. dos Santos",
            "Sandra Avila"
        ],
        "title": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification",
        "abstract": "arXiv:2509.26457v1 Announce Type: new  Abstract: Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.",
        "arxiv_id": "2509.26457",
        "ARXIVID": "2509.26457",
        "COMMENT": "Matches criterion 1: Proposes a new method for spatial understanding in indoor scene classification using attention over scene graphs, focusing on spatial relationships and explainability. Also relevant to embodied agents due to application in robotics and privacy-preserving sensitive content analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25811": {
        "authors": [
            "Zichen Liang",
            "Jingjing Fei",
            "Jie Wang",
            "Zheming Yang",
            "Changqing Li",
            "Pei Wu",
            "Minghui Qiu",
            "Fei Yang",
            "Xialei Liu"
        ],
        "title": "Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition",
        "abstract": "arXiv:2509.25811v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have been primarily evaluated on general-purpose benchmarks, while their applications in domain-specific scenarios, such as intelligent product moderation, remain underexplored. To address this gap, we introduce an open-world logo recognition benchmark, a core challenge in product moderation. Unlike traditional logo recognition methods that rely on memorizing representations of tens of thousands of brands-an impractical approach in real-world settings-our proposed method, Logo-VGR, enables generalization to large-scale brand recognition with supervision from only a small subset of brands. Specifically, we reformulate logo recognition as a comparison-based task, requiring the model to match product images with candidate logos rather than directly generating brand labels. We further observe that existing models tend to overfit by memorizing brand distributions instead of learning robust multimodal reasoning, which results in poor performance on unseen brands. To overcome this limitation, Logo-VGR introduces a new paradigm of domain-specific multimodal reasoning: Logo Perception Grounding injects domain knowledge, and Logo-Guided Visual Grounded Reasoning enhances the model's reasoning capability. Experimental results show that Logo-VGR outperforms strong baselines by nearly 10 points in OOD settings, demonstrating superior generalization.",
        "arxiv_id": "2509.25811",
        "ARXIVID": "2509.25811",
        "COMMENT": "This paper introduces Logo-VGR, a new paradigm and benchmark for open-world logo recognition using multimodal reasoning, and proposes domain-specific visual grounding methods. Closely matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25711": {
        "authors": [
            "Yuan Gao",
            "Sangwook Kim",
            "Jianzhong You",
            "Chris McIntosh"
        ],
        "title": "ProbMed: A Probabilistic Framework for Medical Multimodal Binding",
        "abstract": "arXiv:2509.25711v1 Announce Type: new  Abstract: Medical decision-making requires integrating diverse medical information, from imaging to clinical narratives. These medical modalities are often acquired in a many-to-many manner. However, current medical vision-language pretraining models (Med-VLPMs) fail to directly account for this many-to-many mapping in their model training and embeddings. To address this, we present Probabilistic Modality-Enhanced Diagnosis (ProbMED), a multimodal Med-VLPM that employs probabilistic contrastive learning to model distributions over embeddings rather than deterministic estimates. ProbMED aligns four distinct modalities--chest X-rays, electrocardiograms, echocardiograms, and clinical text--into a unified probabilistic embedding space. We use InfoNCE loss with Hellinger distance to integrate inter-modality distributions. We introduce a probabilistic synthetic sampling loss that captures modality-specific mean and variance to improve intra-modality binding. Extensive experiments across 13 medical datasets demonstrate that our model outperforms current Med-VLPMs in cross-modality retrieval, zero-shot, and few-shot classification. We also demonstrate the robust integration of multiple modalities for prognostication, showing improved intra- and inter-medical modality binding.",
        "arxiv_id": "2509.25711",
        "ARXIVID": "2509.25711",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications) by introducing a probabilistic multimodal vision-language model for medical data, with a novel probabilistic contrastive learning approach.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.25594": {
        "authors": [
            "Bangwei Guo",
            "Yunhe Gao",
            "Meng Ye",
            "Difei Gu",
            "Yang Zhou",
            "Leon Axel",
            "Dimitris Metaxas"
        ],
        "title": "K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model",
        "abstract": "arXiv:2509.25594v1 Announce Type: new  Abstract: Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\\textit{semantic priors}$ learned from annotated datasets, (ii) $\\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\\textit{what}$ to segment and 2-D dense prompts indicating $\\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.",
        "arxiv_id": "2509.25594",
        "ARXIVID": "2509.25594",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) via a universal segmentation model that integrates multiple knowledge paradigms and prompt-based control, with strong generalization across modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.26539": {
        "authors": [
            "Zhen Yang",
            "Zi-Yi Dou",
            "Di Feng",
            "Forrest Huang",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Yuhao Yang",
            "Michael Feng",
            "Haotian Zhang",
            "Ram Ramrakhya",
            "Chao Jia",
            "Jeffrey Nichols",
            "Alexander Toshev",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
        "abstract": "arXiv:2509.26539v1 Announce Type: new  Abstract: Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of $91.6\\%$, $53.3\\%$, and $61.2\\%$ on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of $28.0\\%$ on AndroidWorld and $19.8\\%$ on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.",
        "arxiv_id": "2509.26539",
        "ARXIVID": "2509.26539",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (embodied AI methods for GUI agents). Presents a small, on-device GUI agent (Ferret-UI Lite) with new training and inference strategies, and reports competitive results on multiple benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.25559": {
        "authors": [
            "Suvrankar Datta",
            "Divya Buchireddygari",
            "Lakshmi Vennela Chowdary Kaza",
            "Mrudula Bhalke",
            "Kautik Singh",
            "Ayush Pandey",
            "Sonit Sai Vasipalli",
            "Upasana Karnwal",
            "Hakikat Bir Singh Bhatti",
            "Bhavya Ratan Maroo",
            "Sanjana Hebbar",
            "Rahul Joseph",
            "Gurkawal Kaur",
            "Devyani Singh",
            "Akhil V",
            "Dheeksha Devasya Shama Prasad",
            "Nishtha Mahajan",
            "Ayinaparthi Arisha",
            "Rajesh Vanagundi",
            "Reet Nandy",
            "Kartik Vuthoo",
            "Snigdhaa Rajvanshi",
            "Nikhileswar Kondaveeti",
            "Suyash Gunjal",
            "Rishabh Jain",
            "Rajat Jain",
            "Anurag Agrawal"
        ],
        "title": "Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology",
        "abstract": "arXiv:2509.25559v1 Announce Type: new  Abstract: Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level \"spot diagnosis\" cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development.",
        "arxiv_id": "2509.25559",
        "ARXIVID": "2509.25559",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Benchmarks frontier multimodal AI models (VLMs/LLMs) against human experts in radiology, introduces a taxonomy of visual reasoning errors, and provides insights into model limitations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.26498": {
        "authors": [
            "Jijun Xiang",
            "Longliang Liu",
            "Xuan Zhu",
            "Xianqi Wang",
            "Min Lin",
            "Xin Yang"
        ],
        "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance",
        "abstract": "arXiv:2509.26498v1 Announce Type: new  Abstract: Depth enhancement, which converts raw dToF signals into dense depth maps using RGB guidance, is crucial for improving depth perception in high-precision tasks such as 3D reconstruction and SLAM. However, existing methods often assume ideal dToF inputs and perfect dToF-RGB alignment, overlooking calibration errors and anomalies, thus limiting real-world applicability. This work systematically analyzes the noise characteristics of real-world lightweight dToF sensors and proposes a practical and novel depth completion framework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three key aspects. First, we introduce a simulation method based on synthetic datasets to generate realistic training samples for robust model training. Second, we propose a learnable-parameter-free anomaly detection mechanism to identify and remove erroneous dToF measurements, preventing misleading propagation during completion. Third, we design a depth completion network tailored to noisy dToF inputs, which integrates RGB images and pre-trained monocular depth estimation priors to improve depth recovery in challenging regions. On the ZJU-L5 dataset and real-world samples, our training strategy significantly boosts existing depth completion models, with our model achieving state-of-the-art performance, improving RMSE and Rel by 22% and 11% on average. On the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our model improves upon the previous SOTA by 37% in mirror regions. On the Hammer dataset, using simulated low-cost dToF data from RealSense L515, our method surpasses the L515 measurements with an average gain of 22%, demonstrating its potential to enable low-cost sensors to outperform higher-end devices. Qualitative results across diverse real-world datasets further validate the effectiveness and generalizability of our approach.",
        "arxiv_id": "2509.26498",
        "ARXIVID": "2509.26498",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) by proposing a robust depth enhancement framework using RGB guidance and anomaly detection, with practical improvements for real-world sensors.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.26281": {
        "authors": [
            "Teng Zhang",
            "Ziqian Fan",
            "Mingxin Liu",
            "Xin Zhang",
            "Xudong Lu",
            "Wentong Li",
            "Yue Zhou",
            "Yi Yu",
            "Xiang Li",
            "Junchi Yan",
            "Xue Yang"
        ],
        "title": "Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization",
        "abstract": "arXiv:2509.26281v1 Announce Type: new  Abstract: Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.",
        "arxiv_id": "2509.26281",
        "ARXIVID": "2509.26281",
        "COMMENT": "Matches criterion 4: Presents a new method for weakly-supervised oriented object detection, leveraging SAM (Segment Anything Model) and watershed algorithms for improved pseudo-labeling. This is a vision foundation model application with clever statistical tricks for label assignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.26025": {
        "authors": [
            "Shian Du",
            "Menghan Xia",
            "Chang Liu",
            "Xintao Wang",
            "Jing Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Xiangyang Ji"
        ],
        "title": "PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution",
        "abstract": "arXiv:2509.26025v1 Announce Type: new  Abstract: Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR. This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused by incomplete semantics of patches. Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame. Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency.",
        "arxiv_id": "2509.26025",
        "ARXIVID": "2509.26025",
        "COMMENT": "PatchVSR proposes a patch-wise video super-resolution method using video diffusion priors, with a dual-stream adapter and multi-patch joint modulation. This is relevant to vision foundation models and generative modeling (criterion 4), but not directly to spatial intelligence or embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.26455": {
        "authors": [
            "Hanzhou Liu",
            "Jia Huang",
            "Mi Lu",
            "Srikanth Saripalli",
            "Peng Jiang"
        ],
        "title": "Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting",
        "abstract": "arXiv:2509.26455v1 Announce Type: new  Abstract: We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings.",
        "arxiv_id": "2509.26455",
        "ARXIVID": "2509.26455",
        "COMMENT": "Stylos introduces a new framework for 3D style transfer using single-forward Gaussian splatting, with a transformer backbone and a novel 3D style loss. It is relevant to vision foundation models and their applications (criterion 4), especially in 3D vision and generative modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.26161": {
        "authors": [
            "Runxin Yang",
            "Yuxuan Wan",
            "Shuqing Li",
            "Michael R. Lyu"
        ],
        "title": "90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development",
        "abstract": "arXiv:2509.26161v1 Announce Type: new  Abstract: Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine.   To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at https://github.com/yxwan123/UniGen. A video demonstration is available at https://www.youtube.com/watch?v=xyJjFfnxUx0.",
        "arxiv_id": "2509.26161",
        "ARXIVID": "2509.26161",
        "COMMENT": "This paper presents UniGen, an MLLM-driven framework for zero-code 3D game development. It is relevant to criterion 2 (MLLMs) and also touches on vision foundation models in application, but the main focus is on automation and multi-agent coordination rather than core advances in MLLMs or spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.26219": {
        "authors": [
            "Chenyang Jiang",
            "Zhengcen Li",
            "Hang Zhao",
            "Qiben Shan",
            "Shaocong Wu",
            "Jingyong Su"
        ],
        "title": "Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation",
        "abstract": "arXiv:2509.26219v1 Announce Type: new  Abstract: Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.",
        "arxiv_id": "2509.26219",
        "ARXIVID": "2509.26219",
        "COMMENT": "This paper proposes a new method for dataset distillation using sparse Gaussian representations, which is a clever statistical trick in generative modeling and vision. It is relevant to vision foundation models (criterion 4) as it can be used to train them more efficiently, but does not directly address spatial intelligence, VLLMs/MLLMs, or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.25934": {
        "authors": [
            "Yuan Zhao",
            "Youwei Pang",
            "Lihe Zhang",
            "Hanqi Liu",
            "Jiaming Zuo",
            "Huchuan Lu",
            "Xiaoqi Zhao"
        ],
        "title": "UniMMAD: Unified Multi-Modal and Multi-Class Anomaly Detection via MoE-Driven Feature Decompression",
        "abstract": "arXiv:2509.25934v1 Announce Type: new  Abstract: Existing anomaly detection (AD) methods often treat the modality and class as independent factors. Although this paradigm has enriched the development of AD research branches and produced many specialized models, it has also led to fragmented solutions and excessive memory overhead. Moreover, reconstruction-based multi-class approaches typically rely on shared decoding paths, which struggle to handle large variations across domains, resulting in distorted normality boundaries, domain interference, and high false alarm rates. To address these limitations, we propose UniMMAD, a unified framework for multi-modal and multi-class anomaly detection. At the core of UniMMAD is a Mixture-of-Experts (MoE)-driven feature decompression mechanism, which enables adaptive and disentangled reconstruction tailored to specific domains. This process is guided by a ``general to specific'' paradigm. In the encoding stage, multi-modal inputs of varying combinations are compressed into compact, general-purpose features. The encoder incorporates a feature compression module to suppress latent anomalies, encourage cross-modal interaction, and avoid shortcut learning. In the decoding stage, the general features are decompressed into modality-specific and class-specific forms via a sparsely-gated cross MoE, which dynamically selects expert pathways based on input modality and class. To further improve efficiency, we design a grouped dynamic filtering mechanism and a MoE-in-MoE structure, reducing parameter usage by 75\\% while maintaining sparse activation and fast inference. UniMMAD achieves state-of-the-art performance on 9 anomaly detection datasets, spanning 3 fields, 12 modalities, and 66 classes.   The source code will be available at https://github.com/yuanzhao-CVLAB/UniMMAD.",
        "arxiv_id": "2509.25934",
        "ARXIVID": "2509.25934",
        "COMMENT": "UniMMAD introduces a unified multi-modal, multi-class anomaly detection framework using a Mixture-of-Experts approach. It is relevant to multi-modal learning and vision foundation models (criterion 4), but not directly to spatial intelligence or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.25299": {
        "authors": [
            "Daniel Platnick",
            "Mohamed E. Bengueddache",
            "Marjan Alirezaie",
            "Dava J. Newman",
            "Alex ''Sandy'' Pentland",
            "Hossein Rahnama"
        ],
        "title": "ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents",
        "abstract": "arXiv:2509.25299v1 Announce Type: new  Abstract: Generative agents powered by language models are increasingly deployed for long-horizon tasks. However, as long-term memory context grows over time, they struggle to maintain coherence. This deficiency leads to critical failures, including identity drift, ignoring established beliefs, and the propagation of hallucinations in multi-agent systems. To mitigate these challenges, this paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism designed to ground an agent's persona and persistent preferences in a dynamic, structured identity model: a knowledge graph of core beliefs, traits, and values. During the agent's decision loop, this model is queried to retrieve relevant identity context, which directly informs action selection. We demonstrate this approach by introducing and implementing a new class of ID-RAG enabled agents called Human-AI Agents (HAis), where the identity model is inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic knowledge graph learned from a real-world entity's digital footprint. In social simulations of a mayoral election, HAis using ID-RAG outperformed baseline agents in long-horizon persona coherence - achieving higher identity recall across all tested models by the fourth timestep - and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as an explicit, retrievable knowledge structure, ID-RAG offers a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents. Our code is open-source and available at: https://github.com/flybits/humanai-agents.",
        "arxiv_id": "2509.25299",
        "ARXIVID": "2509.25299",
        "COMMENT": "This paper proposes ID-RAG, a new method for maintaining long-horizon persona coherence in generative agents using a knowledge graph-based identity model. It is a new method for generative agents, but not focused on spatial intelligence or embodied AI in the sense of physical agents. Closest to criterion 3 (new methods for agent coherence), but not simulator/embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.26227": {
        "authors": [
            "Haiyang Zheng",
            "Nan Pu",
            "Wenjing Li",
            "Nicu Sebe",
            "Zhun Zhong"
        ],
        "title": "Generalized Fine-Grained Category Discovery with Multi-Granularity Conceptual Experts",
        "abstract": "arXiv:2509.26227v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) is an open-world problem that clusters unlabeled data by leveraging knowledge from partially labeled categories. A key challenge is that unlabeled data may contain both known and novel categories. Existing approaches suffer from two main limitations. First, they fail to exploit multi-granularity conceptual information in visual data, which limits representation quality. Second, most assume that the number of unlabeled categories is known during training, which is impractical in real-world scenarios. To address these issues, we propose a Multi-Granularity Conceptual Experts (MGCE) framework that adaptively mines visual concepts and integrates multi-granularity knowledge for accurate category discovery. MGCE consists of two modules: (1) Dynamic Conceptual Contrastive Learning (DCCL), which alternates between concept mining and dual-level representation learning to jointly optimize feature learning and category discovery; and (2) Multi-Granularity Experts Collaborative Learning (MECL), which extends the single-expert paradigm by introducing additional experts at different granularities and by employing a concept alignment matrix for effective cross-expert collaboration. Importantly, MGCE can automatically estimate the number of categories in unlabeled data, making it suitable for practical open-world settings. Extensive experiments on nine fine-grained visual recognition benchmarks demonstrate that MGCE achieves state-of-the-art results, particularly in novel-class accuracy. Notably, even without prior knowledge of category numbers, MGCE outperforms parametric approaches that require knowing the exact number of categories, with an average improvement of 3.6\\%. Code is available at https://github.com/HaiyangZheng/MGCE.",
        "arxiv_id": "2509.26227",
        "ARXIVID": "2509.26227",
        "COMMENT": "Partially matches criterion 4: Proposes a new method for open-world fine-grained category discovery using multi-granularity conceptual experts. While not directly about vision foundation models, it is relevant to advanced visual representation learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.26037": {
        "authors": [
            "Zhe Li",
            "Zhiwei Lin",
            "Yongtao Wang"
        ],
        "title": "CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search",
        "abstract": "arXiv:2509.26037v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) with Neural Architecture Search (NAS) has introduced new possibilities for automating the design of neural architectures. However, most existing methods face critical limitations, including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS. In this work, we present Collaborative LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided search driven by two complementary LLMs. Specifically, we propose a Navigator LLM to guide search direction and a Generator LLM to synthesize high-quality candidates, with a dedicated Coordinator module to manage their interaction. CoLLM-NAS efficiently guides the search process by combining LLMs' inherent knowledge of structured neural architectures with progressive knowledge from iterative feedback and historical trajectory. Experimental results on ImageNet and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and conventional search algorithms, achieving new state-of-the-art results. Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its excellent generalization.",
        "arxiv_id": "2509.26037",
        "ARXIVID": "2509.26037",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) via LLMs for neural architecture search, but not a direct match to vision-language or spatial intelligence criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.25374": {
        "authors": [
            "Jialin Wu",
            "Xiaofeng Liu"
        ],
        "title": "Saliency Guided Longitudinal Medical Visual Question Answering",
        "abstract": "arXiv:2509.25374v1 Announce Type: new  Abstract: Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.",
        "arxiv_id": "2509.25374",
        "ARXIVID": "2509.25374",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) via a novel saliency-guided encoder-decoder for longitudinal medical VQA, but not a direct match to any criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.25816": {
        "authors": [
            "Christophe Botella",
            "Benjamin Deneu",
            "Diego Marcos",
            "Maximilien Servajean",
            "Theo Larcher",
            "Cesar Leblanc",
            "Joaquim Estopinan",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "title": "Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing",
        "abstract": "arXiv:2509.25816v1 Announce Type: new  Abstract: Understanding the spatio-temporal distribution of species is a cornerstone of ecology and conservation. By pairing species observations with geographic and environmental predictors, researchers can model the relationship between an environment and the species which may be found there. To advance the state- of-the-art in this area with deep learning models and remote sensing data, we organized an open machine learning challenge called GeoLifeCLEF 2023. The training dataset comprised 5 million plant species observations (single positive label per sample) distributed across Europe and covering most of its flora, high-resolution rasters: remote sensing imagery, land cover, elevation, in addition to coarse-resolution data: climate, soil and human footprint variables. In this multi-label classification task, we evaluated models ability to predict the species composition in 22 thousand small plots based on standardized surveys. This paper presents an overview of the competition, synthesizes the approaches used by the participating teams, and analyzes the main results. In particular, we highlight the biases faced by the methods fitted to single positive labels when it comes to the multi-label evaluation, and the new and effective learning strategy combining single and multi-label data in training.",
        "arxiv_id": "2509.25816",
        "ARXIVID": "2509.25816",
        "COMMENT": "Partially matches criterion 4: Presents a large-scale benchmark and challenge for species composition prediction using remote sensing and deep learning. While not directly about vision foundation models, it is relevant to large-scale vision applications and benchmarking.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.25705": {
        "authors": [
            "Juyeop Kim",
            "Songkuk Kim",
            "Jong-Seok Lee"
        ],
        "title": "How Diffusion Models Memorize",
        "abstract": "arXiv:2509.25705v1 Announce Type: new  Abstract: Despite their success in image generation, diffusion models can memorize training data, raising serious privacy and copyright concerns. Although prior work has sought to characterize, detect, and mitigate memorization, the fundamental question of why and how it occurs remains unresolved. In this paper, we revisit the diffusion and denoising process and analyze latent space dynamics to address the question: \"How do diffusion models memorize?\" We show that memorization is driven by the overestimation of training samples during early denoising, which reduces diversity, collapses denoising trajectories, and accelerates convergence toward the memorized image. Specifically: (i) memorization cannot be explained by overfitting alone, as training loss is larger under memorization due to classifier-free guidance amplifying predictions and inducing overestimation; (ii) memorized prompts inject training images into noise predictions, forcing latent trajectories to converge and steering denoising toward their paired samples; and (iii) a decomposition of intermediate latents reveals how initial randomness is quickly suppressed and replaced by memorized content, with deviations from the theoretical denoising schedule correlating almost perfectly with memorization severity. Together, these results identify early overestimation as the central underlying mechanism of memorization in diffusion models.",
        "arxiv_id": "2509.25705",
        "ARXIVID": "2509.25705",
        "COMMENT": "This paper analyzes memorization in diffusion models, providing new insights into generative modeling. While it is relevant to your friend's interests in generative modeling and clever statistical analysis, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.25682": {
        "authors": [
            "Shiyu Wu",
            "Shuyan Li",
            "Jing Li",
            "Jing Liu",
            "Yequan Wang"
        ],
        "title": "OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution",
        "abstract": "arXiv:2509.25682v1 Announce Type: new  Abstract: AI-generated image (AIGI) detection and source model attribution remain central challenges in combating deepfake abuses, primarily due to the structural diversity of generative models. Current detection methods are prone to overfitting specific forgery traits, whereas source attribution offers a robust alternative through fine-grained feature discrimination. However, synthetic image attribution remains constrained by the scarcity of large-scale, well-categorized synthetic datasets, limiting its practicality and compatibility with detection systems. In this work, we propose a new paradigm for image attribution called open-set, few-shot source identification. This paradigm is designed to reliably identify unseen generators using only limited samples, making it highly suitable for real-world application. To this end, we introduce OmniDFA (Omni Detector and Few-shot Attributor), a novel framework for AIGI that not only assesses the authenticity of images, but also determines the synthesis origins in a few-shot manner. To facilitate this work, we construct OmniFake, a large class-aware synthetic image dataset that curates $1.17$ M images from $45$ distinct generative models, substantially enriching the foundational resources for research on both AIGI detection and attribution. Experiments demonstrate that OmniDFA exhibits excellent capability in open-set attribution and achieves state-of-the-art generalization performance on AIGI detection. Our dataset and code will be made available.",
        "arxiv_id": "2509.25682",
        "ARXIVID": "2509.25682",
        "COMMENT": "This paper introduces a new framework (OmniDFA) and a large synthetic dataset (OmniFake) for open-set, few-shot attribution of AI-generated images. While it is relevant to computer vision and generative modeling, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models per se. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.26574": {
        "authors": [
            "Minhui Zhu",
            "Minyang Tian",
            "Xiaocheng Yang",
            "Tianci Zhou",
            "Penghao Zhu",
            "Eli Chertkov",
            "Shengyan Liu",
            "Yufeng Du",
            "Lifan Yuan",
            "Ziming Ji",
            "Indranil Das",
            "Junyi Cao",
            "Yufeng Du",
            "Jinchen He",
            "Yifan Su",
            "Jiabin Yu",
            "Yikun Jiang",
            "Yujie Zhang",
            "Chang Liu",
            "Ze-Min Huang",
            "Weizhen Jia",
            "Xinan Chen",
            "Peixue Wu",
            "Yunkai Wang",
            "Juntai Zhou",
            "Yong Zhao",
            "Farshid Jafarpour",
            "Jessie Shelton",
            "Aaron Young",
            "John Bartolotta",
            "Wenchao Xu",
            "Yue Sun",
            "Anjun Chu",
            "Victor Colussi",
            "Chris Akers",
            "Nathan Brooks",
            "Wenbo Fu",
            "Christopher Wilson",
            "Jinchao Zhao",
            "Marvin Qi",
            "Anqi Mu",
            "Yubo Yang",
            "Allen Zang",
            "Yang Lyu",
            "Peizhi Mai",
            "Xuefei Guo",
            "Luyu Gao",
            "Ze Yang",
            "Chi Xue",
            "Dmytro Bandak",
            "Ya\\\"ir Hein",
            "Yonatan Kahn",
            "Kevin Zhou",
            "John Drew Wilson Jarrod T. Reilly",
            "Di Luo",
            "Daniel Inafuku",
            "Hao Tong",
            "Liang Yang",
            "Ruixing Zhang",
            "Xueying Wang",
            "Ofir Press",
            "Nicolas Chia",
            "Eliu Huerta",
            "Hao Peng"
        ],
        "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark",
        "abstract": "arXiv:2509.26574v1 Announce Type: new  Abstract: While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced \"critical point\"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.",
        "arxiv_id": "2509.26574",
        "ARXIVID": "2509.26574",
        "COMMENT": "Does not match any specific criterion. Introduces a new benchmark for LLM reasoning in physics research, but not related to vision, spatial intelligence, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.26100": {
        "authors": [
            "Yixu Wang",
            "Xin Wang",
            "Yang Yao",
            "Xinyuan Li",
            "Yan Teng",
            "Xingjun Ma",
            "Yingchun Wang"
        ],
        "title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs",
        "abstract": "arXiv:2509.26100v1 Announce Type: new  Abstract: The rapid integration of Large Language Models (LLMs) into high-stakes domains necessitates reliable safety and compliance evaluation. However, existing static benchmarks are ill-equipped to address the dynamic nature of AI risks and evolving regulations, creating a critical safety gap. This paper introduces a new paradigm of agentic safety evaluation, reframing evaluation as a continuous and self-evolving process rather than a one-time audit. We then propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests unstructured policy documents to generate and perpetually evolve a comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline of specialized agents and incorporates a Self-evolving Evaluation loop, where the system learns from evaluation results to craft progressively more sophisticated and targeted test cases. Our experiments demonstrate the effectiveness of SafeEvalAgent, showing a consistent decline in model safety as the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act drops from 72.50% to 36.36% over successive iterations. These findings reveal the limitations of static assessments and highlight our framework's ability to uncover deep vulnerabilities missed by traditional methods, underscoring the urgent need for dynamic evaluation ecosystems to ensure the safe and responsible deployment of advanced AI.",
        "arxiv_id": "2509.26100",
        "ARXIVID": "2509.26100",
        "COMMENT": "This paper introduces a new agentic, self-evolving safety evaluation framework for LLMs, with a multi-agent system and dynamic benchmark generation. It is a new benchmark for LLM safety, but not for embodied AI or spatial intelligence. Closest to criterion 3, but not focused on embodied AI or simulators.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.25776": {
        "authors": [
            "Mingyu Kang",
            "Yong Suk Choi"
        ],
        "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation",
        "abstract": "arXiv:2509.25776v1 Announce Type: new  Abstract: Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.",
        "arxiv_id": "2509.25776",
        "ARXIVID": "2509.25776",
        "COMMENT": "This paper proposes a new inversion technique for text-to-image diffusion models, improving image editing. It is relevant to generative modeling and vision, but does not directly address any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.26209": {
        "authors": [
            "Zican Hu",
            "Shilin Zhang",
            "Yafu Li",
            "Jianhao Yan",
            "Xuyang Hu",
            "Leyang Cui",
            "Xiaoye Qu",
            "Chunlin Chen",
            "Yu Cheng",
            "Zhi Wang"
        ],
        "title": "Diversity-Incentivized Exploration for Versatile Reasoning",
        "abstract": "arXiv:2509.26209v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \\textbf{DIVER} (\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for \\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.",
        "arxiv_id": "2509.26209",
        "ARXIVID": "2509.26209",
        "COMMENT": "Does not match any specific criterion. Proposes a diversity-incentivized exploration method for LLM reasoning, not directly related to vision, spatial, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.26157": {
        "authors": [
            "Sachith Abeywickrama",
            "Emadeldeen Eldele",
            "Min Wu",
            "Xiaoli Li",
            "Chau Yuen"
        ],
        "title": "EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting",
        "abstract": "arXiv:2509.26157v1 Announce Type: new  Abstract: Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.",
        "arxiv_id": "2509.26157",
        "ARXIVID": "2509.26157",
        "COMMENT": "Does not match any specific criterion. Focuses on time series forecasting with transformers, not spatial intelligence, vision-language, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25420": {
        "authors": [
            "Yingqian Cui",
            "Zhenwei Dai",
            "Pengfei He",
            "Bing He",
            "Hui Liu",
            "Xianfeng Tang",
            "Jingying Zeng",
            "Suhang Wang",
            "Yue Xing",
            "Jiliang Tang",
            "Benoit Dumoulin"
        ],
        "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search",
        "abstract": "arXiv:2509.25420v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. Although effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually. Specifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.",
        "arxiv_id": "2509.25420",
        "ARXIVID": "2509.25420",
        "COMMENT": "This paper proposes a dual-phase, reward-guided search for adaptive test-time reasoning in LLMs, improving efficiency and accuracy. It is a methodological improvement for LLM reasoning, but not for spatial intelligence, embodied AI, or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25689": {
        "authors": [
            "Yixiao Chen",
            "Yanyue Xie",
            "Ruining Yang",
            "Wei Jiang",
            "Wei Wang",
            "Yong He",
            "Yue Chen",
            "Pu Zhao",
            "Yanzhi Wang"
        ],
        "title": "Collaborative Compression for Large-Scale MoE Deployment on Edge",
        "abstract": "arXiv:2509.25689v1 Announce Type: new  Abstract: The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.",
        "arxiv_id": "2509.25689",
        "ARXIVID": "2509.25689",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to large model deployment and compression. Focuses on MoE compression for edge deployment, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26398": {
        "authors": [
            "Atakan Topaloglu",
            "Ahmet Bilican",
            "Cansu Korkmaz",
            "A. Murat Tekalp"
        ],
        "title": "Image-Difficulty-Aware Evaluation of Super-Resolution Models",
        "abstract": "arXiv:2509.26398v1 Announce Type: new  Abstract: Image super-resolution models are commonly evaluated by average scores (over some benchmark test sets), which fail to reflect the performance of these models on images of varying difficulty and that some models generate artifacts on certain difficult images, which is not reflected by the average scores. We propose difficulty-aware performance evaluation procedures to better differentiate between SISR models that produce visually different results on some images but yield close average performance scores over the entire test set. In particular, we propose two image-difficulty measures, the high-frequency index and rotation-invariant edge index, to predict those test images, where a model would yield significantly better visual results over another model, and an evaluation method where these visual differences are reflected on objective measures. Experimental results demonstrate the effectiveness of the proposed image-difficulty measures and evaluation methodology.",
        "arxiv_id": "2509.26398",
        "ARXIVID": "2509.26398",
        "COMMENT": "This paper proposes new image-difficulty-aware evaluation metrics for super-resolution models. While it is a methodological improvement in evaluation, it is not directly related to spatial intelligence, embodied AI, or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26614": {
        "authors": [
            "Xinjin Li",
            "Yu Ma",
            "Kaisen Ye",
            "Jinghan Cao",
            "Minghao Zhou",
            "Yeyang Zhou"
        ],
        "title": "Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods for Enhanced Facial Expression Classification",
        "abstract": "arXiv:2509.26614v1 Announce Type: new  Abstract: Facial expression classification remains a challenging task due to the high dimensionality and inherent complexity of facial image data. This paper presents Hy-Facial, a hybrid feature extraction framework that integrates both deep learning and traditional image processing techniques, complemented by a systematic investigation of dimensionality reduction strategies. The proposed method fuses deep features extracted from the Visual Geometry Group 19-layer network (VGG19) with handcrafted local descriptors and the scale-invariant feature transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB) algorithms, to obtain rich and diverse image representations. To mitigate feature redundancy and reduce computational complexity, we conduct a comprehensive evaluation of dimensionality reduction techniques and feature extraction. Among these, UMAP is identified as the most effective, preserving both local and global structures of the high-dimensional feature space. The Hy-Facial pipeline integrated VGG19, SIFT, and ORB for feature extraction, followed by K-means clustering and UMAP for dimensionality reduction, resulting in a classification accuracy of 83. 3\\% in the facial expression recognition (FER) dataset. These findings underscore the pivotal role of dimensionality reduction not only as a pre-processing step but as an essential component in improving feature quality and overall classification performance.",
        "arxiv_id": "2509.26614",
        "ARXIVID": "2509.26614",
        "COMMENT": "Hy-Facial presents a hybrid feature extraction and dimensionality reduction pipeline for facial expression classification. It is a vision application, but not related to the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}