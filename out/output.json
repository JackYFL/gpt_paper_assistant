{
    "2511.14131": {
        "authors": [
            "Yu Zhong",
            "Zihao Zhang",
            "Rui Zhang",
            "Lingdong Huang",
            "Haihan Gao",
            "Shuo Wang",
            "Da Li",
            "Ruijian Han",
            "Jiaming Guo",
            "Shaohui Peng",
            "Di Huang",
            "Yunji Chen"
        ],
        "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation",
        "abstract": "arXiv:2511.14131v1 Announce Type: new  Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.",
        "arxiv_id": "2511.14131",
        "ARXIVID": "2511.14131",
        "COMMENT": "Matches criteria 1 and 3: This paper presents a dual-process thinking system for Vision-and-Language Navigation (VLN), integrating LLMs and expert models with a novel regulator for switching modes. It is a new method for embodied AI navigation, focusing on spatial understanding and reasoning, and introduces a novel framework for VLN tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.14259": {
        "authors": [
            "Zitong Xu",
            "Huiyu Duan",
            "Xiaoyu Wang",
            "Zhaolin Cai",
            "Kaiwei Zhang",
            "Qiang Hu",
            "Jing Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation",
        "abstract": "arXiv:2511.14259v1 Announce Type: new  Abstract: With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \\textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \\textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.",
        "arxiv_id": "2511.14259",
        "ARXIVID": "2511.14259",
        "COMMENT": "Matches criteria 2 (new MLLM for image manipulation detection, localization, and explanation) and 3 (introduces a new large-scale benchmark for image manipulation detection and localization, with novel interpretability features). Also relevant to 4 (application of MLLMs to vision foundation tasks).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.14183": {
        "authors": [
            "Jingdong Zhang",
            "Lingzhi Zhang",
            "Qing Liu",
            "Mang Tik Chiu",
            "Connelly Barnes",
            "Yizhou Wang",
            "Haoran You",
            "Xiaoyang Liu",
            "Yuqian Zhou",
            "Zhe Lin",
            "Eli Shechtman",
            "Sohrab Amirghodsi",
            "Xin Li",
            "Wenping Wang",
            "Xiaohang Zhan"
        ],
        "title": "UniSER: A Foundation Model for Unified Soft Effects Removal",
        "abstract": "arXiv:2511.14183v1 Announce Type: new  Abstract: Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.",
        "arxiv_id": "2511.14183",
        "ARXIVID": "2511.14183",
        "COMMENT": "UniSER is a vision foundation model for unified soft effects removal, trained on a massive dataset and outperforming both specialist and generalist models. This directly matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.14336": {
        "authors": [
            "Bohan Zhang",
            "Yiyi Miao",
            "Taoyu Wu",
            "Tong Chen",
            "Ji Jiang",
            "Zhuoxiao Li",
            "Zhe Tang",
            "Limin Yu",
            "Jionglong Su"
        ],
        "title": "ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding",
        "abstract": "arXiv:2511.14336v1 Announce Type: new  Abstract: A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.",
        "arxiv_id": "2511.14336",
        "ARXIVID": "2511.14336",
        "COMMENT": "ArchMap introduces a training-free, knowledge-guided vision-language model for structured dental understanding from 3D intraoral scans. It combines geometric normalization with ontology-guided multimodal reasoning, directly matching criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.14149": {
        "authors": [
            "Hao Wang",
            "Linqing Zhao",
            "Xiuwei Xu",
            "Jiwen Lu",
            "Haibin Yan"
        ],
        "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
        "abstract": "arXiv:2511.14149v1 Announce Type: new  Abstract: Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2{\\deg} while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
        "arxiv_id": "2511.14149",
        "ARXIVID": "2511.14149",
        "COMMENT": "Presents a real-time camera pose estimation method using 3D Gaussian splatting inversion, with direct application to robotics and spatial understanding. Closely matches criteria 1 (spatial intelligence for embodied agents) and 3 (novel method for embodied AI, real-time pose estimation).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.14760": {
        "authors": [
            "Rui Tian",
            "Mingfei Gao",
            "Haiming Gang",
            "Jiasen Lu",
            "Zhe Gan",
            "Yinfei Yang",
            "Zuxuan Wu",
            "Afshin Dehghan"
        ],
        "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning",
        "abstract": "arXiv:2511.14760v1 Announce Type: new  Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.",
        "arxiv_id": "2511.14760",
        "ARXIVID": "2511.14760",
        "COMMENT": "UniGen-1.5 is a unified multimodal large language model (MLLM) for image understanding, generation, and editing, with a novel RL-based reward unification and edit instruction alignment. This is a direct match to criterion 2 (new VLLMs/MLLMs) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.13945": {
        "authors": [
            "Zachary Shinnick",
            "Liangze Jiang",
            "Hemanth Saratchandran",
            "Damien Teney",
            "Anton van den Hengel"
        ],
        "title": "Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers",
        "abstract": "arXiv:2511.13945v1 Announce Type: new  Abstract: Transformers show remarkable versatility across domains, suggesting the existence of inductive biases beneficial across modalities. In this work, we explore a new way to instil such generic biases in vision transformers (ViTs) by pretraining on procedurally-generated data devoid of visual or semantic content. We generate this data with simple algorithms such as formal grammars, so the results bear no relationship to either natural or synthetic images. We use this procedurally-generated data to pretrain ViTs in a warm-up phase that bypasses their visual patch embedding mechanisms, thus encouraging the models to internalise abstract computational priors. When followed by standard image-based training, this warm-up significantly improves data efficiency, convergence speed, and downstream performance. On ImageNet-1k for example, allocating just 1% of the training budget to procedural data improves final accuracy by over 1.7%. In terms of its effect on performance, 1% procedurally generated data is thus equivalent to 28% of the ImageNet-1k data. These findings suggest a promising path toward new data-efficient and domain-agnostic pretraining strategies.",
        "arxiv_id": "2511.13945",
        "ARXIVID": "2511.13945",
        "COMMENT": "This paper explores pretraining vision transformers on procedurally-generated, non-visual data to instill abstract computational priors, improving data efficiency and performance. This is a direct match to criterion 4 (vision foundation models and their pretraining strategies), and presents a novel, surprising empirical result.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.14169": {
        "authors": [
            "Xinliang Zhang",
            "Lei Zhu",
            "Hangzhou He",
            "Shuang Zeng",
            "Ourui Fu",
            "Jiakui Hu",
            "Zhengjian Yao",
            "Yanye Lu"
        ],
        "title": "AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs",
        "abstract": "arXiv:2511.14169v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.",
        "arxiv_id": "2511.14169",
        "ARXIVID": "2511.14169",
        "COMMENT": "Proposes an object-aware adaptive token compression for efficient MLLMs, directly matching criteria 2 (MLLMs) and 4 (vision foundation model efficiency).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.14394": {
        "authors": [
            "Joachim Tesch",
            "Giorgio Becherini",
            "Prerana Achar",
            "Anastasios Yiannakidis",
            "Muhammed Kocabas",
            "Priyanka Patel",
            "Michael J. Black"
        ],
        "title": "BEDLAM2.0: Synthetic Humans and Cameras in Motion",
        "abstract": "arXiv:2511.14394v1 Announce Type: new  Abstract: Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.",
        "arxiv_id": "2511.14394",
        "ARXIVID": "2511.14394",
        "COMMENT": "BEDLAM2.0 is a new dataset for 3D human motion and camera motion estimation, providing ground truth for both. This is a new benchmark for embodied AI (criterion 3), especially for spatial understanding in dynamic scenes.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.14291": {
        "authors": [
            "Yuxin Zhang",
            "Ziyu Lu",
            "Hongbo Duan",
            "Keyu Fan",
            "Pengting Luo",
            "Peiyu Zhuang",
            "Mengyu Yang",
            "Houde Liu"
        ],
        "title": "GEN3D: Generating Domain-Free 3D Scenes from a Single Image",
        "abstract": "arXiv:2511.14291v1 Announce Type: new  Abstract: Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.",
        "arxiv_id": "2511.14291",
        "ARXIVID": "2511.14291",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for world model/scene generation) and criterion 4 (vision foundation models and applications). Proposes a new method for generating 3D scenes from a single image, which is important for embodied AI and world models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.14093": {
        "authors": [
            "Fan Zhang",
            "Haoyuan Ren",
            "Fei Ma",
            "Qiang Yin",
            "Yongsheng Zhou"
        ],
        "title": "SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts",
        "abstract": "arXiv:2511.14093v1 Announce Type: new  Abstract: Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images. Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage \"retrieval-matching\" pipelines are prone to cumulative errors. To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization. This model supports click prompting and can output object Geo-localization in real time when prompted to allow for interactive use. The model employs a fully transformer-based architecture, utilizing a Swin-Transformer for joint feature encoding of both drone and satellite imagery and an anchor-free transformer detection head for coordinate regression. In order to better capture both inter-modal and intra-view dependencies, we introduce a grid-level sparse Mixture-of-Experts (GMoE) into the cross-view encoder, allowing it to adaptively activate specialized experts according to the content, scale and source of each grid. We also employ an anchor-free detection head for coordinate regression, directly predicting object locations via heat-map supervision in the reference images. This approach avoids scale bias and matching complexity introduced by predefined anchor boxes. On the drone-to-satellite task, SMGeo achieves leading performance in accuracy at IoU=0.25 and mIoU metrics (e.g., 87.51%, 62.50%, and 61.45% in the test set, respectively), significantly outperforming representative methods such as DetGeo (61.97%, 57.66%, and 54.05%, respectively). Ablation studies demonstrate complementary gains from shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts.",
        "arxiv_id": "2511.14093",
        "ARXIVID": "2511.14093",
        "COMMENT": "Matches criterion 1: Proposes a transformer-based model (SMGeo) for cross-view object geo-localization, with a novel grid-level mixture-of-experts for spatial reasoning across drone and satellite imagery. This is a methodological improvement in spatial intelligence and cross-view understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13864": {
        "authors": [
            "Changyang Li",
            "Xuejian Ma",
            "Lixiang Liu",
            "Zhan Li",
            "Qingan Yan",
            "Yi Xu"
        ],
        "title": "GRLoc: Geometric Representation Regression for Visual Localization",
        "abstract": "arXiv:2511.13864v1 Announce Type: new  Abstract: Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.",
        "arxiv_id": "2511.13864",
        "ARXIVID": "2511.13864",
        "COMMENT": "Matches criterion 1: This paper introduces a new geometric representation regression paradigm for visual localization, explicitly modeling 3D scene geometry and decoupling rotation/translation. This is a methodological improvement to spatial understanding for embodied agents and vision tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.14751": {
        "authors": [
            "Yutian Chen",
            "Yuheng Qiu",
            "Ruogu Li",
            "Ali Agha",
            "Shayegan Omidshafiei",
            "Jay Patrikar",
            "Sebastian Scherer"
        ],
        "title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers",
        "abstract": "arXiv:2511.14751v1 Announce Type: new  Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.",
        "arxiv_id": "2511.14751",
        "ARXIVID": "2511.14751",
        "COMMENT": "Presents a new token merging acceleration method for visual geometric transformers, with a focus on spatial coverage and real-time 3D perception. Closely matches criteria 1 (spatial understanding in embodied agents) and 4 (vision foundation model acceleration/application).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13889": {
        "authors": [
            "Abdul Rehman",
            "Iqra Rasool",
            "Ayesha Imran",
            "Mohsen Ali",
            "Waqas Sultani"
        ],
        "title": "Uni-Hema: Unified Model for Digital Hematopathology",
        "abstract": "arXiv:2511.13889v1 Announce Type: new  Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.",
        "arxiv_id": "2511.13889",
        "ARXIVID": "2511.13889",
        "COMMENT": "Uni-Hema is a unified, multi-task, multi-modal model for digital hematopathology, integrating detection, classification, segmentation, and reasoning. It is a new MLLM (criterion 2) and also relates to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.14582": {
        "authors": [
            "Keda Tao",
            "Kele Shao",
            "Bohan Yu",
            "Weiqiang Wang",
            "Jian liu",
            "Huan Wang"
        ],
        "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
        "abstract": "arXiv:2511.14582v1 Announce Type: new  Abstract: Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
        "arxiv_id": "2511.14582",
        "ARXIVID": "2511.14582",
        "COMMENT": "Presents a new token compression framework for omnimodal LLMs (audio-visual), which is relevant to criteria 2 (MLLMs) and 4 (efficient application of vision foundation models).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.13863": {
        "authors": [
            "Kranti Kumar Parida",
            "Omar Emara",
            "Hazel Doughty",
            "Dima Damen"
        ],
        "title": "Segmenting Collision Sound Sources in Egocentric Videos",
        "abstract": "arXiv:2511.13863v1 Announce Type: new  Abstract: Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.   To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\\times$ and $4.7\\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.",
        "arxiv_id": "2511.13863",
        "ARXIVID": "2511.13863",
        "COMMENT": "This paper introduces the novel task of Collision Sound Source Segmentation (CS3) in egocentric videos, using foundation models (CLIP, SAM2) and egocentric cues. This is relevant to criterion 4 (vision foundation models and applications), and also presents a new multi-modal segmentation task (audio-visual), which is of interest for multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.13794": {
        "authors": [
            "Huayi Zhu",
            "Xiu Shu",
            "Youqiang Xiong",
            "Qiao Liu",
            "Rui Chen",
            "Di Yuan",
            "Xiaojun Chang",
            "Zhenyu He"
        ],
        "title": "FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching",
        "abstract": "arXiv:2511.13794v1 Announce Type: new  Abstract: Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.",
        "arxiv_id": "2511.13794",
        "ARXIVID": "2511.13794",
        "COMMENT": "FusionFM proposes a unified multi-modal image fusion model using flow matching, with a novel divide-and-conquer refiner and continual learning mechanisms. This is relevant to criterion 4 (vision foundation models and applications) as it presents a general-purpose, multi-task fusion model, and also to criterion 2 (multi-modal generative modeling).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.14391": {
        "authors": [
            "Fabian Schmidt",
            "Noushiq Mohammed Kayilan Abdul Nazar",
            "Markus Enzweiler",
            "Abhinav Valada"
        ],
        "title": "Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition",
        "abstract": "arXiv:2511.14391v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly used for decision-making and planning in autonomous driving, showing promising reasoning capabilities and potential to generalize across diverse traffic situations. However, current LLM-based driving agents lack explicit mechanisms to enforce traffic rules and often struggle to reliably detect small, safety-critical objects such as traffic lights and signs. To address this limitation, we introduce TLS-Assist, a modular redundancy layer that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition. TLS-Assist converts detections into structured natural language messages that are injected into the LLM input, enforcing explicit attention to safety-critical cues. The framework is plug-and-play, model-agnostic, and supports both single-view and multi-view camera setups. We evaluate TLS-Assist in a closed-loop setup on the LangAuto benchmark in CARLA. The results demonstrate relative driving performance improvements of up to 14% over LMDrive and 7% over BEVDriver, while consistently reducing traffic light and sign infractions. We publicly release the code and models on https://github.com/iis-esslingen/TLS-Assist.",
        "arxiv_id": "2511.14391",
        "ARXIVID": "2511.14391",
        "COMMENT": "This paper introduces TLS-Assist, a modular system that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition, improving safety and performance. This is a direct match to criterion 3 (embodied AI, new methods for agent perception in simulators, novel angle: explicit modular redundancy for safety-critical cues in LLM-based driving).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.14742": {
        "authors": [
            "Stefan Cobeli",
            "Kazi Shahrukh Omar",
            "Rodrigo Valen\\c{c}a",
            "Nivan Ferreira",
            "Fabio Miranda"
        ],
        "title": "A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments",
        "abstract": "arXiv:2511.14742v1 Announce Type: new  Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.",
        "arxiv_id": "2511.14742",
        "ARXIVID": "2511.14742",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications). Proposes a neural field-based method for efficient 3D urban data exploration, with applications to spatial reasoning and viewpoint selection.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.13891": {
        "authors": [
            "Seyed Mohamad Ali Tousi",
            "John A. Lory",
            "G. N. DeSouza"
        ],
        "title": "Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models",
        "abstract": "arXiv:2511.13891v1 Announce Type: new  Abstract: Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.",
        "arxiv_id": "2511.13891",
        "ARXIVID": "2511.13891",
        "COMMENT": "Matches criterion 2 (shows new VLLMs) and criterion 4 (vision foundation models and applications). Uses vision-language models for weakly supervised detection in remote sensing, with a novel teacher-student pipeline and a new dataset.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.14469": {
        "authors": [
            "Mingchen Zhong",
            "Xin Lu",
            "Dong Li",
            "Senyan Xu",
            "Ruixuan Jiang",
            "Xueyang Fu",
            "Baocai Yin"
        ],
        "title": "CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring",
        "abstract": "arXiv:2511.14469v1 Announce Type: new  Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.",
        "arxiv_id": "2511.14469",
        "ARXIVID": "2511.14469",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) due to the complex-valued neural network for spatiotemporal fusion of event and RGB data, which is highly relevant for spatial intelligence in embodied systems. Also relevant to vision foundation models (criterion 4) via complex-valued neural networks for multi-modal fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.14031": {
        "authors": [
            "Rong Zhang",
            "Jinxiao Li",
            "Jingnan Wang",
            "Zhiwen Zuo",
            "Jianfeng Dong",
            "Wei Li",
            "Chi Wang",
            "Weiwei Xu",
            "Xun Wang"
        ],
        "title": "FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization",
        "abstract": "arXiv:2511.14031v1 Announce Type: new  Abstract: Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.",
        "arxiv_id": "2511.14031",
        "ARXIVID": "2511.14031",
        "COMMENT": "Matches criterion 4: Proposes FashionMAC, a diffusion-based, deformation-free image generation framework for fashion, with a novel region-adaptive decoupled attention mechanism. This is an application of vision foundation models (diffusion models) to controllable image synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.14749": {
        "authors": [
            "Alexander Vedernikov",
            "Puneet Kumar",
            "Haoyu Chen",
            "Tapio Sepp\\\"anen",
            "Xiaobai Li"
        ],
        "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis",
        "abstract": "arXiv:2511.14749v1 Announce Type: new  Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.",
        "arxiv_id": "2511.14749",
        "ARXIVID": "2511.14749",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models and applications). Investigates VLLMs for handling noisy labels in engagement analysis, with a clever curriculum learning and label refinement strategy.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.13892": {
        "authors": [
            "Badhan Chandra Das",
            "Md Tasnim Jawad",
            "Md Jueal Mia",
            "M. Hadi Amini",
            "Yanzhao Wu"
        ],
        "title": "Jailbreaking Large Vision Language Models in Intelligent Transportation Systems",
        "abstract": "arXiv:2511.13892v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.",
        "arxiv_id": "2511.13892",
        "ARXIVID": "2511.13892",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models and applications). Analyzes vulnerabilities and jailbreaking attacks on LVLMs in intelligent transportation, with new attack/defense methods.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.14063": {
        "authors": [
            "Dongyang Jin",
            "Ryan Xu",
            "Jianhao Zeng",
            "Rui Lan",
            "Yancheng Bai",
            "Lei Sun",
            "Xiangxiang Chu"
        ],
        "title": "Semantic Context Matters: Improving Conditioning for Autoregressive Models",
        "abstract": "arXiv:2511.14063v1 Announce Type: new  Abstract: Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.",
        "arxiv_id": "2511.14063",
        "ARXIVID": "2511.14063",
        "COMMENT": "Related to generative modeling and conditioning in autoregressive models, which is tangential to criteria 2 and 4, but does not introduce a new VLLM/MLLM or vision foundation model. Focuses on semantic context for AR models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.14315": {
        "authors": [
            "Yiyi Miao",
            "Taoyu Wu",
            "Tong Chen",
            "Ji Jiang",
            "Zhe Tang",
            "Zhengyong Jiang",
            "Angelos Stefanidis",
            "Limin Yu",
            "Jionglong Su"
        ],
        "title": "Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs",
        "abstract": "arXiv:2511.14315v1 Announce Type: new  Abstract: Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \\textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.",
        "arxiv_id": "2511.14315",
        "ARXIVID": "2511.14315",
        "COMMENT": "Dental3R introduces a geometry-aware pairing strategy and wavelet-regularized 3D Gaussian Splatting for intraoral 3D reconstruction from sparse photographs. While it presents a novel method for spatial reconstruction, it is highly domain-specific (dental), and does not directly address embodied agents or general spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.14604": {
        "authors": [
            "Yilin Zhang",
            "Leo D. Westbury",
            "Elaine M. Dennison",
            "Nicholas C. Harvey",
            "Nicholas R. Fuggle",
            "Rahman Attar"
        ],
        "title": "XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation",
        "abstract": "arXiv:2511.14604v1 Announce Type: new  Abstract: Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.",
        "arxiv_id": "2511.14604",
        "ARXIVID": "2511.14604",
        "COMMENT": "Multimodal deep learning for medical imaging, using cross-attention for image and metadata fusion. While it uses cross-modal learning, it does not introduce a new VLLM/MLLM or vision foundation model, nor is it focused on spatial intelligence in embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.14329": {
        "authors": [
            "Dongchen Han",
            "Tianzhu Ye",
            "Zhuofan Xia",
            "Kaiyi Chen",
            "Yulin Wang",
            "Hanting Chen",
            "Gao Huang"
        ],
        "title": "Step by Step Network",
        "abstract": "arXiv:2511.14329v1 Announce Type: new  Abstract: Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.",
        "arxiv_id": "2511.14329",
        "ARXIVID": "2511.14329",
        "COMMENT": "Presents a new neural network architecture for deep models, but does not focus on spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models. General architecture improvement.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.14033": {
        "authors": [
            "Sun Han Neo",
            "Sachith Seneviratne",
            "Herath Mudiyanselage Viraj Vidura Herath",
            "Abhishek Saha",
            "Sanka Rasnayaka",
            "Lucy Amanda Marshall"
        ],
        "title": "Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping",
        "abstract": "arXiv:2511.14033v1 Announce Type: new  Abstract: Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at https://github.com/neosunhan/flood-diff.",
        "arxiv_id": "2511.14033",
        "ARXIVID": "2511.14033",
        "COMMENT": "Flood-LDM proposes a generalizable latent diffusion model for high-resolution flood mapping, with physics-informed inputs. While it is a generative vision model, it does not directly match the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.14310": {
        "authors": [
            "Jiancheng Fang",
            "Shaoyu Wang",
            "Junlin Wang",
            "Weiwen Wu",
            "Yikun Zhang",
            "Qiegen Liu"
        ],
        "title": "Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model",
        "abstract": "arXiv:2511.14310v1 Announce Type: new  Abstract: Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.",
        "arxiv_id": "2511.14310",
        "ARXIVID": "2511.14310",
        "COMMENT": "Proposes an iterative diffusion-refined neural attenuation field for CT reconstruction, combining neural fields and diffusion models. This is a novel generative modeling approach in vision, but not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.14223": {
        "authors": [
            "Yifan Yang",
            "Zhi Cen",
            "Sida Peng",
            "Xiangwei Chen",
            "Yifu Deng",
            "Xinyu Zhu",
            "Fan Jia",
            "Xiaowei Zhou",
            "Hujun Bao"
        ],
        "title": "StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model",
        "abstract": "arXiv:2511.14223v1 Announce Type: new  Abstract: This paper focuses on the task of speech-driven 3D facial animation, which aims to generate realistic and synchronized facial motions driven by speech inputs.Recent methods have employed audio-conditioned diffusion models for 3D facial animation, achieving impressive results in generating expressive and natural animations.However, these methods process the whole audio sequences in a single pass, which poses two major challenges: they tend to perform poorly when handling audio sequences that exceed the training horizon and will suffer from significant latency when processing long audio inputs. To address these limitations, we propose a novel autoregressive diffusion model that processes input audio in a streaming manner. This design ensures flexibility with varying audio lengths and achieves low latency independent of audio duration. Specifically, we select a limited number of past frames as historical motion context and combine them with the audio input to create a dynamic condition. This condition guides the diffusion process to iteratively generate facial motion frames, enabling real-time synthesis with high-quality results. Additionally, we implemented a real-time interactive demo, highlighting the effectiveness and efficiency of our approach. We will release the code at https://zju3dv.github.io/StreamingTalker/.",
        "arxiv_id": "2511.14223",
        "ARXIVID": "2511.14223",
        "COMMENT": "This paper proposes a novel autoregressive diffusion model for real-time, streaming 3D facial animation from audio. While it is a new generative modeling method for audio-to-vision, it does not directly address spatial understanding or embodied agents, nor is it a VLLM/MLLM or vision foundation model. It is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.14283": {
        "authors": [
            "Zi-Chen Xi",
            "Jiahui Huang",
            "Hao-Xiang Chen",
            "Francis Williams",
            "Qun-Ce Xu",
            "Tai-Jiang Mu",
            "Shi-Min Hu"
        ],
        "title": "NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction",
        "abstract": "arXiv:2511.14283v1 Announce Type: new  Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.",
        "arxiv_id": "2511.14283",
        "ARXIVID": "2511.14283",
        "COMMENT": "Relevant to computer vision and 3D reconstruction, but does not directly match any specific criterion. Focuses on implicit surface reconstruction from point clouds.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.14097": {
        "authors": [
            "Weijia Fan",
            "Qiufu Li",
            "Jiajun Wen",
            "Xiaoyang Peng"
        ],
        "title": "BCE3S: Binary Cross-Entropy Based Tripartite Synergistic Learning for Long-tailed Recognition",
        "abstract": "arXiv:2511.14097v1 Announce Type: new  Abstract: For long-tailed recognition (LTR) tasks, high intra-class compactness and inter-class separability in both head and tail classes, as well as balanced separability among all the classifier vectors, are preferred. The existing LTR methods based on cross-entropy (CE) loss not only struggle to learn features with desirable properties but also couple imbalanced classifier vectors in the denominator of its Softmax, amplifying the imbalance effects in LTR. In this paper, for the LTR, we propose a binary cross-entropy (BCE)-based tripartite synergistic learning, termed BCE3S, which consists of three components: (1) BCE-based joint learning optimizes both the classifier and sample features, which achieves better compactness and separability among features than the CE-based joint learning, by decoupling the metrics between feature and the imbalanced classifier vectors in multiple Sigmoid; (2) BCE-based contrastive learning further improves the intra-class compactness of features; (3) BCE-based uniform learning balances the separability among classifier vectors and interactively enhances the feature properties by combining with the joint learning. The extensive experiments show that the LTR model trained by BCE3S not only achieves higher compactness and separability among sample features, but also balances the classifier's separability, achieving SOTA performance on various long-tailed datasets such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and iNaturalist2018.",
        "arxiv_id": "2511.14097",
        "ARXIVID": "2511.14097",
        "COMMENT": "This paper proposes a new loss function (BCE3S) for long-tailed recognition, focusing on feature compactness and separability. While it is a methodological improvement in recognition, it does not directly address spatial understanding, embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about classification under class imbalance.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14219": {
        "authors": [
            "Kumud Tripathi",
            "Aditya Srinivas Menon",
            "Aman Gaurav",
            "Raj Prakash Gohil",
            "Pankaj Wasnik"
        ],
        "title": "Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation",
        "abstract": "arXiv:2511.14219v1 Announce Type: new  Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.",
        "arxiv_id": "2511.14219",
        "ARXIVID": "2511.14219",
        "COMMENT": "Focuses on improving Whisper ASR model robustness to hallucinations using adaptive layer attention and knowledge distillation. While interesting, it is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13775": {
        "authors": [
            "Dongdong Zhao",
            "Ranxin Fang",
            "Changtian Song",
            "Zhihui Liu",
            "Jianwen Xiang"
        ],
        "title": "Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition",
        "abstract": "arXiv:2511.13775v1 Announce Type: new  Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.",
        "arxiv_id": "2511.13775",
        "ARXIVID": "2511.13775",
        "COMMENT": "Addresses overconfidence in open set recognition, which is a general vision problem but not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14730": {
        "authors": [
            "Parya Dolatyabi",
            "Mahdi Khodayar"
        ],
        "title": "Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration",
        "abstract": "arXiv:2511.14730v1 Announce Type: new  Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.",
        "arxiv_id": "2511.14730",
        "ARXIVID": "2511.14730",
        "COMMENT": "Applies heterogeneous multi-agent RL for power distribution system restoration. While it is an RL paper with a simulator, it is not focused on embodied AI or spatial intelligence in the sense of agents in physical or simulated environments for navigation or manipulation. Not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14270": {
        "authors": [
            "Yiming Zeng",
            "Xi-Le Zhao",
            "Wei-Hao Wu",
            "Teng-Yu Ji",
            "Chao Wang"
        ],
        "title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery",
        "abstract": "arXiv:2511.14270v1 Announce Type: new  Abstract: Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.",
        "arxiv_id": "2511.14270",
        "ARXIVID": "2511.14270",
        "COMMENT": "This paper proposes a Gaussian Splatting-based low-rank tensor representation for multi-dimensional image recovery, improving local high-frequency information capture. While relevant to computer vision, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14650": {
        "authors": [
            "Jingyi Jia",
            "Qinbin Li"
        ],
        "title": "AutoTool: Efficient Tool Selection for Large Language Model Agents",
        "abstract": "arXiv:2511.14650v1 Announce Type: new  Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.",
        "arxiv_id": "2511.14650",
        "ARXIVID": "2511.14650",
        "COMMENT": "This paper proposes a novel graph-based framework (AutoTool) for efficient tool selection in LLM agents, integrating statistical structure for efficiency. It is relevant to criterion 1 (methodological improvements for spatial/agent intelligence) only tangentially, as it focuses on tool selection efficiency in LLM agents, not spatial understanding or embodied agents. No direct match to other criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14639": {
        "authors": [
            "Marco Acerbis",
            "Swarnadip Chatterjee",
            "Christophe Avenel",
            "Joakim Lindblad"
        ],
        "title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology",
        "abstract": "arXiv:2511.14639v1 Announce Type: new  Abstract: Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.",
        "arxiv_id": "2511.14639",
        "ARXIVID": "2511.14639",
        "COMMENT": "Does not directly match any specific criterion. Focuses on multi-task pretraining and adaptive gradient surgery in computational cytology, which is tangential to vision foundation models but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13883": {
        "authors": [
            "Yuetan Chu",
            "Zhongyi Han",
            "Gongning Luo",
            "Xin Gao"
        ],
        "title": "Revisiting Data Scaling Law for Medical Segmentation",
        "abstract": "arXiv:2511.13883v1 Announce Type: new  Abstract: The population loss of trained deep neural networks often exhibits power law scaling with the size of the training dataset, guiding significant performance advancements in deep learning applications. In this study, we focus on the scaling relationship with data size in the context of medical anatomical segmentation, a domain that remains underexplored. We analyze scaling laws for anatomical segmentation across 15 semantic tasks and 4 imaging modalities, demonstrating that larger datasets significantly improve segmentation performance, following similar scaling trends. Motivated by the topological isomorphism in images sharing anatomical structures, we evaluate the impact of deformation-guided augmentation strategies on data scaling laws, specifically random elastic deformation and registration-guided deformation. We also propose a novel, scalable image augmentation approach that generates diffeomorphic mappings from geodesic subspace based on image registration to introduce realistic deformation. Our experimental results demonstrate that both registered and generated deformation-based augmentation considerably enhance data utilization efficiency. The proposed generated deformation method notably achieves superior performance and accelerated convergence, surpassing standard power law scaling trends without requiring additional data. Overall, this work provides insights into the understanding of segmentation scalability and topological variation impact in medical imaging, thereby leading to more efficient model development with reduced annotation and computational costs.",
        "arxiv_id": "2511.13883",
        "ARXIVID": "2511.13883",
        "COMMENT": "Does not directly match any specific criterion. Focuses on data scaling laws and augmentation in medical segmentation, which is tangential to vision foundation models but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14558": {
        "authors": [
            "Adam Bajger",
            "Jan Obdr\\v{z}\\'alek",
            "Vojt\\v{e}ch K\\r{u}r",
            "Rudolf Nenutil",
            "Petr Holub",
            "V\\'it Musil",
            "Tom\\'a\\v{s} Br\\'azdil"
        ],
        "title": "Explaining Digital Pathology Models via Clustering Activations",
        "abstract": "arXiv:2511.14558v1 Announce Type: new  Abstract: We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.",
        "arxiv_id": "2511.14558",
        "ARXIVID": "2511.14558",
        "COMMENT": "Presents a clustering-based explainability technique for digital pathology models. While it is a computer vision paper, it does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}