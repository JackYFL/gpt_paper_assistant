{
    "2506.23711": {
        "authors": [
            "Haoyang Chen",
            "Dongfang Sun",
            "Caoyuan Ma",
            "Shiqin Wang",
            "Kewei Zhang",
            "Zheng Wang",
            "Zhixiang Wang"
        ],
        "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion",
        "abstract": "arXiv:2506.23711v1 Announce Type: new  Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that reconstructs real-world scenes from mental impressions through synergistic use of verbal descriptions and progressive rough sketches. This approach overcomes dual limitations of language ambiguity and sketch abstraction by treating the user's drawing sequence as priors, effectively translating subjective perceptual expectations into photorealistic images.   Existing approaches face three fundamental barriers: (1) user-specific subjective input biases, (2) huge modality gap between planar sketch and 3D priors in diffusion, and (3) sketch quality-sensitive performance degradation. Current solutions either demand resource-intensive model adaptation or impose impractical requirements on sketch precision.   Our framework addresses these challenges through concept-sequential generation. (1) We establish robust appearance priors through text-reward optimization, and then implement sequence-aware disentangled generation that processes concepts in sketching order; these steps accommodate user-specific subjective expectation in a train-free way. (2) We employ latent optimization that effectively bridges the modality gap between planar sketches and 3D priors in diffusion. (3) Our hierarchical reward-guided framework enables the use of rough sketches without demanding artistic expertise. Comprehensive evaluation across diverse datasets demonstrates that our approach achieves state-of-the-art performance in maintaining both semantic and spatial coherence.",
        "arxiv_id": "2506.23711",
        "ARXIVID": "2506.23711",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for visual reconstruction using multimodal inputs (text and sketches), which aligns with advancements in VLLMs/MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.22756": {
        "authors": [
            "Tao Tang",
            "Likui Zhang",
            "Youpeng Wen",
            "Kaidong Zhang",
            "Jia-Wang Bian",
            "xia zhou",
            "Tianyi Yan",
            "Kun Zhan",
            "Peng Jia",
            "Hefeng Wu",
            "Liang Lin",
            "Xiaodan Liang"
        ],
        "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
        "abstract": "arXiv:2506.22756v1 Announce Type: new  Abstract: The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.",
        "arxiv_id": "2506.22756",
        "ARXIVID": "2506.22756",
        "COMMENT": "Matches criterion 3 as it introduces a novel simulation framework for robotic manipulation, addressing sim-to-real challenges.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.22992": {
        "authors": [
            "Yulun Jiang",
            "Yekun Chai",
            "Maria Brbi\\'c",
            "Michael Moor"
        ],
        "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
        "abstract": "arXiv:2506.22992v1 Announce Type: new  Abstract: The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.",
        "arxiv_id": "2506.22992",
        "ARXIVID": "2506.22992",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MARBLE) for multimodal spatial reasoning and planning, focusing on novel challenges in reasoning and planning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23563": {
        "authors": [
            "Huanjin Yao",
            "Jiaxing Huang",
            "Yawen Qiu",
            "Michael K. Chen",
            "Wenzheng Liu",
            "Wei Zhang",
            "Wenjie Zeng",
            "Xikun Zhang",
            "Jingyi Zhang",
            "Yuxin Song",
            "Wenhao Wu",
            "Dacheng Tao"
        ],
        "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI",
        "abstract": "arXiv:2506.23563v1 Announce Type: new  Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.",
        "arxiv_id": "2506.23563",
        "ARXIVID": "2506.23563",
        "COMMENT": "Matches criterion 2 as it introduces a new benchmark for multi-modal reasoning in MLLMs, focusing on long-chain reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23361": {
        "authors": [
            "Yuanhao Cai",
            "He Zhang",
            "Xi Chen",
            "Jinbo Xing",
            "Yiwei Hu",
            "Yuqian Zhou",
            "Kai Zhang",
            "Zhifei Zhang",
            "Soo Ye Kim",
            "Tianyu Wang",
            "Yulun Zhang",
            "Xiaokang Yang",
            "Zhe Lin",
            "Alan Yuille"
        ],
        "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions",
        "abstract": "arXiv:2506.23361v1 Announce Type: new  Abstract: Existing feedforward subject-driven video customization methods mainly study single-subject scenarios due to the difficulty of constructing multi-subject training data pairs. Another challenging problem that how to use the signals such as depth, mask, camera, and text prompts to control and edit the subject in the customized video is still less explored. In this paper, we first propose a data construction pipeline, VideoCus-Factory, to produce training data pairs for multi-subject customization from raw videos without labels and control signals such as depth-to-video and mask-to-video pairs. Based on our constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with image editing data to enable instructive editing for the subject in the customized video. Then we propose a diffusion Transformer framework, OmniVCus, with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned Embedding (TAE). LE enables inference with more subjects by using the training subjects to activate more frame embeddings. TAE encourages the generation process to extract guidance from temporally aligned control signals by assigning the same frame embeddings to the control and noise tokens. Experiments demonstrate that our method significantly surpasses state-of-the-art methods in both quantitative and qualitative evaluations. Video demos are at our project page: https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released at https://github.com/caiyuanhao1998/Open-OmniVCus",
        "arxiv_id": "2506.23361",
        "ARXIVID": "2506.23361",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal video customization framework with novel embedding mechanisms, relevant to MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.24019": {
        "authors": [
            "Hongxin Zhang",
            "Zheyuan Zhang",
            "Zeyuan Wang",
            "Zunzhe Zhang",
            "Lixing Fang",
            "Qinhong Zhou",
            "Chuang Gan"
        ],
        "title": "Ella: Embodied Social Agents with Lifelong Memory",
        "abstract": "arXiv:2506.24019v1 Announce Type: new  Abstract: We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.",
        "arxiv_id": "2506.24019",
        "ARXIVID": "2506.24019",
        "COMMENT": "Matches criterion 3. Introduces Ella, an embodied social agent with lifelong memory, focusing on novel memory systems and integration with foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23306": {
        "authors": [
            "Qi Liu",
            "Can Li",
            "Wanjing Ma"
        ],
        "title": "GATSim: Urban Mobility Simulation with Generative Agents",
        "abstract": "arXiv:2506.23306v1 Announce Type: new  Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based systems that fail to capture the complexity, adaptability, and behavioral diversity characteristic of human travel decision-making. Recent advances in large language models and AI agent technology offer opportunities to create agents with reasoning capabilities, persistent memory, and adaptive learning mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advances to create generative agents with rich behavioral characteristics for urban mobility simulation. Unlike conventional approaches, GATSim agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences that shape their mobility decisions through psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms. The main contributions of this study include: (1) a comprehensive architecture combining an urban mobility foundation model with agent cognitive systems and transport simulation environment, (2) a fully functional prototype implementation, and (3) systematic validation demonstrating that generative agents produce believable travel behaviors. Through designed reflection processes, generative agents in this study can transform specific travel experiences into generalized insights, enabling realistic behavioral adaptation over time with specialized mechanisms for activity planning and real-time reactive behaviors tailored to urban mobility contexts. Experiments show that generative agents perform competitively with human annotators in mobility scenarios while naturally producing macroscopic traffic evolution patterns. The code for the prototype system is shared at https://github.com/qiliuchn/gatsim.",
        "arxiv_id": "2506.23306",
        "ARXIVID": "2506.23306",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (GATSim) for urban mobility simulation with generative agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23352": {
        "authors": [
            "Shunsuke Yasuki",
            "Taiki Miyanishi",
            "Nakamasa Inoue",
            "Shuhei Kurita",
            "Koya Sakamoto",
            "Daichi Azuma",
            "Masato Taki",
            "Yutaka Matsuo"
        ],
        "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields",
        "abstract": "arXiv:2506.23352v1 Announce Type: new  Abstract: The advancement of 3D language fields has enabled intuitive interactions with 3D scenes via natural language. However, existing approaches are typically limited to small-scale environments, lacking the scalability and compositional reasoning capabilities necessary for large, complex urban settings. To overcome these limitations, we propose GeoProg3D, a visual programming framework that enables natural language-driven interactions with city-scale high-fidelity 3D scenes. GeoProg3D consists of two key components: (i) a Geography-aware City-scale 3D Language Field (GCLF) that leverages a memory-efficient hierarchical 3D model to handle large-scale data, integrated with geographic information for efficiently filtering vast urban spaces using directional cues, distance measurements, elevation data, and landmark references; and (ii) Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as area segmentation and object detection. Our framework employs large language models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate GCLF, effectively supporting diverse geographic vision tasks. To assess performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive benchmark dataset containing 952 query-answer pairs across five challenging tasks: grounding, spatial reasoning, comparison, counting, and measurement. Experiments demonstrate that GeoProg3D significantly outperforms existing 3D language fields and vision-language models across multiple tasks. To our knowledge, GeoProg3D is the first framework enabling compositional geographic reasoning in high-fidelity city-scale 3D environments via natural language. The code is available at https://snskysk.github.io/GeoProg3D/.",
        "arxiv_id": "2506.23352",
        "ARXIVID": "2506.23352",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (GeoEval3D) and a novel framework (GeoProg3D) for city-scale 3D language fields.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23275": {
        "authors": [
            "Chengyou Jia",
            "Xin Shen",
            "Zhuohang Dang",
            "Zhuohang Dang",
            "Changliang Xia",
            "Weijia Wu",
            "Xinyu Zhang",
            "Hangwei Qian",
            "Ivor W. Tsang",
            "Minnan Luo"
        ],
        "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation",
        "abstract": "arXiv:2506.23275v1 Announce Type: new  Abstract: Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.",
        "arxiv_id": "2506.23275",
        "ARXIVID": "2506.23275",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (T2IS-Bench) and evaluation framework (T2IS-Eval) for Text-to-ImageSet generation, focusing on novel consistency challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24119": {
        "authors": [
            "Bo Liu",
            "Leon Guertler",
            "Simon Yu",
            "Zichen Liu",
            "Penghui Qi",
            "Daniel Balcells",
            "Mickel Liu",
            "Cheston Tan",
            "Weiyan Shi",
            "Min Lin",
            "Wee Sun Lee",
            "Natasha Jaques"
        ],
        "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
        "abstract": "arXiv:2506.24119v1 Announce Type: new  Abstract: Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
        "arxiv_id": "2506.24119",
        "ARXIVID": "2506.24119",
        "COMMENT": "Matches criterion 3 as it discusses a novel reinforcement learning framework for reasoning in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.23271": {
        "authors": [
            "Jinxing Zhou",
            "Zhihui Li",
            "Yongqiang Yu",
            "Yanghao Zhou",
            "Ruohao Guo",
            "Guangyao Li",
            "Yuxin Mao",
            "Mingfei Han",
            "Xiaojun Chang",
            "Meng Wang"
        ],
        "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation",
        "abstract": "arXiv:2506.23271v1 Announce Type: new  Abstract: We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a \\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training time while maintaining parameter efficiency and competitive accuracy.",
        "arxiv_id": "2506.23271",
        "ARXIVID": "2506.23271",
        "COMMENT": "Matches criterion 4 as it introduces a memory-efficient method for adapting large-scale pretrained transformer models to audio-visual tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23115": {
        "authors": [
            "Haonan Chen",
            "Hong Liu",
            "Yuping Luo",
            "Liang Wang",
            "Nan Yang",
            "Furu Wei",
            "Zhicheng Dou"
        ],
        "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings",
        "abstract": "arXiv:2506.23115v1 Announce Type: new  Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
        "arxiv_id": "2506.23115",
        "ARXIVID": "2506.23115",
        "COMMENT": "Matches criterion 2 as it proposes a new framework for transforming pre-trained Vision Language Models (VLMs) into bidirectional multimodal embedding models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23856": {
        "authors": [
            "Ji Zhang",
            "Shihan Wu",
            "Lianli Gao",
            "Jingkuan Song",
            "Nicu Sebe",
            "Heng Tao Shen"
        ],
        "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models",
        "abstract": "arXiv:2506.23856v1 Announce Type: new  Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better tuned to a base task, their ability to generalize to new tasks diminishes. Recent work on conditional PT addresses this problem by replacing static prompts with dynamic Visual Image Information (VII)-conditioned prompts, improving the model's generalization to new tasks to some extent. In this work, we first identify a critical issue with existing conditional PT methods: using VII as the \"condition\" of prompts yields suboptimal performance, and even random noise-conditioned prompts can outperform the VII-conditioned counterparts. On further analysis, we find that learning dynamic prompts conditioned on Textual Class Information (TCI) is the key to solving the BNT problem. Motivated by this, we then propose Class-adaptive Prompt Tuning (CaPT), which enables fast adaptation of tuned models to new classes by learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be used as a plugin to mitigate the BNT problem for existing unconditional PT schemes. Extensive experiments on 11 datasets show that CaPT consistently improves the performance of five strong unconditional PT baselines with negligible additional computational cost. Additionally, by integrating CaPT with our recently proposed DePT framework, we devise a new conditional PT approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art conditional PT scheme by 3.49%, averaged over the 11 datasets. Code: https://github.com/Koorye/CaPT.",
        "arxiv_id": "2506.23856",
        "ARXIVID": "2506.23856",
        "COMMENT": "Matches criterion 4 as it focuses on improving vision-language models (VLPMs) through conditional prompt tuning, addressing a novel issue in generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23607": {
        "authors": [
            "Shiqi Zhang",
            "Sha Zhang",
            "Jiajun Deng",
            "Yedong Shen",
            "Mingxiao MA",
            "Yanyong Zhang"
        ],
        "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum",
        "abstract": "arXiv:2506.23607v1 Announce Type: new  Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise 3D segmentation models by merging text-aligned features (e.g., CLIP) extracted from multi-view images onto 3D points. However, such approaches treat multi-view images merely as intermediaries for transferring open-vocabulary information, overlooking their rich semantic content and cross-view correspondences, which limits model effectiveness. To address this, we propose PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for improving open-vocabulary 3D semantic segmentation. The key innovation lies in a two-stage training strategy. In the first stage, we pre-train the model on partial scenes that provide dense semantic information but relatively simple geometry. These partial point clouds are derived from multi-view RGB-D inputs via pixel-wise depth projection. To enable open-vocabulary learning, we leverage a multi-modal large language model (MLLM) and a 2D segmentation foundation model to generate open-vocabulary labels for each viewpoint, offering rich and aligned supervision. An auxiliary inter-frame consistency module is introduced to enforce feature consistency across varying viewpoints and enhance spatial understanding. In the second stage, we fine-tune the model on complete scene-level point clouds, which are sparser and structurally more complex. We aggregate the partial vocabularies associated with each scene and generate pseudo labels using the pre-trained model, effectively bridging the semantic gap between dense partial observations and large-scale 3D environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS benchmarks demonstrate that PGOV3D achieves competitive performance in open-vocabulary 3D semantic segmentation.",
        "arxiv_id": "2506.23607",
        "ARXIVID": "2506.23607",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework (PGOV3D) for improving spatial understanding in open-vocabulary 3D semantic segmentation and focuses on embodied AI benchmarks like ScanNet.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23308": {
        "authors": [
            "Yiming Huang",
            "Long Bai",
            "Beilei Cui",
            "Yanheng Li",
            "Tong Chen",
            "Jie Wang",
            "Jinlin Wu",
            "Zhen Lei",
            "Hongbin Liu",
            "Hongliang Ren"
        ],
        "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
        "abstract": "arXiv:2506.23308v1 Announce Type: new  Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
        "arxiv_id": "2506.23308",
        "ARXIVID": "2506.23308",
        "COMMENT": "Matches criterion 3 as it proposes a novel reconstruction method for endoscopic scenes with illumination correction, relevant to embodied AI and novel methods for simulators.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24113": {
        "authors": [
            "Kaiwen Zhang",
            "Zhenyu Tang",
            "Xiaotao Hu",
            "Xingang Pan",
            "Xiaoyang Guo",
            "Yuan Liu",
            "Jingwei Huang",
            "Li Yuan",
            "Qian Zhang",
            "Xiao-Xiao Long",
            "Xun Cao",
            "Wei Yin"
        ],
        "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
        "abstract": "arXiv:2506.24113v1 Announce Type: new  Abstract: Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.",
        "arxiv_id": "2506.24113",
        "ARXIVID": "2506.24113",
        "COMMENT": "Matches criterion 3 as it proposes a diffusion-based world model for autonomous driving, which is relevant to embodied AI and novel methods for simulators.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23207": {
        "authors": [
            "Zhen Tan",
            "Xieyuanli Chen",
            "Lei Feng",
            "Yangbing Ge",
            "Shuaifeng Zhi",
            "Jiaxiong Liu",
            "Dewen Hu"
        ],
        "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints",
        "abstract": "arXiv:2506.23207v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
        "arxiv_id": "2506.23207",
        "ARXIVID": "2506.23207",
        "COMMENT": "Matches criterion 3 as it proposes a novel SLAM system with robust tracking and mapping improvements, which is relevant to embodied AI and simulator-related benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22800": {
        "authors": [
            "Sicong Du",
            "Jiarun Liu",
            "Qifeng Chen",
            "Hao-Xiang Chen",
            "Tai-Jiang Mu",
            "Sheng Yang"
        ],
        "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors",
        "abstract": "arXiv:2506.22800v1 Announce Type: new  Abstract: A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version incorporating reviewer suggestions will be updated soon.)",
        "arxiv_id": "2506.22800",
        "ARXIVID": "2506.22800",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for expansive driving scene reconstruction, focusing on reward-guided Gaussian integration, a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24063": {
        "authors": [
            "Deng Li",
            "Aming Wu",
            "Yang Li",
            "Yaowei Wang",
            "Yahong Han"
        ],
        "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios",
        "abstract": "arXiv:2506.24063v1 Announce Type: new  Abstract: In practice, environments constantly change over time and space, posing significant challenges for object detectors trained based on a closed-set assumption, i.e., training and test data share the same distribution. To this end, continual test-time adaptation has attracted much attention, aiming to improve detectors' generalization by fine-tuning a few specific parameters, e.g., BatchNorm layers. However, based on a small number of test images, fine-tuning certain parameters may affect the representation ability of other fixed parameters, leading to performance degradation. Instead, we explore a new mechanism, i.e., converting the fine-tuning process to a specific-parameter generation. Particularly, we first design a dual-path LoRA-based domain-aware adapter that disentangles features into domain-invariant and domain-specific components, enabling efficient adaptation. Additionally, a conditional diffusion-based parameter generation mechanism is presented to synthesize the adapter's parameters based on the current environment, preventing the optimization from getting stuck in local optima. Finally, we propose a class-centered optimal transport alignment method to mitigate catastrophic forgetting. Extensive experiments conducted on various continuous domain adaptive object detection tasks demonstrate the effectiveness. Meanwhile, visualization results show that the representation extracted by the generated parameters can capture more object-related information and strengthen the generalization ability.",
        "arxiv_id": "2506.24063",
        "ARXIVID": "2506.24063",
        "COMMENT": "Matches criterion 3 as it proposes a new mechanism for continual adaptation in object detection, focusing on dynamic scenarios, which is a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23263": {
        "authors": [
            "Lei-lei Li",
            "Jianwu Fang",
            "Junbin Xiao",
            "Shanmin Pang",
            "Hongkai Yu",
            "Chen Lv",
            "Jianru Xue",
            "Tat-Seng Chua"
        ],
        "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis",
        "abstract": "arXiv:2506.23263v1 Announce Type: new  Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial for the safety of self-driving cars, and synthesizing causal-entity reflected accident videos can facilitate the capability test to respond to unaffordable accidents in reality. However, incorporating causal relations as seen in real-world videos into synthetic videos remains challenging. This work argues that precisely identifying the accident participants and capturing their related behaviors are of critical importance. In this regard, we propose a novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic accident videos. To enable causal entity grounding in video diffusion, Causal-VidSyn leverages the cause descriptions and driver fixations to identify the accident participants and behaviors, facilitated by accident reason answering and gaze-conditioned selection modules. To support Causal-VidSyn, we further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M frames of fixations) in driving accident scenarios. Extensive experiments show that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms of frame quality and causal sensitivity in various tasks, including accident video editing, normal-to-accident video diffusion, and text-to-video generation.",
        "arxiv_id": "2506.23263",
        "ARXIVID": "2506.23263",
        "COMMENT": "Matches criterion 3 as it introduces a novel diffusion model for synthesizing egocentric traffic accident videos, focusing on causal-entity grounding, a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22500": {
        "authors": [
            "Weiyi Zhao",
            "Xiaoyu Tan",
            "Liang Liu",
            "Sijia Li",
            "Youwei Song",
            "Xihe Qiu"
        ],
        "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models",
        "abstract": "arXiv:2506.22500v1 Announce Type: new  Abstract: Surgical risk identification is critical for patient safety and reducing preventable medical errors. While multimodal large language models (MLLMs) show promise for automated operating room (OR) risk detection, they often exhibit visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety violations despite understanding textual rules. To address this, we introduce a dataset comprising over 34,000 synthetic images generated by diffusion models, depicting operating room scenes containing entities that violate established safety rules. These images were created to alleviate data scarcity and examine MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated images that serve as a gold-standard reference for validation. This comprehensive dataset, spanning diverse perspectives, stages, and configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC significantly improves MLLMs' detection of trained conflict entities and generalizes well to new viewpoints for these entities, but performance on untrained entity types remains poor, highlighting learning specificity and the need for comprehensive training. The main contributions of this work include: (1) a data generation methodology tailored for rule-violation scenarios; (2) the release of the OR-VSKC dataset and its associated benchmark as open-source resources; and (3) an empirical analysis of violation-sensitive knowledge consistency in representative MLLMs. The dataset and appendix are available at https://github.com/zgg2577/VS-KC.",
        "arxiv_id": "2506.22500",
        "ARXIVID": "2506.22500",
        "COMMENT": "Matches criterion 2. Focuses on multimodal large language models (MLLMs) and their application to surgical risk perception, addressing visual-semantic knowledge conflicts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23135": {
        "authors": [
            "Yu Shang",
            "Xin Zhang",
            "Yinzhou Tang",
            "Lei Jin",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "RoboScape: Physics-informed Embodied World Model",
        "abstract": "arXiv:2506.23135v1 Announce Type: new  Abstract: World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.",
        "arxiv_id": "2506.23135",
        "ARXIVID": "2506.23135",
        "COMMENT": "Matches criterion 3. Introduces a physics-informed world model for embodied AI, focusing on novel aspects like temporal depth prediction and keypoint dynamics learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22567": {
        "authors": [
            "Shansong Wang",
            "Zhecheng Jin",
            "Mingzhe Hu",
            "Mojtaba Safari",
            "Feng Zhao",
            "Chih-Wei Chang",
            "Richard LJ Qiu",
            "Justin Roper",
            "David S. Yu",
            "Xiaofeng Yang"
        ],
        "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation",
        "abstract": "arXiv:2506.22567v1 Announce Type: new  Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs have demonstrated impressive capabilities in zero-shot classification, cross-modal retrieval, and open-ended visual answering. However, transferring this success to biomedicine is hindered by the scarcity of large-scale biomedical image-text corpora, the heterogeneity of image modalities, and fragmented data standards across institutions. These limitations hinder the development of a unified and generalizable biomedical foundation model trained from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical foundation model developed via Multiple Medical CLIP Knowledge Distillation. Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge from nine state-of-the-art domain-specific or generalist biomedical CLIP models, each pretrained on millions of biomedical image-text pairs. Our two-stage training pipeline first performs CLIP-style pretraining on over 2.9 million biomedical image-text pairs from 26 image modalities, followed by feature-level distillation using over 19.2 million feature pairs extracted from teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets, encompassing over 10.8 million biomedical images across nine image modalities. The evaluation spans six core task types: zero-shot classification, linear probing, cross-modal retrieval, visual question answering, survival prediction, and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models while demonstrating remarkable robustness and generalization across image domains and task settings. These results underscore that multi-teacher knowledge distillation is a scalable and effective paradigm for building high-performing biomedical foundation models under the practical constraints of real-world data availability.",
        "arxiv_id": "2506.22567",
        "ARXIVID": "2506.22567",
        "COMMENT": "Matches criterion 4 as it focuses on a biomedical vision-language foundation model (MMKD-CLIP) and its applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22967": {
        "authors": [
            "Amir Aghdam",
            "Vincent Tao Hu"
        ],
        "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment",
        "abstract": "arXiv:2506.22967v1 Announce Type: new  Abstract: We address the task of zero-shot fine-grained video classification, where no video examples or temporal annotations are available for unseen action classes. While contrastive vision-language models such as SigLIP demonstrate strong open-set recognition via mean-pooled image-text similarity, they fail to capture the temporal structure critical for distinguishing fine-grained activities. We introduce ActAlign, a zero-shot framework that formulates video classification as sequence alignment. For each class, a large language model generates an ordered sub-action sequence, which is aligned with video frames using Dynamic Time Warping (DTW) in a shared embedding space. Without any video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the extremely challenging ActionAtlas benchmark, where human accuracy is only 61.6%. ActAlign outperforms billion-parameter video-language models while using approximately 8x less parameters. These results demonstrate that structured language priors, combined with classical alignment techniques, offer a scalable and general approach to unlocking the open-set recognition potential of vision-language models for fine-grained video understanding.",
        "arxiv_id": "2506.22967",
        "ARXIVID": "2506.22967",
        "COMMENT": "Matches criterion 2 as it introduces a novel zero-shot framework (ActAlign) leveraging vision-language models for fine-grained video classification.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24086": {
        "authors": [
            "Bingfan Zhu",
            "Biao Jiang",
            "Sunyi Wang",
            "Shixiang Tang",
            "Tao Chen",
            "Linjie Luo",
            "Youyi Zheng",
            "Xin Chen"
        ],
        "title": "MotionGPT3: Human Motion as a Second Modality",
        "abstract": "arXiv:2506.24086v1 Announce Type: new  Abstract: Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.",
        "arxiv_id": "2506.24086",
        "ARXIVID": "2506.24086",
        "COMMENT": "Matches criterion 2 as it introduces a new visual large language model (MotionGPT3) for motion-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22604": {
        "authors": [
            "David Porfirio",
            "Vincent Hsiao",
            "Morgan Fine-Morris",
            "Leslie Smith",
            "Laura M. Hiatt"
        ],
        "title": "Bootstrapping Human-Like Planning via LLMs",
        "abstract": "arXiv:2506.22604v1 Announce Type: new  Abstract: Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.",
        "arxiv_id": "2506.22604",
        "ARXIVID": "2506.22604",
        "COMMENT": "Matches criterion 3 as it explores combining natural language and drag-and-drop interfaces for human-like planning in robots.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.23606": {
        "authors": [
            "Zhengkang Xiang",
            "Zizhao Li",
            "Amir Khodabandeh",
            "Kourosh Khoshelham"
        ],
        "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion",
        "abstract": "arXiv:2506.23606v1 Announce Type: new  Abstract: Lidar point cloud synthesis based on generative models offers a promising solution to augment deep learning pipelines, particularly when real-world data is scarce or lacks diversity. By enabling flexible object manipulation, this synthesis approach can significantly enrich training datasets and enhance discriminative models. However, existing methods focus on unconditional lidar point cloud generation, overlooking their potential for real-world applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar Diffusion Model that employs latent alignment to enable robust semantic-to-lidar synthesis. By directly operating in the native lidar space and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds guided by semantic labels. Moreover, we propose the first diffusion-based lidar translation framework based on SG-LDM, which enables cross-domain translation as a domain adaptation strategy to enhance downstream perception performance. Systematic experiments demonstrate that SG-LDM significantly outperforms existing lidar diffusion models and the proposed lidar translation framework further improves data augmentation performance in the downstream lidar segmentation task.",
        "arxiv_id": "2506.23606",
        "ARXIVID": "2506.23606",
        "COMMENT": "Matches criteria 3 as it proposes a novel diffusion-based framework (SG-LDM) for lidar point cloud generation and cross-domain translation, which is relevant to embodied AI and benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.23418": {
        "authors": [
            "Parham Rezaei",
            "Arash Marioriyad",
            "Mahdieh Soleymani Baghshah",
            "Mohammad Hossein Rohban"
        ],
        "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models",
        "abstract": "arXiv:2506.23418v1 Announce Type: new  Abstract: Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks.",
        "arxiv_id": "2506.23418",
        "ARXIVID": "2506.23418",
        "COMMENT": "Matches criterion 1 as it proposes a probabilistic framework for spatial relationship alignment in text-to-image models, improving spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.24044": {
        "authors": [
            "Sicong Jiang",
            "Zilin Huang",
            "Kangan Qian",
            "Ziang Luo",
            "Tianze Zhu",
            "Yang Zhong",
            "Yihong Tang",
            "Menglin Kong",
            "Yunlong Wang",
            "Siwen Jiao",
            "Hao Ye",
            "Zihao Sheng",
            "Xin Zhao",
            "Tuopu Wen",
            "Zheng Fu",
            "Sikai Chen",
            "Kun Jiang",
            "Diange Yang",
            "Seongjin Choi",
            "Lijun Sun"
        ],
        "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "abstract": "arXiv:2506.24044v1 Announce Type: new  Abstract: The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
        "arxiv_id": "2506.24044",
        "ARXIVID": "2506.24044",
        "COMMENT": "Matches criterion 2 as it discusses Vision-Language-Action (VLA) models, which are closely related to MLLMs, and criterion 3 as it consolidates datasets and benchmarks for autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.24039": {
        "authors": [
            "Shubhabrata Mukherjee",
            "Jack Lang",
            "Obeen Kwon",
            "Iryna Zenyuk",
            "Valerie Brogden",
            "Adam Weber",
            "Daniela Ushizima"
        ],
        "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data",
        "abstract": "arXiv:2506.24039v1 Announce Type: new  Abstract: Zero-shot and prompt-based technologies capitalized on using frequently occurring images to transform visual reasoning tasks, which explains why such technologies struggle with valuable yet scarce scientific image sets. In this work, we propose Zenesis, a comprehensive no-code interactive platform designed to minimize barriers posed by data readiness for scientific images. We develop lightweight multi-modal adaptation techniques that enable zero-shot operation on raw scientific data, along with human-in-the-loop refinement and heuristic-based temporal enhancement options. We demonstrate the performance of our approach through comprehensive comparison and validation on challenging Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded membranes. Zenesis significantly outperforms baseline methods, achieving an average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results mark a substantial improvement over traditional methods like Otsu thresholding and even advanced models like Segment Anything Model (SAM) when used in isolation. Our results demonstrate that Zenesis is a powerful tool for scientific applications, particularly in fields where high-quality annotated datasets are unavailable, accelerating accurate analysis of experimental imaging.",
        "arxiv_id": "2506.24039",
        "ARXIVID": "2506.24039",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models (e.g., Segment Anything Model) and their application to scientific image segmentation, with novel zero-shot and human-in-the-loop techniques.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23863": {
        "authors": [
            "Jiahao Ma",
            "Lei Wang",
            "Miaomiao liu",
            "David Ahmedt-Aristizabal",
            "Chuong Nguyen"
        ],
        "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction",
        "abstract": "arXiv:2506.23863v1 Announce Type: new  Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision. Recent methods, such as DUST3R and its successors, directly regress pointmaps from image pairs without relying on known scene geometry or camera parameters. However, the performance of these models is constrained by the diversity and scale of available training data. In this work, we introduce Puzzles, a data augmentation strategy that synthesizes an unbounded volume of high-quality posed video-depth data from a single image or video clip. By simulating diverse camera trajectories and realistic scene geometry through targeted image transformations, Puzzles significantly enhances data variety. Extensive experiments show that integrating Puzzles into existing video-based 3D reconstruction pipelines consistently boosts performance without modifying the underlying network architecture. Notably, models trained on only ten percent of the original data augmented with Puzzles still achieve accuracy comparable to those trained on the full dataset. Code is available at https://jiahao-ma.github.io/puzzles/.",
        "arxiv_id": "2506.23863",
        "ARXIVID": "2506.23863",
        "COMMENT": "Matches criterion 3 as it introduces a novel data augmentation strategy for 3D reconstruction pipelines.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.23785": {
        "authors": [
            "Yongjian Wu",
            "Yang Zhou",
            "Jiya Saiyin",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "Visual Textualization for Image Prompted Object Detection",
        "abstract": "arXiv:2506.23785v1 Announce Type: new  Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that introduces visual textualization -- a process that projects a few visual exemplars into the text feature space to enhance Object-level Vision-Language Models' (OVLMs) capability in detecting rare categories that are difficult to describe textually and nearly absent from their pre-training data, while preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM leverages multi-scale textualizing blocks and a multi-stage fusion strategy to integrate visual information from visual exemplars, generating textualized visual tokens that effectively guide OVLMs alongside text prompts. Unlike previous methods, our method maintains the original architecture of OVLM, maintaining its generalization capabilities while enhancing performance in few-shot settings. VisTex-OVLM demonstrates superior performance across open-set datasets which have minimal overlap with OVLM's pre-training data and achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO. The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.",
        "arxiv_id": "2506.23785",
        "ARXIVID": "2506.23785",
        "COMMENT": "Matches criterion 2 as it introduces a novel method for enhancing object-level vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.23690": {
        "authors": [
            "Shuai Tan",
            "Biao Gong",
            "Yujie Wei",
            "Shiwei Zhang",
            "Zhuoxin Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Yan Wang",
            "Hao Ouyang",
            "Kecheng Zheng",
            "Yujun Shen"
        ],
        "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation",
        "abstract": "arXiv:2506.23690v1 Announce Type: new  Abstract: Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \\textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \\method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/",
        "arxiv_id": "2506.23690",
        "ARXIVID": "2506.23690",
        "COMMENT": "Matches criterion 4 as it focuses on motion-customized video generation using diffusion-based models, integrating semantic and visual adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22900": {
        "authors": [
            "Mai A. Shaaban",
            "Tausifa Jan Saleem",
            "Vijay Ram Papineni",
            "Mohammad Yaqub"
        ],
        "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering",
        "abstract": "arXiv:2506.22900v1 Announce Type: new  Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical decision-making by providing contextually rich answers to image-based queries. Although vision-language models (VLMs) are widely used for this task, they often generate factually incorrect answers. Retrieval-augmented generation addresses this challenge by providing information from external sources, but risks retrieving irrelevant context, which can degrade the reasoning capabilities of VLMs. Re-ranking retrievals, as introduced in existing approaches, enhances retrieval relevance by focusing on query-text alignment. However, these approaches neglect the visual or multimodal context, which is particularly crucial for medical diagnosis. We propose MOTOR, a novel multimodal retrieval and re-ranking approach that leverages grounded captions and optimal transport. It captures the underlying relationships between the query and the retrieved context based on textual and visual information. Consequently, our approach identifies more clinically relevant contexts to augment the VLM input. Empirical analysis and human expert evaluation demonstrate that MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by an average of 6.45%. Code is available at https://github.com/BioMedIA-MBZUAI/MOTOR.",
        "arxiv_id": "2506.22900",
        "ARXIVID": "2506.22900",
        "COMMENT": "Matches criterion 2 as it proposes a novel multimodal retrieval and re-ranking approach for medical visual question answering, leveraging grounded captions and optimal transport.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23641": {
        "authors": [
            "Peng Huang",
            "Junhu Fu",
            "Bowen Guo",
            "Zeju Li",
            "Yuanyuan Wang",
            "Yi Guo"
        ],
        "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation",
        "abstract": "arXiv:2506.23641v1 Announce Type: new  Abstract: As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.",
        "arxiv_id": "2506.23641",
        "ARXIVID": "2506.23641",
        "COMMENT": "Matches criterion 2 as it explores the use of MLLMs for medical image generation, leveraging external knowledge from pre-trained models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22890": {
        "authors": [
            "Senkang Hu",
            "Yihang Tao",
            "Guowen Xu",
            "Xinyuan Qian",
            "Yiqin Deng",
            "Xianhao Chen",
            "Sam Tak Wu Kwong",
            "Yuguang Fang"
        ],
        "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems",
        "abstract": "arXiv:2506.22890v1 Announce Type: new  Abstract: Collaborative Perception (CP) has been shown to be a promising technique for multi-agent autonomous driving and multi-agent robotic systems, where multiple agents share their perception information to enhance the overall perception performance and expand the perception range. However, in CP, an ego agent needs to receive messages from its collaborators, which makes it vulnerable to attacks from malicious agents. To address this critical issue, we propose a unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which is a tailored defense mechanism for CP deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network. Our key idea is to enable CP to reach a consensus rather than a conflict against an ego agent's perception results. Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. Furthermore, we define collaborative consistency loss (CCLoss) for object detection task and bird's eye view (BEV) segmentation task to capture the discrepancy between an ego agent and its collaborators, which is used as a verification criterion for consensus. In addition, we propose online adaptive threshold via dual sliding windows to dynamically adjust the threshold for consensus verification and ensure the reliability of the systems in dynamic environments. Finally, we conduct extensive experiments and demonstrate the effectiveness of our framework. Code will be released at https://github.com/CP-Security/CP-Guard",
        "arxiv_id": "2506.22890",
        "ARXIVID": "2506.22890",
        "COMMENT": "Matches criteria 3 as it introduces CP-Guard, a novel framework for malicious agent detection in multi-agent embodied perception systems, focusing on collaborative perception.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22636": {
        "authors": [
            "Sotirios Panagiotis Chytas",
            "Miso Choi",
            "Hyunwoo J. Kim",
            "Vikas Singh"
        ],
        "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models",
        "abstract": "arXiv:2506.22636v1 Announce Type: new  Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and reasoning with both visual and language data. But these models make mistakes. A common finding -- similar to LLMs -- is their tendency to hallucinate, i.e., generate plausible sounding text which is not grounded in the visual input, or at worst, is contradictory. A growing consensus attributes this behavior to an over-reliance on language -- especially as the generation progresses, the model suffers from a ``fading memory effect'' with respect to the provided visual input. We study mechanisms by which this behavior can be controlled. Specifically, using ideas from geometric algebra and relational compositions, we propose the addition of a small, trainable module (named ReCo) on top of any VLM -- no other modification is needed. We show that such a lightweight module is able to mitigate the fading memory effect on three of the most widely used VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on multiple benchmarks. Additionally, we show that our module can be combined with many of the other approaches for reducing hallucination where we achieve improved results for each one.",
        "arxiv_id": "2506.22636",
        "ARXIVID": "2506.22636",
        "COMMENT": "Matches criterion 2 as it proposes a lightweight module (ReCo) to improve vision-language models by mitigating hallucinations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23590": {
        "authors": [
            "Qiming Li",
            "Zekai Ye",
            "Xiaocheng Feng",
            "Weihong Zhong",
            "Libo Qin",
            "Ruihan Chen",
            "Baohang Li",
            "Kui Jiang",
            "Yaowei Wang",
            "Ting Liu",
            "Bing Qin"
        ],
        "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
        "abstract": "arXiv:2506.23590v1 Announce Type: new  Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful capabilities in interpreting visual information, they frequently produce content that deviates from visual information, leading to object hallucination. To tackle this, recent works mostly depend on expensive manual annotations and training cost, or significantly increase inference time. In this work, we observe that LVLMs' attention to visual information is significantly stronger when answering caption queries compared to non-caption queries. Inspired by this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a training-free, plug-and-play hallucination mitigation method that leverages the attention activation pattern in response to caption queries to enhance LVLMs' visual perception capability. Extensive experimental results across four benchmarks covering both discriminative and generative tasks, demonstrate that CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only with minimal additional inference cost.",
        "arxiv_id": "2506.23590",
        "ARXIVID": "2506.23590",
        "COMMENT": "Matches criteria 2 as it proposes a novel method (CAI) to mitigate object hallucination in large vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.23903": {
        "authors": [
            "Hamza Rasaee",
            "Taha Koleilat",
            "Hassan Rivaz"
        ],
        "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models",
        "abstract": "arXiv:2506.23903v1 Announce Type: new  Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.",
        "arxiv_id": "2506.23903",
        "ARXIVID": "2506.23903",
        "COMMENT": "Matches criteria 2 as it integrates vision-language models (GroundingDINO and SAM) for multi-organ segmentation in ultrasound imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.23440": {
        "authors": [
            "Mahesh Bhosale",
            "Abdul Wasi",
            "Yuanhao Zhai",
            "Yunjie Tian",
            "Samuel Border",
            "Nan Xi",
            "Pinaki Sarder",
            "Junsong Yuan",
            "David Doermann",
            "Xuan Gong"
        ],
        "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions",
        "abstract": "arXiv:2506.23440v1 Announce Type: new  Abstract: Diffusion-based generative models have shown promise in synthesizing histopathology images to address data scarcity caused by privacy constraints. Diagnostic text reports provide high-level semantic descriptions, and masks offer fine-grained spatial structures essential for representing distinct morphological regions. However, public datasets lack paired text and mask data for the same histopathological images, limiting their joint use in image generation. This constraint restricts the ability to fully exploit the benefits of combining both modalities for enhanced control over semantics and spatial details. To overcome this, we propose PathDiff, a diffusion framework that effectively learns from unpaired mask-text data by integrating both modalities into a unified conditioning space. PathDiff allows precise control over structural and contextual features, generating high-quality, semantically accurate images. PathDiff also improves image fidelity, text-image alignment, and faithfulness, enhancing data augmentation for downstream tasks like nuclei segmentation and classification. Extensive experiments demonstrate its superiority over existing methods.",
        "arxiv_id": "2506.23440",
        "ARXIVID": "2506.23440",
        "COMMENT": "Matches criterion 4 as it discusses a diffusion-based generative model for histopathology image synthesis, which is an application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.22819": {
        "authors": [
            "Ramya Hebbalaguppe",
            "Tamoghno Kandar",
            "Abhinav Nagpal",
            "Chetan Arora"
        ],
        "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration",
        "abstract": "arXiv:2506.22819v1 Announce Type: new  Abstract: Vision-language models (VLM) have demonstrated impressive performance in image recognition by leveraging self-supervised training on large datasets. Their performance can be further improved by adapting to the test sample using test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT approaches on improving the accuracy suffers from tunnel vision, and leads to degradation in confidence calibration. This limits the applicability of TPT in critical applications.   We make three contributions in this work. (1) We posit that random or naive initialization of prompts leads to overfitting on a particular test sample, and is the main reason for miscalibration of the VLM after TPT. To mitigate the problem, we propose careful initialization of test time prompt using prior knowledge about the target label attributes from a large language model (LLM); (2) To further maintain the quality of prompts during \\tpt, we propose a novel regularization loss to reduce intraclass distance, and increase inter-class distance between the learnt   Through extensive experiments on different CLIP architectures and 15 datasets, we show that our approach can effectively improve the calibration after TPT. We report an average expected calibration error (ECE) of 4.11 with our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24), 6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is publicly accessible at: https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.",
        "arxiv_id": "2506.22819",
        "ARXIVID": "2506.22819",
        "COMMENT": "Matches criterion 2 as it focuses on improving vision-language models with test-time calibration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23639": {
        "authors": [
            "Wanpeng Zhang",
            "Yicheng Feng",
            "Hao Luo",
            "Yijiang Li",
            "Zihao Yue",
            "Sipeng Zheng",
            "Zongqing Lu"
        ],
        "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding",
        "abstract": "arXiv:2506.23639v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.",
        "arxiv_id": "2506.23639",
        "ARXIVID": "2506.23639",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for multimodal large language models (MLLMs) using byte-pair visual encoding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.24096": {
        "authors": [
            "Antoine Gu\\'edon",
            "Diego Gomez",
            "Nissim Maruani",
            "Bingchen Gong",
            "George Drettakis",
            "Maks Ovsjanikov"
        ],
        "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction",
        "abstract": "arXiv:2506.24096v1 Announce Type: new  Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
        "arxiv_id": "2506.24096",
        "ARXIVID": "2506.24096",
        "COMMENT": "Matches criterion 4 as it focuses on improving 3D scene reconstruction and mesh extraction using Gaussian Splatting.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23518": {
        "authors": [
            "Jiwoo Park",
            "Tae Eun Choi",
            "Youngjun Jun",
            "Seong Jae Hwang"
        ],
        "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image",
        "abstract": "arXiv:2506.23518v1 Announce Type: new  Abstract: Generating high-quality novel views of a scene from a single image requires maintaining structural coherence across different views, referred to as view consistency. While diffusion models have driven advancements in novel view synthesis, they still struggle to preserve spatial continuity across views. Diffusion models have been combined with 3D models to address the issue, but such approaches lack efficiency due to their complex multi-step pipelines. This paper proposes a novel view-consistent image generation method which utilizes diffusion models without additional modules. Our key idea is to enhance diffusion models with a training-free method that enables adaptive attention manipulation and noise reinitialization by leveraging view-guided warping to ensure view consistency. Through our comprehensive metric framework suitable for novel-view datasets, we show that our method improves view consistency across various diffusion models, demonstrating its broader applicability.",
        "arxiv_id": "2506.23518",
        "ARXIVID": "2506.23518",
        "COMMENT": "Matches criterion 4 as it proposes a novel method for view-consistent image generation using diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23575": {
        "authors": [
            "Nuo Chen",
            "Chao Xiao",
            "Yimian Dai",
            "Shiman He",
            "Miao Li",
            "Wei An"
        ],
        "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline",
        "abstract": "arXiv:2506.23575v1 Announce Type: new  Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to the small size of UAVs and complex backgrounds. Traditional frame-based cameras struggle to detect small objects in complex environments due to their low frame rates, limited dynamic range, and data redundancy. Event cameras, with microsecond temporal resolution and high dynamic range, provide a more effective solution for SOD. However, existing event-based object detection datasets are limited in scale, feature large targets size, and lack diverse backgrounds, making them unsuitable for SOD benchmarks. In this paper, we introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV), the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes 147 sequences with over 2.3 million event-level annotations, featuring extremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse scenarios such as urban clutter and extreme lighting conditions. Furthermore, based on the observation that small moving targets form continuous curves in spatiotemporal event point clouds, we propose Event based Sparse Segmentation Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud space, along with a Spatiotemporal Correlation (STC) loss that leverages motion continuity to guide the network in retaining target events. Extensive experiments on the EV-UAV dataset demonstrate the superiority of our method and provide a benchmark for future research in EVSOD. The dataset and code are at https://github.com/ChenYichen9527/Ev-UAV.",
        "arxiv_id": "2506.23575",
        "ARXIVID": "2506.23575",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (EV-UAV) for event-based small object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23128": {
        "authors": [
            "Chi Chiu So",
            "Yueyue Sun",
            "Jun-Min Wang",
            "Siu Pang Yung",
            "Anthony Wai Keung Loh",
            "Chun Pong Chau"
        ],
        "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons",
        "abstract": "arXiv:2506.23128v1 Announce Type: new  Abstract: How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMs' internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMs' reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.",
        "arxiv_id": "2506.23128",
        "ARXIVID": "2506.23128",
        "COMMENT": "Matches criterion 2 as it evaluates reasoning capabilities of large language models, though it does not focus on visual or multi-modal aspects.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23854": {
        "authors": [
            "Yida Wang",
            "Xueyang Zhang",
            "Kun Zhan",
            "Peng Jia",
            "Xianpeng Lang"
        ],
        "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity",
        "abstract": "arXiv:2506.23854v1 Announce Type: new  Abstract: Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate state-of-the-art performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting.",
        "arxiv_id": "2506.23854",
        "ARXIVID": "2506.23854",
        "COMMENT": "Does not match any specific criteria but focuses on neural surface reconstruction with novel techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23532": {
        "authors": [
            "Jefferson Hernandez",
            "Ruozhen He",
            "Guha Balakrishnan",
            "Alexander C. Berg",
            "Vicente Ordonez"
        ],
        "title": "GViT: Representing Images as Gaussians for Visual Recognition",
        "abstract": "arXiv:2506.23532v1 Announce Type: new  Abstract: We introduce GVIT, a classification framework that abandons conventional pixel or patch grid input representations in favor of a compact set of learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose positions, scales, orientations, colors, and opacities are optimized jointly with a ViT classifier trained on top of these representations. We reuse the classifier gradients as constructive guidance, steering the Gaussians toward class-salient regions while a differentiable renderer optimizes an image reconstruction loss. We demonstrate that by 2D Gaussian input representations coupled with our GVIT guidance, using a relatively standard ViT architecture, closely matches the performance of a traditional patch-based ViT, reaching a 76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.",
        "arxiv_id": "2506.23532",
        "ARXIVID": "2506.23532",
        "COMMENT": "Does not match any specific criteria but introduces a novel representation for images using Gaussians.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23692": {
        "authors": [
            "Boyuan Zheng",
            "Zerui Fang",
            "Zhe Xu",
            "Rui Wang",
            "Yiwen Chen",
            "Cunshi Wang",
            "Mengwei Qu",
            "Lei Lei",
            "Zhen Feng",
            "Yan Liu",
            "Yuyang Li",
            "Mingzhou Tan",
            "Jiaji Wu",
            "Jianwei Shuai",
            "Jia Li",
            "Fangfu Ye"
        ],
        "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models",
        "abstract": "arXiv:2506.23692v1 Announce Type: new  Abstract: While AI for Science (AI4S) serves as an analytical tool in the current research paradigm, it doesn't solve its core inefficiency. We propose \"Agent for Science\" (Agent4S)-the use of LLM-driven agents to automate the entire research workflow-as the true Fifth Scientific Paradigm. This paper introduces a five-level classification for Agent4S, outlining a clear roadmap from simple task automation to fully autonomous, collaborative \"AI Scientists.\" This framework defines the next revolutionary step in scientific discovery.",
        "arxiv_id": "2506.23692",
        "ARXIVID": "2506.23692",
        "COMMENT": "Does not match any specific criteria. Discusses a conceptual framework for LLM-driven agents in scientific research, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23074": {
        "authors": [
            "Yu Zheng",
            "Boyang Gong",
            "Fanye Kong",
            "Yueqi Duan",
            "Bingyao Yu",
            "Wenzhao Zheng",
            "Lei Chen",
            "Jiwen Lu",
            "Jie Zhou"
        ],
        "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution",
        "abstract": "arXiv:2506.23074v1 Announce Type: new  Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning (CDAL) method for open-world model attribution. Existing methods rely on handcrafted design of region partitioning or feature space, which could be confounded by the spurious statistical correlations and struggle with novel attacks in open-world scenarios. To address this, CDAL explicitly models the causal relationships between the attentional visual traces and source model attribution, and counterfactually decouples the discriminative model-specific artifacts from confounding source biases for comparison. In this way, the resulting causal effect provides a quantification on the quality of learned attention maps, thus encouraging the network to capture essential generation patterns that generalize to unseen source models by maximizing the effect. Extensive experiments on existing open-world model attribution benchmarks show that with minimal computational overhead, our method consistently improves state-of-the-art models by large margins, particularly for unseen novel attacks. Source code: https://github.com/yzheng97/CDAL.",
        "arxiv_id": "2506.23074",
        "ARXIVID": "2506.23074",
        "COMMENT": "Does not match any specific criteria. Focuses on open-world model attribution using counterfactual attention learning, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23618": {
        "authors": [
            "Zhongdao Wang",
            "Guodongfang Zhao",
            "Jingjing Ren",
            "Bailan Feng",
            "Shifeng Zhang",
            "Wenbo Li"
        ],
        "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them",
        "abstract": "arXiv:2506.23618v1 Announce Type: new  Abstract: Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image SR show surprising fine details.",
        "arxiv_id": "2506.23618",
        "ARXIVID": "2506.23618",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling and efficient methods.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23549": {
        "authors": [
            "Huai-Chih Wang",
            "Hsiang-Chun Chuang",
            "Hsi-Chun Cheng",
            "Dai-Jie Wu",
            "Shao-Hua Sun"
        ],
        "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers",
        "abstract": "arXiv:2506.23549v1 Announce Type: new  Abstract: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.",
        "arxiv_id": "2506.23549",
        "ARXIVID": "2506.23549",
        "COMMENT": "Does not match any specific criteria. Focuses on coordination transformers for multi-agent systems, which is not directly related to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23714": {
        "authors": [
            "Md Moinul Islam",
            "Sofoklis Kakouros",
            "Janne Heikkil\\\"a",
            "Mourad Oussalah"
        ],
        "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
        "abstract": "arXiv:2506.23714v1 Announce Type: new  Abstract: The increasing volume of video content in educational, professional, and social domains necessitates effective summarization techniques that go beyond traditional unimodal approaches. This paper proposes a behaviour-aware multimodal video summarization framework that integrates textual, audio, and visual cues to generate timestamp-aligned summaries. By extracting prosodic features, textual cues and visual indicators, the framework identifies semantically and emotionally important moments. A key contribution is the identification of bonus words, which are terms emphasized across multiple modalities and used to improve the semantic relevance and expressive clarity of the summaries. The approach is evaluated against pseudo-ground truth (pGT) summaries generated using LLM-based extractive method. Experimental results demonstrate significant improvements over traditional extractive method, such as the Edmundson method, in both text and video-based evaluation metrics. Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework improves F1-Score by almost 23%. The findings underscore the potential of multimodal integration in producing comprehensive and behaviourally informed video summaries.",
        "arxiv_id": "2506.23714",
        "ARXIVID": "2506.23714",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multimodal learning and video summarization, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23547": {
        "authors": [
            "Jiwon Kim",
            "Soohyun Hwang",
            "Dong-O Kim",
            "Changsu Han",
            "Min Kyu Park",
            "Chang-Su Kim"
        ],
        "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions",
        "abstract": "arXiv:2506.23547v1 Announce Type: new  Abstract: The first algorithm, called Oneta, for a novel task of multi-style image enhancement is proposed in this work. Oneta uses two point operators sequentially: intensity enhancement with a transformation function (TF) and color correction with a color correction matrix (CCM). This two-step enhancement model, though simple, achieves a high performance upper bound. Also, we introduce eigentransformation function (eigenTF) to represent TF compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and CCM parameters, respectively. To support $K$ styles, Oneta employs $K$ learnable tokens. During training, each style token is learned using image pairs from the corresponding dataset. In testing, Oneta selects one of the $K$ style tokens to enhance an image accordingly. Extensive experiments show that the single Oneta network can effectively undertake six enhancement tasks -- retouching, image signal processing, low-light image enhancement, dehazing, underwater image enhancement, and white balancing -- across 30 datasets.",
        "arxiv_id": "2506.23547",
        "ARXIVID": "2506.23547",
        "COMMENT": "Does not match any specific criteria but introduces a multi-style image enhancement method.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23482": {
        "authors": [
            "Jun Huang",
            "Ting Liu",
            "Yihang Wu",
            "Xiaochao Qu",
            "Luoqi Liu",
            "Xiaolin Hu"
        ],
        "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting",
        "abstract": "arXiv:2506.23482v1 Announce Type: new  Abstract: Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods.",
        "arxiv_id": "2506.23482",
        "ARXIVID": "2506.23482",
        "COMMENT": "Does not match any specific criteria but focuses on generative modeling for object inpainting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.24092": {
        "authors": [
            "Moein Heidari",
            "Yasamin Medghalchi",
            "Mahdi Khoursha",
            "Reza Rezaeian",
            "Ilker Hacihaliloglu"
        ],
        "title": "WaRA: Wavelet Low Rank Adaptation",
        "abstract": "arXiv:2506.24092v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its extensions have emerged as particularly effective, allowing efficient model adaptation while significantly reducing computational overhead. However, existing approaches typically rely on global low-rank factorizations, which overlook local or multi-scale structure, failing to capture complex patterns in the weight updates. To address this, we propose WaRA, a novel PEFT method that leverages wavelet transforms to decompose the weight update matrix into a multi-resolution representation. By performing low-rank factorization in the wavelet domain and reconstructing updates through an inverse transform, WaRA obtains compressed adaptation parameters that harness multi-resolution analysis, enabling it to capture both coarse and fine-grained features while providing greater flexibility and sparser representations than standard LoRA. Through comprehensive experiments and analysis, we demonstrate that WaRA performs superior on diverse vision tasks, including image generation, classification, and semantic segmentation, significantly enhancing generated image quality while reducing computational complexity. Although WaRA was primarily designed for vision tasks, we further showcase its effectiveness in language tasks, highlighting its broader applicability and generalizability. The code is publicly available at \\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.",
        "arxiv_id": "2506.24092",
        "ARXIVID": "2506.24092",
        "COMMENT": "Does not match any specific criteria but discusses parameter-efficient fine-tuning for vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23881": {
        "authors": [
            "Reihaneh Zohrabi",
            "Hosein Hasani",
            "Mahdieh Soleymani Baghshah",
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Mohammad Hossein Rohban"
        ],
        "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection",
        "abstract": "arXiv:2506.23881v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications, where they frequently face data distributions unseen during training. Despite progress, existing methods are often vulnerable to spurious correlations that mislead models and compromise robustness. To address this, we propose SPROD, a novel prototype-based OOD detection approach that explicitly addresses the challenge posed by unknown spurious correlations. Our post-hoc method refines class prototypes to mitigate bias from spurious features without additional data or hyperparameter tuning, and is broadly applicable across diverse backbones and OOD detection settings. We conduct a comprehensive spurious correlation OOD detection benchmarking, comparing our method against existing approaches and demonstrating its superior performance across challenging OOD datasets, such as CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3% over the second best.",
        "arxiv_id": "2506.23881",
        "ARXIVID": "2506.23881",
        "COMMENT": "Does not match any specific criteria. Focuses on out-of-distribution detection with prototype refinement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22554": {
        "authors": [
            "Vasu Agrawal",
            "Akinniyi Akinyemi",
            "Kathryn Alvero",
            "Morteza Behrooz",
            "Julia Buffalini",
            "Fabio Maria Carlucci",
            "Joy Chen",
            "Junming Chen",
            "Zhang Chen",
            "Shiyang Cheng",
            "Praveen Chowdary",
            "Joe Chuang",
            "Antony D'Avirro",
            "Jon Daly",
            "Ning Dong",
            "Mark Duppenthaler",
            "Cynthia Gao",
            "Jeff Girard",
            "Martin Gleize",
            "Sahir Gomez",
            "Hongyu Gong",
            "Srivathsan Govindarajan",
            "Brandon Han",
            "Sen He",
            "Denise Hernandez",
            "Yordan Hristov",
            "Rongjie Huang",
            "Hirofumi Inaguma",
            "Somya Jain",
            "Raj Janardhan",
            "Qingyao Jia",
            "Christopher Klaiber",
            "Dejan Kovachev",
            "Moneish Kumar",
            "Hang Li",
            "Yilei Li",
            "Pavel Litvin",
            "Wei Liu",
            "Guangyao Ma",
            "Jing Ma",
            "Martin Ma",
            "Xutai Ma",
            "Lucas Mantovani",
            "Sagar Miglani",
            "Sreyas Mohan",
            "Louis-Philippe Morency",
            "Evonne Ng",
            "Kam-Woh Ng",
            "Tu Anh Nguyen",
            "Amia Oberai",
            "Benjamin Peloquin",
            "Juan Pino",
            "Jovan Popovic",
            "Omid Poursaeed",
            "Fabian Prada",
            "Alice Rakotoarison",
            "Alexander Richard",
            "Christophe Ropers",
            "Safiyyah Saleem",
            "Vasu Sharma",
            "Alex Shcherbyna",
            "Jia Shen",
            "Jie Shen",
            "Anastasis Stathopoulos",
            "Anna Sun",
            "Paden Tomasello",
            "Tuan Tran",
            "Arina Turkatenko",
            "Bo Wan",
            "Chao Wang",
            "Jeff Wang",
            "Mary Williamson",
            "Carleigh Wood",
            "Tao Xiang",
            "Yilin Yang",
            "Julien Yao",
            "Chen Zhang",
            "Jiemin Zhang",
            "Xinyue Zhang",
            "Jason Zheng",
            "Pavlo Zhyzheria",
            "Jan Zikes",
            "Michael Zollhoefer"
        ],
        "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset",
        "abstract": "arXiv:2506.22554v1 Announce Type: new  Abstract: Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.",
        "arxiv_id": "2506.22554",
        "ARXIVID": "2506.22554",
        "COMMENT": "Does not match any specific criteria. Focuses on dyadic audiovisual motion modeling and dataset creation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23705": {
        "authors": [
            "Smriti Joshi",
            "Richard Osuala",
            "Lidia Garrucho",
            "Kaisar Kushibar",
            "Dimitri Kessler",
            "Oliver Diaz",
            "Karim Lekadir"
        ],
        "title": "Single Image Test-Time Adaptation via Multi-View Co-Training",
        "abstract": "arXiv:2506.23705v1 Announce Type: new  Abstract: Test-time adaptation enables a trained model to adjust to a new domain during inference, making it particularly valuable in clinical settings where such on-the-fly adaptation is required. However, existing techniques depend on large target domain datasets, which are often impractical and unavailable in medical scenarios that demand per-patient, real-time inference. Moreover, current methods commonly focus on two-dimensional images, failing to leverage the volumetric richness of medical imaging data. Bridging this gap, we propose a Patch-Based Multi-View Co-Training method for Single Image Test-Time adaptation. Our method enforces feature and prediction consistency through uncertainty-guided self-training, enabling effective volumetric segmentation in the target domain with only a single test-time image. Validated on three publicly available breast magnetic resonance imaging datasets for tumor segmentation, our method achieves performance close to the upper bound supervised benchmark while also outperforming all existing state-of-the-art methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly share our accessible codebase, readily integrable with the popular nnUNet framework, at https://github.com/smriti-joshi/muvi.git.",
        "arxiv_id": "2506.23705",
        "ARXIVID": "2506.23705",
        "COMMENT": "Does not match any specific criteria. Focuses on test-time adaptation for medical imaging, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23538": {
        "authors": [
            "Yuhao Huang",
            "Yueyue Xu",
            "Haoran Dou",
            "Jiaxiao Deng",
            "Xin Yang",
            "Hongyu Zheng",
            "Dong Ni"
        ],
        "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound",
        "abstract": "arXiv:2506.23538v1 Announce Type: new  Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.",
        "arxiv_id": "2506.23538",
        "ARXIVID": "2506.23538",
        "COMMENT": "Does not match any specific criteria. Focuses on medical imaging and reinforcement learning for ultrasound diagnosis, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22726": {
        "authors": [
            "Yu Zhang",
            "Xi Zhang",
            "Hualin zhou",
            "Xinyuan Chen",
            "Shang Gao",
            "Hong Jia",
            "Jianfei Yang",
            "Yuankai Qi",
            "Tao Gu"
        ],
        "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge",
        "abstract": "arXiv:2506.22726v1 Announce Type: new  Abstract: Deep learning for human sensing on edge systems offers significant opportunities for smart applications. However, its training and development are hindered by the limited availability of sensor data and resource constraints of edge systems. Current methods that rely on transferring pre-trained models often encounter issues such as modality shift and high resource demands, resulting in substantial accuracy loss, resource overhead, and poor adaptability across different sensing applications. In this paper, we propose XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic model transfer. XTransfer freely leverages single or multiple pre-trained models and transfers knowledge across different modalities by (i) model repairing that safely repairs modality shift in pre-trained model layers with only few sensor data, and (ii) layer recombining that efficiently searches and recombines layers of interest from source models in a layer-wise manner to create compact models. We benchmark various baselines across diverse human sensing datasets spanning different modalities. Comprehensive results demonstrate that XTransfer achieves state-of-the-art performance on human sensing tasks while significantly reducing the costs of sensor data collection, model training, and edge deployment.",
        "arxiv_id": "2506.22726",
        "ARXIVID": "2506.22726",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of multi-modal learning and edge systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23088": {
        "authors": [
            "Yuchen Zhou",
            "Jiayu Tang",
            "Xiaoyan Xiao",
            "Yueyao Lin",
            "Linkai Liu",
            "Zipeng Guo",
            "Hao Fei",
            "Xiaobo Xia",
            "Chao Gou"
        ],
        "title": "Where, What, Why: Towards Explainable Driver Attention Prediction",
        "abstract": "arXiv:2506.23088v1 Announce Type: new  Abstract: Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction.",
        "arxiv_id": "2506.23088",
        "ARXIVID": "2506.23088",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of vision and cognitive modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23827": {
        "authors": [
            "Mingcheng Qu",
            "Yuncong Wu",
            "Donglin Di",
            "Yue Gao",
            "Tonghua Su",
            "Yang Song",
            "Lei Fan"
        ],
        "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning",
        "abstract": "arXiv:2506.23827v1 Announce Type: new  Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue micro-environments, but is limited to its high cost and complexity. As an alternative, predicting gene expression from pathology whole slide images (WSI) is gaining increasing attention. However, existing methods typically rely on single patches or a single pathology modality, neglecting the complex spatial and molecular interactions between target and neighboring information (e.g., gene co-expression). This leads to a failure in establishing connections among adjacent regions and capturing intricate cross-modal relationships. To address these issues, we propose NH2ST, a framework that integrates spatial context and both pathology and gene modalities for gene expression prediction. Our model comprises a query branch and a neighbor branch to process paired target patch and gene data and their neighboring regions, where cross-attention and contrastive learning are employed to capture intrinsic associations and ensure alignments between pathology and gene expression. Extensive experiments on six datasets demonstrate that our model consistently outperforms existing methods, achieving over 20% in PCC metrics. Codes are available at https://github.com/MCPathology/NH2ST",
        "arxiv_id": "2506.23827",
        "ARXIVID": "2506.23827",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of multi-modal learning and spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23049": {
        "authors": [
            "Leander Melroy Maben",
            "Gayathri Ganesh Lakshmy",
            "Srijith Radhakrishnan",
            "Siddhant Arora",
            "Shinji Watanabe"
        ],
        "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks",
        "abstract": "arXiv:2506.23049v1 Announce Type: new  Abstract: Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
        "arxiv_id": "2506.23049",
        "ARXIVID": "2506.23049",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of multi-modal learning and embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23724": {
        "authors": [
            "Chang'an Yi",
            "Xiaohui Deng",
            "Guohao Chen",
            "Yan Zhou",
            "Qinghua Lu",
            "Shuaicheng Niu"
        ],
        "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation",
        "abstract": "arXiv:2506.23724v1 Announce Type: new  Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with potential domain shifts through online unsupervised learning, yielding impressive performance. However, to date, existing TTA methods primarily focus on single-model adaptation. In this work, we investigate an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that, in TTA's unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there are substantial differences in model size. For instance, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base (86.6M parameters). In light of this, we propose COCA, a Cross-Model Co-Learning framework for TTA, which mainly consists of two main strategies. 1) Co-adaptation adaptively integrates complementary knowledge from other models throughout the TTA process, reducing individual model biases. 2) Self-adaptation enhances each model's unique strengths via unsupervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, which can also serve as a plug-and-play module, significantly boosts existing SOTAs, on models with various sizes--including ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example, with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy on ImageNet-C from 51.7% to 64.5%. The code is publicly available at https://github.com/ycarobot/COCA.",
        "arxiv_id": "2506.23724",
        "ARXIVID": "2506.23724",
        "COMMENT": "Does not match any specific criteria but introduces a cross-model co-learning framework for test-time adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23461": {
        "authors": [
            "Yun Xing",
            "Qing Guo",
            "Xiaoguang Li",
            "Yihao Huang",
            "Xiaofeng Cao",
            "Di Lin",
            "Ivor Tsang",
            "Lei Ma"
        ],
        "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation",
        "abstract": "arXiv:2506.23461v1 Announce Type: new  Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image's source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.",
        "arxiv_id": "2506.23461",
        "ARXIVID": "2506.23461",
        "COMMENT": "Does not match any specific criteria but focuses on a novel image inpainting task (TAMP) with a diffusion-based approach.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22749": {
        "authors": [
            "Yun Zhang",
            "Feifan Chen",
            "Na Li",
            "Zhiwei Guo",
            "Xu Wang",
            "Fen Miao",
            "Sam Kwong"
        ],
        "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds",
        "abstract": "arXiv:2506.22749v1 Announce Type: new  Abstract: Colored point cloud, which includes geometry and attribute components, is a mainstream representation enabling realistic and immersive 3D applications. To generate large-scale and denser colored point clouds, we propose a deep learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that learns to model both geometry and attribute patterns while leveraging spatial attribute correlations. First, we establish and release a large-scale dataset for colored point cloud up-sampling called SYSU-PCUD, containing 121 large-scale colored point clouds with diverse geometry and attribute complexities across six categories and four sampling rates. Second, to improve the quality of up-sampled point clouds, we propose a deep learning-based JGAU framework that jointly up-samples geometry and attributes. It consists of a geometry up-sampling network and an attribute up-sampling network, where the latter leverages the up-sampled auxiliary geometry to model neighborhood correlations of the attributes. Third, we propose two coarse attribute up-sampling methods, Geometric Distance Weighted Attribute Interpolation (GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate coarse up-sampled attributes for each point. Then, an attribute enhancement module is introduced to refine these up-sampled attributes and produce high-quality point clouds by further exploiting intrinsic attribute and geometry patterns. Extensive experiments show that the Peak Signal-to-Noise Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10 decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times, 8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28 decibels, and 2.11 decibels at these four up-sampling rates, demonstrating significant improvement.",
        "arxiv_id": "2506.22749",
        "ARXIVID": "2506.22749",
        "COMMENT": "Does not match any specific criteria. Focuses on up-sampling for colored point clouds, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23542": {
        "authors": [
            "Weida Wang",
            "Changyong He",
            "Jin Zeng",
            "Di Qiu"
        ],
        "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention",
        "abstract": "arXiv:2506.23542v1 Announce Type: new  Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.",
        "arxiv_id": "2506.23542",
        "ARXIVID": "2506.23542",
        "COMMENT": "Does not match any specific criteria. Focuses on depth denoising for ToF sensors, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22850": {
        "authors": [
            "Aalok Gangopadhyay",
            "Shashikant Verma",
            "Shanmuganathan Raman"
        ],
        "title": "DMD-Net: Deep Mesh Denoising Network",
        "abstract": "arXiv:2506.22850v1 Announce Type: new  Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning framework, for solving the mesh denoising problem. DMD-Net consists of a Graph Convolutional Neural Network in which aggregation is performed in both the primal as well as the dual graph. This is realized in the form of an asymmetric two-stream network, which contains a primal-dual fusion block that enables communication between the primal-stream and the dual-stream. We develop a Feature Guided Transformer (FGT) paradigm, which consists of a feature extractor, a transformer, and a denoiser. The feature extractor estimates the local features, that guide the transformer to compute a transformation, which is applied to the noisy input mesh to obtain a useful intermediate representation. This is further processed by the denoiser to obtain the denoised mesh. Our network is trained on a large scale dataset of 3D objects. We perform exhaustive ablation studies to demonstrate that each component in our network is essential for obtaining the best performance. We show that our method obtains competitive or better results when compared with the state-of-the-art mesh denoising algorithms. We demonstrate that our method is robust to various kinds of noise. We observe that even in the presence of extremely high noise, our method achieves excellent performance.",
        "arxiv_id": "2506.22850",
        "ARXIVID": "2506.22850",
        "COMMENT": "Does not match any specific criteria. Focuses on mesh denoising, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23077": {
        "authors": [
            "Suofei Zhang",
            "Xinxin Wang",
            "Xiaofu Wu",
            "Quan Zhou",
            "Haifeng Hu"
        ],
        "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization",
        "abstract": "arXiv:2506.23077v1 Announce Type: new  Abstract: Existing deep learning-based cross-view geo-localization methods primarily focus on improving the accuracy of cross-domain image matching, rather than enabling models to comprehensively capture contextual information around the target and minimize the cost of localization errors. To support systematic research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem, we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs multi-view imagery with precise distance annotations across three spatial resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical retrieval problem across different domains. Our study further reveals that, due to the inherent complexity of spatial relationships among buildings, this problem can only be addressed via a contrastive learning paradigm, rather than conventional metric learning. To tackle this challenge, we propose Dynamic Contrastive Learning (DyCL), a novel framework that progressively aligns feature representations according to hierarchical spatial margins. Extensive experiments demonstrate that DyCL is highly complementary to existing multi-scale metric learning methods and yields substantial improvements in both hierarchical retrieval performance and overall cross-view geo-localization accuracy. Our code and benchmark are publicly available at https://github.com/anocodetest1/DyCL.",
        "arxiv_id": "2506.23077",
        "ARXIVID": "2506.23077",
        "COMMENT": "Does not match any specific criterion but discusses hierarchical retrieval for geo-localization, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23519": {
        "authors": [
            "Qi Qin",
            "Runmin Cong",
            "Gen Zhan",
            "Yiting Liao",
            "Sam Kwong"
        ],
        "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection",
        "abstract": "arXiv:2506.23519v1 Announce Type: new  Abstract: The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.",
        "arxiv_id": "2506.23519",
        "ARXIVID": "2506.23519",
        "COMMENT": "Does not match any specific criterion but discusses weakly supervised video salient object detection, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22463": {
        "authors": [
            "Weizhi Gao",
            "Zhichao Hou",
            "Junqi Yin",
            "Feiyi Wang",
            "Linyu Peng",
            "Xiaorui Liu"
        ],
        "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization",
        "abstract": "arXiv:2506.22463v1 Announce Type: new  Abstract: Diffusion models have emerged as powerful generative models, but their high computation cost in iterative sampling remains a significant bottleneck. In this work, we present an in-depth and insightful study of state-of-the-art acceleration techniques for diffusion models, including caching and quantization, revealing their limitations in computation error and generation quality. To break these limits, this work introduces Modulated Diffusion (MoDiff), an innovative, rigorous, and principled framework that accelerates generative modeling through modulated quantization and error compensation. MoDiff not only inherents the advantages of existing caching and quantization methods but also serves as a general framework to accelerate all diffusion models. The advantages of MoDiff are supported by solid theoretical insight and analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate that MoDiff significant reduces activation quantization from 8 bits to 3 bits without performance degradation in post-training quantization (PTQ). Our code implementation is available at https://github.com/WeizhiGao/MoDiff.",
        "arxiv_id": "2506.22463",
        "ARXIVID": "2506.22463",
        "COMMENT": "Does not match any specific criterion but discusses acceleration techniques for diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23926": {
        "authors": [
            "Junping Wang",
            "Bicheng Wang",
            "Yibo Xuea",
            "Yuan Xie"
        ],
        "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system",
        "abstract": "arXiv:2506.23926v1 Announce Type: new  Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental functionality amidst failures and errors, is crucial for scientific management and engineering applications of industrial chain. The problem is particularly challenging when the number or types of multiple co-evolution of resilience (for example, randomly placed) are extremely chaos. Existing end-to-end deep learning ordinarily do not generalize well to unseen full-feld reconstruction of spatiotemporal co-evolution structure, and predict resilience of network topology, especially in multiple chaos data regimes typically seen in real-world applications. To address this challenge, here we propose industrial brain, a human-like autonomous cognitive decision-making and planning framework integrating higher-order activity-driven neuro network and CT-OODA symbolic reasoning to autonomous plan resilience directly from observational data of global variable. The industrial brain not only understands and model structure of node activity dynamics and network co-evolution topology without simplifying assumptions, and reveal the underlying laws hidden behind complex networks, but also enabling accurate resilience prediction, inference, and planning. Experimental results show that industrial brain significantly outperforms resilience prediction and planning methods, with an accurate improvement of up to 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension reduction. It also generalizes to unseen topologies and dynamics and maintains robust performance despite observational disturbances. Our findings suggest that industrial brain addresses an important gap in resilience prediction and planning for industrial chain.",
        "arxiv_id": "2506.23926",
        "ARXIVID": "2506.23926",
        "COMMENT": "Does not match any specific criterion but discusses a novel neuro-symbolic cognitive decision-making system, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22881": {
        "authors": [
            "Fumiya Uchiyama",
            "Rintaro Yanagi",
            "Shohei Taniguchi",
            "Shota Takashiro",
            "Masahiro Suzuki",
            "Hirokatsu Kataoka",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings",
        "abstract": "arXiv:2506.22881v1 Announce Type: new  Abstract: Contrastive learning has the capacity to model multimodal probability distributions by embedding and aligning visual representations with semantics from captions. This approach enables the estimation of relational semantic similarity; however, it remains unclear whether it can also represent absolute semantic informativeness. In this work, we introduce a semantic informativeness metric for an image calculated from text samples via a contrastive learning model; similarly, the informativeness of a text is calculated from image samples. We propose a redefinition of the concept of Information Gain, a concept previously explored in natural language processing, extending its application to the domains of vision and language. Our metric quantifies how conditioning on an image distorts the distribution of associated texts, and vice versa for text conditioning on image distributions. In OpenCLIP's empirical results, we observe that images with the lowest Information Gain scores often correspond to placeholder icons such as \"image not found.\" Furthermore, we propose to measure a norm-based metric of the embedding to estimate the Information Gain, following the theoretical results for Skip-Gram with Negative Sampling (SGNS) word embedding. Information Gain can be measured using either CLIP or SigLIP, and the results demonstrate a strong correlation with a coefficient of determination ranging from 0.98 to 1.00. After obtaining the mean and the covariance of the sample embedding, the computational cost of this method is independent of the sample size, and it is compatible with publicly available, open-weight models.",
        "arxiv_id": "2506.22881",
        "ARXIVID": "2506.22881",
        "COMMENT": "Does not match any specific criteria. Focuses on measuring semantic informativeness in contrastive learning embeddings, which is not directly related to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23104": {
        "authors": [
            "Jihun Kim",
            "Hoyong Kwon",
            "Hyeokjun Kweon",
            "Wooseong Jeong",
            "Kuk-Jin Yoon"
        ],
        "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation",
        "abstract": "arXiv:2506.23104v1 Announce Type: new  Abstract: Interactive segmentation (IS) allows users to iteratively refine object boundaries with minimal cues, such as positive and negative clicks. While the Segment Anything Model (SAM) has garnered attention in the IS community for its promptable segmentation capabilities, it often struggles in specialized domains or when handling complex scenarios (e.g., camouflaged or multi-part objects). To overcome these challenges, we propose DC-TTA, a novel test-time adaptation (TTA) framework that adapts SAM on a per-sample basis by leveraging user interactions as supervision. Instead of forcing a single model to incorporate all user clicks at once, DC-TTA partitions the clicks into more coherent subsets, each processed independently via TTA with a separated model. This Divide-and-Conquer strategy reduces conflicts among diverse cues and enables more localized updates. Finally, we merge the adapted models to form a unified predictor that integrates the specialized knowledge from each subset. Experimental results across various benchmarks demonstrate that DC-TTA significantly outperforms SAM's zero-shot results and conventional TTA methods, effectively handling complex tasks such as camouflaged object segmentation with fewer interactions and improved accuracy.",
        "arxiv_id": "2506.23104",
        "ARXIVID": "2506.23104",
        "COMMENT": "Does not match any specific criteria. Focuses on test-time adaptation for interactive segmentation, which is not directly related to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23783": {
        "authors": [
            "Shiao Wang",
            "Ju Huang",
            "Qingchuan Ma",
            "Jinfeng Gao",
            "Chunyi Xu",
            "Xiao Wang",
            "Lan Chen",
            "Bo Jiang"
        ],
        "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking",
        "abstract": "arXiv:2506.23783v1 Announce Type: new  Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack",
        "arxiv_id": "2506.23783",
        "ARXIVID": "2506.23783",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient RGB-Event object tracking, which is not directly related to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23157": {
        "authors": [
            "Hanyu Zhou",
            "Haonan Wang",
            "Haoyue Liu",
            "Yuxing Duan",
            "Luxin Yan",
            "Gim Hee Lee"
        ],
        "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene",
        "abstract": "arXiv:2506.23157v1 Announce Type: new  Abstract: High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
        "arxiv_id": "2506.23157",
        "ARXIVID": "2506.23157",
        "COMMENT": "Does not match any specific criteria. Focuses on spatiotemporal disentanglement for high-dynamic scene reconstruction, which is not directly related to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23141": {
        "authors": [
            "Siyuan Li",
            "Ruitong Liu",
            "Yan Wen",
            "Te Sun"
        ],
        "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing",
        "abstract": "arXiv:2506.23141v1 Announce Type: new  Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a \\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a \\textbf{multi-head attention aggregator} to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.",
        "arxiv_id": "2506.23141",
        "ARXIVID": "2506.23141",
        "COMMENT": "Does not match any specific criterion but is relevant to knowledge graph completion and semantic reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23543": {
        "authors": [
            "Hui Li",
            "Baoyou Chen",
            "Liwei Zhang",
            "Jiaye Li",
            "Jingdong Wang",
            "Siyu Zhu"
        ],
        "title": "Pyramidal Patchification Flow for Visual Generation",
        "abstract": "arXiv:2506.23543v1 Announce Type: new  Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations to token representations through linear projections, to adjust the number of tokens input to DiT blocks and thus the computation cost. Instead of a single patch size for all the timesteps, we introduce a Pyramidal Patchification Flow (PPFlow) approach: Large patch sizes are used for high noise timesteps and small patch sizes for low noise timesteps; Linear projections are learned for each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow, our approach operates over full latent representations other than pyramid representations, and adopts the normal denoising process without requiring the renoising trick. We demonstrate the effectiveness of our approach through two training manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$) inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with slightly lower training FLOPs and similar image generation performance. Training from pretrained normal DiTs achieves even better performance with small training time. The code and checkpoint are at https://github.com/fudan-generative-vision/PPFlow.",
        "arxiv_id": "2506.23543",
        "ARXIVID": "2506.23543",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22833": {
        "authors": [
            "Shashikant Verma",
            "Shanmuganathan Raman"
        ],
        "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds",
        "abstract": "arXiv:2506.22833v1 Announce Type: new  Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the resulting images often lack the capacity for localized editing. In response, generative radiance manifolds emerge as an efficient approach for constrained point sampling within volumes, effectively reducing computational demands and enabling the learning of fine details. This work introduces SemFaceEdit, a novel method that streamlines the appearance and geometric editing process by generating semantic fields on generative radiance manifolds. Utilizing latent codes, our method effectively disentangles the geometry and appearance associated with different facial semantics within the generated image. In contrast to existing methods that can change the appearance of the entire radiance field, our method enables the precise editing of particular facial semantics while preserving the integrity of other regions. Our network comprises two key modules: the Geometry module, which generates semantic radiance and occupancy fields, and the Appearance module, which is responsible for predicting RGB radiance. We jointly train both modules in adversarial settings to learn semantic-aware geometry and appearance descriptors. The appearance descriptors are then conditioned on their respective semantic latent codes by the Appearance Module, facilitating disentanglement and enhanced control. Our experiments highlight SemFaceEdit's superior performance in semantic field-based editing, particularly in achieving improved radiance field disentanglement.",
        "arxiv_id": "2506.22833",
        "ARXIVID": "2506.22833",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23852": {
        "authors": [
            "Jianing Jin",
            "Jiangyong Ying",
            "Huiyu Duan",
            "Liu Yang",
            "Sijing Wu",
            "Yunhao Li",
            "Yushuo Zheng",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment",
        "abstract": "arXiv:2506.23852v1 Announce Type: new  Abstract: As camera-equipped robotic platforms become increasingly integrated into daily life, robotic-generated videos have begun to appear on streaming media platforms, enabling us to envision a future where humans and robots coexist. We innovatively propose the concept of Robotic-Generated Content (RGC) to term these videos generated from egocentric perspective of robots. The perceptual quality of RGC videos is critical in human-robot interaction scenarios, and RGC videos exhibit unique distortions and visual requirements that differ markedly from those of professionally-generated content (PGC) videos and user-generated content (UGC) videos. However, dedicated research on quality assessment of RGC videos is still lacking. To address this gap and to support broader robotic applications, we establish the first Robotic-Generated Content Database (RGCD), which contains a total of 2,100 videos drawn from three robot categories and sourced from diverse platforms. A subjective VQA experiment is conducted subsequently to assess human visual perception of robotic-generated videos. Finally, we conduct a benchmark experiment to evaluate the performance of 11 state-of-the-art VQA models on our database. Experimental results reveal significant limitations in existing VQA models when applied to complex, robotic-generated content, highlighting a critical need for RGC-specific VQA models. Our RGCD is publicly available at: https://github.com/IntMeGroup/RGC-VQA.",
        "arxiv_id": "2506.23852",
        "ARXIVID": "2506.23852",
        "COMMENT": "Does not match any specific criteria. Focuses on quality assessment of robotic-generated videos.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.22843": {
        "authors": [
            "Kien Nguyen",
            "Clinton Fookes",
            "Sridha Sridharan",
            "Huy Nguyen",
            "Feng Liu",
            "Xiaoming Liu",
            "Arun Ross",
            "Dana Michalski",
            "Tam\\'as Endrei",
            "Ivan DeAndres-Tame",
            "Ruben Tolosana",
            "Ruben Vera-Rodriguez",
            "Aythami Morales",
            "Julian Fierrez",
            "Javier Ortega-Garcia",
            "Zijing Gong",
            "Yuhao Wang",
            "Xuehu Liu",
            "Pingping Zhang",
            "Md Rashidunnabi",
            "Hugo Proen\\c{c}a",
            "Kailash A. Hambarde",
            "Saeid Rezaei"
        ],
        "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results",
        "abstract": "arXiv:2506.22843v1 Announce Type: new  Abstract: Person re-identification (ReID) across aerial and ground vantage points has become crucial for large-scale surveillance and public safety applications. Although significant progress has been made in ground-only scenarios, bridging the aerial-ground domain gap remains a formidable challenge due to extreme viewpoint differences, scale variations, and occlusions. Building upon the achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID 2025 Challenge - the first large-scale video-based competition focused on high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7 million frames captured from UAVs, CCTV, and wearable cameras, the challenge featured four international teams. These teams developed solutions ranging from multi-stream architectures to transformer-based temporal reasoning and physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained 72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the ground-to-aerial ReID setting, surpassing existing baselines while highlighting the dataset's complexity. For additional details, please refer to the official website at https://agvpreid25.github.io.",
        "arxiv_id": "2506.22843",
        "ARXIVID": "2506.22843",
        "COMMENT": "Does not match any specific criteria. Focuses on person re-identification across aerial and ground domains.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23202": {
        "authors": [
            "Qilin Shu",
            "Qixian Zhang",
            "Qi Zhang",
            "Hongyun Zhang",
            "Duoqian Miao",
            "Cairong Zhao"
        ],
        "title": "Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing",
        "abstract": "arXiv:2506.23202v1 Announce Type: new  Abstract: The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face three primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person search. HAMW is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs containing additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a strategy based on multi-level Haar wavelet fusion to capture multi-scale features. This not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multi-scale information. Extensive experiments demonstrate that HAMW achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.",
        "arxiv_id": "2506.23202",
        "ARXIVID": "2506.23202",
        "COMMENT": "Does not match any specific criteria but proposes a novel transformer-based method for person search.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23676": {
        "authors": [
            "Gaozheng Pei",
            "Ke Ma",
            "Dongpeng Zhang",
            "Chengzhi Sun",
            "Qianqian Xu",
            "Qingming Huang"
        ],
        "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement",
        "abstract": "arXiv:2506.23676v1 Announce Type: new  Abstract: Due to their powerful image generation capabilities, diffusion-based adversarial example generation methods through image editing are rapidly gaining popularity. However, due to reliance on the discriminative capability of the diffusion model, these diffusion-based methods often struggle to generalize beyond conventional image classification tasks, such as in Deepfake detection. Moreover, traditional strategies for enhancing adversarial example transferability are challenging to adapt to these methods. To address these challenges, we propose a unified framework that seamlessly incorporates traditional transferability enhancement strategies into diffusion model-based adversarial example generation via image editing, enabling their application across a wider range of downstream tasks. Our method won first place in the \"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Media\" competition at ACM MM25, which validates the effectiveness of our approach.",
        "arxiv_id": "2506.23676",
        "ARXIVID": "2506.23676",
        "COMMENT": "Does not match any specific criteria but focuses on adversarial example generation using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23467": {
        "authors": [
            "Chenlang Yi",
            "Zizhan Xiong",
            "Qi Qi",
            "Xiyuan Wei",
            "Girish Bathla",
            "Ching-Long Lin",
            "Bobak Jack Mortazavi",
            "Tianbao Yang"
        ],
        "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays",
        "abstract": "arXiv:2506.23467v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.",
        "arxiv_id": "2506.23467",
        "ARXIVID": "2506.23467",
        "COMMENT": "Does not match any specific criteria but focuses on fairness in CLIP models for medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.22836": {
        "authors": [
            "Hongyan An",
            "Kuan Zhu",
            "Xin He",
            "Haiyun Guo",
            "Chaoyang Zhao",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition",
        "abstract": "arXiv:2506.22836v1 Announce Type: new  Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in intelligent transportation and security. To tackle this fine-grained task, most existing methods focus on extracting regional features to enrich attribute information. However, a regional feature is typically used to predict a fixed set of pre-defined attributes in these methods, which limits the performance and practicality in two aspects: 1) Regional features may compromise fine-grained patterns unique to certain attributes in favor of capturing common characteristics shared across attributes. 2) Regional features cannot generalize to predict unseen attributes in the test time. In this paper, we propose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C} g\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which adaptively extracts fine-grained attribute-level features for each attribute individually, regardless of whether the attributes are seen or not during training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to capture latent features at varying levels of visual granularity, thereby enriching the diversity of the extracted information. Next, we introduce the Attribute-guided Visual Feature Extraction (AVFE) module, which leverages textual attributes as queries to retrieve their corresponding visual attribute features from the Mix Tokens using a cross-attention mechanism. To ensure that textual attributes focus on the appropriate Mix Tokens, we further incorporate a Region-Aware Contrastive Learning (RACL) method, encouraging attributes within the same region to share consistent attention maps. Extensive experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness and strong generalization ability of our method.",
        "arxiv_id": "2506.22836",
        "ARXIVID": "2506.22836",
        "COMMENT": "Does not match any specific criteria. Focuses on pedestrian attribute recognition, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23523": {
        "authors": [
            "Tuong Do",
            "Binh X. Nguyen",
            "Quang D. Tran",
            "Erman Tjiputra",
            "Te-Chuan Chiu",
            "Anh Nguyen"
        ],
        "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving",
        "abstract": "arXiv:2506.23523v1 Announce Type: new  Abstract: Traditional vision-based autonomous driving systems often face difficulties in navigating complex environments when relying solely on single-image inputs. To overcome this limitation, incorporating temporal data such as past image frames or steering sequences, has proven effective in enhancing robustness and adaptability in challenging scenarios. While previous high-performance methods exist, they often rely on resource-intensive fusion networks, making them impractical for training and unsuitable for federated learning. To address these challenges, we propose lightweight temporal transformer decomposition, a method that processes sequential image frames and temporal steering data by breaking down large attention maps into smaller matrices. This approach reduces model complexity, enabling efficient weight updates for convergence and real-time predictions while leveraging temporal information to enhance autonomous driving performance. Intensive experiments on three datasets demonstrate that our method outperforms recent approaches by a clear margin while achieving real-time performance. Additionally, real robot experiments further confirm the effectiveness of our method.",
        "arxiv_id": "2506.23523",
        "ARXIVID": "2506.23523",
        "COMMENT": "Does not match any specific criterion but is relevant to autonomous driving and temporal modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23844": {
        "authors": [
            "Hang Su",
            "Jun Luo",
            "Chang Liu",
            "Xiao Yang",
            "Yichi Zhang",
            "Yinpeng Dong",
            "Jun Zhu"
        ],
        "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
        "abstract": "arXiv:2506.23844v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.",
        "arxiv_id": "2506.23844",
        "ARXIVID": "2506.23844",
        "COMMENT": "Does not match any specific criteria but provides a survey on security risks in large model-based agents.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}