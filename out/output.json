{
    "2511.13719": {
        "authors": [
            "Zhongang Cai",
            "Ruisi Wang",
            "Chenyang Gu",
            "Fanyi Pu",
            "Junxiang Xu",
            "Yubo Wang",
            "Wanqi Yin",
            "Zhitao Yang",
            "Chen Wei",
            "Qingping Sun",
            "Tongxi Zhou",
            "Jiaqi Li",
            "Hui En Pang",
            "Oscar Qian",
            "Yukun Wei",
            "Zhiqian Lin",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Liang Pan",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
        "abstract": "arXiv:2511.13719v1 Announce Type: new  Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
        "arxiv_id": "2511.13719",
        "ARXIVID": "2511.13719",
        "COMMENT": "Directly matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications). Presents a new family of multimodal foundation models (SenseNova-SI) focused on spatial intelligence, with new benchmarks and analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2511.12263": {
        "authors": [
            "Jingyao Li",
            "Jingyun Wang",
            "Molin Tan",
            "Haochen Wang",
            "Cilin Yan",
            "Likun Shi",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu"
        ],
        "title": "CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models",
        "abstract": "arXiv:2511.12263v1 Announce Type: new  Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.",
        "arxiv_id": "2511.12263",
        "ARXIVID": "2511.12263",
        "COMMENT": "This paper introduces CrossVid, a comprehensive benchmark for evaluating cross-video reasoning in MLLMs. It is the first benchmark to assess spatial-temporal reasoning across multiple videos, directly matching criterion 3 (new embodied AI benchmarks with novel angles) and criterion 2 (MLLMs). It also provides empirical insights into current MLLMs' limitations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.13704": {
        "authors": [
            "Harold Haodong Chen",
            "Disen Lan",
            "Wen-Jie Shu",
            "Qingyang Liu",
            "Zihan Wang",
            "Sirui Chen",
            "Wenkai Cheng",
            "Kanghao Chen",
            "Hongfei Zhang",
            "Zixin Zhang",
            "Rongjin Guo",
            "Yu Cheng",
            "Ying-Cong Chen"
        ],
        "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
        "abstract": "arXiv:2511.13704v1 Announce Type: new  Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
        "arxiv_id": "2511.13704",
        "ARXIVID": "2511.13704",
        "COMMENT": "This paper introduces TiViBench, a new benchmark for reasoning in video generative models, and proposes a test-time optimization method (VideoTPO) for improving reasoning. This directly matches criterion 3 (new benchmarks for embodied AI/video models) and is also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.12908": {
        "authors": [
            "Junbo Zou",
            "Haotian Xia",
            "Zhen Ye",
            "Shengjie Zhang",
            "Christopher Lai",
            "Vicente Ordonez",
            "Weining Shen",
            "Hanjie Chen"
        ],
        "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning",
        "abstract": "arXiv:2511.12908v1 Announce Type: new  Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.",
        "arxiv_id": "2511.12908",
        "ARXIVID": "2511.12908",
        "COMMENT": "DeepSport introduces a new MLLM for comprehensive sports video reasoning, using agentic reinforcement learning and a large, diverse dataset. This matches criterion 2 (new MLLMs) and is highly relevant to multi-modal learning and video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.13713": {
        "authors": [
            "Xincheng Shuai",
            "Zhenyuan Qin",
            "Henghui Ding",
            "Dacheng Tao"
        ],
        "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
        "abstract": "arXiv:2511.13713v1 Announce Type: new  Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
        "arxiv_id": "2511.13713",
        "ARXIVID": "2511.13713",
        "COMMENT": "This paper presents FFSE, a 3D-aware autoregressive framework for multi-round object manipulation in real-world images, modeling editing as a sequence of learned 3D transformations. It introduces a new dataset (3DObjectEditor) for multi-round editing. This is relevant to spatial understanding and intelligence (criterion 1) and also introduces a new method and dataset for embodied 3D scene editing (criterion 3).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.12868": {
        "authors": [
            "Ruiqi Yang",
            "Tian Yun",
            "Zihan Wang",
            "Ellie Pavlick"
        ],
        "title": "Video Finetuning Improves Reasoning Between Frames",
        "abstract": "arXiv:2511.12868v1 Announce Type: new  Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.",
        "arxiv_id": "2511.12868",
        "ARXIVID": "2511.12868",
        "COMMENT": "This paper investigates video finetuning for multimodal LLMs and introduces Visual Chain-of-Thought (vCoT) for explicit reasoning between video frames. It systematically compares image-only LVLMs and video-finetuned models, providing empirical insights into temporal reasoning. This directly matches criterion 2 (new VLLMs/MLLMs) and also provides surprising empirical results about video models' transfer abilities.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.12027": {
        "authors": [
            "Jeong Hun Yeo",
            "Sangyun Chung",
            "Sungjune Park",
            "Dae Hoe Kim",
            "Jinyoung Moon",
            "Yong Man Ro"
        ],
        "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory",
        "abstract": "arXiv:2511.12027v1 Announce Type: new  Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.",
        "arxiv_id": "2511.12027",
        "ARXIVID": "2511.12027",
        "COMMENT": "GCAgent introduces a new agent framework for long-video understanding using schematic and narrative episodic memory, targeting MLLMs. This is a strong match for criterion 2 (new MLLMs) and criterion 1 (spatial/temporal intelligence in embodied agents), as it addresses long-term temporal dependencies and reasoning in video.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.12207": {
        "authors": [
            "Haozhe Liu",
            "Ding Liu",
            "Mingchen Zhuge",
            "Zijian Zhou",
            "Tian Xie",
            "Sen He",
            "Yukang Yang",
            "Shuming Liu",
            "Yuren Cong",
            "Jiadong Guo",
            "Hongyu Xu",
            "Ke Xu",
            "Kam-Woh Ng",
            "Juan C. P\\'erez",
            "Juan-Manuel~P\\'erez-R\\'ua",
            "Tao Xiang",
            "Wei Liu",
            "Shikun Liu",
            "J\\\"urgen Schmidhuber"
        ],
        "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
        "abstract": "arXiv:2511.12207v1 Announce Type: new  Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\\epsilon$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
        "arxiv_id": "2511.12207",
        "ARXIVID": "2511.12207",
        "COMMENT": "Proposes a new fusion paradigm (Mixture of States) for multimodal diffusion models, with state-of-the-art results in text-to-image generation and editing. This matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.12428": {
        "authors": [
            "Jingqi Xu",
            "Jingxi Lu",
            "Chenghao Li",
            "Sreetama Sarkar",
            "Souvik Kundu",
            "Peter A. Beerel"
        ],
        "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning",
        "abstract": "arXiv:2511.12428v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.",
        "arxiv_id": "2511.12428",
        "ARXIVID": "2511.12428",
        "COMMENT": "Introduces a training-free visual token pruning method for diffusion vision-language models, directly matching criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.13211": {
        "authors": [
            "Yijia Fan",
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Jian Wang",
            "Keze Wang"
        ],
        "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale",
        "abstract": "arXiv:2511.13211v1 Announce Type: new  Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.",
        "arxiv_id": "2511.13211",
        "ARXIVID": "2511.13211",
        "COMMENT": "3DAlign-DAER proposes a unified framework for fine-grained 3D-text alignment at scale, with a new dynamic attention policy and efficient retrieval strategy, and introduces a large-scale dataset. This matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications), especially in cross-modal retrieval and alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.13283": {
        "authors": [
            "Jongha Kim",
            "Minseong Bae",
            "Sanghyeok Lee",
            "Jinsung Yoon",
            "Hyunwoo J. Kim"
        ],
        "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing",
        "abstract": "arXiv:2511.13283v1 Announce Type: new  Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.",
        "arxiv_id": "2511.13283",
        "ARXIVID": "2511.13283",
        "COMMENT": "This paper introduces TabFlash, an efficient MLLM for table understanding, with new methods for progressive question conditioning and token focusing. It directly matches criterion 2 (new MLLMs) and also presents clever empirical and architectural tricks for multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12151": {
        "authors": [
            "Kaixiang Yang",
            "Boyang Shen",
            "Xin Li",
            "Yuchen Dai",
            "Yuxuan Luo",
            "Yueran Ma",
            "Wei Fang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing",
        "abstract": "arXiv:2511.12151v1 Announce Type: new  Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.",
        "arxiv_id": "2511.12151",
        "ARXIVID": "2511.12151",
        "COMMENT": "This paper presents FIA-Edit, a new inversion-free, frequency-interactive attention framework for text-guided image editing, with applications to clinical images. It is a methodological improvement in vision foundation models and their applications (criterion 4), and also relates to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13102": {
        "authors": [
            "Yu Zhu",
            "Dan Zeng",
            "Shuiwang Li",
            "Qijun Zhao",
            "Qiaomu Shen",
            "Bo Tang"
        ],
        "title": "CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation",
        "abstract": "arXiv:2511.13102v1 Announce Type: new  Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.",
        "arxiv_id": "2511.13102",
        "ARXIVID": "2511.13102",
        "COMMENT": "This paper proposes a new framework for category-agnostic pose estimation that integrates hierarchical cross-modal interaction and dual-stream feature refinement. This is a methodological improvement in spatial understanding (criterion 1), especially in vision models, and is relevant to your friend's interests in clever statistical tricks and spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12026": {
        "authors": [
            "Rulin Zhou",
            "Wenlong He",
            "An Wang",
            "Jianhang Zhang",
            "Xuanhui Zeng",
            "Xi Zhang",
            "Chaowei Zhu",
            "Haijun Hu",
            "Hongliang Ren"
        ],
        "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark",
        "abstract": "arXiv:2511.12026v1 Announce Type: new  Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.",
        "arxiv_id": "2511.12026",
        "ARXIVID": "2511.12026",
        "COMMENT": "VL-SurgPT introduces a large-scale multimodal dataset and benchmark for surgical point tracking, bridging vision and language. This matches criterion 3 (new embodied AI benchmarks with a novel angle, i.e., combining visual and linguistic modalities for robust tracking) and is also relevant to criterion 2 (multimodal models).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12594": {
        "authors": [
            "Rongkun Zheng",
            "Lu Qi",
            "Xi Chen",
            "Yi Wang",
            "Kun Wang",
            "Hengshuang Zhao"
        ],
        "title": "Seg-VAR: Image Segmentation with Visual Autoregressive Modeling",
        "abstract": "arXiv:2511.12594v1 Announce Type: new  Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.",
        "arxiv_id": "2511.12594",
        "ARXIVID": "2511.12594",
        "COMMENT": "This paper proposes Seg-VAR, a novel framework that reframes image segmentation as a conditional autoregressive mask generation problem, introducing autoregressive reasoning into spatial-aware vision systems. This matches criterion 1 (new methodological improvements to spatial understanding) and is also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10705": {
        "authors": [
            "Yuan Zhao",
            "Hualei Zhu",
            "Tingyu Jiang",
            "Shen Li",
            "Xiaohang Xu",
            "Hao Henry Wang"
        ],
        "title": "Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents",
        "abstract": "arXiv:2511.10705v1 Announce Type: new  Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.",
        "arxiv_id": "2511.10705",
        "ARXIVID": "2511.10705",
        "COMMENT": "Co-EPG introduces a new self-iterative training framework for GUI agents, focusing on the co-evolution of planning and grounding. This is relevant to criterion 1 (spatial intelligence in embodied agents) and criterion 3 (new methods for embodied AI agents).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13135": {
        "authors": [
            "Junjie Yang",
            "Yuhao Yan",
            "Gang Wu",
            "Yuxuan Wang",
            "Ruoyu Liang",
            "Xinjie Jiang",
            "Xiang Wan",
            "Fenglei Fan",
            "Yongquan Zhang",
            "Feiwei Qin",
            "Changmiao Wan"
        ],
        "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation",
        "abstract": "arXiv:2511.13135v1 Announce Type: new  Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \\textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.",
        "arxiv_id": "2511.13135",
        "ARXIVID": "2511.13135",
        "COMMENT": "MedGEN-Bench is a new multimodal benchmark for open-ended medical image and text generation, with a focus on cross-modal reasoning and generative outputs. This matches criterion 3 (new benchmarks for embodied/multimodal AI) and is also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12280": {
        "authors": [
            "Shuochen Chang",
            "Xiaofeng Zhang",
            "Qingyang Liu",
            "Li Niu"
        ],
        "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs",
        "abstract": "arXiv:2511.12280v1 Announce Type: new  Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.",
        "arxiv_id": "2511.12280",
        "ARXIVID": "2511.12280",
        "COMMENT": "Proposes D$^{3}$ToM, a decider-guided dynamic token merging method for accelerating diffusion MLLMs, matching criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13001": {
        "authors": [
            "Pengcheng Shi",
            "Jiawei Chen",
            "Jiaqi Liu",
            "Xinglin Zhang",
            "Tao Chen",
            "Lei Li"
        ],
        "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
        "abstract": "arXiv:2511.13001v1 Announce Type: new  Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
        "arxiv_id": "2511.13001",
        "ARXIVID": "2511.13001",
        "COMMENT": "Introduces Medal S, a medical segmentation foundation model supporting spatial and textual prompts, with strong spatial understanding and efficiency. This matches criterion 4 (vision foundation models and applications) and partially criterion 1 (spatial intelligence).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12220": {
        "authors": [
            "Ameen Ali",
            "Tamim Zoabi",
            "Lior Wolf"
        ],
        "title": "Suppressing VLM Hallucinations with Spectral Representation Filtering",
        "abstract": "arXiv:2511.12220v1 Announce Type: new  Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.",
        "arxiv_id": "2511.12220",
        "ARXIVID": "2511.12220",
        "COMMENT": "Presents a training-free, post-hoc method to suppress hallucinations in vision-language models, matching criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.12034": {
        "authors": [
            "Xiaohao Liu",
            "Xiaobo Xia",
            "Jiaheng Wei",
            "Shuo Yang",
            "Xiu Su",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "title": "Calibrated Multimodal Representation Learning with Missing Modalities",
        "abstract": "arXiv:2511.12034v1 Announce Type: new  Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.",
        "arxiv_id": "2511.12034",
        "ARXIVID": "2511.12034",
        "COMMENT": "This paper introduces CalMRL, a method for multimodal representation learning with missing modalities, providing theoretical and practical advances in handling incomplete data. This is a methodological improvement in multi-modal learning (criterion 2), and is relevant to your friend's interest in multi-modal generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.12702": {
        "authors": [
            "Safaeid Hossain Arib",
            "Rabeya Akter",
            "Abdul Monaf Chowdhury",
            "Md Jubair Ahmed Sourov",
            "Md Mehedi Hasan"
        ],
        "title": "Counting Through Occlusion: Framework for Open World Amodal Counting",
        "abstract": "arXiv:2511.12702v1 Announce Type: new  Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.",
        "arxiv_id": "2511.12702",
        "ARXIVID": "2511.12702",
        "COMMENT": "This paper presents CountOCC, a framework for amodal object counting under occlusion using hierarchical multimodal guidance and visual equivalence objectives. It leverages spatial context and semantic priors for robust counting, which is a methodological improvement in spatial understanding (criterion 1) and uses multimodal cues (criterion 2, weakly).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.13011": {
        "authors": [
            "Qingsen Ma",
            "Chen Zou",
            "Dianyun Wang",
            "Jia Wang",
            "Liuyu Xiang",
            "Zhaofeng He"
        ],
        "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis",
        "abstract": "arXiv:2511.13011v1 Announce Type: new  Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.",
        "arxiv_id": "2511.13011",
        "ARXIVID": "2511.13011",
        "COMMENT": "This paper introduces DTGS, a framework for low-light novel view synthesis using thermal supervision and Retinex-inspired decomposition within 3D Gaussian Splatting. It also introduces a new dataset (RGBT-LOW) for low-light thermal multi-view data. This is relevant to vision foundation models and their application to novel view synthesis (criterion 4), and also introduces a new benchmark (criterion 3).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.12215": {
        "authors": [
            "Peng Zhang",
            "Zhihui Lai",
            "Wenting Chen",
            "Xu Wu",
            "Heng Kong"
        ],
        "title": "FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention",
        "abstract": "arXiv:2511.12215v1 Announce Type: new  Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.",
        "arxiv_id": "2511.12215",
        "ARXIVID": "2511.12215",
        "COMMENT": "FaNe introduces a new framework for medical vision-language pretraining with fine-grained cross-modal contrast and false-negative reduction. This is relevant to criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications), especially in the medical domain.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.11519": {
        "authors": [
            "Adam Stein",
            "Matthew Trager",
            "Benjamin Bowman",
            "Michael Kleinman",
            "Aditya Chattopadhyay",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
        "abstract": "arXiv:2511.11519v1 Announce Type: new  Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
        "arxiv_id": "2511.11519",
        "ARXIVID": "2511.11519",
        "COMMENT": "This paper introduces a meta-strategy for dynamically adapting inference-time reasoning strategies in agentic AI systems, which is a methodological improvement relevant to spatial intelligence and embodied agents (criterion 1).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.11257": {
        "authors": [
            "Yuqi Yin",
            "Yibo Fu",
            "Siyuan Wang",
            "Peng Sun",
            "Hongyu Wang",
            "Xiaohui Wang",
            "Lei Zheng",
            "Zhiyong Li",
            "Zhirong Liu",
            "Jianji Wang",
            "Zhaoxi Sun"
        ],
        "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
        "abstract": "arXiv:2511.11257v1 Announce Type: new  Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
        "arxiv_id": "2511.11257",
        "ARXIVID": "2511.11257",
        "COMMENT": "AIonopedia presents an LLM agent orchestrating multimodal learning for ionic liquid discovery, powered by a multimodal domain foundation model. This matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications), though the application is in chemistry rather than vision.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.13065": {
        "authors": [
            "Reeshoon Sayera",
            "Akash Kumar",
            "Sirshapan Mitra",
            "Prudvi Kamtam",
            "Yogesh S Rawat"
        ],
        "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
        "abstract": "arXiv:2511.13065v1 Announce Type: new  Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.",
        "arxiv_id": "2511.13065",
        "ARXIVID": "2511.13065",
        "COMMENT": "RobustGait introduces a new benchmark for robustness evaluation in gait recognition, with insights into how perturbations affect downstream systems. This matches criterion 3 (embodied AI papers on building new benchmarks with novel angles), as it provides a new evaluation framework and highlights overlooked sources of bias.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.13019": {
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Ge Wu",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "title": "MeanFlow Transformers with Representation Autoencoders",
        "abstract": "arXiv:2511.13019v1 Announce Type: new  Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.",
        "arxiv_id": "2511.13019",
        "ARXIVID": "2511.13019",
        "COMMENT": "This paper proposes an efficient training and sampling scheme for MeanFlow, a diffusion-motivated generative model, in the latent space of a representation autoencoder. It introduces clever statistical tricks for stabilizing training and reducing computation, and achieves strong empirical results. This is relevant to generative modeling in vision foundation models (criterion 4), with a focus on efficiency and empirical performance.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.12150": {
        "authors": [
            "Yuqi Xie",
            "Shuhan Ye",
            "Yi Yu",
            "Chong Wang",
            "Qixin Zhang",
            "Jiazhen Xu",
            "Le Shen",
            "Yuanbin Qian",
            "Jiangbo Qian",
            "Guoqi Li"
        ],
        "title": "Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain",
        "abstract": "arXiv:2511.12150v1 Announce Type: new  Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.",
        "arxiv_id": "2511.12150",
        "ARXIVID": "2511.12150",
        "COMMENT": "This paper introduces a new cross-modal training framework for transferring knowledge from RGB to event-based (DVS) data using spiking neural networks. The method (TMKT) is a novel statistical trick for cross-modal transfer, which is relevant to spatial intelligence and multi-modal learning (criterion 1).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.12110": {
        "authors": [
            "Qinyue Tong",
            "Ziqian Lu",
            "Jun Liu",
            "Rui Zuo",
            "Zheming Lu"
        ],
        "title": "MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images",
        "abstract": "arXiv:2511.12110v1 Announce Type: new  Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.",
        "arxiv_id": "2511.12110",
        "ARXIVID": "2511.12110",
        "COMMENT": "Presents a new multi-round entity-level reasoning segmentation task and dataset for medical images, with a novel model and error correction mechanism. This is a new benchmark and method for spatial reasoning in vision (criteria 1 and 3).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.11837": {
        "authors": [
            "Fatemeh Elhambakhsh",
            "Gaurav Ameta",
            "Aditi Roy",
            "Hyunwoong Ko"
        ],
        "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning",
        "abstract": "arXiv:2511.11837v1 Announce Type: new  Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.",
        "arxiv_id": "2511.11837",
        "ARXIVID": "2511.11837",
        "COMMENT": "This paper proposes MP-GFormer, a 3D-geometry-aware dynamic graph transformer for machining process planning, integrating evolving 3D geometric representations. It is a methodological improvement for spatial understanding in a specific industrial context, matching criterion 1 (spatial intelligence on embodied agents) but in a manufacturing domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.12047": {
        "authors": [
            "Huimin Cheng",
            "Xiaowei Yu",
            "Shushan Wu",
            "Luyang Fang",
            "Chao Cao",
            "Jing Zhang",
            "Tianming Liu",
            "Dajiang Zhu",
            "Wenxuan Zhong",
            "Ping Ma"
        ],
        "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging",
        "abstract": "arXiv:2511.12047v1 Announce Type: new  Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.",
        "arxiv_id": "2511.12047",
        "ARXIVID": "2511.12047",
        "COMMENT": "This paper introduces DCMM-Transformer, a novel ViT architecture for medical imaging that incorporates community structure and degree heterogeneity in a differentiable and interpretable way. While it is a methodological improvement in vision transformers, it is not specifically about spatial intelligence in embodied agents, VLLMs/MLLMs, or benchmarks. It is relevant to vision foundation models (criterion 4), especially in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.12662": {
        "authors": [
            "Hongbin Huang",
            "Junwei Li",
            "Tianxin Xie",
            "Zhuang Li",
            "Cekai Weng",
            "Yaodong Yang",
            "Yue Luo",
            "Li Liu",
            "Jing Tang",
            "Zhijing Shao",
            "Zeyu Wang"
        ],
        "title": "Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans",
        "abstract": "arXiv:2511.12662v1 Announce Type: new  Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.",
        "arxiv_id": "2511.12662",
        "ARXIVID": "2511.12662",
        "COMMENT": "This paper presents a high-fidelity, real-time conversational digital human system that integrates multi-modal components, including knowledge-grounded dialogue and expressive speech. It leverages novel retrieval-augmented methods and asynchronous pipelines. While it is relevant to embodied agents and multi-modal systems, it does not introduce a new VLLM/MLLM or a new benchmark, but does show a novel system for digital humans. Closest match is criterion 1 (spatial intelligence on embodied agents), but the focus is more on conversational interaction than spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.13110": {
        "authors": [
            "Shuaibin Fan",
            "Senming Zhong",
            "Wenchao Yan",
            "Minglong Xue"
        ],
        "title": "Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing",
        "abstract": "arXiv:2511.13110v1 Announce Type: new  Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.",
        "arxiv_id": "2511.13110",
        "ARXIVID": "2511.13110",
        "COMMENT": "This paper proposes an implicit neural representation for unpaired image dehazing, focusing on modeling spatially varying haze. While it is a vision foundation model application, it is not a new foundation model or a multi-modal method.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.12100": {
        "authors": [
            "Yannan Chen",
            "Ruoyu Chen",
            "Bin Zeng",
            "Wei Wang",
            "Shiming Liu",
            "Qunli Zhang",
            "Zheng Hu",
            "Laiyuan Wang",
            "Yaowei Wang",
            "Xiaochun Cao"
        ],
        "title": "Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation",
        "abstract": "arXiv:2511.12100v1 Announce Type: new  Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.",
        "arxiv_id": "2511.12100",
        "ARXIVID": "2511.12100",
        "COMMENT": "This paper proposes a counterfactual augmentation strategy guided by attribution methods to improve visual model generalization and robustness. While it is a clever training trick and uses interpretability, it is not directly about spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.12097": {
        "authors": [
            "Shuhan Ye",
            "Yi Yu",
            "Qixin Zhang",
            "Chenqi Kong",
            "Qiangqiang Wu",
            "Xudong Jiang",
            "Dacheng Tao"
        ],
        "title": "Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks",
        "abstract": "arXiv:2511.12097v1 Announce Type: new  Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \\emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \\emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \\textbf{SpikeNM}, the first SNN-oriented \\emph{semi-structured} \\(N{:}M\\) pruning framework that learns sparse SNNs \\emph{from scratch}, enforcing \\emph{at most \\(N\\)} non-zeros per \\(M\\)-weight block. To avoid the combinatorial space complexity \\(\\sum_{k=1}^{N}\\binom{M}{k}\\) growing exponentially with \\(M\\), SpikeNM adopts an \\(M\\)-way basis-logit parameterization with a differentiable top-\\(k\\) sampler, \\emph{linearizing} per-block complexity to \\(\\mathcal O(M)\\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \\emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \\(2{:}4\\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.",
        "arxiv_id": "2511.12097",
        "ARXIVID": "2511.12097",
        "COMMENT": "This paper introduces a new semi-structured pruning framework for spiking neural networks, with a neuroscience-inspired distillation method. While it is a methodological advance in neural network sparsity, it is not directly related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2511.13587": {
        "authors": [
            "Haotian Dong",
            "Ye Li",
            "Rongwei Lu",
            "Chen Tang",
            "Shu-Tao Xia",
            "Zhi Wang"
        ],
        "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
        "abstract": "arXiv:2511.13587v1 Announce Type: new  Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
        "arxiv_id": "2511.13587",
        "ARXIVID": "2511.13587",
        "COMMENT": "VVS introduces a new speculative decoding framework for visual autoregressive generation, accelerating inference via partial verification skipping. This is relevant to generative modeling in vision, but does not directly match the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.12321": {
        "authors": [
            "Xi Ding",
            "Lei Wang",
            "Piotr Koniusz",
            "Yongsheng Gao"
        ],
        "title": "Learning Time in Static Classifiers",
        "abstract": "arXiv:2511.12321v1 Announce Type: new  Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.",
        "arxiv_id": "2511.12321",
        "ARXIVID": "2511.12321",
        "COMMENT": "This paper proposes a new framework for equipping static classifiers with temporal reasoning using a novel loss design, bridging static and temporal learning. While it is a clever statistical trick and relevant to temporal modeling, it does not directly match any of the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.13127": {
        "authors": [
            "Zonghao Ying",
            "Moyang Chen",
            "Nizhang Li",
            "Zhiqiang Wang",
            "Wenxin Zhang",
            "Quanchen Zou",
            "Zonglei Jing",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language",
        "abstract": "arXiv:2511.13127v1 Announce Type: new  Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.",
        "arxiv_id": "2511.13127",
        "ARXIVID": "2511.13127",
        "COMMENT": "VEIL proposes a new jailbreak attack on text-to-video models using implicit language cues. While it is related to VLLMs and vision-language models, the focus is on adversarial attacks rather than new models or benchmarks. It is tangentially relevant to criterion 4.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.12107": {
        "authors": [
            "Tianxiang Zhang",
            "Peipeng Yu",
            "Zhihua Xia",
            "Longchen Dai",
            "Xiaoyu Zhou",
            "Hui Gao"
        ],
        "title": "Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection",
        "abstract": "arXiv:2511.12107v1 Announce Type: new  Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.",
        "arxiv_id": "2511.12107",
        "ARXIVID": "2511.12107",
        "COMMENT": "This paper proposes a new fine-grained adapter for DINOv2 for face forgery detection, with a multi-task approach. It is related to vision foundation models (criterion 4), but focuses on a specific application (deepfake detection) rather than general-purpose spatial understanding or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2511.12893": {
        "authors": [
            "Kaixin Zhang",
            "Ruiqing Yang",
            "Yuan Zhang",
            "Shan You",
            "Tao Huang"
        ],
        "title": "ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation",
        "abstract": "arXiv:2511.12893v1 Announce Type: new  Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\\%$ FLOPs reduction with minimal performance degradation.",
        "arxiv_id": "2511.12893",
        "ARXIVID": "2511.12893",
        "COMMENT": "Presents ActVAR, a dynamic activation framework for efficient visual autoregressive generation. While relevant to generative modeling, it does not directly match the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.12104": {
        "authors": [
            "Tammy Glazer",
            "Gilles Q. Hacheme",
            "Akram Zaytar",
            "Luana Marotti",
            "Amy Michaels",
            "Girmaw Abebe Tadesse",
            "Kevin White",
            "Rahul Dodhia",
            "Andrew Zolli",
            "Inbal Becker-Reshef",
            "Juan M. Lavista Ferres",
            "Caleb Robinson"
        ],
        "title": "TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery",
        "abstract": "arXiv:2511.12104v1 Announce Type: new  Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.",
        "arxiv_id": "2511.12104",
        "ARXIVID": "2511.12104",
        "COMMENT": "TEMPO introduces a global, temporally resolved dataset and deep learning model for building density and height estimation from satellite imagery. This is a vision application with a new dataset and model, but it does not focus on embodied AI, spatial intelligence for agents, VLLMs/MLLMs, or vision foundation models. It is more of a remote sensing application.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.13285": {
        "authors": [
            "Yunjie Yu",
            "Jingchen Wu",
            "Junchen Zhu",
            "Chunze Lin",
            "Guibin Chen"
        ],
        "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design",
        "abstract": "arXiv:2511.13285v1 Announce Type: new  Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.",
        "arxiv_id": "2511.13285",
        "ARXIVID": "2511.13285",
        "COMMENT": "This paper presents SkyReels-Text, a novel font-controllable text editing framework for poster design. While it is a vision application and involves generative modeling, it does not directly match any of the four criteria (no spatial intelligence, VLLM/MLLM, embodied AI, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.11551": {
        "authors": [
            "Dena Mujtaba",
            "Brian Hu",
            "Anthony Hoogs",
            "Arslan Basharat"
        ],
        "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
        "abstract": "arXiv:2511.11551v2 Announce Type: new  Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining alignment. For pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.",
        "arxiv_id": "2511.11551",
        "ARXIVID": "2511.11551",
        "COMMENT": "This paper introduces a test-time alignment technique for RL agents using policy shaping, evaluated on the MACHIAVELLI benchmark. While it is about agent behavior and alignment, it does not focus on spatial intelligence, embodied AI benchmarks, or vision/multimodal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.13399": {
        "authors": [
            "Yuchen Bao",
            "Yiting Wang",
            "Wenjian Huang",
            "Haowei Wang",
            "Shen Chen",
            "Taiping Yao",
            "Shouhong Ding",
            "Jianguo Zhang"
        ],
        "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
        "abstract": "arXiv:2511.13399v1 Announce Type: new  Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
        "arxiv_id": "2511.13399",
        "ARXIVID": "2511.13399",
        "COMMENT": "This paper proposes a new framework for scene text editing with disentangled modular attributes, which is a methodological improvement in spatial understanding and manipulation, but it is not directly about embodied agents or spatial intelligence in agents. It is more about generative modeling and disentanglement in vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.12422": {
        "authors": [
            "Nuolin Sun",
            "Linyuan Wang",
            "Haonan Wei",
            "Lei Li",
            "Bin Yan"
        ],
        "title": "MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation",
        "abstract": "arXiv:2511.12422v1 Announce Type: new  Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.",
        "arxiv_id": "2511.12422",
        "ARXIVID": "2511.12422",
        "COMMENT": "This paper introduces a new method for optimizing ResNet architectures using generative flow-fields, providing a novel perspective on the relationship between generative modeling and discriminative learning. While it is a methodological improvement in deep learning, it does not directly address spatial understanding in embodied agents (criterion 1), VLLMs/MLLMs (criterion 2), embodied AI benchmarks (criterion 3), or vision foundation models (criterion 4).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.11732": {
        "authors": [
            "Aditya Mehta",
            "Swarnim Chaudhary",
            "Pratik Narang",
            "Jagat Sesh Challa"
        ],
        "title": "Exposing DeepFakes via Hyperspectral Domain Mapping",
        "abstract": "arXiv:2511.11732v1 Announce Type: new  Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.",
        "arxiv_id": "2511.11732",
        "ARXIVID": "2511.11732",
        "COMMENT": "HSI-Detect proposes a hyperspectral domain mapping approach for deepfake detection. While it is a novel application of spectral analysis, it is not directly related to spatial intelligence, VLLMs/MLLMs, or embodied AI benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.12899": {
        "authors": [
            "Hao Li",
            "Zhenfeng Zhuang",
            "Jingyu Lin",
            "Yu Liu",
            "Yifei Chen",
            "Qiong Peng",
            "Lequan Yu",
            "Liansheng Wang"
        ],
        "title": "FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI",
        "abstract": "arXiv:2511.12899v1 Announce Type: new  Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.",
        "arxiv_id": "2511.12899",
        "ARXIVID": "2511.12899",
        "COMMENT": "Proposes a frequency-domain preprocessing pipeline for unsupervised anomaly detection in brain MRI. While interesting for generative modeling, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.12090": {
        "authors": [
            "Shengqin Jiang",
            "Tianqi Kong",
            "Yuankai Qi",
            "Haokui Zhang",
            "Lina Yao",
            "Quan Z. Sheng",
            "Qingshan Liu",
            "Ming-Hsuan Yang"
        ],
        "title": "Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning",
        "abstract": "arXiv:2511.12090v1 Announce Type: new  Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.",
        "arxiv_id": "2511.12090",
        "ARXIVID": "2511.12090",
        "COMMENT": "This paper proposes a new hierarchical layer-grouped prompt tuning method for continual learning, which is a methodological improvement in prompt-based adaptation. However, it does not focus on spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about continual learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12066": {
        "authors": [
            "Jialang Lu",
            "Shuning Sun",
            "Pu Wang",
            "Chen Wu",
            "Feng Gao",
            "Lina Gong",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "title": "DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal",
        "abstract": "arXiv:2511.12066v1 Announce Type: new  Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel\", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.",
        "arxiv_id": "2511.12066",
        "ARXIVID": "2511.12066",
        "COMMENT": "This paper introduces DCA-LUT, a deep learning framework for purple fringing removal using a novel chromatic-aware coordinate transformation and a 5D LUT. It is an application of vision models, but not a new foundation model or multi-modal method.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13115": {
        "authors": [
            "Hanzhe Liang",
            "Jie Zhou",
            "Can Gao",
            "Bingyang Guo",
            "Jinbao Wang",
            "Linlin Shen"
        ],
        "title": "A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features",
        "abstract": "arXiv:2511.13115v1 Announce Type: new  Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.",
        "arxiv_id": "2511.13115",
        "ARXIVID": "2511.13115",
        "COMMENT": "This paper proposes a lightweight 3D anomaly detection method with rotationally invariant features. While it is a methodological improvement in 3D vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, new embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12162": {
        "authors": [
            "Shuo Yin",
            "Zhiyuan Yin",
            "Yuqing Hou",
            "Rui Liu",
            "Yong Chen",
            "Dell Zhang"
        ],
        "title": "Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function",
        "abstract": "arXiv:2511.12162v1 Announce Type: new  Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.",
        "arxiv_id": "2511.12162",
        "ARXIVID": "2511.12162",
        "COMMENT": "This paper proposes Center-Reassigned Hashing (CRH), an end-to-end deep hashing framework for semantic retrieval. While it introduces a novel method for learning hash centers, it is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12084": {
        "authors": [
            "Ji-Ping Jin",
            "Chen-Bin Feng",
            "Rui Fan",
            "Chi-Man Vong"
        ],
        "title": "SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving",
        "abstract": "arXiv:2511.12084v1 Announce Type: new  Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.",
        "arxiv_id": "2511.12084",
        "ARXIVID": "2511.12084",
        "COMMENT": "SemanticStitch proposes a deep learning-based image stitching method that uses semantic priors for better visual coherence. While it is a vision method, it does not directly address spatial intelligence in embodied agents or foundation models, nor does it introduce a new benchmark or VLLM/MLLM.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}