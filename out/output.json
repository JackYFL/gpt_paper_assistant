{
    "2601.01547": {
        "authors": [
            "Tianjun Gu",
            "Chenghua Gong",
            "Jingyu Gong",
            "Zhizhong Zhang",
            "Yuan Xie",
            "Lizhuang Ma",
            "Xin Tan"
        ],
        "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding",
        "abstract": "arXiv:2601.01547v1 Announce Type: new  Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.",
        "arxiv_id": "2601.01547",
        "ARXIVID": "2601.01547",
        "COMMENT": "Matches criteria 1 and 3 very closely: introduces a new benchmark (EscherVerse) for teleo-spatial intelligence, focusing on both physical-dynamic and intent-driven reasoning, and provides a novel data curation pipeline for embodied spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2601.02204": {
        "authors": [
            "Huichao Zhang",
            "Liao Qu",
            "Yiheng Liu",
            "Hang Chen",
            "Yangyang Song",
            "Yongsheng Dong",
            "Shikun Sun",
            "Xian Li",
            "Xu Wang",
            "Yi Jiang",
            "Hu Ye",
            "Bo Chen",
            "Yiming Gao",
            "Peng Liu",
            "Akide Liu",
            "Zhipeng Yang",
            "Qili Deng",
            "Linjie Xing",
            "Jiyang Liu",
            "Zhao Wang",
            "Yang Zhou",
            "Mingcong Liu",
            "Yi Zhang",
            "Qian He",
            "Xiwei Hu",
            "Zhongqi Qi",
            "Jie Shao",
            "Zhiye Fu",
            "Shuai Wang",
            "Fangmin Chen",
            "Xuezhi Chai",
            "Zhihua Wu",
            "Yitong Wang",
            "Zehuan Yuan",
            "Daniel K. Du",
            "Xinglong Wu"
        ],
        "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
        "abstract": "arXiv:2601.02204v1 Announce Type: new  Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
        "arxiv_id": "2601.02204",
        "ARXIVID": "2601.02204",
        "COMMENT": "NextFlow is a unified decoder-only autoregressive transformer trained on 6T interleaved text-image tokens, enabling multimodal understanding and generation, including image editing and video generation. It introduces a novel next-scale prediction for images and achieves SOTA among unified models. This is a strong match for criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2601.00998": {
        "authors": [
            "Yue Zhou",
            "Jue Chen",
            "Zilun Zhang",
            "Penghui Huang",
            "Ran Ding",
            "Zhentao Zou",
            "PengFei Gao",
            "Yuchen Wei",
            "Ke Li",
            "Xue Yang",
            "Xue Jiang",
            "Hongxin Yang",
            "Jonathan Li"
        ],
        "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models",
        "abstract": "arXiv:2601.00998v1 Announce Type: new  Abstract: Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench",
        "arxiv_id": "2601.00998",
        "ARXIVID": "2601.00998",
        "COMMENT": "Matches criteria 2 and 3: introduces a new benchmark (DVGBench) for visual grounding in UAV imagery with LVLMs, and proposes a novel method (I2E-CoT) for implicit-to-explicit reasoning in vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.01984": {
        "authors": [
            "Weijian Ma",
            "Shizhao Sun",
            "Tianyu Yu",
            "Ruiyu Wang",
            "Tat-Seng Chua",
            "Jiang Bian"
        ],
        "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
        "abstract": "arXiv:2601.01984v1 Announce Type: new  Abstract: Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
        "arxiv_id": "2601.01984",
        "ARXIVID": "2601.01984",
        "COMMENT": "Strongly matches criterion 1 (new method for spatial reasoning in VLMs using structured object-centric blueprints). Also relevant to criterion 2 (improves VLMs for spatial intelligence).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.02281": {
        "authors": [
            "Shuai Yuan",
            "Yantai Yang",
            "Xiaotian Yang",
            "Xupeng Zhang",
            "Zhonghao Zhao",
            "Lingming Zhang",
            "Zhipeng Zhang"
        ],
        "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
        "abstract": "arXiv:2601.02281v1 Announce Type: new  Abstract: The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
        "arxiv_id": "2601.02281",
        "ARXIVID": "2601.02281",
        "COMMENT": "Strongly matches criterion 3 (embodied AI, new benchmark and method for long-term 3D geometry understanding in streaming/online settings) and criterion 4 (vision foundation models and applications). Introduces a new benchmark (Long3D) and a novel streaming transformer architecture for persistent 3D spatial understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.01322": {
        "authors": [
            "Hongjie Wang",
            "Niraj K. Jha"
        ],
        "title": "LinMU: Multimodal Understanding Made Linear",
        "abstract": "arXiv:2601.01322v1 Announce Type: new  Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
        "arxiv_id": "2601.01322",
        "ARXIVID": "2601.01322",
        "COMMENT": "Introduces LinMU, a new VLM architecture with linear complexity, replacing self-attention with a dual-branch state-space and local attention module. This is a new VLLM (visual large language model) and matches criterion 2.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.01416": {
        "authors": [
            "Yue Zhou",
            "Ran Ding",
            "Xue Yang",
            "Xue Jiang",
            "Xingzhao Liu"
        ],
        "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval",
        "abstract": "arXiv:2601.01416v1 Announce Type: new  Abstract: Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot",
        "arxiv_id": "2601.01416",
        "ARXIVID": "2601.01416",
        "COMMENT": "AirSpatialBot introduces a spatially-aware aerial agent for fine-grained vehicle attribute recognition and retrieval, with a new dataset (AirSpatial) and tasks (Spatial Grounding, Spatial QA). It directly addresses criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (new benchmark for embodied AI with a novel angle: remote sensing and spatial grounding).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.01050": {
        "authors": [
            "Hongming Fu",
            "Wenjia Wang",
            "Xiaozhen Qiao",
            "Shuo Yang",
            "Zheng Liu",
            "Bo Zhao"
        ],
        "title": "EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos",
        "abstract": "arXiv:2601.01050v1 Announce Type: new  Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.",
        "arxiv_id": "2601.01050",
        "ARXIVID": "2601.01050",
        "COMMENT": "Matches criteria 1: proposes a new method for reconstructing world-space hand-object interactions from egocentric videos, leveraging spatial intelligence models and diffusion-based priors for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.01095": {
        "authors": [
            "Hyeonjeong Ha",
            "Jinjin Ge",
            "Bo Feng",
            "Kaixin Ma",
            "Gargi Chakraborty"
        ],
        "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame",
        "abstract": "arXiv:2601.01095v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.",
        "arxiv_id": "2601.01095",
        "ARXIVID": "2601.01095",
        "COMMENT": "Strongly matches criterion 3 (new benchmark for evaluating narrative and temporal reasoning in MLLMs, with a novel compositional reasoning progression). Also relevant to criterion 2 (systematic evaluation of MLLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.02029": {
        "authors": [
            "Toshihiko Nishimura",
            "Hirofumi Abe",
            "Kazuhiko Murasaki",
            "Taiga Yoshida",
            "Ryuichi Tanida"
        ],
        "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding",
        "abstract": "arXiv:2601.02029v1 Announce Type: new  Abstract: This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.",
        "arxiv_id": "2601.02029",
        "ARXIVID": "2601.02029",
        "COMMENT": "This paper presents a novel 3D semantic segmentation method for large-scale point clouds using 2D vision-language models (VLMs) and natural language prompts, achieving label-free 3D segmentation and open-vocabulary recognition. This is a strong match for criterion 4 (vision foundation models and applications), and also touches on spatial understanding (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.01804": {
        "authors": [
            "Zhengjian Kang",
            "Qi Chen",
            "Rui Liu",
            "Kangtong Mo",
            "Xingyu Zhang",
            "Xiaoyu Deng",
            "Ye Zhang"
        ],
        "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
        "abstract": "arXiv:2601.01804v1 Announce Type: new  Abstract: Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
        "arxiv_id": "2601.01804",
        "ARXIVID": "2601.01804",
        "COMMENT": "Matches criterion 2 (new Video-LLM architecture with explicit temporal and causal reasoning improvements). Also relevant to criterion 1 (spatial-temporal intelligence in video understanding).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.02211": {
        "authors": [
            "Binglei Li",
            "Mengping Yang",
            "Zhiyu Tan",
            "Junping Zhang",
            "Hao Li"
        ],
        "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion",
        "abstract": "arXiv:2601.02211v1 Announce Type: new  Abstract: Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.",
        "arxiv_id": "2601.02211",
        "ARXIVID": "2601.02211",
        "COMMENT": "Directly matches criterion 2 (new VLLMs/MLLMs) by analyzing and enhancing Multimodal Diffusion Transformers (MMDiT) for text-to-image generation and editing. Also relevant to criterion 4 (vision foundation models and applications). Presents new training-free strategies and insightful empirical findings.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.02249": {
        "authors": [
            "Xiantai Xiang",
            "Guangyao Zhou",
            "Zixiao Wen",
            "Wenshuai Li",
            "Ben Niu",
            "Feng Wang",
            "Lijia Huang",
            "Qiantong Wang",
            "Yuhan Liu",
            "Zongxu Pan",
            "Yuxin Hu"
        ],
        "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection",
        "abstract": "arXiv:2601.02249v1 Announce Type: new  Abstract: Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.",
        "arxiv_id": "2601.02249",
        "ARXIVID": "2601.02249",
        "COMMENT": "SLGNet is a parameter-efficient multimodal object detection framework leveraging vision foundation models (ViT) and language-guided modulation. It directly matches criterion 4 (vision foundation models and applications) and also touches on criterion 2 (VLM-driven captions).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.01939": {
        "authors": [
            "Victor Sanchez",
            "Chris Reinke",
            "Ahamed Mohamed",
            "Xavier Alameda-Pineda"
        ],
        "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation",
        "abstract": "arXiv:2601.01939v1 Announce Type: new  Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.",
        "arxiv_id": "2601.01939",
        "ARXIVID": "2601.01939",
        "COMMENT": "This paper introduces OpenSocInt, a new open-source simulator for multi-modal social navigation, which is a new benchmark and environment for embodied AI. This directly matches criterion 3.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.01363": {
        "authors": [
            "Xiaomeng Yang",
            "Zhiyu Tan",
            "Xiaohui Zhong",
            "Mengping Yang",
            "Qiusheng Huang",
            "Lei Chen",
            "Libo Wu",
            "Hao Li"
        ],
        "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research",
        "abstract": "arXiv:2601.01363v1 Announce Type: new  Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25{\\deg} resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.",
        "arxiv_id": "2601.01363",
        "ARXIVID": "2601.01363",
        "COMMENT": "FuXi-Uni is a unified multimodal model for scientific understanding and generation across domains, supporting both language and scientific data. It is a new MLLM (criterion 2) and demonstrates strong cross-domain performance, including vision-language and scientific modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2601.01085": {
        "authors": [
            "Jiayi Xu",
            "Zhang Zhang",
            "Yuanrui Zhang",
            "Ruitao Chen",
            "Yixian Xu",
            "Tianyu He",
            "Di He"
        ],
        "title": "Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models",
        "abstract": "arXiv:2601.01085v1 Announce Type: new  Abstract: In this paper, we introduce \\emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \\emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.",
        "arxiv_id": "2601.01085",
        "ARXIVID": "2601.01085",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications). Introduces a training-free, probabilistically-certified watermarking method for general vision generative models, which is a clever statistical trick and broadly applicable.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.01192": {
        "authors": [
            "Hao Lu",
            "Xuhui Zhu",
            "Wenjing Zhang",
            "Yanan Li",
            "Xiang Bai"
        ],
        "title": "Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors",
        "abstract": "arXiv:2601.01192v1 Announce Type: new  Abstract: Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.",
        "arxiv_id": "2601.01192",
        "ARXIVID": "2601.01192",
        "COMMENT": "Matches criterion 1 (spatial understanding in video individual counting) and criterion 3 (new benchmark for crowded video individual counting, WuhanMetroCrowd, and new methods using social grouping and spatial-temporal displacement priors). The OMAN++ method and dataset are novel for crowded scene analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.00988": {
        "authors": [
            "Lin Xi",
            "Yingliang Ma",
            "Xiahai Zhuang"
        ],
        "title": "Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss",
        "abstract": "arXiv:2601.00988v1 Announce Type: new  Abstract: We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.",
        "arxiv_id": "2601.00988",
        "ARXIVID": "2601.00988",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (new benchmark for video segmentation in medical imaging, with a novel local matching and spatio-temporal consistency loss). The introduction of a new dataset and a non-parametric, direction-based local sampling mechanism for spatial understanding is notable.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.02018": {
        "authors": [
            "Guangqian Guo",
            "Aixi Ren",
            "Yong Guo",
            "Xuehui Yu",
            "Jiacheng Tian",
            "Wenli Li",
            "Yaoxing Wang",
            "Shan Gao"
        ],
        "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement",
        "abstract": "arXiv:2601.02018v1 Announce Type: new  Abstract: Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.",
        "arxiv_id": "2601.02018",
        "ARXIVID": "2601.02018",
        "COMMENT": "Matches criterion 4 (improves vision foundation models, specifically SAM, for robust segmentation across image qualities using generative modeling).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.01204": {
        "authors": [
            "Zunhai Su",
            "Weihao Ye",
            "Hansen Feng",
            "Keyu Fan",
            "Jing Zhang",
            "Dahai Yu",
            "Zhengwu Liu",
            "Ngai Wong"
        ],
        "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression",
        "abstract": "arXiv:2601.01204v1 Announce Type: new  Abstract: Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.",
        "arxiv_id": "2601.01204",
        "ARXIVID": "2601.01204",
        "COMMENT": "Matches criterion 3 (new method for memory-efficient streaming 3D vision geometry, relevant for embodied AI and simulators). Also relevant to criterion 4 (transformer-based vision foundation model for 3D applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.01769": {
        "authors": [
            "Hao Lu",
            "Ziniu Qian",
            "Yifu Li",
            "Yang Zhou",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology",
        "abstract": "arXiv:2601.01769v1 Announce Type: new  Abstract: In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.",
        "arxiv_id": "2601.01769",
        "ARXIVID": "2601.01769",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and method) by introducing a new VQA benchmark (CTIS-Bench) and a novel model (CTIS-QA) for clinically grounded slide-level question answering in pathology. Also relevant to spatial understanding in vision-language models (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.01386": {
        "authors": [
            "Xiaobao Wei",
            "Zhangjie Ye",
            "Yuxiang Gu",
            "Zunjie Zhu",
            "Yunfei Guo",
            "Yingying Shen",
            "Shan Zhao",
            "Ming Lu",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Rongfeng Lu",
            "Hangjun Ye"
        ],
        "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking",
        "abstract": "arXiv:2601.01386v1 Announce Type: new  Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian",
        "arxiv_id": "2601.01386",
        "ARXIVID": "2601.01386",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and method) by introducing ParkRecon3D, a new benchmark for 3D parking scene reconstruction, and ParkGaussian, a novel 3D Gaussian Splatting method tailored for autonomous parking. Also relevant to spatial intelligence (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.02046": {
        "authors": [
            "Shaocheng Shen",
            "Jianfeng Liang. Chunlei Cai",
            "Cong Geng",
            "Huiyu Duan",
            "Xiaoyun Zhang",
            "Qiang Hu",
            "Guangtao Zhai"
        ],
        "title": "Agentic Retoucher for Text-To-Image Generation",
        "abstract": "arXiv:2601.02046v1 Announce Type: new  Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
        "arxiv_id": "2601.02046",
        "ARXIVID": "2601.02046",
        "COMMENT": "Agentic Retoucher is a hierarchical, decision-driven framework for post-generation correction in text-to-image diffusion models, integrating perception, reasoning, and action agents. It introduces a new dataset (GenBlemish-27K) and a self-corrective process. This is relevant to vision foundation models and their applications (criterion 4), especially in improving generative model outputs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.01181": {
        "authors": [
            "Chenglizhao Chen",
            "Shaojiang Yuan",
            "Xiaoxue Lu",
            "Mengke Song",
            "Jia Song",
            "Zhenyu Wu",
            "Wenfeng Song",
            "Shuai Li"
        ],
        "title": "GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation",
        "abstract": "arXiv:2601.01181v1 Announce Type: new  Abstract: Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.",
        "arxiv_id": "2601.01181",
        "ARXIVID": "2601.01181",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications) and partially criterion 3 (new synthetic dataset and generative framework for camouflage object detection/segmentation). The use of generative models for dense annotation and scene-graph contextual decoupling is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.01926": {
        "authors": [
            "Zhifei Li",
            "Yiran Wang",
            "Chenyi Xiong",
            "Yujing Xia",
            "Xiaoju Hou",
            "Yue Zhao",
            "Miao Zhang",
            "Kui Xiao",
            "Bing Yang"
        ],
        "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering",
        "abstract": "arXiv:2601.01926v1 Announce Type: new  Abstract: Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.",
        "arxiv_id": "2601.01926",
        "ARXIVID": "2601.01926",
        "COMMENT": "Matches criterion 2 (multi-modal large language models, as it is about continual visual question answering) and criterion 4 (application of vision foundation models in continual learning for VQA). The adaptive memory allocation and global noise filtering are novel for continual VQA.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.01891": {
        "authors": [
            "Niloufar Alipour Talemi",
            "Julia Boone",
            "Fatemeh Afghah"
        ],
        "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
        "abstract": "arXiv:2601.01891v1 Announce Type: new  Abstract: The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
        "arxiv_id": "2601.01891",
        "ARXIVID": "2601.01891",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and criterion 3 (embodied AI, new benchmarks) by surveying agentic AI in remote sensing, including new benchmarks and agentic architectures. Focuses on novel evaluation and orchestration in geospatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.01439": {
        "authors": [
            "Wenqi Ren",
            "Weijie Wang",
            "Meng Zheng",
            "Ziyan Wu",
            "Yang Tang",
            "Zhun Zhong",
            "Nicu Sebe"
        ],
        "title": "In defense of the two-stage framework for open-set domain adaptive semantic segmentation",
        "abstract": "arXiv:2601.01439v1 Announce Type: new  Abstract: Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.",
        "arxiv_id": "2601.01439",
        "ARXIVID": "2601.01439",
        "COMMENT": "Proposes a new two-stage framework for open-set domain adaptive semantic segmentation, with a novel training strategy and data augmentation. This is a methodological improvement in spatial understanding (semantic segmentation), matching criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.01167": {
        "authors": [
            "Tianheng Cheng",
            "Xinggang Wang",
            "Junchao Liao",
            "Wenyu Liu"
        ],
        "title": "Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation",
        "abstract": "arXiv:2601.01167v1 Announce Type: new  Abstract: Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.",
        "arxiv_id": "2601.01167",
        "ARXIVID": "2601.01167",
        "COMMENT": "This paper proposes a new method (Guided Attentive Interpolation) for efficient semantic segmentation, which is a core spatial understanding task in computer vision. The method focuses on spatial feature upsampling and can be integrated into embodied agents for better spatial intelligence. Closely matches criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.00905": {
        "authors": [
            "Eliot Park",
            "Abhi Kumar",
            "Pranav Rajpurkar"
        ],
        "title": "Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems",
        "abstract": "arXiv:2601.00905v1 Announce Type: new  Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.",
        "arxiv_id": "2601.00905",
        "ARXIVID": "2601.00905",
        "COMMENT": "Matches criterion 2 (evaluation of new VLLMs/MLLMs for contextual spatial reasoning in recyclability). Also relevant to criterion 1 (spatial intelligence in VLMs for embodied tasks).",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2601.01875": {
        "authors": [
            "Kewen Cao",
            "Jianxu Chen",
            "Yongbing Zhang",
            "Ye Zhang",
            "Hongxiao Wang"
        ],
        "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
        "abstract": "arXiv:2601.01875v1 Announce Type: new  Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
        "arxiv_id": "2601.01875",
        "ARXIVID": "2601.01875",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding in embodied agents) via auditable neuro-symbolic reasoning and explicit evidence tracing in pathology VQA; also relevant to criterion 4 (vision foundation models and applications) due to use of vision-language models for interpretability.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.00939": {
        "authors": [
            "Feng Luo",
            "Hongbo Pan",
            "Xiang Yang",
            "Baoyu Jiang",
            "Fengqing Liu",
            "Tao Huang"
        ],
        "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery",
        "abstract": "arXiv:2601.00939v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.",
        "arxiv_id": "2601.00939",
        "ARXIVID": "2601.00939",
        "COMMENT": "Presents ShadowGS, a novel 3D Gaussian Splatting method for satellite imagery, focusing on shadow-aware 3D reconstruction. While it is a new method in 3D vision, it does not directly address embodied agents, VLLMs, or benchmarks, but is relevant to vision foundation models and spatial understanding. Somewhat matches criterion 4.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.01856": {
        "authors": [
            "Joongwon Chae",
            "Lihui Luo",
            "Yang Liu",
            "Runming Wang",
            "Dongmei Yu",
            "Zeming Liang",
            "Xi Yuan",
            "Dayan Zhang",
            "Zhenglin Chen",
            "Peiwu Qin",
            "Ilmoon Chae"
        ],
        "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection",
        "abstract": "arXiv:2601.01856v1 Announce Type: new  Abstract: Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.   We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.   Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR",
        "arxiv_id": "2601.01856",
        "ARXIVID": "2601.01856",
        "COMMENT": "Somewhat relevant to criterion 4 (uses large vision encoders for anomaly detection, but focus is on continual learning and routing, not foundation models per se).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.01749": {
        "authors": [
            "Lei Zhu",
            "Lijian Lin",
            "Ye Zhu",
            "Jiahao Wu",
            "Xuehan Hou",
            "Yu Li",
            "Yunfei Liu",
            "Jie Chen"
        ],
        "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement",
        "abstract": "arXiv:2601.01749v1 Announce Type: new  Abstract: Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.",
        "arxiv_id": "2601.01749",
        "ARXIVID": "2601.01749",
        "COMMENT": "MANGO introduces a two-stage framework for natural multi-speaker 3D talking head generation, using diffusion-based transformers and a new dataset (MANGO-Dialog). It is relevant to multi-modal generative modeling and introduces a new dataset, but does not directly address embodied AI benchmarks or spatial intelligence in agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.01425": {
        "authors": [
            "Xu Guo",
            "Fulong Ye",
            "Xinghui Li",
            "Pengqi Tu",
            "Pengze Zhang",
            "Qichao Sun",
            "Songtao Zhao",
            "Xiangwang Hou",
            "Qian He"
        ],
        "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
        "abstract": "arXiv:2601.01425v1 Announce Type: new  Abstract: Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
        "arxiv_id": "2601.01425",
        "ARXIVID": "2601.01425",
        "COMMENT": "This paper presents DreamID-V, a diffusion transformer-based framework for high-fidelity video face swapping, with a new benchmark (IDBench-V). It is relevant to generative modeling and introduces a new benchmark, but is not focused on embodied AI or spatial intelligence. It is more about video synthesis and identity preservation.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.02098": {
        "authors": [
            "Jinlong Fan",
            "Shanshan Zhao",
            "Liang Zheng",
            "Jing Zhang",
            "Yuxiang Yang",
            "Mingming Gong"
        ],
        "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
        "abstract": "arXiv:2601.02098v1 Announce Type: new  Abstract: Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
        "arxiv_id": "2601.02098",
        "ARXIVID": "2601.02098",
        "COMMENT": "This paper introduces a novel method for reconstructing occluded humans in 3D from monocular videos using multi-scale UV mapping and identity-preserving diffusion inpainting. It is relevant to generative modeling in multi-modal learning and 3D vision, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.02256": {
        "authors": [
            "Shikun Sun",
            "Liao Qu",
            "Huichao Zhang",
            "Yiheng Liu",
            "Yangyang Song",
            "Xian Li",
            "Xu Wang",
            "Yi Jiang",
            "Daniel K. Du",
            "Xinglong Wu",
            "Jia Jia"
        ],
        "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
        "abstract": "arXiv:2601.02256v1 Announce Type: new  Abstract: Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
        "arxiv_id": "2601.02256",
        "ARXIVID": "2601.02256",
        "COMMENT": "Related to visual autoregressive generation and reinforcement learning, but does not directly match any of the four criteria (not about spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.02359": {
        "authors": [
            "Kaede Shiohara",
            "Toshihiko Yamasaki",
            "Vladislav Golyanik"
        ],
        "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
        "abstract": "arXiv:2601.02359v1 Announce Type: new  Abstract: Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
        "arxiv_id": "2601.02359",
        "ARXIVID": "2601.02359",
        "COMMENT": "Focuses on face forgery detection using diffusion models, which is generative modeling in vision, but does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.02103": {
        "authors": [
            "Yating Wang",
            "Yuan Sun",
            "Xuan Wang",
            "Ran Yi",
            "Boyao Zhou",
            "Yipengjing Sun",
            "Hongyu Liu",
            "Yinuo Wang",
            "Lizhuang Ma"
        ],
        "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
        "abstract": "arXiv:2601.02103v1 Announce Type: new  Abstract: Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
        "arxiv_id": "2601.02103",
        "ARXIVID": "2601.02103",
        "COMMENT": "Related to generative modeling and 3D vision, but does not directly match any of the four criteria. It is not about spatial intelligence in embodied agents, VLLMs/MLLMs, new embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.01064": {
        "authors": [
            "Jianan Li",
            "Wangcai Zhao",
            "Tingfa Xu"
        ],
        "title": "Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers",
        "abstract": "arXiv:2601.01064v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.",
        "arxiv_id": "2601.01064",
        "ARXIVID": "2601.01064",
        "COMMENT": "Somewhat relevant to criterion 4 (vision transformer for hyperspectral image reconstruction, but not a foundation model or multi-modal).",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.01321": {
        "authors": [
            "Rong Zhou",
            "Dongping Chen",
            "Zihan Jia",
            "Yao Su",
            "Yixin Liu",
            "Yiwen Lu",
            "Dongwei Shi",
            "Yue Huang",
            "Tianyang Xu",
            "Yi Pan",
            "Xinliang Li",
            "Yohannes Abate",
            "Qingyu Chen",
            "Zhengzhong Tu",
            "Yu Yang",
            "Yu Zhang",
            "Qingsong Wen",
            "Gengchen Mai",
            "Sunyang Fu",
            "Jiachen Li",
            "Xuyu Wang",
            "Ziran Wang",
            "Jing Huang",
            "Tianming Liu",
            "Yong Chen",
            "Lichao Sun",
            "Lifang He"
        ],
        "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
        "abstract": "arXiv:2601.01321v1 Announce Type: new  Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
        "arxiv_id": "2601.01321",
        "ARXIVID": "2601.01321",
        "COMMENT": "Broad survey on digital twins and AI, including large language models and world models. Matches criterion 4 (vision foundation models and applications) and is tangentially relevant to embodied AI, but is a high-level review rather than a new method or benchmark.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2601.01743": {
        "authors": [
            "Bin Xu"
        ],
        "title": "AI Agent Systems: Architectures, Applications, and Evaluation",
        "abstract": "arXiv:2601.01743v1 Announce Type: new  Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.",
        "arxiv_id": "2601.01743",
        "ARXIVID": "2601.01743",
        "COMMENT": "Survey of AI agent systems, including architectures and evaluation, with some discussion of vision foundation models and agentic reasoning. Matches criterion 4 (vision foundation models and applications) and is tangentially relevant to criterion 1 (spatial intelligence in agents) but is a survey, not a new method.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2601.02008": {
        "authors": [
            "Midhat Urooj",
            "Ayan Banerjee",
            "Sandeep Gupta"
        ],
        "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging",
        "abstract": "arXiv:2601.02008v1 Announce Type: new  Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.",
        "arxiv_id": "2601.02008",
        "ARXIVID": "2601.02008",
        "COMMENT": "XAI-MeD is a neuro-symbolic framework for explainable, robust, and generalizable medical imaging. While it is multimodal and uses symbolic reasoning, it is focused on medical imaging and does not directly address the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.01352": {
        "authors": [
            "Yixuan Lai",
            "He Wang",
            "Kun Zhou",
            "Tianjia Shao"
        ],
        "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding",
        "abstract": "arXiv:2601.01352v1 Announce Type: new  Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.",
        "arxiv_id": "2601.01352",
        "ARXIVID": "2601.01352",
        "COMMENT": "Proposes Slot-ID, a new video generation method that preserves identity using slot-based temporal encoding. While it is a generative model for video, it does not directly address spatial intelligence, VLLMs, embodied AI, or vision foundation models. Relevant to generative modeling but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.01950": {
        "authors": [
            "Meng Wang",
            "Wenjing Dai",
            "Jiawan Zhang",
            "Xiaojie Guo"
        ],
        "title": "Face Normal Estimation from Rags to Riches",
        "abstract": "arXiv:2601.01950v1 Announce Type: new  Abstract: Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
        "arxiv_id": "2601.01950",
        "ARXIVID": "2601.01950",
        "COMMENT": "Relevant to computer vision (face normal estimation), but does not directly match any of the four criteria (not spatial intelligence, VLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.01041": {
        "authors": [
            "Xiang Zhang",
            "Wenliang Weng",
            "Daoyong Fu",
            "Ziqiang Li",
            "Zhangjie Fu"
        ],
        "title": "Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking",
        "abstract": "arXiv:2601.01041v1 Announce Type: new  Abstract: Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.",
        "arxiv_id": "2601.01041",
        "ARXIVID": "2601.01041",
        "COMMENT": "Relevant to computer vision (deepfake detection), but does not match any of the four criteria (not spatial intelligence, VLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.01240": {
        "authors": [
            "Ziqian Guan",
            "Xieyi Fu",
            "Yuting Wang",
            "Haowen Xiao",
            "Jiarui Zhu",
            "Yingying Zhu",
            "Yongtao Liu",
            "Lin Gu"
        ],
        "title": "RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection",
        "abstract": "arXiv:2601.01240v1 Announce Type: new  Abstract: Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.",
        "arxiv_id": "2601.01240",
        "ARXIVID": "2601.01240",
        "COMMENT": "Relevant to computer vision and object detection, but does not directly match any of the four criteria (not spatial intelligence, VLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.02203": {
        "authors": [
            "Oliver Custance",
            "Saad Khan",
            "Simon Parkinson",
            "Quan Z. Sheng"
        ],
        "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules",
        "abstract": "arXiv:2601.02203v1 Announce Type: new  Abstract: Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.",
        "arxiv_id": "2601.02203",
        "ARXIVID": "2601.02203",
        "COMMENT": "Presents parameter-efficient domain adaptation for WiFi-based crowd counting, which is not directly related to any of the four criteria. Some relevance to general machine learning and sensing, but not to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.02126": {
        "authors": [
            "Xavier Bou",
            "Elliot Vincent",
            "Gabriele Facciolo",
            "Rafael Grompone von Gioi",
            "Jean-Michel Morel",
            "Thibaud Ehret"
        ],
        "title": "Remote Sensing Change Detection via Weak Temporal Supervision",
        "abstract": "arXiv:2601.02126v1 Announce Type: new  Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.",
        "arxiv_id": "2601.02126",
        "ARXIVID": "2601.02126",
        "COMMENT": "Addresses remote sensing change detection with weak temporal supervision, but does not directly match any of the four criteria. Some relevance to computer vision and learning, but not to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.01364": {
        "authors": [
            "Mostofa Rafid Uddin",
            "Mahek Vora",
            "Qifeng Wu",
            "Muyuan Chen",
            "Min Xu"
        ],
        "title": "Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography",
        "abstract": "arXiv:2601.01364v1 Announce Type: new  Abstract: Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.",
        "arxiv_id": "2601.01364",
        "ARXIVID": "2601.01364",
        "COMMENT": "This paper proposes an unsupervised deep learning framework for SE(3) disentanglement in cryo-ET data, enabling improved morphology identification. While it is a novel statistical method for 3D data, it is not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.01696": {
        "authors": [
            "Yian Liu",
            "Xiong Wang",
            "Ping Xu",
            "Lei Zhu",
            "Ming Yan",
            "Linyun Xue"
        ],
        "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems",
        "abstract": "arXiv:2601.01696v1 Announce Type: new  Abstract: Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.",
        "arxiv_id": "2601.01696",
        "ARXIVID": "2601.01696",
        "COMMENT": "This paper proposes a new Covariance Distribution Optimization (CDO) module for real-time lane detection on embedded systems. While it is a computer vision method, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. It is an application-specific improvement for lane detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}