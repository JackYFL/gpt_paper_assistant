{
    "2602.09878": {
        "authors": [
            "Jiaxu Wang",
            "Yicheng Jiang",
            "Tianlun He",
            "Jingkai Sun",
            "Qiang Zhang",
            "Junhao He",
            "Jiahang Cao",
            "Zesen Gan",
            "Mingyuan Sun",
            "Qiming Shao",
            "Xiangyu Yue"
        ],
        "title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation",
        "abstract": "arXiv:2602.09878v1 Announce Type: new  Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.",
        "arxiv_id": "2602.09878",
        "ARXIVID": "2602.09878",
        "COMMENT": "Matches criterion 1 and 3: proposes MVISTA-4D, a novel embodied 4D world model for robotic manipulation, enabling view-consistent, arbitrary-view RGBD generation and test-time action inference. Strong focus on spatial understanding and new methods for embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.10116": {
        "authors": [
            "Hongchi Xia",
            "Xuan Li",
            "Zhaoshuo Li",
            "Qianli Ma",
            "Jiashu Xu",
            "Ming-Yu Liu",
            "Yin Cui",
            "Tsung-Yi Lin",
            "Wei-Chiu Ma",
            "Shenlong Wang",
            "Shuran Song",
            "Fangyin Wei"
        ],
        "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
        "abstract": "arXiv:2602.10116v1 Announce Type: new  Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
        "arxiv_id": "2602.10116",
        "ARXIVID": "2602.10116",
        "COMMENT": "Matches criterion 3: introduces SAGE, a new agentic framework for scalable, simulation-ready 3D scene generation for embodied AI, with a novel self-refinement loop and a new dataset (SAGE-10k). Also relevant to criterion 1 (spatial understanding for embodied agents) and criterion 4 (vision foundation models for simulation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.09638": {
        "authors": [
            "Hanqing Wang",
            "Mingyu Liu",
            "Xiaoyu Chen",
            "Chengwei MA",
            "Yiming Zhong",
            "Wenti Yin",
            "Yuhao Liu",
            "Zhiqing Cui",
            "Jiahao Yuan",
            "Lu Dai",
            "Zhiyuan Ma",
            "Hui Xiong"
        ],
        "title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model",
        "abstract": "arXiv:2602.09638v1 Announce Type: new  Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \\textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \\textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \\textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.",
        "arxiv_id": "2602.09638",
        "ARXIVID": "2602.09638",
        "COMMENT": "Matches criteria 1 (spatial understanding in embodied agents), 2 (new MLLM for affordance), and 3 (new benchmark/dataset for embodied AI). Introduces a video-based 3D affordance dataset and a multimodal LLM baseline for 3D affordance grounding, with spatial-aware loss and dynamic action encoding. Strong fit for spatial intelligence and embodied AI with novel dataset and method.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.09609": {
        "authors": [
            "Jialun Liu",
            "Yukuo Ma",
            "Xiao Cao",
            "Tian Li",
            "Gonghu Shang",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li",
            "Cong Liu",
            "Junqi Liu",
            "Jiakui Hu",
            "Robby T. Tan",
            "Shiwen Zhang",
            "Liying Yang",
            "Xiaoyan Yang",
            "Qizhen Weng",
            "Xiangzhen Chang",
            "Yuanzhi Liang",
            "Yifan Xu",
            "Zhiyong Huang",
            "Zuoxin Li",
            "Xuelong Li"
        ],
        "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing",
        "abstract": "arXiv:2602.09609v1 Announce Type: new  Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.",
        "arxiv_id": "2602.09609",
        "ARXIVID": "2602.09609",
        "COMMENT": "Matches criterion 2 and 4: introduces Tele-Omni, a unified multimodal framework for video generation and editing using pretrained multimodal LLMs and diffusion models, supporting a wide range of video-centric tasks and composable multimodal control.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.09825": {
        "authors": [
            "Zhaoxu Li",
            "Chenqi Kong",
            "Peijun Bao",
            "Song Xia",
            "Yi Tu",
            "Yi Yu",
            "Xinghao Jiang",
            "Xudong Jiang"
        ],
        "title": "SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding",
        "abstract": "arXiv:2602.09825v1 Announce Type: new  Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.",
        "arxiv_id": "2602.09825",
        "ARXIVID": "2602.09825",
        "COMMENT": "Directly matches criteria 2 (VLLMs/MLLMs). Proposes a new decoding method (SAKED) to mitigate hallucination in large vision-language models, with empirical analysis and a novel, training-free approach. Strong fit for vision-language model methodology.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2602.09523": {
        "authors": [
            "Zhen Qiu",
            "Kaiwen Xiao",
            "Zhengwei Lu",
            "Xiangyu Liu",
            "Lei Zhao",
            "Hao Zhang"
        ],
        "title": "Singpath-VL Technical Report",
        "abstract": "arXiv:2602.09523v1 Announce Type: new  Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.",
        "arxiv_id": "2602.09523",
        "ARXIVID": "2602.09523",
        "COMMENT": "Matches criterion 2. Introduces Singpath-VL, a new vision-language large model (VLLM) for cytopathology, with a novel synthetic dataset and fine-tuning pipeline.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2602.08990": {
        "authors": [
            "Shiyang Feng",
            "Runmin Ma",
            "Xiangchao Yan",
            "Yue Fan",
            "Yusong Hu",
            "Songtao Huang",
            "Shuaiyu Zhang",
            "Zongsheng Cao",
            "Tianshuo Peng",
            "Jiakang Yuan",
            "Zijie Guo",
            "Zhijie Zhong",
            "Shangheng Du",
            "Weida Wang",
            "Jinxin Shi",
            "Yuhao Zhou",
            "Xiaohan He",
            "Zhiyin Yu",
            "Fangchen Yu",
            "Qihao Zheng",
            "Jiamin Wu",
            "Mianxin Liu",
            "Chi Zhang",
            "Shaowei Hou",
            "Shuya Li",
            "Yankai Jiang",
            "Wenjie Lou",
            "Lilong Wang",
            "Zifu Wang",
            "Jiong Wang",
            "Wanghan Xu",
            "Yue Deng",
            "Dongrui Liu",
            "Yiheng Wang",
            "Wenlong Zhang",
            "Fenghua Ling",
            "Shufei Zhang",
            "Xiaosong Wang",
            "Shuangjia Zheng",
            "Xun Huang",
            "Siqi Sun",
            "Shuyue Hu",
            "Peng Ye",
            "Chunfeng Song",
            "Bin Wang",
            "Conghui He",
            "Yihao Liu",
            "Xin Li",
            "Qibin Hou",
            "Tao Chen",
            "Xiangyu Yue",
            "Bin Wang",
            "Liang He",
            "Dahua Lin",
            "Bowen Zhou",
            "Bo Zhang",
            "Lei Bai"
        ],
        "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
        "abstract": "arXiv:2602.08990v1 Announce Type: new  Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
        "arxiv_id": "2602.08990",
        "ARXIVID": "2602.08990",
        "COMMENT": "Matches criterion 2: Introduces a unified agentic framework (InternAgent-1.5) for autonomous scientific discovery, with strong multi-modal and agentic capabilities. Also relevant to criterion 3 as it presents a new system for embodied AI with long-horizon tasks and benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.09483": {
        "authors": [
            "Lin Chen",
            "Xiaoke Zhao",
            "Kun Ding",
            "Weiwei Feng",
            "Changtao Miao",
            "Zili Wang",
            "Wenxuan Guo",
            "Ying Wang",
            "Kaiyuan Zheng",
            "Bo Zhang",
            "Zhe Li",
            "Shiming Xiang"
        ],
        "title": "Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions",
        "abstract": "arXiv:2602.09483v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.",
        "arxiv_id": "2602.09483",
        "ARXIVID": "2602.09483",
        "COMMENT": "Matches criterion 2: Shows a new distillation framework for MLLMs (multi-modal large language models), with a novel focus on token interactions for knowledge distillation. Also provides empirical results showing a small model outperforming a much larger MLLM.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.09586": {
        "authors": [
            "Bo Peng",
            "Yuanwei Hu",
            "Bo Liu",
            "Ling Chen",
            "Jie Lu",
            "Zhen Fang"
        ],
        "title": "Delving into Spectral Clustering with Vision-Language Representations",
        "abstract": "arXiv:2602.09586v1 Announce Type: new  Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \\textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.",
        "arxiv_id": "2602.09586",
        "ARXIVID": "2602.09586",
        "COMMENT": "Matches criterion 4: proposes a novel spectral clustering method leveraging vision-language model representations, with a new neural tangent kernel and prompt-based affinity diffusion, directly applying vision foundation models to unsupervised learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.09662": {
        "authors": [
            "Deyang Jiang",
            "Jing Huang",
            "Xuanle Zhao",
            "Lei Chen",
            "Liming Zheng",
            "Fanfan Liu",
            "Haibo Qiu",
            "Peng Shi",
            "Zhixiong Zeng"
        ],
        "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution",
        "abstract": "arXiv:2602.09662v1 Announce Type: new  Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\\emph{i.e.}, trajectory difficulty) and breadth (\\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.",
        "arxiv_id": "2602.09662",
        "ARXIVID": "2602.09662",
        "COMMENT": "Matches criterion 3: introduces TreeCUA, a new multi-agent framework and method for GUI automation in embodied agents, with a novel tree-structured exploration and planning approach, and a new benchmark (TreeCUA-DPO).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.09252": {
        "authors": [
            "Ange Lou",
            "Yamin Li",
            "Qi Chang",
            "Nan Xi",
            "Luyuan Xie",
            "Zichao Li",
            "Tianyu Luan"
        ],
        "title": "VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models",
        "abstract": "arXiv:2602.09252v1 Announce Type: new  Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.",
        "arxiv_id": "2602.09252",
        "ARXIVID": "2602.09252",
        "COMMENT": "Matches criteria 1 and 4. Proposes a new method for spatial understanding in surgical image segmentation using VLMs and foundation models, with adaptive refinement and clinician-in-the-loop interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.09214": {
        "authors": [
            "Chenyu Wang",
            "Tianle Chen",
            "H. M. Sabbir Ahmad",
            "Kayhan Batmanghelich",
            "Wenchao Li"
        ],
        "title": "VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models",
        "abstract": "arXiv:2602.09214v1 Announce Type: new  Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.",
        "arxiv_id": "2602.09214",
        "ARXIVID": "2602.09214",
        "COMMENT": "Matches criterion 4: Proposes a new benchmark (VLM-UQBench) for uncertainty quantification in vision-language models, with novel metrics and empirical findings about modality-specific uncertainty and hallucinations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09413": {
        "authors": [
            "Xinyu Wang",
            "Ke Deng",
            "Fei Dou",
            "Jinbo Bi",
            "Jin Lu"
        ],
        "title": "LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging",
        "abstract": "arXiv:2602.09413v1 Announce Type: new  Abstract: Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.",
        "arxiv_id": "2602.09413",
        "ARXIVID": "2602.09413",
        "COMMENT": "Matches criterion 4: Proposes a novel, data-free, layer-wise adaptive rescaling method for model merging in vision transformers, directly improving vision foundation model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09082": {
        "authors": [
            "Veuns-Team",
            ":",
            "Changlong Gao",
            "Zhangxuan Gu",
            "Yulin Liu",
            "Xinyu Qiu",
            "Shuheng Shen",
            "Yue Wen",
            "Tianyu Xia",
            "Zhenyu Xu",
            "Zhengwen Zeng",
            "Beitong Zhou",
            "Xingran Zhou",
            "Weizhi Chen",
            "Sunhao Dai",
            "Jingya Dou",
            "Yichen Gong",
            "Yuan Guo",
            "Zhenlin Guo",
            "Feng Li",
            "Qian Li",
            "Jinzhen Lin",
            "Yuqi Zhou",
            "Linchao Zhu",
            "Liang Chen",
            "Zhenyu Guo",
            "Changhua Meng",
            "Weiqiang Wang"
        ],
        "title": "UI-Venus-1.5 Technical Report",
        "abstract": "arXiv:2602.09082v1 Announce Type: new  Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
        "arxiv_id": "2602.09082",
        "ARXIVID": "2602.09082",
        "COMMENT": "Matches criterion 2: UI-Venus-1.5 is a new large vision-language model for GUI agents, with technical advances in training and model merging, and strong empirical results on benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09713": {
        "authors": [
            "Ruisi Zhao",
            "Haoren Zheng",
            "Zongxin Yang",
            "Hehe Fan",
            "Yi Yang"
        ],
        "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
        "abstract": "arXiv:2602.09713v1 Announce Type: new  Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
        "arxiv_id": "2602.09713",
        "ARXIVID": "2602.09713",
        "COMMENT": "Relevant to criterion 4: introduces Stroke3D, a framework for generating rigged 3D models from 2D strokes and text using latent diffusion models, with a novel two-stage pipeline and new dataset. Strong generative modeling and vision foundation model application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09611": {
        "authors": [
            "Yue Li",
            "Xin Yi",
            "Dongsheng Shi",
            "Yongyi Cui",
            "Gerard de Melo",
            "Linlin Wang"
        ],
        "title": "AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models",
        "abstract": "arXiv:2602.09611v1 Announce Type: new  Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\\% AUC) and robust attack resilience (at least 88.61\\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.",
        "arxiv_id": "2602.09611",
        "ARXIVID": "2602.09611",
        "COMMENT": "Matches criterion 2: proposes AGMark, a new attention-guided dynamic watermarking method for Large Vision-Language Models (LVLMs), with empirical improvements in watermarking and visual semantic fidelity.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09439": {
        "authors": [
            "Xu Ma",
            "Yitian Zhang",
            "Qihua Dong",
            "Yun Fu"
        ],
        "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
        "abstract": "arXiv:2602.09439v1 Announce Type: new  Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
        "arxiv_id": "2602.09439",
        "ARXIVID": "2602.09439",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications). Introduces a large, high-quality, open dataset for text-to-image fine-tuning, with demonstrated improvements for vision foundation models. Relevant for generative modeling and vision-language applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09007": {
        "authors": [
            "Haodong Li",
            "Jingwei Wu",
            "Quan Sun",
            "Guopeng Li",
            "Juanxi Tian",
            "Huanyu Zhang",
            "Yanlin Lai",
            "Ruichuan An",
            "Hongbo Peng",
            "Yuhong Dai",
            "Chenxi Li",
            "Chunmei Qing",
            "Jia Wang",
            "Ziyang Meng",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
        "abstract": "arXiv:2602.09007v2 Announce Type: new  Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
        "arxiv_id": "2602.09007",
        "ARXIVID": "2602.09007",
        "COMMENT": "Matches criteria 3 (new benchmark for embodied AI/simulator-related). Introduces GEBench, a benchmark for evaluating image generation models as GUI environments, with a novel metric for dynamic interaction and temporal coherence. Focuses on generative GUI environments, which is a novel angle for embodied AI evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09411": {
        "authors": [
            "Zhikai Li",
            "Jiatong Li",
            "Xuewen Liu",
            "Wangbo Zhao",
            "Pan Du",
            "Kaicheng Zhou",
            "Qingyi Gu",
            "Yang You",
            "Zhen Dong",
            "Kurt Keutzer"
        ],
        "title": "K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge",
        "abstract": "arXiv:2602.09411v1 Announce Type: new  Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.",
        "arxiv_id": "2602.09411",
        "ARXIVID": "2602.09411",
        "COMMENT": "Matches criterion 4. Proposes a new VLM-based evaluation framework for visual generative models, with statistical tricks for improved alignment and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.07153": {
        "authors": [
            "Jinbiao Wei",
            "Yilun Zhao",
            "Kangqi Ni",
            "Arman Cohan"
        ],
        "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
        "abstract": "arXiv:2602.07153v1 Announce Type: new  Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
        "arxiv_id": "2602.07153",
        "ARXIVID": "2602.07153",
        "COMMENT": "Matches criterion 3. Proposes a new data generation framework (Anchor) for GUI agents, expanding high-quality interaction data for embodied AI benchmarks with novel trajectory expansion and verification.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.09518": {
        "authors": [
            "Hung-Shuo Chang",
            "Yue-Cheng Yang",
            "Yu-Hsi Chen",
            "Wei-Hsin Chen",
            "Chien-Yao Wang",
            "James C. Liao",
            "Chien-Chang Chen",
            "Hen-Hsen Huang",
            "Hong-Yuan Mark Liao"
        ],
        "title": "A Universal Action Space for General Behavior Analysis",
        "abstract": "arXiv:2602.09518v1 Announce Type: new  Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.",
        "arxiv_id": "2602.09518",
        "ARXIVID": "2602.09518",
        "COMMENT": "Matches criterion 4: Proposes a Universal Action Space for behavior analysis, leveraging large-scale vision foundation models and datasets for general behavior understanding. Relevant for vision foundation model applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.07830": {
        "authors": [
            "Jiahui Zhou",
            "Dan Li",
            "Boxin Li",
            "Xiao Zhang",
            "Erli Meng",
            "Lin Li",
            "Zhuomin Chen",
            "Jian Lou",
            "See-Kiong Ng"
        ],
        "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning",
        "abstract": "arXiv:2602.07830v1 Announce Type: new  Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.",
        "arxiv_id": "2602.07830",
        "ARXIVID": "2602.07830",
        "COMMENT": "Matches criterion 2: introduces a new multi-modal (TS-text) dataset and RL training for LLMs on time series reasoning, with process-verifiable data synthesis and scheduling, relevant to multi-modal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.09534": {
        "authors": [
            "Jiayi Lyu",
            "Leigang Qu",
            "Wenjing Zhang",
            "Hanyu Jiang",
            "Kai Liu",
            "Zhenglin Zhou",
            "Xiaobo Xia",
            "Jian Xue",
            "Tat-Seng Chua"
        ],
        "title": "AUHead: Realistic Emotional Talking Head Generation via Action Units Control",
        "abstract": "arXiv:2602.09534v1 Announce Type: new  Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an \"emotion-then-AU\" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR",
        "arxiv_id": "2602.09534",
        "ARXIVID": "2602.09534",
        "COMMENT": "Matches criterion 4: proposes a new method for realistic talking-head generation using action units and large audio-language models, with a novel cross-modal diffusion model and AU disentanglement, relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.08335": {
        "authors": [
            "Yanming Li",
            "Xuelin Zhang",
            "WenJie Lu",
            "Ziye Tang",
            "Maodong Wu",
            "Haotian Luo",
            "Tongtong Wu",
            "Zijie Peng",
            "Hongze Mi",
            "Yibo Feng",
            "Naiqiang Tan",
            "Chao Huang",
            "Hong Chen",
            "Li Shen"
        ],
        "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System",
        "abstract": "arXiv:2602.08335v1 Announce Type: new  Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.",
        "arxiv_id": "2602.08335",
        "ARXIVID": "2602.08335",
        "COMMENT": "Matches criterion 3: proposes a novel Shapley-based credit assignment method (SHARP) for multi-agent systems, addressing a key challenge in multi-agent RL with a new statistical approach.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.08412": {
        "authors": [
            "Yuhang Wang",
            "Feiming Xu",
            "Zheng Lin",
            "Guangyu He",
            "Yuzhe Huang",
            "Haichang Gao",
            "Zhenxing Niu"
        ],
        "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent",
        "abstract": "arXiv:2602.08412v1 Announce Type: new  Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.",
        "arxiv_id": "2602.08412",
        "ARXIVID": "2602.08412",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (PASB) for evaluating security in personalized AI agents, focusing on real-world, long-horizon, tool-using agents, which is a novel angle in embodied agent evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.10094": {
        "authors": [
            "Yihang Luo",
            "Shangchen Zhou",
            "Yushi Lan",
            "Xingang Pan",
            "Chen Change Loy"
        ],
        "title": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere",
        "abstract": "arXiv:2602.10094v1 Announce Type: new  Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.",
        "arxiv_id": "2602.10094",
        "ARXIVID": "2602.10094",
        "COMMENT": "Relevant to criterion 1: introduces a new 4D reconstruction framework (4RC) that jointly models geometry and motion from monocular videos, with a novel encode-once, query-anywhere paradigm. Strong spatial intelligence and representation learning angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.09587": {
        "authors": [
            "Yongfan Lai",
            "Wen Qian",
            "Bo Liu",
            "Hongyan Li",
            "Hao Luo",
            "Fan Wang",
            "Bohan Zhuang",
            "Shenda Hong"
        ],
        "title": "MieDB-100k: A Comprehensive Dataset for Medical Image Editing",
        "abstract": "arXiv:2602.09587v1 Announce Type: new  Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.",
        "arxiv_id": "2602.09587",
        "ARXIVID": "2602.09587",
        "COMMENT": "Matches criterion 4: Introduces a large, diverse dataset for medical image editing, enabling new applications for multimodal generative models and vision foundation models in the medical domain.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.08734": {
        "authors": [
            "David Hud\\'ak",
            "Maris F. L. Galesloot",
            "Martin Tappler",
            "Martin Kure\\v{c}ka",
            "Nils Jansen",
            "Milan \\v{C}e\\v{s}ka"
        ],
        "title": "Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning",
        "abstract": "arXiv:2602.08734v1 Announce Type: new  Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.",
        "arxiv_id": "2602.08734",
        "ARXIVID": "2602.08734",
        "COMMENT": "This paper proposes a new framework for POMDPs using deep RL and finite-state controllers, which is relevant to embodied AI (criterion 3) as it addresses robust policy learning in partially observable environments, a key challenge in embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.09648": {
        "authors": [
            "Siyu Chen",
            "Ting Han",
            "Haoling Huang",
            "Chaolei Wang",
            "Chengzheng Fu",
            "Duxin Zhu",
            "Guorong Cai",
            "Jinhe Su"
        ],
        "title": "Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation",
        "abstract": "arXiv:2602.09648v1 Announce Type: new  Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.",
        "arxiv_id": "2602.09648",
        "ARXIVID": "2602.09648",
        "COMMENT": "Addresses spatiotemporal invariant representations for video semantic segmentation, with a focus on domain generalization and temporal consistency. This is a methodological improvement in spatial and temporal understanding, matching criteria 1 (spatial intelligence in vision/embodied agents).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.09927": {
        "authors": [
            "Isabel Funke",
            "Sebastian Bodenstedt",
            "Felix von Bechtolsheim",
            "Florian Oehme",
            "Michael Maruschke",
            "Stefanie Herrlich",
            "J\\\"urgen Weitz",
            "Marius Distler",
            "S\\\"oren Torge Mees",
            "Stefanie Speidel"
        ],
        "title": "A benchmark for video-based laparoscopic skill analysis and assessment",
        "abstract": "arXiv:2602.09927v1 Announce Type: new  Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.",
        "arxiv_id": "2602.09927",
        "ARXIVID": "2602.09927",
        "COMMENT": "Matches criterion 3: introduces a new benchmark dataset (LASANA) for video-based skill assessment in laparoscopic surgery, enabling benchmarking of deep learning models for spatially complex embodied tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.09510": {
        "authors": [
            "Kun Wang",
            "Yun Zhu",
            "Pan Zhou",
            "Na Zhao"
        ],
        "title": "Robust Depth Super-Resolution via Adaptive Diffusion Sampling",
        "abstract": "arXiv:2602.09510v1 Announce Type: new  Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.",
        "arxiv_id": "2602.09510",
        "ARXIVID": "2602.09510",
        "COMMENT": "Applies diffusion models to depth super-resolution, which is related to vision foundation models (criterion 4), but is more of an application than a new foundation model or method for spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.08533": {
        "authors": [
            "Kun Peng",
            "Conghui Tan",
            "Yu Liu",
            "Guohua Tang",
            "Zhongqian Sun",
            "Wei Yang",
            "Zining Zhu",
            "Lei Jiang",
            "Yanbing Liu",
            "Hao Peng"
        ],
        "title": "Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO",
        "abstract": "arXiv:2602.08533v2 Announce Type: new  Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.",
        "arxiv_id": "2602.08533",
        "ARXIVID": "2602.08533",
        "COMMENT": "Somewhat matches criterion 3: proposes a new RL framework for dialogue agents with a tree-based optimization method, but not directly focused on spatial or embodied AI; more about dialogue optimization.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.08354": {
        "authors": [
            "Zixuan Huang",
            "Xin Xia",
            "Yuxi Ren",
            "Jianbin Zheng",
            "Xuanda Wang",
            "Zhixia Zhang",
            "Hongyan Xie",
            "Songshi Liang",
            "Zehao Chen",
            "Xuefeng Xiao",
            "Fuzhen Zhuang",
            "Jianxin Li",
            "Yikun Ban",
            "Deqing Wang"
        ],
        "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
        "abstract": "arXiv:2602.08354v1 Announce Type: new  Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
        "arxiv_id": "2602.08354",
        "ARXIVID": "2602.08354",
        "COMMENT": "This paper is about reasoning models and efficient sampling for large reasoning models, but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about LLM reasoning efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.07824": {
        "authors": [
            "Yiwei Qin",
            "Zhen Huang",
            "Tiantian Mi",
            "Weiye Si",
            "Chenyang Zhou",
            "Qipeng Guo",
            "Siyuan Feng",
            "Pengfei Liu"
        ],
        "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
        "abstract": "arXiv:2602.07824v1 Announce Type: new  Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.   To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.",
        "arxiv_id": "2602.07824",
        "ARXIVID": "2602.07824",
        "COMMENT": "This paper introduces a new data processing taxonomy and a large scientific corpus for pre-training, but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about data curation for LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.07883": {
        "authors": [
            "Jingqi Zhou",
            "Sheng Wang",
            "DeZhao Deng",
            "Junwen Lu",
            "Junwei Su",
            "Qintong Li",
            "Jiahui Gao",
            "Hao Wu",
            "Jiyue Jiang",
            "Lingpeng Kong",
            "Chuan Wu"
        ],
        "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation",
        "abstract": "arXiv:2602.07883v1 Announce Type: new  Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.",
        "arxiv_id": "2602.07883",
        "ARXIVID": "2602.07883",
        "COMMENT": "Does not directly match any criterion. Focuses on agentic LLMs and self-reconfiguration via tool-driven adaptation, not spatial intelligence or vision/multimodal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.09736": {
        "authors": [
            "Shaoyang Xie",
            "Xiaofeng Cong",
            "Baosheng Yu",
            "Zhipeng Gui",
            "Jie Gui",
            "Yuan Yan Tang",
            "James Tin-Yau Kwok"
        ],
        "title": "Toward Fine-Grained Facial Control in 3D Talking Head Generation",
        "abstract": "arXiv:2602.09736v1 Announce Type: new  Abstract: Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.",
        "arxiv_id": "2602.09736",
        "ARXIVID": "2602.09736",
        "COMMENT": "Focuses on 3D talking head generation with fine-grained facial control. While it is a vision/ML paper, it does not match any of the four criteria directly (no spatial intelligence for embodied agents, no VLLM/MLLM, no embodied AI benchmark, not vision foundation model work).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.07276": {
        "authors": [
            "Pengrui Han",
            "Xueqiang Xu",
            "Keyang Xuan",
            "Peiyang Song",
            "Siru Ouyang",
            "Runchu Tian",
            "Yuqing Jiang",
            "Cheng Qian",
            "Pengcheng Jiang",
            "Jiashuo Sun",
            "Junxia Cui",
            "Ming Zhong",
            "Ge Liu",
            "Jiawei Han",
            "Jiaxuan You"
        ],
        "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
        "abstract": "arXiv:2602.07276v1 Announce Type: new  Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
        "arxiv_id": "2602.07276",
        "ARXIVID": "2602.07276",
        "COMMENT": "Presents a new method for LLM adaptation via steering vectors, but not specific to vision, spatial intelligence, or VLLMs/MLLMs. No embodied AI or vision foundation model focus.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.07543": {
        "authors": [
            "Heewoong Noh",
            "Gyoung S. Na",
            "Namkyeong Lee",
            "Chanyoung Park"
        ],
        "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
        "abstract": "arXiv:2602.07543v2 Announce Type: new  Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.",
        "arxiv_id": "2602.07543",
        "ARXIVID": "2602.07543",
        "COMMENT": "This paper presents a unified LLM-based framework for material synthesis planning, but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about LLMs for scientific planning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.08222": {
        "authors": [
            "Zehao Chen",
            "Gongxun Li",
            "Tianxiang Ai",
            "Yifei Li",
            "Zixuan Huang",
            "Wang Zhou",
            "Fuzhen Zhuang",
            "Xianglong Liu",
            "Jianxin Li",
            "Deqing Wang",
            "Yikun Ban"
        ],
        "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
        "abstract": "arXiv:2602.08222v1 Announce Type: new  Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
        "arxiv_id": "2602.08222",
        "ARXIVID": "2602.08222",
        "COMMENT": "Does not match any specific criterion. Focuses on post-training optimization for LLMs, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.08400": {
        "authors": [
            "Longkun Li",
            "Yuanben Zou",
            "Jinghan Wu",
            "Yuqing Wen",
            "Jing Li",
            "Hangwei Qian",
            "Ivor Tsang"
        ],
        "title": "SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains",
        "abstract": "arXiv:2602.08400v1 Announce Type: new  Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \\textbf{SCOUT-RAG} (\\textit{\\underline{S}calable and \\underline{CO}st-efficient \\underline{U}nifying \\underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.",
        "arxiv_id": "2602.08400",
        "ARXIVID": "2602.08400",
        "COMMENT": "Does not directly match any criterion. Focuses on distributed knowledge retrieval for LLMs using agentic graph traversal, not spatial or vision-based embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.09740": {
        "authors": [
            "Sandeep Gupta",
            "Roberto Passerone"
        ],
        "title": "Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors",
        "abstract": "arXiv:2602.09740v1 Announce Type: new  Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.",
        "arxiv_id": "2602.09740",
        "ARXIVID": "2602.09740",
        "COMMENT": "Focuses on security and robustness of vision systems in autonomous vehicles. While relevant to computer vision, it does not match any of the four criteria directly.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.07359": {
        "authors": [
            "Xiaoqiang Lin",
            "Jun Hao Liew",
            "Silvio Savarese",
            "Junnan Li"
        ],
        "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents",
        "abstract": "arXiv:2602.07359v1 Announce Type: new  Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.",
        "arxiv_id": "2602.07359",
        "ARXIVID": "2602.07359",
        "COMMENT": "Focuses on research agents and tool calling, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.07905": {
        "authors": [
            "Yu Zhao",
            "Hao Guan",
            "Yongcheng Jing",
            "Ying Zhang",
            "Dacheng Tao"
        ],
        "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
        "abstract": "arXiv:2602.07905v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.",
        "arxiv_id": "2602.07905",
        "ARXIVID": "2602.07905",
        "COMMENT": "Touches on LLMs and meta-cognition in medical reasoning, but does not introduce a new VLLM/MLLM or vision foundation model. No spatial or embodied AI focus.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.09989": {
        "authors": [
            "Oskar Thaeter",
            "Christian Grashei",
            "Anette Haas",
            "Elisa Schmoeckel",
            "Han Li",
            "Peter J. Sch\\\"uffler"
        ],
        "title": "Efficient Special Stain Classification",
        "abstract": "arXiv:2602.09989v1 Announce Type: new  Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently   utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for   the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly   used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.   On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and   0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of   magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control   in digital pathology workflows.",
        "arxiv_id": "2602.09989",
        "ARXIVID": "2602.09989",
        "COMMENT": "Does not match any specific criteria. Focuses on stain classification in pathology images, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}