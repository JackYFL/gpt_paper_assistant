{
    "2509.05263": {
        "authors": [
            "Yinglin Duan",
            "Zhengxia Zou",
            "Tongwei Gu",
            "Wei Jia",
            "Zhan Zhao",
            "Luyi Xu",
            "Xinzhu Liu",
            "Hao Jiang",
            "Kang Chen",
            "Shuang Qiu"
        ],
        "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
        "abstract": "arXiv:2509.05263v1 Announce Type: new  Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18",
        "arxiv_id": "2509.05263",
        "ARXIVID": "2509.05263",
        "COMMENT": "Matches criterion 3 and 2: Proposes LatticeWorld, a new MLLM-empowered framework for interactive 3D world generation, with applications in embodied AI and new simulation/benchmarking environments. Uses multimodal LLMs for world generation and agent interaction.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2509.04548": {
        "authors": [
            "Hongyang Wei",
            "Baixin Xu",
            "Hongbo Liu",
            "Cyrus Wu",
            "Jie Liu",
            "Yi Peng",
            "Peiyu Wang",
            "Zexiang Liu",
            "Jingwen He",
            "Yidan Xietian",
            "Chuanxin Tang",
            "Zidong Wang",
            "Yichen Wei",
            "Liang Hu",
            "Boyi Jiang",
            "William Li",
            "Ying He",
            "Yang Liu",
            "Xuchen Song",
            "Eric Li",
            "Yahui Zhou"
        ],
        "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model",
        "abstract": "arXiv:2509.04548v1 Announce Type: new  Abstract: Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.",
        "arxiv_id": "2509.04548",
        "ARXIVID": "2509.04548",
        "COMMENT": "Matches criterion 2 and 4: Introduces a new unified multimodal model (UniPic2-Metaquery) with novel reinforcement learning strategies for image generation, editing, and understanding, and connects to a VLLM (Qwen2.5-VL-7B).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.04908": {
        "authors": [
            "Hongyi Jing",
            "Jiafu Chen",
            "Chen Rao",
            "Ziqiang Dang",
            "Jiajie Teng",
            "Tianyi Chu",
            "Juncheng Mo",
            "Shuo Fang",
            "Huaizhong Lin",
            "Rui Lv",
            "Chenguang Ma",
            "Lei Zhao"
        ],
        "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing",
        "abstract": "arXiv:2509.04908v1 Announce Type: new  Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.",
        "arxiv_id": "2509.04908",
        "ARXIVID": "2509.04908",
        "COMMENT": "Matches criterion 2: Proposes a new MLLM-based framework (SparkUI-Parser) for GUI perception, with a new benchmark (ScreenParse) and improved grounding/parsing. Also relevant to criterion 4 as it applies vision-language models to GUI understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.04859": {
        "authors": [
            "Hannah Schieber",
            "Dominik Frischmann",
            "Simon Boche",
            "Victor Schaack",
            "Angela Schoellig",
            "Stefan Leutenegger",
            "Daniel Roth"
        ],
        "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
        "abstract": "arXiv:2509.04859v1 Announce Type: new  Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential for critical applications such as tele-guidance and disaster response. These tasks demand both accurate 3D reconstruction and fast scene processing. Instead of reconstructing the entire scene in detail, it is often more efficient to focus on specific objects, i.e., points of interest (PoIs). Mobile robots equipped with advanced sensing can usually detect these early during data acquisition or preliminary analysis, reducing the need for full-scene optimization. Gaussian Splatting (GS) has recently shown promise in delivering high-quality novel view synthesis and 3D representation by an incremental learning process. Extending GS with scene editing, semantics adds useful per-splat features to isolate objects effectively.   Semantic 3D Gaussian editing can already be achieved before the full training cycle is completed, reducing the overall training time. Moreover, the semantically relevant area, the PoI, is usually already known during capturing. To balance high-quality reconstruction with reduced training time, we propose CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS and then refine it for the semantic object using our novel color-based effective filtering for effective object isolation. This is speeding up the training process to be about a quarter less than a full training cycle for semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world, outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher novel-view-synthesis quality.",
        "arxiv_id": "2509.04859",
        "ARXIVID": "2509.04859",
        "COMMENT": "Matches criterion 1 (spatial understanding for embodied agents) and criterion 3 (new method for efficient, semantic-focused 3D reconstruction for mobile robotics, with a novel coarse-to-refined Gaussian Splatting approach). The focus on semantic object isolation and runtime efficiency is a novel angle for embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.05012": {
        "authors": [
            "Hulin Li",
            "Qiliang Ren",
            "Jun Li",
            "Hanbing Wei",
            "Zheng Liu",
            "Linfang Fan"
        ],
        "title": "A biologically inspired separable learning vision model for real-time traffic object perception in Dark",
        "abstract": "arXiv:2509.05012v1 Announce Type: new  Abstract: Fast and accurate object perception in low-light traffic scenes has attracted increasing attention. However, due to severe illumination degradation and the lack of reliable visual cues, existing perception models and methods struggle to quickly adapt to and accurately predict in low-light environments. Moreover, there is the absence of available large-scale benchmark specifically focused on low-light traffic scenes. To bridge this gap, we introduce a physically grounded illumination degradation method tailored to real-world low-light settings and construct Dark-traffic, the largest densely annotated dataset to date for low-light traffic scenes, supporting object detection, instance segmentation, and optical flow estimation. We further propose the Separable Learning Vision Model (SLVM), a biologically inspired framework designed to enhance perception under adverse lighting. SLVM integrates four key components: a light-adaptive pupillary mechanism for illumination-sensitive feature extraction, a feature-level separable learning strategy for efficient representation, task-specific decoupled branches for multi-task separable learning, and a spatial misalignment-aware fusion module for precise multi-feature alignment. Extensive experiments demonstrate that SLVM achieves state-of-the-art performance with reduced computational overhead. Notably, it outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1 percentage points in instance segmentation, and reduces endpoint error (EPE) of baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end trained SLVM surpasses Swin Transformer+EnlightenGAN and ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage points. The Dark-traffic dataset and complete code is released at https://github.com/alanli1997/slvm.",
        "arxiv_id": "2509.05012",
        "ARXIVID": "2509.05012",
        "COMMENT": "Matches criterion 3: The paper introduces a new large-scale benchmark (Dark-traffic) for low-light traffic scenes, supporting multiple tasks (object detection, instance segmentation, optical flow), and proposes a novel biologically inspired vision model (SLVM) with several new components for spatial understanding under adverse lighting. The focus on low-light conditions and the combination of new dataset and method is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05144": {
        "authors": [
            "Chaolei Wang",
            "Yang Luo",
            "Jing Du",
            "Siyu Chen",
            "Yiping Chen",
            "Ting Han"
        ],
        "title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing",
        "abstract": "arXiv:2509.05144v1 Announce Type: new  Abstract: Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available in the supplementary materials.",
        "arxiv_id": "2509.05144",
        "ARXIVID": "2509.05144",
        "COMMENT": "Matches criterion 1 (spatial understanding in 3D vision) and criterion 4 (vision foundation models and applications). Proposes a training-free, split-then-grow framework for high-fidelity 3D instance segmentation, fusing semantic and geometric information. The mask filtering and growing approach is a novel refinement method.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04545": {
        "authors": [
            "Linqing Wang",
            "Ximing Xing",
            "Yiji Cheng",
            "Zhiyuan Zhao",
            "Jiale Tao",
            "Qixun Wang",
            "Ruihuang Li",
            "Xin Li",
            "Mingrui Wu",
            "Xinchi Deng",
            "Chunyu Wang",
            "Qinglin Lu"
        ],
        "title": "PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting",
        "abstract": "arXiv:2509.04545v1 Announce Type: new  Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.",
        "arxiv_id": "2509.04545",
        "ARXIVID": "2509.04545",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a universal prompt rewriting framework for text-to-image diffusion models, using chain-of-thought and reinforcement learning, with a new reward model and benchmark. This is a clever statistical trick for improving T2I models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04809": {
        "authors": [
            "Haechang Kim",
            "Hao Chen",
            "Can Li",
            "Jong Min Lee"
        ],
        "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models",
        "abstract": "arXiv:2509.04809v1 Announce Type: new  Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.",
        "arxiv_id": "2509.04809",
        "ARXIVID": "2509.04809",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel XRL framework for explainable RL agents, with interactive LLM-based explanations and counterfactuals). The use of LLMs for explainability in RL is a novel angle, especially with the multi-agent LLM architecture and mapping user queries to XRL tools.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04824": {
        "authors": [
            "Haosong Liu",
            "Xiancheng Zhu",
            "Huanqiang Zeng",
            "Jianqing Zhu",
            "Jiuwen Cao",
            "Junhui Hou"
        ],
        "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution",
        "abstract": "arXiv:2509.04824v1 Announce Type: new  Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.",
        "arxiv_id": "2509.04824",
        "ARXIVID": "2509.04824",
        "COMMENT": "Matches criterion 1: Proposes a new hybrid Mamba-Transformer framework (LFMT) for light field super-resolution, focusing on non-local spatial-angular correlations and efficient feature extraction, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.05030": {
        "authors": [
            "Cong Cao",
            "Xianhang Cheng",
            "Jingyuan Liu",
            "Yujian Zheng",
            "Zhenhui Lin",
            "Meriem Chkir",
            "Hao Li"
        ],
        "title": "LUIVITON: Learned Universal Interoperable VIrtual Try-ON",
        "abstract": "arXiv:2509.05030v1 Announce Type: new  Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on, capable of draping complex, multi-layer clothing onto diverse and arbitrarily posed humanoid characters. To address the challenge of aligning complex garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy representation and separate the clothing-to-body draping problem into two correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence, where each has its unique challenges. While we address the clothing-to-SMPL fitting problem using a geometric learning-based approach for partial-to-complete shape correspondence prediction, we introduce a diffusion model-based approach for body-to-SMPL correspondence using multi-view consistent appearance features and a pre-trained 2D foundation model. Our method can handle complex geometries, non-manifold meshes, and generalizes effectively to a wide range of humanoid characters -- including humans, robots, cartoon subjects, creatures, and aliens, while maintaining computational efficiency for practical adoption. In addition to offering a fully automatic fitting solution, LUIVITON supports fast customization of clothing size, allowing users to adjust clothing sizes and material properties after they have been draped. We show that our system can produce high-quality 3D clothing fittings without any human labor, even when 2D clothing sewing patterns are not available.",
        "arxiv_id": "2509.05030",
        "ARXIVID": "2509.05030",
        "COMMENT": "This paper presents a virtual try-on system using geometric learning and diffusion models, leveraging a 2D foundation model for correspondence. It is relevant to criterion 4 (vision foundation models and applications), with a novel approach to 3D clothing fitting and generalization to diverse characters.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.05075": {
        "authors": [
            "Yangming Li",
            "Chaoyu Liu",
            "Lihao Liu",
            "Simon Masnou",
            "Carola-Bibian Sch\\\"onlieb"
        ],
        "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
        "abstract": "arXiv:2509.05075v1 Announce Type: new  Abstract: A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they are also unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework: GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.",
        "arxiv_id": "2509.05075",
        "ARXIVID": "2509.05075",
        "COMMENT": "Matches criterion 4: Proposes GeoSplat, a geometry-constrained optimization framework for Gaussian splatting in novel view synthesis, improving vision foundation model performance.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.05034": {
        "authors": [
            "Jingqi Wu",
            "Hanxi Li",
            "Lin Yuanbo Wu",
            "Hao Chen",
            "Deyin Liu",
            "Peng Wang"
        ],
        "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization",
        "abstract": "arXiv:2509.05034v1 Announce Type: new  Abstract: Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC = 99.1\\% on MVTec AD).",
        "arxiv_id": "2509.05034",
        "ARXIVID": "2509.05034",
        "COMMENT": "Matches criterion 4: Proposes a cross-modal framework (ADClick-Seg) for industrial anomaly detection and localization, leveraging vision-language alignment and prototype-based approaches, which is an application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.04597": {
        "authors": [
            "Jin Ma",
            "Mohammed Aldeen",
            "Christopher Salas",
            "Feng Luo",
            "Mashrur Chowdhury",
            "Mert Pes\\'e",
            "Long Cheng"
        ],
        "title": "DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models",
        "abstract": "arXiv:2509.04597v1 Announce Type: new  Abstract: Object detection is fundamental to various real-world applications, such as security monitoring and surveillance video analysis. Despite their advancements, state-of-theart object detectors are still vulnerable to adversarial patch attacks, which can be easily applied to real-world objects to either conceal actual items or create non-existent ones, leading to severe consequences. Given the current diversity of adversarial patch attacks and potential unknown threats, an ideal defense method should be effective, generalizable, and robust against adaptive attacks. In this work, we introduce DISPATCH, the first diffusion-based defense framework for object detection. Unlike previous works that aim to \"detect and remove\" adversarial patches, DISPATCH adopts a \"regenerate and rectify\" strategy, leveraging generative models to disarm attack effects while preserving the integrity of the input image. Specifically, we utilize the in-distribution generative power of diffusion models to regenerate the entire image, aligning it with benign data. A rectification process is then employed to identify and replace adversarial regions with their regenerated benign counterparts. DISPATCH is attack-agnostic and requires no prior knowledge of the existing patches. Extensive experiments across multiple detectors and attacks demonstrate that DISPATCH consistently outperforms state-of-the-art defenses on both hiding attacks and creating attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and lowering the attack success rate to 24.8% on untargeted creating attacks. Moreover, it maintains strong robustness against adaptive attacks, making it a practical and reliable defense for object detection systems.",
        "arxiv_id": "2509.04597",
        "ARXIVID": "2509.04597",
        "COMMENT": "Relevant to vision foundation models and generative modeling (criterion 4), as it uses diffusion models for adversarial defense in object detection. The 'regenerate and rectify' strategy is a clever generative approach, but not a new VLLM or MLLM.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.04848": {
        "authors": [
            "Enze Ye",
            "Wei Lin",
            "Shaochi Ren",
            "Yakun Liu",
            "Xiaoping Li",
            "Hao Wang",
            "He Sun",
            "Feng Pan"
        ],
        "title": "Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations",
        "abstract": "arXiv:2509.04848v1 Announce Type: new  Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables label-free, volumetric characterization of individual cells by reconstructing their refractive index (RI) distributions from multiple viewing angles during flow through microfluidic channels. However, current imaging methods assume that cells undergo uniform, single-axis rotation, which require their poses to be known at each frame. This assumption restricts applicability to near-spherical cells and prevents accurate imaging of irregularly shaped cells with complex rotations. As a result, only a subset of the cellular population can be analyzed, limiting the ability of flow-based assays to perform robust statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction framework that leverages the Fourier diffraction theorem and implicit neural representations (INRs) for high-throughput flow cytometry tomographic imaging. By jointly optimizing each cell's unknown rotational trajectory and volumetric structure under weak scattering assumptions, OmniFHT supports arbitrary cell geometries and multi-axis rotations. Its continuous representation also allows accurate reconstruction from sparsely sampled projections and restricted angular coverage, producing high-fidelity results with as few as 10 views or only 120 degrees of angular range. OmniFHT enables, for the first time, in situ, high-throughput tomographic imaging of entire flowing cell populations, providing a scalable and unbiased solution for label-free morphometric analysis in flow cytometry platforms.",
        "arxiv_id": "2509.04848",
        "ARXIVID": "2509.04848",
        "COMMENT": "This paper presents a new method for pose-free 3D quantitative phase imaging using implicit neural representations and joint optimization of pose and structure. While it is a methodological advance in 3D imaging, it is not directly about spatial intelligence in embodied agents or vision-language models. Closest to criterion 1, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2509.04847": {
        "authors": [
            "Mukul Singh",
            "Arjun Radhakrishna",
            "Sumit Gulwani"
        ],
        "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory",
        "abstract": "arXiv:2509.04847v1 Announce Type: new  Abstract: Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled \"strategy switch\" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.",
        "arxiv_id": "2509.04847",
        "ARXIVID": "2509.04847",
        "COMMENT": "This paper studies language models in game-theoretic, long-horizon social environments. While interesting for LLM behavior, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.04736": {
        "authors": [
            "Taeyoung Yeon",
            "Vasco Xu",
            "Henry Hoffmann",
            "Karan Ahuja"
        ],
        "title": "WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches",
        "abstract": "arXiv:2509.04736v1 Announce Type: new  Abstract: Despite advances in practical and multimodal fine-grained Human Activity Recognition (HAR), a system that runs entirely on smartwatches in unconstrained environments remains elusive. We present WatchHAR, an audio and inertial-based HAR system that operates fully on smartwatches, addressing privacy and latency issues associated with external data processing. By optimizing each component of the pipeline, WatchHAR achieves compounding performance gains. We introduce a novel architecture that unifies sensor data preprocessing and inference into an end-to-end trainable module, achieving 5x faster processing while maintaining over 90% accuracy across more than 25 activity classes. WatchHAR outperforms state-of-the-art models for event detection and activity classification while running directly on the smartwatch, achieving 9.3 ms processing time for activity event detection and 11.8 ms for multimodal activity classification. This research advances on-device activity recognition, realizing smartwatches' potential as standalone, privacy-aware, and minimally-invasive continuous activity tracking devices.",
        "arxiv_id": "2509.04736",
        "ARXIVID": "2509.04736",
        "COMMENT": "Does not match any specific criterion. Focuses on on-device human activity recognition using audio and inertial data, not spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04642": {
        "authors": [
            "Wenxiao Wang",
            "Priyatham Kattakinda",
            "Soheil Feizi"
        ],
        "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents",
        "abstract": "arXiv:2509.04642v1 Announce Type: new  Abstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.",
        "arxiv_id": "2509.04642",
        "ARXIVID": "2509.04642",
        "COMMENT": "Does not match any specific criterion. Focuses on LLM agent optimization, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.05019": {
        "authors": [
            "Mohsine El Khayati",
            "Ayyad Maafiri",
            "Yassine Himeur",
            "Hamzah Ali Alkhazaleh",
            "Shadi Atalla",
            "Wathiq Mansoor"
        ],
        "title": "Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition",
        "abstract": "arXiv:2509.05019v1 Announce Type: new  Abstract: The study explores the integration of transfer learning (TL) with mobile-enabled convolutional neural networks (MbNets) to enhance Arabic Handwritten Character Recognition (AHCR). Addressing challenges like extensive computational requirements and dataset scarcity, this research evaluates three TL strategies--full fine-tuning, partial fine-tuning, and training from scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD, HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently achieving superior accuracy, robustness, and efficiency, with ShuffleNet excelling in generalization, particularly under full fine-tuning. The IFHCDB dataset yielded the highest results, with 99% accuracy using MnasNet under full fine-tuning, highlighting its suitability for robust character recognition. The AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA posed significant challenges due to its variability, achieving a peak accuracy of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall performance, balancing accuracy and convergence speed, while partial fine-tuning underperformed across metrics. These findings underscore the potential of combining TL and MbNets for resource-efficient AHCR, paving the way for further optimizations and broader applications. Future work will explore architectural modifications, in-depth dataset feature analysis, data augmentation, and advanced sensitivity analysis to enhance model robustness and generalizability.",
        "arxiv_id": "2509.05019",
        "ARXIVID": "2509.05019",
        "COMMENT": "This is about transfer learning and mobile CNNs for Arabic handwritten character recognition. It is a vision application, but not related to the specific criteria (no spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.04624": {
        "authors": [
            "Ali Khanpour",
            "Tianyi Wang",
            "Afra Vahidi-Shams",
            "Wim Ectors",
            "Farzam Nakhaie",
            "Amirhossein Taheri",
            "Christian Claudel"
        ],
        "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis",
        "abstract": "arXiv:2509.04624v1 Announce Type: new  Abstract: Traffic congestion and violations pose significant challenges for urban mobility and road safety. Traditional traffic monitoring systems, such as fixed cameras and sensor-based methods, are often constrained by limited coverage, low adaptability, and poor scalability. To address these challenges, this paper introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance system capable of accurate vehicle detection, classification, tracking, and behavioral analysis in real-world, unconstrained urban environments. The system leverages multi-scale and multi-angle template matching, Kalman filtering, and homography-based calibration to process aerial video data collected from altitudes of approximately 200 meters. A case study in urban area demonstrates robust performance, achieving a detection precision of 91.8%, an F1-score of 90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively. Beyond precise detection, the system classifies five vehicle types and automatically detects critical traffic violations, including unsafe lane changes, illegal double parking, and crosswalk obstructions, through the fusion of geofencing, motion filtering, and trajectory deviation analysis. The integrated analytics module supports origin-destination tracking, vehicle count visualization, inter-class correlation analysis, and heatmap-based congestion modeling. Additionally, the system enables entry-exit trajectory profiling, vehicle density estimation across road segments, and movement direction logging, supporting comprehensive multi-scale urban mobility analytics. Experimental results confirms the system's scalability, accuracy, and practical relevance, highlighting its potential as an enforcement-aware, infrastructure-independent traffic monitoring solution for next-generation smart cities.",
        "arxiv_id": "2509.04624",
        "ARXIVID": "2509.04624",
        "COMMENT": "Does not match any specific criterion. Focuses on UAV-based traffic surveillance using computer vision, but not on embodied AI benchmarks, spatial intelligence, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}