{
    "2509.09154": {
        "authors": [
            "Bui Duc Manh",
            "Soumyaratna Debnath",
            "Zetong Zhang",
            "Shriram Damodaran",
            "Arvind Kumar",
            "Yueyi Zhang",
            "Lu Mi",
            "Erik Cambria",
            "Lin Wang"
        ],
        "title": "Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective",
        "abstract": "arXiv:2509.09154v1 Announce Type: new  Abstract: Recent advances in agentic AI have led to systems capable of autonomous task execution and language-based reasoning, yet their spatial reasoning abilities remain limited and underexplored, largely constrained to symbolic and sequential processing. In contrast, human spatial intelligence, rooted in integrated multisensory perception, spatial memory, and cognitive maps, enables flexible, context-aware decision-making in unstructured environments. Therefore, bridging this gap is critical for advancing Agentic Spatial Intelligence toward better interaction with the physical 3D world. To this end, we first start from scrutinizing the spatial neural models as studied in computational neuroscience, and accordingly introduce a novel computational framework grounded in neuroscience principles. This framework maps core biological functions to six essential computation modules: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, an artificial cognitive map, spatial memory, and spatial reasoning. Together, these modules form a perspective landscape for agentic spatial reasoning capability across both virtual and physical environments. On top, we conduct a framework-guided analysis of recent methods, evaluating their relevance to each module and identifying critical gaps that hinder the development of more neuroscience-grounded spatial reasoning modules. We further examine emerging benchmarks and datasets and explore potential application domains ranging from virtual to embodied systems, such as robotics. Finally, we outline potential research directions, emphasizing the promising roadmap that can generalize spatial reasoning across dynamic or unstructured environments. We hope this work will benefit the research community with a neuroscience-grounded perspective and a structured pathway. Our project page can be found at Github.",
        "arxiv_id": "2509.09154",
        "ARXIVID": "2509.09154",
        "COMMENT": "Matches criterion 1: proposes a neuroscience-inspired computational framework for agentic spatial intelligence, mapping biological functions to computational modules for embodied agents. Offers a novel perspective and identifies gaps in current methods.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.09666": {
        "authors": [
            "Zhiyuan Yan",
            "Kaiqing Lin",
            "Zongjian Li",
            "Junyan Ye",
            "Hui Han",
            "Zhendong Wang",
            "Hao Liu",
            "Bin Lin",
            "Hao Li",
            "Xue Xu",
            "Xinyan Xiao",
            "Jingdong Wang",
            "Haifeng Wang",
            "Li Yuan"
        ],
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
        "abstract": "arXiv:2509.09666v1 Announce Type: new  Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.",
        "arxiv_id": "2509.09666",
        "ARXIVID": "2509.09666",
        "COMMENT": "Matches criterion 2: proposes a unified multimodal learning framework (UAE) that tightly couples image-to-text and text-to-image, with a new benchmark (Unified-Bench) and surprising empirical findings about mutual benefit in VLLMs/MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.09680": {
        "authors": [
            "Rongyao Fang",
            "Aldrich Yu",
            "Chengqi Duan",
            "Linjiang Huang",
            "Shuai Bai",
            "Yuxuan Cai",
            "Kun Wang",
            "Si Liu",
            "Xihui Liu",
            "Hongsheng Li"
        ],
        "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark",
        "abstract": "arXiv:2509.09680v1 Announce Type: new  Abstract: The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
        "arxiv_id": "2509.09680",
        "ARXIVID": "2509.09680",
        "COMMENT": "Matches criterion 3 (new benchmark for reasoning in text-to-image generation, with a large-scale dataset and comprehensive evaluation) and criterion 2 (evaluation of VLLMs/MLLMs on the new benchmark). Also relevant to criterion 4 (vision foundation models and their evaluation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.08897": {
        "authors": [
            "Davide Caffagni",
            "Sara Sarto",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ],
        "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval",
        "abstract": "arXiv:2509.08897v1 Announce Type: new  Abstract: With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2",
        "arxiv_id": "2509.08897",
        "ARXIVID": "2509.08897",
        "COMMENT": "This paper introduces ReT-2, a unified retrieval model for multimodal queries and documents, using a recurrent Transformer with LSTM-inspired gating. It is a new VLLM/MLLM (criterion 2), and also relevant to vision foundation models (criterion 4).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.09676": {
        "authors": [
            "Jiahao Wang",
            "Yufeng Yuan",
            "Rujie Zheng",
            "Youtian Lin",
            "Jian Gao",
            "Lin-Zhuo Chen",
            "Yajie Bao",
            "Yi Zhang",
            "Chang Zeng",
            "Yanxi Zhou",
            "Xiaoxiao Long",
            "Hao Zhu",
            "Zhaoxiang Zhang",
            "Xun Cao",
            "Yao Yao"
        ],
        "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
        "abstract": "arXiv:2509.09676v1 Announce Type: new  Abstract: Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
        "arxiv_id": "2509.09676",
        "ARXIVID": "2509.09676",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks/simulators) and criterion 1 (spatial understanding for embodied agents). Introduces a large-scale video dataset with rich spatial annotations, enabling new research in spatial intelligence and 3D vision.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.09356": {
        "authors": [
            "Abdel Hakim Drid",
            "Vincenzo Suriani",
            "Daniele Nardi",
            "Abderrezzak Debilou"
        ],
        "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning",
        "abstract": "arXiv:2509.09356v1 Announce Type: new  Abstract: Navigating and understanding complex and unknown environments autonomously demands more than just basic perception and movement from embodied agents. Truly effective exploration requires agents to possess higher-level cognitive abilities, the ability to reason about their surroundings, and make more informed decisions regarding exploration strategies. However, traditional RL approaches struggle to balance efficient exploration and semantic understanding due to limited cognitive capabilities embedded in the small policies for the agents, leading often to human drivers when dealing with semantic exploration. In this paper, we address this challenge by presenting a novel Deep Reinforcement Learning (DRL) architecture that is specifically designed for resource efficient semantic exploration. A key methodological contribution is the integration of a Vision-Language Model (VLM) common-sense through a layered reward function. The VLM query is modeled as a dedicated action, allowing the agent to strategically query the VLM only when deemed necessary for gaining external guidance, thereby conserving resources. This mechanism is combined with a curriculum learning strategy designed to guide learning at different levels of complexity to ensure robust and stable learning. Our experimental evaluation results convincingly demonstrate that our agent achieves significantly enhanced object discovery rates and develops a learned capability to effectively navigate towards semantically rich regions. Furthermore, it also shows a strategic mastery of when to prompt for external environmental information. By demonstrating a practical and scalable method for embedding common-sense semantic reasoning with autonomous agents, this research provides a novel approach to pursuing a fully intelligent and self-guided exploration in robotics.",
        "arxiv_id": "2509.09356",
        "ARXIVID": "2509.09356",
        "COMMENT": "Matches criterion 1 (new methodological improvement for spatial understanding and intelligence in embodied agents, integrating VLMs for semantic exploration) and criterion 3 (novel method for embodied AI with curriculum learning and VLM querying).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.09064": {
        "authors": [
            "Qiuhui Chen",
            "Xuancheng Yao",
            "Huping Ye",
            "Yi Hong"
        ],
        "title": "Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models",
        "abstract": "arXiv:2509.09064v1 Announce Type: new  Abstract: Understanding 3D medical image volumes is critical in the medical field, yet existing 3D medical convolution and transformer-based self-supervised learning (SSL) methods often lack deep semantic comprehension. Recent advancements in multimodal large language models (MLLMs) provide a promising approach to enhance image understanding through text descriptions. To leverage these 2D MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a novel pretraining framework that integrates 3D image encoders with 2D MLLMs via a specially designed plane-slice-aware transformer module. Additionally, our model employs a partial optimal transport based alignment, demonstrating greater tolerance to noise introduced by potential noises in LLM-generated content. Med3DInsight introduces a new paradigm for scalable multimodal 3D medical representation learning without requiring human annotations. Extensive experiments demonstrate our state-of-the-art performance on two downstream tasks, i.e., segmentation and classification, across various public datasets with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can be seamlessly integrated into existing 3D medical image understanding networks, potentially enhancing their performance. Our source code, generated datasets, and pre-trained models will be available at https://github.com/Qybc/Med3DInsight.",
        "arxiv_id": "2509.09064",
        "ARXIVID": "2509.09064",
        "COMMENT": "Matches criteria 2 (new MLLMs) and 4 (vision foundation models and applications). Proposes a new pretraining framework integrating 3D image encoders with 2D MLLMs for medical image understanding, using a novel plane-slice-aware transformer and partial optimal transport alignment. Strong focus on multi-modal learning and vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09143": {
        "authors": [
            "Yuiko Uchida",
            "Ren Togo",
            "Keisuke Maeda",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation",
        "abstract": "arXiv:2509.09143v1 Announce Type: new  Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric for 3D scenes that explicitly focuses on \"objects,\" which are fundamental units of human visual perception. Existing metrics assess overall image quality, leading to discrepancies with human perception. Inspired by neuropsychological insights, we hypothesize that human recognition of 3D scenes fundamentally involves attention to individual objects. OSIM enables object-centric evaluations by leveraging an object detection model and its feature representations to quantify the \"objectness\" of each object in the scene. Our user study demonstrates that OSIM aligns more closely with human perception compared to existing metrics. We also analyze the characteristics of OSIM using various approaches. Moreover, we re-evaluate recent 3D reconstruction and generation models under a standardized experimental setup to clarify advancements in this field. The code is available at https://github.com/Objectness-Similarity/OSIM.",
        "arxiv_id": "2509.09143",
        "ARXIVID": "2509.09143",
        "COMMENT": "This paper introduces OSIM, a new object-centric evaluation metric for 3D scene understanding, focusing on object-level fidelity. This is a methodological improvement for spatial understanding (criterion 1), and also relevant to vision foundation model evaluation (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.09555": {
        "authors": [
            "Sirui Xu",
            "Dongting Li",
            "Yucheng Zhang",
            "Xiyan Xu",
            "Qi Long",
            "Ziyin Wang",
            "Yunzhi Lu",
            "Shuchang Dong",
            "Hezi Jiang",
            "Akshat Gupta",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation",
        "abstract": "arXiv:2509.09555v1 Announce Type: new  Abstract: While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.",
        "arxiv_id": "2509.09555",
        "ARXIVID": "2509.09555",
        "COMMENT": "Matches criterion 3: introduces InterAct, a large-scale 3D human-object interaction benchmark and new methods for HOI generation, with a focus on data quality and benchmarking tasks. Highly relevant for embodied AI and novel benchmarking.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09658": {
        "authors": [
            "Bingkui Tong",
            "Jiaer Xia",
            "Sifeng Shang",
            "Kaiyang Zhou"
        ],
        "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
        "abstract": "arXiv:2509.09658v1 Announce Type: new  Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a \"None of the above\" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.",
        "arxiv_id": "2509.09658",
        "ARXIVID": "2509.09658",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new benchmark (HumbleBench) for evaluating epistemic humility in MLLMs, focusing on hallucination and false-option rejection.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.09311": {
        "authors": [
            "Illia Volkov",
            "Nikita Kisel",
            "Klara Janouskova",
            "Jiri Matas"
        ],
        "title": "Image Recognition with Vision and Language Embeddings of VLMs",
        "abstract": "arXiv:2509.09311v1 Announce Type: new  Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification through image-text alignment. Yet, their purely visual inference capabilities remain under-explored. In this work, we conduct a comprehensive evaluation of both language-guided and vision-only image classification with a diverse set of dual-encoder VLMs, including both well-established and recent models such as SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the ImageNet-1k validation set and its label-corrected variant. The key factors affecting accuracy are analysed, including prompt design, class diversity, the number of neighbours in k-NN, and reference set size. We show that language and vision offer complementary strengths, with some classes favouring textual prompts and others better handled by visual similarity. To exploit this complementarity, we introduce a simple, learning-free fusion method based on per-class precision that improves classification performance. The code is available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.",
        "arxiv_id": "2509.09311",
        "ARXIVID": "2509.09311",
        "COMMENT": "Matches criterion 2 (comprehensive evaluation of VLLMs for image recognition, including new models and fusion methods), and criterion 4 (vision foundation models and their applications).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09324": {
        "authors": [
            "Hui Li",
            "Yi You",
            "Qiqi Chen",
            "Bingfeng Zhang",
            "George Q. Huang"
        ],
        "title": "Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM",
        "abstract": "arXiv:2509.09324v1 Announce Type: new  Abstract: Generative AI evolves the execution of complex workflows in industry, where the large multimodal model empowers fashion design in the garment industry. Current generation AI models magically transform brainstorming into fancy designs easily, but the fine-grained customization still suffers from text uncertainty without professional background knowledge from end-users. Thus, we propose the Better Understanding Generation (BUG) workflow with LMM to automatically create and fine-grain customize the cloth designs from chat with image-into-prompt. Our framework unleashes users' creative potential beyond words and also lowers the barriers of clothing design/editing without further human involvement. To prove the effectiveness of our model, we propose a new FashionEdit dataset that simulates the real-world clothing design workflow, evaluated from generation similarity, user satisfaction, and quality. The code and dataset: https://github.com/detectiveli/FashionEdit.",
        "arxiv_id": "2509.09324",
        "ARXIVID": "2509.09324",
        "COMMENT": "This paper presents a new workflow and dataset for fine-grained fashion design using large multimodal models (LMMs), and proposes a new benchmark (criterion 2: new MLLMs, and criterion 4: vision foundation model applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09307": {
        "authors": [
            "Zhengzhao Lai",
            "Youbin Zheng",
            "Zhenyang Cai",
            "Haonan Lyu",
            "Jinpu Yang",
            "Hongqing Liang",
            "Yan Hu",
            "Benyou Wang"
        ],
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
        "abstract": "arXiv:2509.09307v1 Announce Type: new  Abstract: Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
        "arxiv_id": "2509.09307",
        "ARXIVID": "2509.09307",
        "COMMENT": "Matches criterion 2: presents MatCha, a new multimodal benchmark for evaluating MLLMs on materials characterization images, revealing surprising empirical limitations of current models. Relevant for VLLM/MLLM evaluation and insightful results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09584": {
        "authors": [
            "Lingdong Kong",
            "Dongyue Lu",
            "Ao Liang",
            "Rong Li",
            "Yuhao Dong",
            "Tianshuai Hu",
            "Lai Xing Ng",
            "Wei Tsang Ooi",
            "Benoit R. Cottereau"
        ],
        "title": "Visual Grounding from Event Cameras",
        "abstract": "arXiv:2509.09584v1 Announce Type: new  Abstract: Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.",
        "arxiv_id": "2509.09584",
        "ARXIVID": "2509.09584",
        "COMMENT": "Matches criterion 3: introduces Talk2Event, a new benchmark for language-driven visual grounding using event cameras, with structured attributes for spatial, temporal, and relational cues. Focuses on multimodal perception in dynamic environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09190": {
        "authors": [
            "Hanwei Zhu",
            "Haoning Wu",
            "Zicheng Zhang",
            "Lingyu Zhu",
            "Yixuan Li",
            "Peilin Chen",
            "Shiqi Wang",
            "Chris Wei Zhou",
            "Linhan Cao",
            "Wei Sun",
            "Xiangyang Zhu",
            "Weixia Zhang",
            "Yucheng Zhu",
            "Jing Liu",
            "Dandan Zhu",
            "Guangtao Zhai",
            "Xiongkuo Min",
            "Zhichao Zhang",
            "Xinyue Li",
            "Shubo Xu",
            "Anh Dao",
            "Yifan Li",
            "Hongyuan Yu",
            "Jiaojiao Yi",
            "Yiding Tian",
            "Yupeng Wu",
            "Feiran Sun",
            "Lijuan Liao",
            "Song Jiang"
        ],
        "title": "VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results",
        "abstract": "arXiv:2509.09190v1 Announce Type: new  Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems.",
        "arxiv_id": "2509.09190",
        "ARXIVID": "2509.09190",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). The paper presents a new benchmark and challenge for evaluating visual quality reasoning in large multimodal models, with a focus on interpretable and human-aligned evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09572": {
        "authors": [
            "Sijun Dong",
            "Yuxuan Hu",
            "LiBo Wang",
            "Geng Chen",
            "Xiaoliang Meng"
        ],
        "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
        "abstract": "arXiv:2509.09572v1 Announce Type: new  Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.",
        "arxiv_id": "2509.09572",
        "ARXIVID": "2509.09572",
        "COMMENT": "Matches criterion 4 (application of vision foundation models with parameter-efficient fine-tuning for remote sensing change detection).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08982": {
        "authors": [
            "Karim Slimani",
            "Catherine Achard",
            "Brahim Tamadazte"
        ],
        "title": "iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning",
        "abstract": "arXiv:2509.08982v1 Announce Type: new  Abstract: This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.",
        "arxiv_id": "2509.08982",
        "ARXIVID": "2509.08982",
        "COMMENT": "Matches criteria 1 (new methodological improvements to spatial understanding on embodied agents). Proposes a new differentiable framework for point cloud registration using local-to-global geometric consistency learning, with strong empirical results on spatial tasks. Relevant for spatial intelligence and geometric reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.09498": {
        "authors": [
            "Haoran Xu",
            "Jiacong Hu",
            "Ke Zhang",
            "Lei Yu",
            "Yuxin Tang",
            "Xinyuan Song",
            "Yiqun Duan",
            "Lynn Ai",
            "Bill Shi"
        ],
        "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
        "abstract": "arXiv:2509.09498v1 Announce Type: new  Abstract: Long-term multi-agent systems inevitably generate vast amounts of trajectories and historical interactions, which makes efficient memory management essential for both performance and scalability. Existing methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, we present SEDM, Self-Evolving Distributed Memory, a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory mechanism for open-ended multi-agent collaboration. The code will be released in the later stage of this project.",
        "arxiv_id": "2509.09498",
        "ARXIVID": "2509.09498",
        "COMMENT": "This paper presents SEDM, a scalable, self-evolving distributed memory for multi-agent systems. It is relevant to embodied AI (criterion 3), as it proposes a new memory mechanism for agents, which is a novel angle for multi-agent collaboration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.08908": {
        "authors": [
            "Rogerio Guimaraes",
            "Frank Xiao",
            "Pietro Perona",
            "Markus Marks"
        ],
        "title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains",
        "abstract": "arXiv:2509.08908v1 Announce Type: new  Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\\href{https://www.vision.caltech.edu/actiondiff/}{\\texttt{vision.caltech.edu/actiondiff}}$ Code: $\\href{https://github.com/frankyaoxiao/ActionDiff}{\\texttt{github.com/frankyaoxiao/ActionDiff}}$",
        "arxiv_id": "2509.08908",
        "ARXIVID": "2509.08908",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in vision. Proposes using features from a vision diffusion model for robust action recognition across domains, showing surprising empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.09547": {
        "authors": [
            "Dohun Lee",
            "Hyeonho Jeong",
            "Jiwook Kim",
            "Duygu Ceylan",
            "Jong Chul Ye"
        ],
        "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders",
        "abstract": "arXiv:2509.09547v1 Announce Type: new  Abstract: Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/",
        "arxiv_id": "2509.09547",
        "ARXIVID": "2509.09547",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. The paper proposes a novel method for improving video diffusion transformer training by aligning with self-supervised vision encoders.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.09130": {
        "authors": [
            "Bin Huang",
            "Kang Chen",
            "Bingxuan Li",
            "Huafeng Liu",
            "Qiegen Liu"
        ],
        "title": "ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain",
        "abstract": "arXiv:2509.09130v1 Announce Type: new  Abstract: Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.",
        "arxiv_id": "2509.09130",
        "ARXIVID": "2509.09130",
        "COMMENT": "This paper introduces a PET imaging foundation model using latent diffusion, with innovations in data augmentation and attention. It is a vision foundation model (criterion 4), though focused on medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.09111": {
        "authors": [
            "Jianqin Gao",
            "Tianqi Wang",
            "Yu Zhang",
            "Yishu Zhang",
            "Chenyuan Wang",
            "Allan Dong",
            "Zihao Wang"
        ],
        "title": "FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding",
        "abstract": "arXiv:2509.09111v1 Announce Type: new  Abstract: The widespread use of mobile devices has created new challenges for vision systems in safety monitoring, workplace productivity assessment, and attention management. Detecting whether a person is using a phone requires not only object recognition but also an understanding of behavioral context, which involves reasoning about the relationship between faces, hands, and devices under diverse conditions. Existing generic benchmarks do not fully capture such fine-grained human--device interactions. To address this gap, we introduce the FPI-Det, containing 22{,}879 images with synchronized annotations for faces and phones across workplace, education, transportation, and public scenarios. The dataset features extreme scale variation, frequent occlusions, and varied capture conditions. We evaluate representative YOLO and DETR detectors, providing baseline results and an analysis of performance across object sizes, occlusion levels, and environments. Source code and dataset is available at https://github.com/KvCgRv/FPI-Det.",
        "arxiv_id": "2509.09111",
        "ARXIVID": "2509.09111",
        "COMMENT": "Matches criterion 3: introduces a new benchmark dataset (FPI-Det) for fine-grained human-device interaction, with challenging conditions and baseline analysis. Relevant for embodied AI and spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09667": {
        "authors": [
            "Zhengdi Yu",
            "Simone Foti",
            "Linguang Zhang",
            "Amy Zhao",
            "Cem Keskin",
            "Stefanos Zafeiriou",
            "Tolga Birdal"
        ],
        "title": "Geometric Neural Distance Fields for Learning Human Motion Priors",
        "abstract": "arXiv:2509.09667v1 Announce Type: new  Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to \"roll out\" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.",
        "arxiv_id": "2509.09667",
        "ARXIVID": "2509.09667",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and generative modeling. Proposes a novel geometric neural distance field approach for learning human motion priors, with strong generalization and new geometric integrators.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.09314": {
        "authors": [
            "Thuy Ngoc Nguyen",
            "Anita Williams Woolley",
            "Cleotilde Gonzalez"
        ],
        "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance",
        "abstract": "arXiv:2509.09314v1 Announce Type: new  Abstract: Coordinated teamwork is essential in fast-paced decision-making environments that require dynamic adaptation, often without an opportunity for explicit communication. Although implicit coordination has been extensively considered in the existing literature, the majority of work has focused on co-located, synchronous teamwork (such as sports teams) or, in distributed teams, primarily on coordination of knowledge work. However, many teams (firefighters, military, law enforcement, emergency response) must coordinate their movements in physical space without the benefit of visual cues or extensive explicit communication. This paper investigates how three dimensions of spatial coordination, namely exploration diversity, movement specialization, and adaptive spatial proximity, influence team performance in a collaborative online search and rescue task where explicit communication is restricted and team members rely on movement patterns to infer others' intentions and coordinate actions. Our metrics capture the relational aspects of teamwork by measuring spatial proximity, distribution patterns, and alignment of movements within shared environments. We analyze data from 34 four-person teams (136 participants) assigned to specialized roles in a search and rescue task. Results show that spatial specialization positively predicts performance, while adaptive spatial proximity exhibits a marginal inverted U-shaped relationship, suggesting moderate levels of adaptation are optimal. Furthermore, the temporal dynamics of these metrics differentiate high- from low-performing teams over time. These findings provide insights into implicit spatial coordination in role-based teamwork and highlight the importance of balanced adaptive strategies, with implications for training and AI-assisted team support systems.",
        "arxiv_id": "2509.09314",
        "ARXIVID": "2509.09314",
        "COMMENT": "Matches criterion 1 (new metrics and analysis for spatial coordination and intelligence in teams, with implications for embodied AI and spatial understanding).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09527": {
        "authors": [
            "Jian Zhu",
            "Xin Zou",
            "Xi Wang",
            "Ning Zhang",
            "Bian Wu",
            "Yao Yang",
            "Ying Zhou",
            "Lingfang Zeng",
            "Chang Tang",
            "Cheng Luo"
        ],
        "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
        "abstract": "arXiv:2509.09527v1 Announce Type: new  Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced under the influence of deep learning. By integrating heterogeneous data from multiple views, MVC enhances clustering analysis, making multi-view fusion critical to clustering performance. However, there is a problem of low-quality data in multi-view fusion. This problem primarily arises from two reasons: 1) Certain views are contaminated by noisy data. 2) Some views suffer from missing data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF) method to address this problem. SGDF leverages a multiple generative mechanism for the multi-view feature of each sample. It is robust to low-quality data. Building on SGDF, we further present the Generative Diffusion Contrastive Network (GDCN). Extensive experiments show that GDCN achieves the state-of-the-art results in deep MVC tasks. The source code is publicly available at https://github.com/HackerHyper/GDCN.",
        "arxiv_id": "2509.09527",
        "ARXIVID": "2509.09527",
        "COMMENT": "This paper proposes a generative diffusion contrastive network for multi-view clustering. It uses generative diffusion models in a multi-modal context, which is relevant to generative modeling in multi-modal learning (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.09610": {
        "authors": [
            "Daria Laslo",
            "Efthymios Georgiou",
            "Marius George Linguraru",
            "Andreas Rauschecker",
            "Sabine Muller",
            "Catherine R. Jutzeler",
            "Sarah Bruningk"
        ],
        "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth",
        "abstract": "arXiv:2509.09610v1 Announce Type: new  Abstract: Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
        "arxiv_id": "2509.09610",
        "ARXIVID": "2509.09610",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) and generative modeling, as it combines mechanistic models with diffusion models for spatio-temporal prediction in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09172": {
        "authors": [
            "Chunxiao Li",
            "Xiaoxiao Wang",
            "Meiling Li",
            "Boming Miao",
            "Peng Sun",
            "Yunjian Zhang",
            "Xiangyang Ji",
            "Yao Zhu"
        ],
        "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios",
        "abstract": "arXiv:2509.09172v1 Announce Type: new  Abstract: With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.",
        "arxiv_id": "2509.09172",
        "ARXIVID": "2509.09172",
        "COMMENT": "Partially matches criterion 4 (benchmarking vision-language models for AI-generated image detection in real-world scenarios), but focus is more on detection robustness than foundation model development or application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09054": {
        "authors": [
            "Binxu Li",
            "Wei Peng",
            "Mingjie Li",
            "Ehsan Adeli",
            "Kilian M. Pohl"
        ],
        "title": "Integrating Anatomical Priors into a Causal Diffusion Model",
        "abstract": "arXiv:2509.09054v1 Announce Type: new  Abstract: 3D brain MRI studies often examine subtle morphometric differences between cohorts that are hard to detect visually. Given the high cost of MRI acquisition, these studies could greatly benefit from image syntheses, particularly counterfactual image generation, as seen in other domains, such as computer vision. However, counterfactual models struggle to produce anatomically plausible MRIs due to the lack of explicit inductive biases to preserve fine-grained anatomical details. This shortcoming arises from the training of the models aiming to optimize for the overall appearance of the images (e.g., via cross-entropy) rather than preserving subtle, yet medically relevant, local variations across subjects. To preserve subtle variations, we propose to explicitly integrate anatomical constraints on a voxel-level as prior into a generative diffusion framework. Called Probabilistic Causal Graph Model (PCGM), the approach captures anatomical constraints via a probabilistic graph module and translates those constraints into spatial binary masks of regions where subtle variations occur. The masks (encoded by a 3D extension of ControlNet) constrain a novel counterfactual denoising UNet, whose encodings are then transferred into high-quality brain MRIs via our 3D diffusion decoder. Extensive experiments on multiple datasets demonstrate that PCGM generates structural brain MRIs of higher quality than several baseline approaches. Furthermore, we show for the first time that brain measurements extracted from counterfactuals (generated by PCGM) replicate the subtle effects of a disease on cortical brain regions previously reported in the neuroscience literature. This achievement is an important milestone in the use of synthetic MRIs in studies investigating subtle morphological differences.",
        "arxiv_id": "2509.09054",
        "ARXIVID": "2509.09054",
        "COMMENT": "Somewhat related to criterion 4: proposes a new method for integrating anatomical priors into diffusion models for brain MRI, which is a generative model in vision, but not a general-purpose vision foundation model.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.09495": {
        "authors": [
            "Victor Livernoche",
            "Akshatha Arodi",
            "Andreea Musulan",
            "Zachary Yang",
            "Adam Salvail",
            "Ga\\'etan Marceau Caron",
            "Jean-Fran\\c{c}ois Godbout",
            "Reihaneh Rabbany"
        ],
        "title": "OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection",
        "abstract": "arXiv:2509.09495v1 Announce Type: new  Abstract: Deepfakes, synthetic media created using advanced AI techniques, have intensified the spread of misinformation, particularly in politically sensitive contexts. Existing deepfake detection datasets are often limited, relying on outdated generation methods, low realism, or single-face imagery, restricting the effectiveness for general synthetic image detection. By analyzing social media posts, we identify multiple modalities through which deepfakes propagate misinformation. Furthermore, our human perception study demonstrates that recently developed proprietary models produce synthetic images increasingly indistinguishable from real ones, complicating accurate identification by the general public. Consequently, we present a comprehensive, politically-focused dataset specifically crafted for benchmarking detection against modern generative models. This dataset contains three million real images paired with descriptive captions, which are used for generating 963k corresponding high-quality synthetic images from a mix of proprietary and open-source models. Recognizing the continual evolution of generative techniques, we introduce an innovative crowdsourced adversarial platform, where participants are incentivized to generate and submit challenging synthetic images. This ongoing community-driven initiative ensures that deepfake detection methods remain robust and adaptive, proactively safeguarding public discourse from sophisticated misinformation threats.",
        "arxiv_id": "2509.09495",
        "ARXIVID": "2509.09495",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications), as it provides a large-scale dataset for deepfake detection using generative models, but does not introduce new methods for spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.09541": {
        "authors": [
            "Hala Hawashin",
            "Mina Abbaszadeh",
            "Nicholas Joseph",
            "Beth Pearson",
            "Martha Lewis",
            "Mehrnoosh sadrzadeh"
        ],
        "title": "Compositional Concept Generalization with Variational Quantum Circuits",
        "abstract": "arXiv:2509.09541v1 Announce Type: new  Abstract: Compositional generalization is a key facet of human cognition, but lacking in current AI tools such as vision-language models. Previous work examined whether a compositional tensor-based sentence semantics can overcome the challenge, but led to negative results. We conjecture that the increased training efficiency of quantum models will improve performance in these tasks. We interpret the representations of compositional tensor-based models in Hilbert spaces and train Variational Quantum Circuits to learn these representations on an image captioning task requiring compositional generalization. We used two image encoding techniques: a multi-hot encoding (MHE) on binary image vectors and an angle/amplitude encoding on image vectors taken from the vision-language model CLIP. We achieve good proof-of-concept results using noisy MHE encodings. Performance on CLIP image vectors was more mixed, but still outperformed classical compositional models.",
        "arxiv_id": "2509.09541",
        "ARXIVID": "2509.09541",
        "COMMENT": "Touches on compositional generalization in vision-language models (criterion 2), but the main novelty is the use of quantum circuits, which is less central to the listed criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.09327": {
        "authors": [
            "Dimitrios Anastasiou",
            "Razvan Caramalau",
            "Nazir Sirajudeen",
            "Matthew Boal",
            "Philip Edwards",
            "Justin Collins",
            "John Kelly",
            "Ashwin Sridhar",
            "Maxine Tran",
            "Faiz Mumtaz",
            "Nevil Pavithran",
            "Nader Francis",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos"
        ],
        "title": "Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment",
        "abstract": "arXiv:2509.09327v1 Announce Type: new  Abstract: Automated surgical skill assessment (SSA) is a central task in surgical computer vision. Developing robust SSA models is challenging due to the scarcity of skill annotations, which are time-consuming to produce and require expert consensus. Few-shot learning (FSL) offers a scalable alternative enabling model development with minimal supervision, though its success critically depends on effective pre-training. While widely studied for several surgical downstream tasks, pre-training has remained largely unexplored in SSA. In this work, we formulate SSA as a few-shot task and investigate how self-supervised pre-training strategies affect downstream few-shot SSA performance. We annotate a publicly available robotic surgery dataset with Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate various pre-training sources across three few-shot settings. We quantify domain similarity and analyze how domain gap and the inclusion of procedure-specific data into pre-training influence transferability. Our results show that small but domain-relevant datasets can outperform large scale, less aligned ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot settings, respectively. Moreover, incorporating procedure-specific data into pre-training with a domain-relevant external dataset significantly boosts downstream performance, with an average gain of +1.22% in accuracy and +2.28% in F1-score; however, applying the same strategy with less similar but large-scale sources can instead lead to performance degradation. Code and models are available at https://github.com/anastadimi/ssa-fsl.",
        "arxiv_id": "2509.09327",
        "ARXIVID": "2509.09327",
        "COMMENT": "Somewhat related to criteria 4 (vision foundation models and applications) as it explores pre-training strategies for surgical skill assessment, but does not introduce a new VLLM/MLLM or foundation model. Focuses on empirical analysis of pre-training domain similarity for few-shot learning in medical vision.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.09677": {
        "authors": [
            "Akshit Sinha",
            "Arvindh Arun",
            "Shashwat Goel",
            "Steffen Staab",
            "Jonas Geiping"
        ],
        "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
        "abstract": "arXiv:2509.09677v1 Announce Type: new  Abstract: Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
        "arxiv_id": "2509.09677",
        "ARXIVID": "2509.09677",
        "COMMENT": "This paper investigates long-horizon execution in LLMs, focusing on execution and reasoning over long tasks. While interesting for LLMs, it does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09365": {
        "authors": [
            "Xiaodong Wang",
            "Ping Wang",
            "Zhangyuan Li",
            "Xin Yuan"
        ],
        "title": "Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection",
        "abstract": "arXiv:2509.09365v1 Announce Type: new  Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.",
        "arxiv_id": "2509.09365",
        "ARXIVID": "2509.09365",
        "COMMENT": "Somewhat related to criterion 4: explores plug-and-play diffusion models for image compressive sensing, which is a generative vision model, but not a foundation model or directly about spatial intelligence or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.09183": {
        "authors": [
            "Jiasheng Guo",
            "Xin Gao",
            "Yuxiang Yan",
            "Guanghao Li",
            "Jian Pu"
        ],
        "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection",
        "abstract": "arXiv:2509.09183v1 Announce Type: new  Abstract: Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.",
        "arxiv_id": "2509.09183",
        "ARXIVID": "2509.09183",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it proposes a new ISP plugin for low-light object detection, but not directly about spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.09429": {
        "authors": [
            "Peisong Wen",
            "Qianqian Xu",
            "Siran Dai",
            "Runmin Cong",
            "Qingming Huang"
        ],
        "title": "Semantic Concentration for Self-Supervised Dense Representations Learning",
        "abstract": "arXiv:2509.09429v1 Announce Type: new  Abstract: Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.",
        "arxiv_id": "2509.09429",
        "ARXIVID": "2509.09429",
        "COMMENT": "Relevant to general computer vision and self-supervised learning, but does not directly match any specific criterion (no embodied AI, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.08926": {
        "authors": [
            "Waqar Ahmad",
            "Evan Murphy",
            "Vladimir A. Krylov"
        ],
        "title": "Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures",
        "abstract": "arXiv:2509.08926v1 Announce Type: new  Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed.The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: https://github.com/waqar3411/Beta-SOD",
        "arxiv_id": "2509.08926",
        "ARXIVID": "2509.08926",
        "COMMENT": "Does not directly match any specific criterion. Focuses on statistical outlier detection for object re-identification, which is not about spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09290": {
        "authors": [
            "Anthony P. Addison",
            "Felix Wagner",
            "Wentian Xu",
            "Natalie Voets",
            "Konstantinos Kamnitsas"
        ],
        "title": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training",
        "abstract": "arXiv:2509.09290v1 Announce Type: new  Abstract: Segmentation models are important tools for the detection and analysis of lesions in brain MRI. Depending on the type of brain pathology that is imaged, MRI scanners can acquire multiple, different image modalities (contrasts). Most segmentation models for multimodal brain MRI are restricted to fixed modalities and cannot effectively process new ones at inference. Some models generalize to unseen modalities but may lose discriminative modality-specific information. This work aims to develop a model that can perform inference on data that contain image modalities unseen during training, previously seen modalities, and heterogeneous combinations of both, thus allowing a user to utilize any available imaging modalities. We demonstrate this is possible with a simple, thus practical alteration to the U-net architecture, by integrating a modality-agnostic input channel or pathway, alongside modality-specific input channels. To train this modality-agnostic component, we develop an image augmentation scheme that synthesizes artificial MRI modalities. Augmentations differentially alter the appearance of pathological and healthy brain tissue to create artificial contrasts between them while maintaining realistic anatomical integrity. We evaluate the method using 8 MRI databases that include 5 types of pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI, DWI, ADC and FLAIR). The results demonstrate that the approach preserves the ability to effectively process MRI modalities encountered during training, while being able to process new, unseen modalities to improve its segmentation. Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG",
        "arxiv_id": "2509.09290",
        "ARXIVID": "2509.09290",
        "COMMENT": "Does not directly match any specific criterion. Focuses on medical image segmentation with modality-agnostic input channels, which is outside the main scope of spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09210": {
        "authors": [
            "Xing Gao",
            "Zherui Huang",
            "Weiyao Lin",
            "Xiao Sun"
        ],
        "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting",
        "abstract": "arXiv:2509.09210v1 Announce Type: new  Abstract: Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.",
        "arxiv_id": "2509.09210",
        "ARXIVID": "2509.09210",
        "COMMENT": "Relevant to multi-agent motion forecasting and spatio-temporal modeling, but does not directly address embodied AI benchmarks, spatial intelligence in embodied agents, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09272": {
        "authors": [
            "Vaibhav Chaudhary",
            "Neha Soni",
            "Narotam Singh",
            "Amita Kapoor"
        ],
        "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs",
        "abstract": "arXiv:2509.09272v1 Announce Type: new  Abstract: Knowledge graphs, a powerful tool for structuring information through relational triplets, have recently become the new front-runner in enhancing question-answering systems. While traditional Retrieval Augmented Generation (RAG) approaches are proficient in fact-based and local context-based extraction from concise texts, they encounter limitations when addressing the thematic and holistic understanding of complex, extensive texts, requiring a deeper analysis of both text and context. This paper presents a comprehensive technical comparative study of three different methodologies for constructing knowledge graph triplets and integrating them with Large Language Models (LLMs) for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all leveraging open source technologies. We evaluate the effectiveness, feasibility, and adaptability of these methods by analyzing their capabilities, state of development, and their impact on the performance of LLM-based question answering. Experimental results indicate that while OpenIE provides the most comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning abilities among the three. We conclude with a discussion on the strengths and limitations of each method and provide insights into future directions for improving knowledge graph-based question answering.",
        "arxiv_id": "2509.09272",
        "ARXIVID": "2509.09272",
        "COMMENT": "This paper compares knowledge graph-based question answering methods with LLMs. It is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.09558": {
        "authors": [
            "Akshit Achara",
            "Esther Puyol Anton",
            "Alexander Hammers",
            "Andrew P. King"
        ],
        "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification",
        "abstract": "arXiv:2509.09558v1 Announce Type: new  Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR",
        "arxiv_id": "2509.09558",
        "ARXIVID": "2509.09558",
        "COMMENT": "This paper explores demographic bias in MRI-based Alzheimer's classification using deep learning. It is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models, but rather fairness in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.09006": {
        "authors": [
            "Samuel Felipe dos Santos",
            "Tiago Agostinho de Almeida",
            "Jurandy Almeida"
        ],
        "title": "E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting",
        "abstract": "arXiv:2509.09006v1 Announce Type: new  Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a labeled source to an unlabeled target domain without assuming any relationship between their label sets, requiring models to classify known samples while rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet) use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization (OEM). However, this strategy treats all classifiers equally, diluting the learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet), which integrates a dynamic weighting strategy to OEM. By leveraging the closed-set classifier's predictions, E-MLNet focuses adaptation on the most relevant class boundaries for each target sample, sharpening the distinction between known and unknown classes. We conduct extensive experiments on four challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The results demonstrate that E-MLNet achieves the highest average H-scores on VisDA and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet outperforms the strong MLNet baseline in the majority of individual adaptation tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of 31 in the Open-Set DA setting -- confirming the benefits of our focused adaptation strategy.",
        "arxiv_id": "2509.09006",
        "ARXIVID": "2509.09006",
        "COMMENT": "This paper proposes a new method for universal domain adaptation with sample-specific weighting, but it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about domain adaptation in vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.08949": {
        "authors": [
            "Yibin Wang",
            "Wondimagegn Beshah",
            "Padmanava Dash",
            "Haifeng Wang"
        ],
        "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery",
        "abstract": "arXiv:2509.08949v1 Announce Type: new  Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the current decade. They have significantly advanced remote sensing with the capability to deploy and image the terrain as per required spatial, spectral, temporal, and radiometric resolutions for various remote sensing applications. One of the major advantages of UAS imagery is that images can be acquired in cloudy conditions by flying the UAS under the clouds. The limitation to the technology is that the imagery is often sullied by cloud shadows. Images taken over water are additionally affected by sun glint. These are two pose serious issues for estimating water quality parameters from the UAS images. This study proposes a novel machine learning approach first to identify and extract regions with cloud shadows and sun glint and separate such regions from non-obstructed clear sky regions and sun-glint unaffected regions. The data was extracted from the images at pixel level to train an U-Net based deep learning model and best settings for model training was identified based on the various evaluation metrics from test cases. Using this evaluation, a high-quality image correction model was determined, which was used to recover the cloud shadow and sun glint areas in the images.",
        "arxiv_id": "2509.08949",
        "ARXIVID": "2509.08949",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it applies deep learning to UAS imagery correction, but does not introduce new methods for spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}