{
    "2506.06218": {
        "authors": [
            "Christian Fruhwirth-Reisinger",
            "Du\\v{s}an Mali\\'c",
            "Wei Lin",
            "David Schinagl",
            "Samuel Schulter",
            "Horst Possegger"
        ],
        "title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving",
        "abstract": "arXiv:2506.06218v1 Announce Type: new  Abstract: We introduce STSBench, a scenario-based framework to benchmark the holistic understanding of vision-language models (VLMs) for autonomous driving. The framework automatically mines pre-defined traffic scenarios from any dataset using ground-truth annotations, provides an intuitive user interface for efficient human verification, and generates multiple-choice questions for model evaluation. Applied to the NuScenes dataset, we present STSnu, the first benchmark that evaluates the spatio-temporal reasoning capabilities of VLMs based on comprehensive 3D perception. Existing benchmarks typically target off-the-shelf or fine-tuned VLMs for images or videos from a single viewpoint and focus on semantic tasks such as object recognition, dense captioning, risk assessment, or scene understanding. In contrast, STSnu evaluates driving expert VLMs for end-to-end driving, operating on videos from multi-view cameras or LiDAR. It specifically assesses their ability to reason about both ego-vehicle actions and complex interactions among traffic participants, a crucial capability for autonomous vehicles. The benchmark features 43 diverse scenarios spanning multiple views and frames, resulting in 971 human-verified multiple-choice questions. A thorough evaluation uncovers critical shortcomings in existing models' ability to reason about fundamental traffic dynamics in complex environments. These findings highlight the urgent need for architectural advances that explicitly model spatio-temporal reasoning. By addressing a core gap in spatio-temporal evaluation, STSBench enables the development of more robust and explainable VLMs for autonomous driving.",
        "arxiv_id": "2506.06218",
        "ARXIVID": "2506.06218",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (STSBench) for spatio-temporal reasoning in autonomous driving, focusing on novel evaluation angles.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.05414": {
        "authors": [
            "Mingfei Chen",
            "Zijun Cui",
            "Xiulong Liu",
            "Jinlin Xiang",
            "Caleb Zheng",
            "Jingyuan Li",
            "Eli Shlizerman"
        ],
        "title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
        "abstract": "arXiv:2506.05414v1 Announce Type: new  Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.",
        "arxiv_id": "2506.05414",
        "ARXIVID": "2506.05414",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a benchmark (SAVVY-Bench) for 3D spatial reasoning in dynamic audio-visual environments, focusing on embodied AI and spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.05904": {
        "authors": [
            "Yichi Zhang",
            "Xin Luna Dong",
            "Zhaojiang Lin",
            "Andrea Madotto",
            "Anuj Kumar",
            "Babak Damavandi",
            "Joyce Chai",
            "Seungwhan Moon"
        ],
        "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
        "abstract": "arXiv:2506.05904v1 Announce Type: new  Abstract: Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/",
        "arxiv_id": "2506.05904",
        "ARXIVID": "2506.05904",
        "COMMENT": "Matches criterion 3. Proposes a framework for proactive assistant dialogue generation from egocentric videos, which is relevant to embodied AI and novel methods for real-time task guidance.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.06279": {
        "authors": [
            "Shi Liu",
            "Weijie Su",
            "Xizhou Zhu",
            "Wenhai Wang",
            "Jifeng Dai"
        ],
        "title": "CoMemo: LVLMs Need Image Context with Image Memory",
        "abstract": "arXiv:2506.06279v1 Announce Type: new  Abstract: Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.",
        "arxiv_id": "2506.06279",
        "ARXIVID": "2506.06279",
        "COMMENT": "Matches criterion 2 as it introduces CoMemo, a novel LVLM architecture addressing limitations in multimodal processing with innovative positional encoding and memory mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.05543": {
        "authors": [
            "Sethuraman TV",
            "Savya Khosla",
            "Vignesh Srinivasakumar",
            "Jiahui Huang",
            "Seoung Wug Oh",
            "Simon Jenni",
            "Derek Hoiem",
            "Joon-Young Lee"
        ],
        "title": "FRAME: Pre-Training Video Feature Representations via Anticipation and Memory",
        "abstract": "arXiv:2506.05543v1 Announce Type: new  Abstract: Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame. However, existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks. We address this gap with FRAME, a self-supervised video frame encoder tailored for dense video understanding. FRAME learns to predict current and future DINO patch features from past and present RGB frames, leading to spatially precise and temporally coherent representations. To our knowledge, FRAME is the first video encoder to leverage image-based models for dense prediction while outperforming them on tasks requiring fine-grained visual correspondence. As an auxiliary capability, FRAME aligns its class token with CLIP's semantic space, supporting language-driven tasks such as video classification. We evaluate FRAME across six dense prediction tasks on seven datasets, where it consistently outperforms image encoders and existing self-supervised video models. Despite its versatility, FRAME maintains a compact architecture suitable for a range of downstream applications.",
        "arxiv_id": "2506.05543",
        "ARXIVID": "2506.05543",
        "COMMENT": "Matches criterion 1 as it proposes a new video encoder (FRAME) for dense video understanding, improving spatial and temporal feature consistency.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.06097": {
        "authors": [
            "Zikang Wang",
            "Boyu Chen",
            "Zhengrong Yue",
            "Yi Wang",
            "Yu Qiao",
            "Limin Wang",
            "Yali Wang"
        ],
        "title": "VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning",
        "abstract": "arXiv:2506.06097v1 Announce Type: new  Abstract: The recent advance in video understanding has been driven by multimodal large language models (MLLMs). But these MLLMs are good at analyzing short videos, while suffering from difficulties in understanding videos with a longer context. To address this difficulty, several agent paradigms have recently been proposed, using MLLMs as agents for retrieving extra contextual knowledge in a long video. However, most existing agents ignore the key fact that a long video is composed with multiple shots, i.e., to answer the user question from a long video, it is critical to deeply understand its relevant shots like human. Without such insight, these agents often mistakenly find redundant even noisy temporal context, restricting their capacity for long video understanding. To fill this gap, we propose VideoChat-A1, a novel long video agent paradigm. Different from the previous works, our VideoChat-A1 can deeply think with long videos, via a distinct chain-of-shot reasoning paradigm. More specifically, it can progressively select the relevant shots of user question, and look into these shots in a coarse-to-fine partition. By multi-modal reasoning along the shot chain, VideoChat-A1 can effectively mimic step-by-step human thinking process, allowing to interactively discover preferable temporal context for thoughtful understanding in long videos. Extensive experiments show that, our VideoChat-A1 achieves the state-of-the-art performance on the mainstream long video QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema, outperforming its strong baselines (e.g., Intern2.5VL-8B and InternVideo2.5-8B), by up to 10.8\\% and 6.2\\%. Compared to leading close-source GPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with 7\\% input frames and 12\\% inference time on average.",
        "arxiv_id": "2506.06097",
        "ARXIVID": "2506.06097",
        "COMMENT": "Matches criterion 2 as it discusses a novel MLLM (VideoChat-A1) for long video understanding with chain-of-shot reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.05384": {
        "authors": [
            "Zhuoxuan Cai",
            "Jian Zhang",
            "Xinbin Yuan",
            "Pengtao Jiang",
            "Wenxiang Chen",
            "Bowen Tang",
            "Lujian Yao",
            "Qiyuan Wang",
            "Jinwen Chen",
            "Bo Li"
        ],
        "title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment",
        "abstract": "arXiv:2506.05384v1 Announce Type: new  Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks.",
        "arxiv_id": "2506.05384",
        "ARXIVID": "2506.05384",
        "COMMENT": "Matches criterion 2. Proposes a unified training pipeline for reasoning-based visual quality assessment, which is relevant to VLLMs and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05872": {
        "authors": [
            "Yu Li",
            "Xingyu Qiu",
            "Yuqian Fu",
            "Jie Chen",
            "Tianwen Qian",
            "Xu Zheng",
            "Danda Pani Paudel",
            "Yanwei Fu",
            "Xuanjing Huang",
            "Luc Van Gool",
            "Yu-Gang Jiang"
        ],
        "title": "Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection",
        "abstract": "arXiv:2506.05872v1 Announce Type: new  Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.",
        "arxiv_id": "2506.05872",
        "ARXIVID": "2506.05872",
        "COMMENT": "Matches criterion 4. Proposes a novel compositional image generation framework for cross-domain few-shot object detection, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05558": {
        "authors": [
            "Andreas Meuleman",
            "Ishaan Shah",
            "Alexandre Lanvin",
            "Bernhard Kerbl",
            "George Drettakis"
        ],
        "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images",
        "abstract": "arXiv:2506.05558v1 Announce Type: new  Abstract: Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that our solution can provide on-the-fly processing of all the capture scenarios and scene sizes we target while remaining competitive with other methods that only handle specific capture styles or scene sizes in speed, image quality, or both.",
        "arxiv_id": "2506.05558",
        "ARXIVID": "2506.05558",
        "COMMENT": "Matches criterion 1. Proposes a novel method for on-the-fly reconstruction and spatial understanding in large-scale scenes, which is relevant to spatial intelligence and embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05529": {
        "authors": [
            "Rodney Sanchez",
            "Ferat Sahin",
            "Alexander Ororbia",
            "Jamison Heard"
        ],
        "title": "Avoiding Death through Fear Intrinsic Conditioning",
        "abstract": "arXiv:2506.05529v1 Announce Type: new  Abstract: Biological and psychological concepts have inspired reinforcement learning algorithms to create new complex behaviors that expand agents' capacity. These behaviors can be seen in the rise of techniques like goal decomposition, curriculum, and intrinsic rewards, which have paved the way for these complex behaviors. One limitation in evaluating these methods is the requirement for engineered extrinsic for realistic environments. A central challenge in engineering the necessary reward function(s) comes from these environments containing states that carry high negative rewards, but provide no feedback to the agent. Death is one such stimuli that fails to provide direct feedback to the agent. In this work, we introduce an intrinsic reward function inspired by early amygdala development and produce this intrinsic reward through a novel memory-augmented neural network (MANN) architecture. We show how this intrinsic motivation serves to deter exploration of terminal states and results in avoidance behavior similar to fear conditioning observed in animals. Furthermore, we demonstrate how modifying a threshold where the fear response is active produces a range of behaviors that are described under the paradigm of general anxiety disorders (GADs). We demonstrate this behavior in the Miniworld Sidewalk environment, which provides a partially observable Markov decision process (POMDP) and a sparse reward with a non-descriptive terminal condition, i.e., death. In effect, this study results in a biologically-inspired neural architecture and framework for fear conditioning paradigms; we empirically demonstrate avoidance behavior in a constructed agent that is able to solve environments with non-descriptive terminal conditions.",
        "arxiv_id": "2506.05529",
        "ARXIVID": "2506.05529",
        "COMMENT": "Matches criterion 3 as it introduces a novel biologically-inspired framework for embodied AI with intrinsic motivation for avoiding terminal states.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05883": {
        "authors": [
            "Daming Wang",
            "Yuhao Song",
            "Zijian He",
            "Kangliang Chen",
            "Xing Pan",
            "Lu Deng",
            "Weihao Gu"
        ],
        "title": "HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios",
        "abstract": "arXiv:2506.05883v1 Announce Type: new  Abstract: We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving framework that implements the slow branch of a cognitively inspired fast-slow architecture. A fast controller outputs low-level steering, throttle, and brake commands, while a slow planner-a large vision-language model-generates high-level intents such as \"yield to pedestrian\" or \"merge after the truck\" without compromising latency. HMVLM introduces three upgrades: (1) selective five-view prompting with an embedded 4s history of ego kinematics, (2) multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding -> Driving Decision -> Trajectory Inference reasoning flow, and (3) spline-based trajectory post-processing that removes late-stage jitter and sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025 Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public baseline by 2.77%.",
        "arxiv_id": "2506.05883",
        "ARXIVID": "2506.05883",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model (HMVLM) for autonomous driving with multi-stage reasoning, which is relevant to vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05473": {
        "authors": [
            "Jinhyung Park",
            "Yihan Hu",
            "Chensheng Peng",
            "Wenzhao Zheng",
            "Kris Kitani",
            "Wei Zhan"
        ],
        "title": "S2GO: Streaming Sparse Gaussian Occupancy Prediction",
        "abstract": "arXiv:2506.05473v1 Announce Type: new  Abstract: Despite the demonstrated efficiency and performance of sparse query-based representations for perception, state-of-the-art 3D occupancy prediction methods still rely on voxel-based or dense Gaussian-based 3D representations. However, dense representations are slow, and they lack flexibility in capturing the temporal dynamics of driving scenes. Distinct from prior work, we instead summarize the scene into a compact set of 3D queries which are propagated through time in an online, streaming fashion. These queries are then decoded into semantic Gaussians at each timestep. We couple our framework with a denoising rendering objective to guide the queries and their constituent Gaussians in effectively capturing scene geometry. Owing to its efficient, query-based representation, S2GO achieves state-of-the-art performance on the nuScenes and KITTI occupancy benchmarks, outperforming prior art (e.g., GaussianWorld) by 1.5 IoU with 5.9x faster inference.",
        "arxiv_id": "2506.05473",
        "ARXIVID": "2506.05473",
        "COMMENT": "Matches criterion 3 as it proposes a novel method (S2GO) for 3D occupancy prediction with a focus on efficiency and temporal dynamics, which is a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05667": {
        "authors": [
            "Yuhan Hao",
            "Zhengning Li",
            "Lei Sun",
            "Weilong Wang",
            "Naixin Yi",
            "Sheng Song",
            "Caihong Qin",
            "Mofan Zhou",
            "Yifei Zhan",
            "Peng Jia",
            "Xianpeng Lang"
        ],
        "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models",
        "abstract": "arXiv:2506.05667v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by users of production-level autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from users' actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving.",
        "arxiv_id": "2506.05667",
        "ARXIVID": "2506.05667",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (DriveAction) for Vision-Language-Action models in autonomous driving, focusing on human-like decisions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05719": {
        "authors": [
            "Jingshun Huang",
            "Haitao Lin",
            "Tianyu Wang",
            "Yanwei Fu",
            "Yu-Gang Jiang",
            "Xiangyang Xue"
        ],
        "title": "You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping",
        "abstract": "arXiv:2506.05719v1 Announce Type: new  Abstract: This paper addresses the problem of category-level pose estimation for articulated objects in robotic manipulation tasks. Recent works have shown promising results in estimating part pose and size at the category level. However, these approaches primarily follow a complex multi-stage pipeline that first segments part instances in the point cloud and then estimates the Normalized Part Coordinate Space (NPCS) representation for 6D poses. These approaches suffer from high computational costs and low performance in real-time robotic tasks. To address these limitations, we propose YOEO, a single-stage method that simultaneously outputs instance segmentation and NPCS representations in an end-to-end manner. We use a unified network to generate point-wise semantic labels and centroid offsets, allowing points from the same part instance to vote for the same centroid. We further utilize a clustering algorithm to distinguish points based on their estimated centroid distances. Finally, we first separate the NPCS region of each instance. Then, we align the separated regions with the real point cloud to recover the final pose and size. Experimental results on the GAPart dataset demonstrate the pose estimation capabilities of our proposed single-shot method. We also deploy our synthetically-trained model in a real-world setting, providing real-time visual feedback at 200Hz, enabling a physical Kinova robot to interact with unseen articulated objects. This showcases the utility and effectiveness of our proposed method.",
        "arxiv_id": "2506.05719",
        "ARXIVID": "2506.05719",
        "COMMENT": "Matches criterion 3 as it proposes a novel single-stage method for articulated object pose estimation in robotic manipulation, focusing on embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05787": {
        "authors": [
            "Ivan Rodin",
            "Tz-Ying Wu",
            "Kyle Min",
            "Sharath Nittur Sridhar",
            "Antonino Furnari",
            "Subarna Tripathi",
            "Giovanni Maria Farinella"
        ],
        "title": "EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs",
        "abstract": "arXiv:2506.05787v1 Announce Type: new  Abstract: We introduce EASG-Bench, a question-answering benchmark for egocentric videos where the question-answering pairs are created from spatio-temporally grounded dynamic scene graphs capturing intricate relationships among actors, actions, and objects. We propose a systematic evaluation framework and evaluate several language-only and video large language models (video-LLMs) on this benchmark. We observe a performance gap in language-only and video-LLMs, especially on questions focusing on temporal ordering, thus identifying a research gap in the area of long-context video understanding. To promote the reproducibility of our findings and facilitate further research, the benchmark and accompanying code are available at the following GitHub page: https://github.com/fpv-iplab/EASG-bench.",
        "arxiv_id": "2506.05787",
        "ARXIVID": "2506.05787",
        "COMMENT": "Matches criterion 3 as it introduces EASG-Bench, a new benchmark for egocentric video question answering with dynamic scene graphs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06277": {
        "authors": [
            "Han Yi",
            "Yulu Pan",
            "Feihong He",
            "Xinyu Liu",
            "Benjamin Zhang",
            "Oluwatumininu Oguntola",
            "Gedas Bertasius"
        ],
        "title": "ExAct: A Video-Language Benchmark for Expert Action Analysis",
        "abstract": "arXiv:2506.06277v1 Announce Type: new  Abstract: We present ExAct, a new video-language benchmark for expert-level understanding of skilled physical human activities. Our new benchmark contains 3521 expert-curated video question-answer pairs spanning 11 physical activities in 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct requires the correct answer to be selected from five carefully designed candidate options, thus necessitating a nuanced, fine-grained, expert-level understanding of physical human skills. Evaluating the recent state-of-the-art VLMs on ExAct reveals a substantial performance gap relative to human expert performance. Specifically, the best-performing GPT-4o model achieves only 44.70% accuracy, well below the 82.02% attained by trained human specialists/experts. We believe that ExAct will be beneficial for developing and evaluating VLMs capable of precise understanding of human skills in various physical and procedural domains. Dataset and code are available at https://texaser.github.io/exact_project_page/",
        "arxiv_id": "2506.06277",
        "ARXIVID": "2506.06277",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (ExAct) for video-language understanding with a focus on expert-level human activities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.05587": {
        "authors": [
            "Junjie Xing",
            "Yeye He",
            "Mengyu Zhou",
            "Haoyu Dong",
            "Shi Han",
            "Lingjiao Chen",
            "Dongmei Zhang",
            "Surajit Chaudhuri",
            "H. V. Jagadish"
        ],
        "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark",
        "abstract": "arXiv:2506.05587v1 Announce Type: new  Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.   In this work, we introduce MMTU, a large-scale benchmark with over 30K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis. Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.",
        "arxiv_id": "2506.05587",
        "ARXIVID": "2506.05587",
        "COMMENT": "Matches criterion 4. Introduces a new benchmark for table understanding and reasoning, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06076": {
        "authors": [
            "Julio Silva-Rodr\\'iguez",
            "Leo Fillioux",
            "Paul-Henry Courn\\`ede",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis",
            "Ismail Ben Ayed",
            "Jose Dolz"
        ],
        "title": "Full Conformal Adaptation of Medical Vision-Language Models",
        "abstract": "arXiv:2506.06076v1 Announce Type: new  Abstract: Vision-language models (VLMs) pre-trained at large scale have shown unprecedented transferability capabilities and are being progressively integrated into medical image analysis. Although its discriminative potential has been widely explored, its reliability aspect remains overlooked. This work investigates their behavior under the increasingly popular split conformal prediction (SCP) framework, which theoretically guarantees a given error level on output sets by leveraging a labeled calibration set. However, the zero-shot performance of VLMs is inherently limited, and common practice involves few-shot transfer learning pipelines, which cannot absorb the rigid exchangeability assumptions of SCP. To alleviate this issue, we propose full conformal adaptation, a novel setting for jointly adapting and conformalizing pre-trained foundation models, which operates transductively over each test data point using a few-shot adaptation set. Moreover, we complement this framework with SS-Text, a novel training-free linear probe solver for VLMs that alleviates the computational cost of such a transductive approach. We provide comprehensive experiments using 3 different modality-specialized medical VLMs and 9 adaptation tasks. Our framework requires exactly the same data as SCP, and provides consistent relative improvements of up to 27% on set efficiency while maintaining the same coverage guarantees.",
        "arxiv_id": "2506.06076",
        "ARXIVID": "2506.06076",
        "COMMENT": "Matches criterion 2 as it focuses on adapting vision-language models for medical applications, with a novel conformal adaptation framework.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06006": {
        "authors": [
            "Yifu Qiu",
            "Yftah Ziser",
            "Anna Korhonen",
            "Shay B. Cohen",
            "Edoardo M. Ponti"
        ],
        "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models",
        "abstract": "arXiv:2506.06006v1 Announce Type: new  Abstract: To what extent do vision-and-language foundation models possess a realistic world model (observation $\\times$ action $\\rightarrow$ observation) and a dynamics model (observation $\\times$ observation $\\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.",
        "arxiv_id": "2506.06006",
        "ARXIVID": "2506.06006",
        "COMMENT": "Matches criterion 2 as it explores multimodal foundation models and their ability to model world and dynamics, which is relevant to vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.05782": {
        "authors": [
            "Wei-Cheng Lin",
            "Chih-Ming Lien",
            "Chen Lo",
            "Chia-Hung Yeh"
        ],
        "title": "GazeNLQ @ Ego4D Natural Language Queries Challenge 2025",
        "abstract": "arXiv:2506.05782v1 Announce Type: new  Abstract: This report presents our solution to the Ego4D Natural Language Queries (NLQ) Challenge at CVPR 2025. Egocentric video captures the scene from the wearer's perspective, where gaze serves as a key non-verbal communication cue that reflects visual attention and offer insights into human intention and cognition. Motivated by this, we propose a novel approach, GazeNLQ, which leverages gaze to retrieve video segments that match given natural language queries. Specifically, we introduce a contrastive learning-based pretraining strategy for gaze estimation directly from video. The estimated gaze is used to augment video representations within proposed model, thereby enhancing localization accuracy. Experimental results show that GazeNLQ achieves R1@IoU0.3 and R1@IoU0.5 scores of 27.82 and 18.68, respectively. Our code is available at https://github.com/stevenlin510/GazeNLQ.",
        "arxiv_id": "2506.05782",
        "ARXIVID": "2506.05782",
        "COMMENT": "Matches criterion 3 as it proposes a novel approach (GazeNLQ) for egocentric video understanding using gaze estimation, which is a novel angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06220": {
        "authors": [
            "Diji Yang",
            "Minghao Liu",
            "Chung-Hsiang Lo",
            "Yi Zhang",
            "James Davis"
        ],
        "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
        "abstract": "arXiv:2506.06220v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind, that is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction.",
        "arxiv_id": "2506.06220",
        "ARXIVID": "2506.06220",
        "COMMENT": "Matches criterion 2 as it proposes a generative visual feedback method (GenIR) for mental image retrieval, leveraging diffusion-based image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06242": {
        "authors": [
            "Zahra Babaiee",
            "Peyman M. Kiasari",
            "Daniela Rus",
            "Radu Grosu"
        ],
        "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models",
        "abstract": "arXiv:2506.06242v1 Announce Type: new  Abstract: Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'-the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systems' capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path/cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}",
        "arxiv_id": "2506.06242",
        "ARXIVID": "2506.06242",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal large language models (MLLMs) and their conceptual reasoning capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.06281": {
        "authors": [
            "Muhammad Sohail Danish",
            "Muhammad Akhtar Munir",
            "Syed Roshaan Ali Shah",
            "Muhammad Haris Khan",
            "Rao Muhammad Anwer",
            "Jorma Laaksonen",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation",
        "abstract": "arXiv:2506.06281v1 Announce Type: new  Abstract: Modern Earth observation (EO) increasingly leverages deep learning to harness the scale and diversity of satellite imagery across sensors and regions. While recent foundation models have demonstrated promising generalization across EO tasks, many remain limited by the scale, geographical coverage, and spectral diversity of their training data, factors critical for learning globally transferable representations. In this work, we introduce TerraFM, a scalable self-supervised learning model that leverages globally distributed Sentinel-1 and Sentinel-2 imagery, combined with large spatial tiles and land-cover aware sampling to enrich spatial and semantic coverage. By treating sensing modalities as natural augmentations in our self-supervised approach, we unify radar and optical inputs via modality-specific patch embeddings and adaptive cross-attention fusion. Our training strategy integrates local-global contrastive learning and introduces a dual-centering mechanism that incorporates class-frequency-aware regularization to address long-tailed distributions in land cover.TerraFM achieves strong generalization on both classification and segmentation tasks, outperforming prior models on GEO-Bench and Copernicus-Bench. Our code and pretrained models are publicly available at: https://github.com/mbzuai-oryx/TerraFM .",
        "arxiv_id": "2506.06281",
        "ARXIVID": "2506.06281",
        "COMMENT": "Matches criterion 4 as it introduces a scalable foundation model (TerraFM) for Earth observation with multisensor data.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.05425": {
        "authors": [
            "Fanqi Kong",
            "Weiqin Zu",
            "Xinyu Chen",
            "Yaodong Yang",
            "Song-Chun Zhu",
            "Xue Feng"
        ],
        "title": "SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning",
        "abstract": "arXiv:2506.05425v1 Announce Type: new  Abstract: The rich and multifaceted nature of human social interaction, encompassing multimodal cues, unobservable relations and mental states, and dynamical behavior, presents a formidable challenge for artificial intelligence. To advance research in this area, we introduce SIV-Bench, a novel video benchmark for rigorously evaluating the capabilities of Multimodal Large Language Models (MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and 8,792 meticulously generated question-answer pairs derived from a human-LLM collaborative pipeline. It is originally collected from TikTok and YouTube, covering a wide range of video genres, presentation styles, and linguistic and cultural backgrounds. It also includes a dedicated setup for analyzing the impact of different textual cues-original on-screen text, added dialogue, or no text. Our comprehensive experiments on leading MLLMs reveal that while models adeptly handle SSU, they significantly struggle with SSR and SDP, where Relation Inference (RI) is an acute bottleneck, as further examined in our analysis. Our study also confirms the critical role of transcribed dialogue in aiding comprehension of complex social interactions. By systematically identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial insights to steer the development of more socially intelligent AI. The dataset and code are available at https://kfq20.github.io/sivbench/.",
        "arxiv_id": "2506.05425",
        "ARXIVID": "2506.05425",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SIV-Bench) for evaluating MLLMs in social interaction understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.05965": {
        "authors": [
            "Mingrui Li",
            "Yiming Zhou",
            "Hongxing Zhou",
            "Xinggang Hu",
            "Florian Roemer",
            "Hongyu Wang",
            "Ahmad Osman"
        ],
        "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments",
        "abstract": "arXiv:2506.05965v1 Announce Type: new  Abstract: Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
        "arxiv_id": "2506.05965",
        "ARXIVID": "2506.05965",
        "COMMENT": "Matches criterion 3 as it introduces a new SLAM method for dynamic environments, focusing on monocular RGB input and dynamic scene challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.05765": {
        "authors": [
            "Taiga Shinozaki",
            "Tomoki Doi",
            "Satoshi Nishida",
            "Hitomi Yanaka"
        ],
        "title": "Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?",
        "abstract": "arXiv:2506.05765v1 Announce Type: new  Abstract: Humans are susceptible to optical illusions, which serve as valuable tools for investigating sensory and cognitive processes. Inspired by human vision studies, research has begun exploring whether machines, such as large vision language models (LVLMs), exhibit similar susceptibilities to visual illusions. However, studies often have used non-abstract images and have not distinguished actual and apparent features, leading to ambiguous assessments of machine cognition. To address these limitations, we introduce a visual question answering (VQA) dataset, categorized into genuine and fake illusions, along with corresponding control images. Genuine illusions present discrepancies between actual and apparent features, whereas fake illusions have the same actual and apparent features even though they look illusory due to the similar geometric configuration. We evaluate the performance of LVLMs for genuine and fake illusion VQA tasks and investigate whether the models discern actual and apparent features. Our findings indicate that although LVLMs may appear to recognize illusions by correctly answering questions about both feature types, they predict the same answers for both Genuine Illusion and Fake Illusion VQA questions. This suggests that their responses might be based on prior knowledge of illusions rather than genuine visual understanding. The dataset is available at https://github.com/ynklab/FILM",
        "arxiv_id": "2506.05765",
        "ARXIVID": "2506.05765",
        "COMMENT": "Matches criterion 2 as it evaluates large vision-language models (LVLMs) on visual illusions, providing insights into their reasoning capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05405": {
        "authors": [
            "Shiwei Lin",
            "Chenxu Wang",
            "Xiaozhen Ding",
            "Yi Wang",
            "Boyuan Du",
            "Lei Song",
            "Chenggang Wang",
            "Huaping Liu"
        ],
        "title": "A VLM-based Method for Visual Anomaly Detection in Robotic Scientific Laboratories",
        "abstract": "arXiv:2506.05405v1 Announce Type: new  Abstract: In robot scientific laboratories, visual anomaly detection is important for the timely identification and resolution of potential faults or deviations. It has become a key factor in ensuring the stability and safety of experimental processes. To address this challenge, this paper proposes a VLM-based visual reasoning approach that supports different levels of supervision through four progressively informative prompt configurations. To systematically evaluate its effectiveness, we construct a visual benchmark tailored for process anomaly detection in scientific workflows. Experiments on two representative vision-language models show that detection accuracy improves as more contextual information is provided, confirming the effectiveness and adaptability of the proposed reasoning approach for process anomaly detection in scientific workflows. Furthermore, real-world validations at selected experimental steps confirm that first-person visual observation can effectively identify process-level anomalies. This work provides both a data-driven foundation and an evaluation framework for vision anomaly detection in scientific experiment workflows.",
        "arxiv_id": "2506.05405",
        "ARXIVID": "2506.05405",
        "COMMENT": "Matches criterion 2 as it applies Vision-Language Models to visual anomaly detection in robotic scientific laboratories.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05442": {
        "authors": [
            "Hao Jiang",
            "Chuan Hu",
            "Yukang Shi",
            "Yuan He",
            "Ke Wang",
            "Xi Zhang",
            "Zhipeng Zhang"
        ],
        "title": "Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving",
        "abstract": "arXiv:2506.05442v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) offer a promising approach to end-to-end autonomous driving due to their human-like reasoning capabilities. However, troublesome gaps remains between current VLMs and real-world autonomous driving applications. One major limitation is that existing datasets with loosely formatted language descriptions are not machine-friendly and may introduce redundancy. Additionally, high computational cost and massive scale of VLMs hinder the inference speed and real-world deployment. To bridge the gap, this paper introduces a structured and concise benchmark dataset, NuScenes-S, which is derived from the NuScenes dataset and contains machine-friendly structured representations. Moreover, we present FastDrive, a compact VLM baseline with 0.9B parameters. In contrast to existing VLMs with over 7B parameters and unstructured language processing(e.g., LLaVA-1.5), FastDrive understands structured and concise descriptions and generates machine-friendly driving decisions with high efficiency. Extensive experiments show that FastDrive achieves competitive performance on structured dataset, with approximately 20% accuracy improvement on decision-making tasks, while surpassing massive parameter baseline in inference speed with over 10x speedup. Additionally, ablation studies further focus on the impact of scene annotations (e.g., weather, time of day) on decision-making tasks, demonstrating their importance on decision-making tasks in autonomous driving.",
        "arxiv_id": "2506.05442",
        "ARXIVID": "2506.05442",
        "COMMENT": "Matches criterion 2 as it focuses on a compact Vision-Language Model (FastDrive) for autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.06144": {
        "authors": [
            "David Wan",
            "Han Wang",
            "Elias Stengel-Eskin",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
        "abstract": "arXiv:2506.06144v1 Announce Type: new  Abstract: Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.",
        "arxiv_id": "2506.06144",
        "ARXIVID": "2506.06144",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal retriever for video content, leveraging a unified multimodal backbone.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05551": {
        "authors": [
            "Yan Shu",
            "Hangui Lin",
            "Yexin Liu",
            "Yan Zhang",
            "Gangyan Zeng",
            "Yan Li",
            "Yu Zhou",
            "Ser-Nam Lim",
            "Harry Yang",
            "Nicu Sebe"
        ],
        "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding",
        "abstract": "arXiv:2506.05551v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.",
        "arxiv_id": "2506.05551",
        "ARXIVID": "2506.05551",
        "COMMENT": "Matches criterion 2 as it addresses hallucination issues in large multimodal models for scene text understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05546": {
        "authors": [
            "Vadim Tschernezki",
            "Diane Larlus",
            "Andrea Vedaldi",
            "Iro Laina"
        ],
        "title": "Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos",
        "abstract": "arXiv:2506.05546v1 Announce Type: new  Abstract: Computer vision is largely based on 2D techniques, with 3D vision still relegated to a relatively narrow subset of applications. However, by building on recent advances in 3D models such as neural radiance fields, some authors have shown that 3D techniques can at last improve outputs extracted from independent 2D views, by fusing them into 3D and denoising them. This is particularly helpful in egocentric videos, where the camera motion is significant, but only under the assumption that the scene itself is static. In fact, as shown in the recent analysis conducted by EPIC Fields, 3D techniques are ineffective when it comes to studying dynamic phenomena, and, in particular, when segmenting moving objects. In this paper, we look into this issue in more detail. First, we propose to improve dynamic segmentation in 3D by fusing motion segmentation predictions from a 2D-based model into layered radiance fields (Layered Motion Fusion). However, the high complexity of long, dynamic videos makes it challenging to capture the underlying geometric structure, and, as a result, hinders the fusion of motion cues into the (incomplete) scene geometry. We address this issue through test-time refinement, which helps the model to focus on specific frames, thereby reducing the data complexity. This results in a synergy between motion fusion and the refinement, and in turn leads to segmentation predictions of the 3D model that surpass the 2D baseline by a large margin. This demonstrates that 3D techniques can enhance 2D analysis even for dynamic phenomena in a challenging and realistic setting.",
        "arxiv_id": "2506.05546",
        "ARXIVID": "2506.05546",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for dynamic segmentation in 3D egocentric videos, addressing previously ignored challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05689": {
        "authors": [
            "Hugues Thomas",
            "Chen Chen",
            "Jian Zhang"
        ],
        "title": "Pts3D-LLM: Studying the Impact of Token Structure for 3D Scene Understanding With Large Language Models",
        "abstract": "arXiv:2506.05689v1 Announce Type: new  Abstract: Effectively representing 3D scenes for Multimodal Large Language Models (MLLMs) is crucial yet challenging. Existing approaches commonly only rely on 2D image features and use varied tokenization approaches. This work presents a rigorous study of 3D token structures, systematically comparing video-based and point-based representations while maintaining consistent model backbones and parameters. We propose a novel approach that enriches visual tokens by incorporating 3D point cloud features from a Sonata pretrained Point Transformer V3 encoder. Our experiments demonstrate that merging explicit 3D features significantly boosts performance. Furthermore, we show that point-based token structures can rival video-based ones when the points are cleverly sampled and ordered. Our best models from both structures achieve state-of-the-art results on multiple 3D understanding benchmarks. We emphasize our analysis of token structures as a key contribution, alongside transparent reporting of results averaged over multiple seeds, a practice we believe is vital for robust progress in the field.",
        "arxiv_id": "2506.05689",
        "ARXIVID": "2506.05689",
        "COMMENT": "Matches criterion 2 as it studies token structures for 3D scene understanding with multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.06275": {
        "authors": [
            "Emmanouil Zaranis",
            "Ant\\'onio Farinhas",
            "Saul Santos",
            "Beatriz Canaverde",
            "Miguel Moura Ramos",
            "Aditya K Surikuchi",
            "Andr\\'e Viveiros",
            "Baohao Liao",
            "Elena Bueno-Benito",
            "Nithin Sivakumaran",
            "Pavlo Vasylenko",
            "Shoubin Yu",
            "Sonal Sannigrahi",
            "Wafaa Mohammed",
            "Ben Peters",
            "Danae S\\'anchez Villegas",
            "Elias Stengel-Eskin",
            "Giuseppe Attanasio",
            "Jaehong Yoon",
            "Stella Frank",
            "Alessandro Suglia",
            "Chrysoula Zerva",
            "Desmond Elliott",
            "Mariella Dimiccoli",
            "Mohit Bansal",
            "Oswald Lanz",
            "Raffaella Bernardi",
            "Raquel Fern\\'andez",
            "Sandro Pezzelle",
            "Vlad Niculae",
            "Andr\\'e F. T. Martins"
        ],
        "title": "Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding",
        "abstract": "arXiv:2506.06275v1 Announce Type: new  Abstract: Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, ``needle-in-a-haystack'' details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced by language models themselves) that are easier for models to answer but fail to reflect genuine understanding. In this paper, we introduce MF$^2$, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information from full-length movies (50-170 minutes long). MF$^2$ includes over 50 full-length, open-licensed movies, each paired with manually constructed sets of claim pairs -- one true (fact) and one plausible but false (fib), totalling over 850 pairs. These claims target core narrative elements such as character motivations and emotions, causal chains, and event order, and refer to memorable moments that humans can recall without rewatching the movie. Instead of multiple-choice formats, we adopt a binary claim evaluation protocol: for each pair, models must correctly identify both the true and false claims. This reduces biases like answer ordering and enables a more precise assessment of reasoning. Our experiments demonstrate that both open-weight and closed state-of-the-art models fall well short of human performance, underscoring the relative ease of the task for humans and their superior ability to retain and reason over critical narrative information -- an ability current VLMs lack.",
        "arxiv_id": "2506.06275",
        "ARXIVID": "2506.06275",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating long-form video understanding, focusing on narrative comprehension.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.05599": {
        "authors": [
            "Mo Zhou",
            "Keren Ye",
            "Mauricio Delbracio",
            "Peyman Milanfar",
            "Vishal M. Patel",
            "Hossein Talebi"
        ],
        "title": "UniRes: Universal Image Restoration for Complex Degradations",
        "abstract": "arXiv:2506.05599v1 Announce Type: new  Abstract: Real-world image restoration is hampered by diverse degradations stemming from varying capture conditions, capture devices and post-processing pipelines. Existing works make improvements through simulating those degradations and leveraging image generative priors, however generalization to in-the-wild data remains an unresolved problem. In this paper, we focus on complex degradations, i.e., arbitrary mixtures of multiple types of known degradations, which is frequently seen in the wild. A simple yet flexible diffusionbased framework, named UniRes, is proposed to address such degradations in an end-to-end manner. It combines several specialized models during the diffusion sampling steps, hence transferring the knowledge from several well-isolated restoration tasks to the restoration of complex in-the-wild degradations. This only requires well-isolated training data for several degradation types. The framework is flexible as extensions can be added through a unified formulation, and the fidelity-quality trade-off can be adjusted through a new paradigm. Our proposed method is evaluated on both complex-degradation and single-degradation image restoration datasets. Extensive qualitative and quantitative experimental results show consistent performance gain especially for images with complex degradations.",
        "arxiv_id": "2506.05599",
        "ARXIVID": "2506.05599",
        "COMMENT": "Does not match any specific criterion but is related to vision foundation models and their applications in image restoration.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2506.05488": {
        "authors": [
            "Mary Aiyetigbo",
            "Wanqi Yuan",
            "Feng Luo",
            "Nianyi Li"
        ],
        "title": "Implicit Neural Representation for Video Restoration",
        "abstract": "arXiv:2506.05488v1 Announce Type: new  Abstract: High-resolution (HR) videos play a crucial role in many computer vision applications. Although existing video restoration (VR) methods can significantly enhance video quality by exploiting temporal information across video frames, they are typically trained for fixed upscaling factors and lack the flexibility to handle scales or degradations beyond their training distribution. In this paper, we introduce VR-INR, a novel video restoration approach based on Implicit Neural Representations (INRs) that is trained only on a single upscaling factor ($\\times 4$) but generalizes effectively to arbitrary, unseen super-resolution scales at test time. Notably, VR-INR also performs zero-shot denoising on noisy input, despite never having seen noisy data during training. Our method employs a hierarchical spatial-temporal-texture encoding framework coupled with multi-resolution implicit hash encoding, enabling adaptive decoding of high-resolution and noise-suppressed frames from low-resolution inputs at any desired magnification. Experimental results show that VR-INR consistently maintains high-quality reconstructions at unseen scales and noise during training, significantly outperforming state-of-the-art approaches in sharpness, detail preservation, and denoising efficacy.",
        "arxiv_id": "2506.05488",
        "ARXIVID": "2506.05488",
        "COMMENT": "Does not match any specific criteria. Focuses on video restoration using implicit neural representations, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05745": {
        "authors": [
            "Emil Biju",
            "Shayan Talaei",
            "Zhemin Huang",
            "Mohammadreza Pourreza",
            "Azalia Mirhoseini",
            "Amin Saberi"
        ],
        "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models",
        "abstract": "arXiv:2506.05745v1 Announce Type: new  Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we show that the models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to ~39% fewer sequential tokens on problems requiring more than 8000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65% reduction in average sequential tokens for longer reasoning trajectories, while achieving the performance of the fine-tuned reasoning model.",
        "arxiv_id": "2506.05745",
        "ARXIVID": "2506.05745",
        "COMMENT": "Does not match any specific criteria. Focuses on improving reasoning models with parallelized execution, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06085": {
        "authors": [
            "Koulischer Felix",
            "Handke Florian",
            "Deleu Johannes",
            "Demeester Thomas",
            "Ambrogioni Luca"
        ],
        "title": "Feedback Guidance of Diffusion Models",
        "abstract": "arXiv:2506.06085v1 Announce Type: new  Abstract: While Classifier-Free Guidance (CFG) has become standard for improving sample fidelity in conditional diffusion models, it can harm diversity and induce memorization by applying constant guidance regardless of whether a particular sample needs correction. We propose FeedBack Guidance (FBG), which uses a state-dependent coefficient to self-regulate guidance amounts based on need. Our approach is derived from first principles by assuming the learned conditional distribution is linearly corrupted by the unconditional distribution, contrasting with CFG's implicit multiplicative assumption. Our scheme relies on feedback of its own predictions about the conditional signal informativeness to adapt guidance dynamically during inference, challenging the view of guidance as a fixed hyperparameter. The approach is benchmarked on ImageNet512x512, where it significantly outperforms Classifier-Free Guidance and is competitive to Limited Interval Guidance (LIG) while benefitting from a strong mathematical framework. On Text-To-Image generation, we demonstrate that, as anticipated, our approach automatically applies higher guidance scales for complex prompts than for simpler ones and that it can be easily combined with existing guidance schemes such as CFG or LIG.",
        "arxiv_id": "2506.06085",
        "ARXIVID": "2506.06085",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling with diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06271": {
        "authors": [
            "Jonathan Schmidt",
            "Simon Giebenhain",
            "Matthias Niessner"
        ],
        "title": "BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading",
        "abstract": "arXiv:2506.06271v1 Announce Type: new  Abstract: We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin.",
        "arxiv_id": "2506.06271",
        "ARXIVID": "2506.06271",
        "COMMENT": "This paper does not directly match any of the criteria but is related to relightable avatars, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05501": {
        "authors": [
            "Kaihang Pan",
            "Wendong Bu",
            "Yuruo Wu",
            "Yang Wu",
            "Kai Shen",
            "Yunfei Li",
            "Hang Zhao",
            "Juncheng Li",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "title": "FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL",
        "abstract": "arXiv:2506.05501v1 Announce Type: new  Abstract: Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark -- featuring test cases of paired prompts with similar syntax but different fine-grained semantics -- reveals that existing models struggle with fine-grained text-image alignment thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing a novel reinforcement learning algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves state-of-the-art performance on existing text-to-image benchmarks and significantly outperforms prior methods on PairComp.",
        "arxiv_id": "2506.05501",
        "ARXIVID": "2506.05501",
        "COMMENT": "This paper does not directly match any of the criteria but is related to fine-grained text-to-image alignment, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05361": {
        "authors": [
            "Tinglin Huang",
            "Tianyu Liu",
            "Mehrtash Babadi",
            "Wengong Jin",
            "Rex Ying"
        ],
        "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching",
        "abstract": "arXiv:2506.05361v1 Announce Type: new  Abstract: Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.",
        "arxiv_id": "2506.05361",
        "ARXIVID": "2506.05361",
        "COMMENT": "Does not match any specific criteria but involves generative modeling and spatial transcriptomics, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06035": {
        "authors": [
            "Shiyi Zhang",
            "Dong Liang",
            "Hairong Zheng",
            "Yihang Zhou"
        ],
        "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion",
        "abstract": "arXiv:2506.06035v1 Announce Type: new  Abstract: Reconstructing visual information from brain activity bridges the gap between neuroscience and computer vision. Even though progress has been made in decoding images from fMRI using generative models, a challenge remains in accurately recovering highly complex visual stimuli. This difficulty stems from their elemental density and diversity, sophisticated spatial structures, and multifaceted semantic information.   To address these challenges, we propose HAVIR that contains two adapters: (1) The AutoKL Adapter transforms fMRI voxels into a latent diffusion prior, capturing topological structures; (2) The CLIP Adapter converts the voxels to CLIP text and image embeddings, containing semantic information. These complementary representations are fused by Versatile Diffusion to generate the final reconstructed image. To extract the most essential semantic information from complex scenarios, the CLIP Adapter is trained with text captions describing the visual stimuli and their corresponding semantic images synthesized from these captions. The experimental results demonstrate that HAVIR effectively reconstructs both structural features and semantic information of visual stimuli even in complex scenarios, outperforming existing models.",
        "arxiv_id": "2506.06035",
        "ARXIVID": "2506.06035",
        "COMMENT": "Does not match any specific criteria but involves generative modeling and neuroscience, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05897": {
        "authors": [
            "Xin Zhang",
            "Dongdong Meng",
            "Sheng Li"
        ],
        "title": "Query Nearby: Offset-Adjusted Mask2Former enhances small-organ segmentation",
        "abstract": "arXiv:2506.05897v1 Announce Type: new  Abstract: Medical segmentation plays an important role in clinical applications like radiation therapy and surgical guidance, but acquiring clinically acceptable results is difficult. In recent years, progress has been witnessed with the success of utilizing transformer-like models, such as combining the attention mechanism with CNN. In particular, transformer-based segmentation models can extract global information more effectively, compensating for the drawbacks of CNN modules that focus on local features. However, utilizing transformer architecture is not easy, because training transformer-based models can be resource-demanding. Moreover, due to the distinct characteristics in the medical field, especially when encountering mid-sized and small organs with compact regions, their results often seem unsatisfactory. For example, using ViT to segment medical images directly only gives a DSC of less than 50\\%, which is far lower than the clinically acceptable score of 80\\%. In this paper, we used Mask2Former with deformable attention to reduce computation and proposed offset adjustment strategies to encourage sampling points within the same organs during attention weights computation, thereby integrating compact foreground information better. Additionally, we utilized the 4th feature map in Mask2Former to provide a coarse location of organs, and employed an FCN-based auxiliary head to help train Mask2Former more quickly using Dice loss. We show that our model achieves SOTA (State-of-the-Art) performance on the HaNSeg and SegRap2023 datasets, especially on mid-sized and small organs.Our code is available at link https://github.com/earis/Offsetadjustment\\_Background-location\\_Decoder\\_Mask2former.",
        "arxiv_id": "2506.05897",
        "ARXIVID": "2506.05897",
        "COMMENT": "Does not match any specific criteria but is related to medical image segmentation using transformer-based models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06261": {
        "authors": [
            "Jihwan Jeong",
            "Xiaoyu Wang",
            "Jingmin Wang",
            "Scott Sanner",
            "Pascal Poupart"
        ],
        "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
        "abstract": "arXiv:2506.06261v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
        "arxiv_id": "2506.06261",
        "ARXIVID": "2506.06261",
        "COMMENT": "Does not match any specific criteria but is related to offline reinforcement learning and planning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06276": {
        "authors": [
            "Jiatao Gu",
            "Tianrong Chen",
            "David Berthelot",
            "Huangjie Zheng",
            "Yuyang Wang",
            "Ruixiang Zhang",
            "Laurent Dinh",
            "Miguel Angel Bautista",
            "Josh Susskind",
            "Shuangfei Zhai"
        ],
        "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
        "abstract": "arXiv:2506.06276v1 Announce Type: new  Abstract: We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",
        "arxiv_id": "2506.06276",
        "ARXIVID": "2506.06276",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05573": {
        "authors": [
            "Yuchen Lin",
            "Chenguo Lin",
            "Panwang Pan",
            "Honglei Yan",
            "Yiqiang Feng",
            "Yadong Mu",
            "Katerina Fragkiadaki"
        ],
        "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers",
        "abstract": "arXiv:2506.05573v1 Announce Type: new  Abstract: We introduce PartCrafter, the first structured 3D generative model that jointly synthesizes multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image. Unlike existing methods that either produce monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an image and then reconstructing each segment, PartCrafter adopts a unified, compositional generation architecture that does not rely on pre-segmented inputs. Conditioned on a single image, it simultaneously denoises multiple 3D parts, enabling end-to-end part-aware generation of both individual objects and complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh diffusion transformer (DiT) trained on whole objects, inheriting the pretrained weights, encoder, and decoder, and introduces two key innovations: (1) A compositional latent space, where each 3D part is represented by a set of disentangled latent tokens; (2) A hierarchical attention mechanism that enables structured information flow both within individual parts and across all parts, ensuring global coherence while preserving part-level detail during generation. To support part-level supervision, we curate a new dataset by mining part-level annotations from large-scale 3D object datasets. Experiments show that PartCrafter outperforms existing approaches in generating decomposable 3D meshes, including parts that are not directly visible in input images, demonstrating the strength of part-aware generative priors for 3D understanding and synthesis. Code and training data will be released.",
        "arxiv_id": "2506.05573",
        "ARXIVID": "2506.05573",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and 3D vision, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05934": {
        "authors": [
            "Yixuan Zhu",
            "Haolin Wang",
            "Shilin Ma",
            "Wenliang Zhao",
            "Yansong Tang",
            "Lei Chen",
            "Jie Zhou"
        ],
        "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing",
        "abstract": "arXiv:2506.05934v1 Announce Type: new  Abstract: Recent advancements in diffusion frameworks have significantly enhanced video editing, achieving high fidelity and strong alignment with textual prompts. However, conventional approaches using image diffusion models fall short in handling video dynamics, particularly for challenging temporal edits like motion adjustments. While current video diffusion models produce high-quality results, adapting them for efficient editing remains difficult due to the heavy computational demands that prevent the direct application of previous image editing techniques. To overcome these limitations, we introduce FADE, a training-free yet highly effective video editing approach that fully leverages the inherent priors from pre-trained video diffusion models via frequency-aware factorization. Rather than simply using these models, we first analyze the attention patterns within the video model to reveal how video priors are distributed across different components. Building on these insights, we propose a factorization strategy to optimize each component's specialized role. Furthermore, we devise spectrum-guided modulation to refine the sampling trajectory with frequency domain cues, preventing information leakage and supporting efficient, versatile edits while preserving the basic spatial and temporal structure. Extensive experiments on real-world videos demonstrate that our method consistently delivers high-quality, realistic and temporally coherent editing results both qualitatively and quantitatively. Code is available at https://github.com/EternalEvan/FADE .",
        "arxiv_id": "2506.05934",
        "ARXIVID": "2506.05934",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in video editing, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.05754": {
        "authors": [
            "Emmanuel Anaya Gonzalez",
            "Sairam Vaidya",
            "Kanghee Park",
            "Ruyi Ji",
            "Taylor Berg-Kirkpatrick",
            "Loris D'Antoni"
        ],
        "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective",
        "abstract": "arXiv:2506.05754v1 Announce Type: new  Abstract: Constrained decoding enables Language Models (LMs) to produce samples that provably satisfy hard constraints. However, existing constrained-decoding approaches often distort the underlying model distribution, a limitation that is especially problematic in applications like program fuzzing, where one wants to generate diverse and valid program inputs for testing purposes. We propose a new constrained sampling framework based on Markov Chain Monte Carlo (MCMC) that simultaneously satisfies three core desiderata: constraint satisfying (every sample satisfies the constraint), monotonically converging (the sampling process converges to the true conditional distribution), and efficient (high-quality samples emerge in few steps). Our method constructs a proposal distribution over valid outputs and applies a Metropolis-Hastings acceptance criterion based on the LM's likelihood, ensuring principled and efficient exploration of the constrained space. Empirically, our sampler outperforms existing methods on both synthetic benchmarks and real-world program fuzzing tasks.",
        "arxiv_id": "2506.05754",
        "ARXIVID": "2506.05754",
        "COMMENT": "Does not match any specific criteria. Focuses on constrained sampling for language models, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05806": {
        "authors": [
            "Haojie Yu",
            "Zhaonian Wang",
            "Yihan Pan",
            "Meng Cheng",
            "Hao Yang",
            "Chao Wang",
            "Tao Xie",
            "Xiaoming Xu",
            "Xiaoming Wei",
            "Xunliang Cai"
        ],
        "title": "LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models",
        "abstract": "arXiv:2506.05806v1 Announce Type: new  Abstract: Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our model's inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively.",
        "arxiv_id": "2506.05806",
        "ARXIVID": "2506.05806",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for real-time avatar applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05843": {
        "authors": [
            "Myungkyu Koo",
            "Subin Kim",
            "Sangkyung Kwak",
            "Jaehyun Nam",
            "Seojin Kim",
            "Jinwoo Shin"
        ],
        "title": "FontAdapter: Instant Font Adaptation in Visual Text Generation",
        "abstract": "arXiv:2506.05843v1 Announce Type: new  Abstract: Text-to-image diffusion models have significantly improved the seamless integration of visual text into diverse image contexts. Recent approaches further improve control over font styles through fine-tuning with predefined font dictionaries. However, adapting unseen fonts outside the preset is computationally expensive, often requiring tens of minutes, making real-time customization impractical. In this paper, we present FontAdapter, a framework that enables visual text generation in unseen fonts within seconds, conditioned on a reference glyph image. To this end, we find that direct training on font datasets fails to capture nuanced font attributes, limiting generalization to new glyphs. To overcome this, we propose a two-stage curriculum learning approach: FontAdapter first learns to extract font attributes from isolated glyphs and then integrates these styles into diverse natural backgrounds. To support this two-stage training scheme, we construct synthetic datasets tailored to each stage, leveraging large-scale online fonts effectively. Experiments demonstrate that FontAdapter enables high-quality, robust font customization across unseen fonts without additional fine-tuning during inference. Furthermore, it supports visual text editing, font style blending, and cross-lingual font transfer, positioning FontAdapter as a versatile framework for font customization tasks.",
        "arxiv_id": "2506.05843",
        "ARXIVID": "2506.05843",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling and visual text generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05821": {
        "authors": [
            "Quansong He",
            "Xiangde Min",
            "Kaishen Wang",
            "Tao He"
        ],
        "title": "FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks",
        "abstract": "arXiv:2506.05821v1 Announce Type: new  Abstract: Medical image segmentation is a critical task in computer vision, with UNet serving as a milestone architecture. The typical component of UNet family is the skip connection, however, their skip connections face two significant limitations: (1) they lack effective interaction between features at different scales, and (2) they rely on simple concatenation or addition operations, which constrain efficient information integration. While recent improvements to UNet have focused on enhancing encoder and decoder capabilities, these limitations remain overlooked. To overcome these challenges, we propose a novel multi-scale feature fusion method that reimagines the UNet decoding process as solving an initial value problem (IVP), treating skip connections as discrete nodes. By leveraging principles from the linear multistep method, we propose an adaptive ordinary differential equation method to enable effective multi-scale feature fusion. Our approach is independent of the encoder and decoder architectures, making it adaptable to various U-Net-like networks. Experiments on ACDC, KiTS2023, MSD brain tumor, and ISIC2017/2018 skin lesion segmentation datasets demonstrate improved feature utilization, reduced network parameters, and maintained high performance. The code is available at https://github.com/nayutayuki/FuseUNet.",
        "arxiv_id": "2506.05821",
        "ARXIVID": "2506.05821",
        "COMMENT": "Does not match any specific criterion but is related to general computer vision and machine learning interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05858": {
        "authors": [
            "Jinjuan Wang",
            "Wenzhang Sun",
            "Ming Li",
            "Yun Zheng",
            "Fanyao Li",
            "Zhulin Tao",
            "Donglin Di",
            "Hao Li",
            "Wei Chen",
            "Xianglin Huang"
        ],
        "title": "ChronoTailor: Harnessing Attention Guidance for Fine-Grained Video Virtual Try-On",
        "abstract": "arXiv:2506.05858v1 Announce Type: new  Abstract: Video virtual try-on aims to seamlessly replace the clothing of a person in a source video with a target garment. Despite significant progress in this field, existing approaches still struggle to maintain continuity and reproduce garment details. In this paper, we introduce ChronoTailor, a diffusion-based framework that generates temporally consistent videos while preserving fine-grained garment details. By employing a precise spatio-temporal attention mechanism to guide the integration of fine-grained garment features, ChronoTailor achieves robust try-on performance. First, ChronoTailor leverages region-aware spatial guidance to steer the evolution of spatial attention and employs an attention-driven temporal feature fusion mechanism to generate more continuous temporal features. This dual approach not only enables fine-grained local editing but also effectively mitigates artifacts arising from video dynamics. Second, ChronoTailor integrates multi-scale garment features to preserve low-level visual details and incorporates a garment-pose feature alignment to ensure temporal continuity during dynamic motion. Additionally, we collect StyleDress, a new dataset featuring intricate garments, varied environments, and diverse poses, offering advantages over existing public datasets, and will be publicly available for research. Extensive experiments show that ChronoTailor maintains spatio-temporal continuity and preserves garment details during motion, significantly outperforming previous methods.",
        "arxiv_id": "2506.05858",
        "ARXIVID": "2506.05858",
        "COMMENT": "This paper does not directly match any of the criteria but is related to video virtual try-on, which is tangentially relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05810": {
        "authors": [
            "Yesheng Zhang",
            "Wenjian Sun",
            "Yuheng Chen",
            "Qingwei Liu",
            "Qi Lin",
            "Rui Zhang",
            "Xu Zhao"
        ],
        "title": "Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction",
        "abstract": "arXiv:2506.05810v1 Announce Type: new  Abstract: Complex interactions among agents present a significant challenge for autonomous driving in real-world scenarios. Recently, a promising approach has emerged, which formulates the interactions of agents as a level-k game framework. It effectively decouples agent policies by hierarchical game levels. However, this framework ignores both the varying driving complexities among agents and the dynamic changes in agent states across game levels, instead treating them uniformly. Consequently, redundant and error-prone computations are introduced into this framework. To tackle the issue, this paper proposes a metric, termed as Trajectory Entropy, to reveal the game status of agents within the level-k game framework. The key insight stems from recognizing the inherit relationship between agent policy uncertainty and the associated driving complexity. Specifically, Trajectory Entropy extracts statistical signals representing uncertainty from the multimodality trajectory prediction results of agents in the game. Then, the signal-to-noise ratio of this signal is utilized to quantify the game status of agents. Based on the proposed Trajectory Entropy, we refine the current level-k game framework through a simple gating mechanism, significantly improving overall accuracy while reducing computational costs. Our method is evaluated on the Waymo and nuPlan datasets, in terms of trajectory prediction, open-loop and closed-loop planning tasks. The results demonstrate the state-of-the-art performance of our method, with precision improved by up to 19.89% for prediction and up to 16.48% for planning.",
        "arxiv_id": "2506.05810",
        "ARXIVID": "2506.05810",
        "COMMENT": "This paper does not directly match any of the criteria but is related to trajectory prediction in autonomous driving, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06023": {
        "authors": [
            "Xingchang Huang",
            "Ashish Kumar Singh",
            "Florian Dubost",
            "Cristina Nader Vasconcelos",
            "Sakar Khattar",
            "Liang Shi",
            "Christian Theobalt",
            "Cengiz Oztireli",
            "Gurprit Singh"
        ],
        "title": "Restereo: Diffusion stereo video generation and restoration",
        "abstract": "arXiv:2506.06023v1 Announce Type: new  Abstract: Stereo video generation has been gaining increasing attention with recent advancements in video diffusion models. However, most existing methods focus on generating 3D stereoscopic videos from monocular 2D videos. These approaches typically assume that the input monocular video is of high quality, making the task primarily about inpainting occluded regions in the warped video while preserving disoccluded areas. In this paper, we introduce a new pipeline that not only generates stereo videos but also enhances both left-view and right-view videos consistently with a single model. Our approach achieves this by fine-tuning the model on degraded data for restoration, as well as conditioning the model on warped masks for consistent stereo generation. As a result, our method can be fine-tuned on a relatively small synthetic stereo video datasets and applied to low-quality real-world videos, performing both stereo video generation and restoration. Experiments demonstrate that our method outperforms existing approaches both qualitatively and quantitatively in stereo video generation from low-resolution inputs.",
        "arxiv_id": "2506.06023",
        "ARXIVID": "2506.06023",
        "COMMENT": "This paper does not directly match any of the criteria but is related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05856": {
        "authors": [
            "Yuqian Fu",
            "Runze Wang",
            "Yanwei Fu",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "title": "Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025",
        "abstract": "arXiv:2506.05856v1 Announce Type: new  Abstract: In this report, we present a cross-view multi-modal object segmentation approach for the object correspondence task in the Ego-Exo4D Correspondence Challenges 2025. Given object queries from one perspective (e.g., ego view), the goal is to predict the corresponding object masks in another perspective (e.g., exo view). To tackle this task, we propose a multimodal condition fusion module that enhances object localization by leveraging both visual masks and textual descriptions as segmentation conditions. Furthermore, to address the visual domain gap between ego and exo views, we introduce a cross-view object alignment module that enforces object-level consistency across perspectives, thereby improving the model's robustness to viewpoint changes. Our proposed method ranked second on the leaderboard of the large-scale Ego-Exo4D object correspondence benchmark. Code will be made available at https://github.com/lovelyqian/ObjectRelator.",
        "arxiv_id": "2506.05856",
        "ARXIVID": "2506.05856",
        "COMMENT": "Does not match any specific criteria but involves multimodal segmentation and cross-view alignment, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06120": {
        "authors": [
            "Zhanwen Liu",
            "Huanna Song",
            "Yang Wang",
            "Nan Yang",
            "Shangyu Xie",
            "Yisheng An",
            "Xiangmo Zhao"
        ],
        "title": "Bidirectional Image-Event Guided Low-Light Image Enhancement",
        "abstract": "arXiv:2506.06120v1 Announce Type: new  Abstract: Under extreme low-light conditions, traditional frame-based cameras, due to their limited dynamic range and temporal resolution, face detail loss and motion blur in captured images. To overcome this bottleneck, researchers have introduced event cameras and proposed event-guided low-light image enhancement algorithms. However, these methods neglect the influence of global low-frequency noise caused by dynamic lighting conditions and local structural discontinuities in sparse event data. To address these issues, we propose an innovative Bidirectional guided Low-light Image Enhancement framework (BiLIE). Specifically, to mitigate the significant low-frequency noise introduced by global illumination step changes, we introduce the frequency high-pass filtering-based Event Feature Enhancement (EFE) module at the event representation level to suppress the interference of low-frequency information, and preserve and highlight the high-frequency edges.Furthermore, we design a Bidirectional Cross Attention Fusion (BCAF) mechanism to acquire high-frequency structures and edges while suppressing structural discontinuities and local noise introduced by sparse event guidance, thereby generating smoother fused representations.Additionally, considering the poor visual quality and color bias in existing datasets, we provide a new dataset (RELIE), with high-quality ground truth through a reliable enhancement scheme. Extensive experimental results demonstrate that our proposed BiLIE outperforms state-of-the-art methods by 0.96dB in PSNR and 0.03 in LPIPS.",
        "arxiv_id": "2506.06120",
        "ARXIVID": "2506.06120",
        "COMMENT": "Does not match any specific criteria but focuses on low-light image enhancement, which may be of general interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05579": {
        "authors": [
            "Quan Shi",
            "Carlos E. Jimenez",
            "Shunyu Yao",
            "Nick Haber",
            "Diyi Yang",
            "Karthik Narasimhan"
        ],
        "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
        "abstract": "arXiv:2506.05579v1 Announce Type: new  Abstract: Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.",
        "arxiv_id": "2506.05579",
        "ARXIVID": "2506.05579",
        "COMMENT": "Does not match any specific criteria but involves human-AI collaboration and reasoning, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06253": {
        "authors": [
            "Yuping He",
            "Yifei Huang",
            "Guo Chen",
            "Lidong Lu",
            "Baoqi Pei",
            "Jilan Xu",
            "Tong Lu",
            "Yoichi Sato"
        ],
        "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision",
        "abstract": "arXiv:2506.06253v1 Announce Type: new  Abstract: Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
        "arxiv_id": "2506.06253",
        "ARXIVID": "2506.06253",
        "COMMENT": "Does not match any specific criteria but is related to video understanding and collaborative intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06254": {
        "authors": [
            "Weizhi Zhang",
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Liangwei Yang",
            "Jingbo Shang",
            "Zhepei Wei",
            "Henry Peng Zou",
            "Zijie Huang",
            "Zhengyang Wang",
            "Yifan Gao",
            "Xiaoman Pan",
            "Lian Xiong",
            "Jingguo Liu",
            "Philip S. Yu",
            "Xian Li"
        ],
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time",
        "abstract": "arXiv:2506.06254v1 Announce Type: new  Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "arxiv_id": "2506.06254",
        "ARXIVID": "2506.06254",
        "COMMENT": "Does not match any specific criteria but is related to personalized LLM agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06155": {
        "authors": [
            "Wenyuan Li",
            "Shunlin Liang",
            "Yuxiang Zhang",
            "Liqin Liu",
            "Keyan Chen",
            "Yongzhe Chen",
            "Han Ma",
            "Jianglei Xu",
            "Yichuan Ma",
            "Shikang Guan",
            "Zhenwei Shi"
        ],
        "title": "A Novel Large-scale Crop Dataset and Dual-stream Transformer Method for Fine-grained Hierarchical Crop Classification from Integrated Hyperspectral EnMAP Data and Multispectral Sentinel-2 Time Series",
        "abstract": "arXiv:2506.06155v1 Announce Type: new  Abstract: Fine-grained crop classification is crucial for precision agriculture and food security monitoring. It requires simultaneous capture of both phenological dynamics (obtained from multi-temporal satellite data like Sentinel-2) and subtle spectral variations (demanding nanometer-scale spectral resolution from hyperspectral imagery). Research combining these two modalities remains scarce currently due to challenges in hyperspectral data acquisition and crop types annotation costs. To address these issues, we construct a hierarchical hyperspectral crop dataset (H2Crop) by integrating 30m-resolution EnMAP hyperspectral data with Sentinel-2 time series. With over one million annotated field parcels organized in a four-tier crop taxonomy, H2Crop establishes a vital benchmark for fine-grained agricultural crop classification and hyperspectral image processing. We propose a dual-stream Transformer architecture that synergistically processes these modalities. It coordinates two specialized pathways: a spectral-spatial Transformer extracts fine-grained signatures from hyperspectral EnMAP data, while a temporal Swin Transformer extracts crop growth patterns from Sentinel-2 time series. The designed hierarchy classification heads with hierarchical fusion then simultaneously delivers multi-level classification across all taxonomic tiers. Experiments demonstrate that adding hyperspectral EnMAP data to Sentinel-2 time series yields a 4.2% average F1-scores improvement (peaking at 6.3%). Extensive comparisons also confirming our method's higher accuracy over existing deep learning approaches for crop type classification and the consistent benefits of hyperspectral data across varying temporal windows and crop change scenarios. Codes and dataset will be available at https://github.com/flyakon/H2Crop and www.glass.hku.hk   Keywords: Crop type classification, precision agriculture, remote sensing, deep learning, hyperspectral data, Sentinel-2 time series, fine-grained crops",
        "arxiv_id": "2506.06155",
        "ARXIVID": "2506.06155",
        "COMMENT": "Does not match any specific criteria. Focuses on crop classification using hyperspectral and multispectral data.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06176": {
        "authors": [
            "Zhenyu Yu",
            "Mohd. Yamani Idna Idris",
            "Pei Wang",
            "Yuelong Xia",
            "Fei Ma",
            "Rizwan Qureshi"
        ],
        "title": "SatelliteFormula: Multi-Modal Symbolic Regression from Remote Sensing Imagery for Physics Discovery",
        "abstract": "arXiv:2506.06176v1 Announce Type: new  Abstract: We propose SatelliteFormula, a novel symbolic regression framework that derives physically interpretable expressions directly from multi-spectral remote sensing imagery. Unlike traditional empirical indices or black-box learning models, SatelliteFormula combines a Vision Transformer-based encoder for spatial-spectral feature extraction with physics-guided constraints to ensure consistency and interpretability. Existing symbolic regression methods struggle with the high-dimensional complexity of multi-spectral data; our method addresses this by integrating transformer representations into a symbolic optimizer that balances accuracy and physical plausibility. Extensive experiments on benchmark datasets and remote sensing tasks demonstrate superior performance, stability, and generalization compared to state-of-the-art baselines. SatelliteFormula enables interpretable modeling of complex environmental variables, bridging the gap between data-driven learning and physical understanding.",
        "arxiv_id": "2506.06176",
        "ARXIVID": "2506.06176",
        "COMMENT": "Does not match any specific criteria. Focuses on symbolic regression for remote sensing imagery.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05655": {
        "authors": [
            "Yimei Liu",
            "Yakun Ju",
            "Yuan Rao",
            "Hao Fan",
            "Junyu Dong",
            "Feng Gao",
            "Qian Du"
        ],
        "title": "Aerial Multi-View Stereo via Adaptive Depth Range Inference and Normal Cues",
        "abstract": "arXiv:2506.05655v1 Announce Type: new  Abstract: Three-dimensional digital urban reconstruction from multi-view aerial images is a critical application where deep multi-view stereo (MVS) methods outperform traditional techniques. However, existing methods commonly overlook the key differences between aerial and close-range settings, such as varying depth ranges along epipolar lines and insensitive feature-matching associated with low-detailed aerial images. To address these issues, we propose an Adaptive Depth Range MVS (ADR-MVS), which integrates monocular geometric cues to improve multi-view depth estimation accuracy. The key component of ADR-MVS is the depth range predictor, which generates adaptive range maps from depth and normal estimates using cross-attention discrepancy learning. In the first stage, the range map derived from monocular cues breaks through predefined depth boundaries, improving feature-matching discriminability and mitigating convergence to local optima. In later stages, the inferred range maps are progressively narrowed, ultimately aligning with the cascaded MVS framework for precise depth regression. Moreover, a normal-guided cost aggregation operation is specially devised for aerial stereo images to improve geometric awareness within the cost volume. Finally, we introduce a normal-guided depth refinement module that surpasses existing RGB-guided techniques. Experimental results demonstrate that ADR-MVS achieves state-of-the-art performance on the WHU, LuoJia-MVS, and M\\\"unchen datasets, while exhibits superior computational complexity.",
        "arxiv_id": "2506.05655",
        "ARXIVID": "2506.05655",
        "COMMENT": "Does not match any specific criteria. Focuses on aerial multi-view stereo and depth estimation improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05917": {
        "authors": [
            "Steven Landgraf",
            "Markus Hillemann",
            "Markus Ulrich"
        ],
        "title": "Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness",
        "abstract": "arXiv:2506.05917v1 Announce Type: new  Abstract: Semantic segmentation is critical for scene understanding but demands costly pixel-wise annotations, attracting increasing attention to semi-supervised approaches to leverage abundant unlabeled data. While semi-supervised segmentation is often promoted as a path toward scalable, real-world deployment, it is astonishing that current evaluation protocols exclusively focus on segmentation accuracy, entirely overlooking reliability and robustness. These qualities, which ensure consistent performance under diverse conditions (robustness) and well-calibrated model confidences as well as meaningful uncertainties (reliability), are essential for safety-critical applications like autonomous driving, where models must handle unpredictable environments and avoid sudden failures at all costs. To address this gap, we introduce the Reliable Segmentation Score (RSS), a novel metric that combines predictive accuracy, calibration, and uncertainty quality measures via a harmonic mean. RSS penalizes deficiencies in any of its components, providing an easy and intuitive way of holistically judging segmentation models. Comprehensive evaluations of UniMatchV2 against its predecessor and a supervised baseline show that semi-supervised methods often trade reliability for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's robustness, they further expose persistent reliability shortcomings. We advocate for a shift in evaluation protocols toward more holistic metrics like RSS to better align semi-supervised learning research with real-world deployment needs.",
        "arxiv_id": "2506.05917",
        "ARXIVID": "2506.05917",
        "COMMENT": "Does not match any specific criterion but discusses evaluation metrics for semi-supervised segmentation, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.05377": {
        "authors": [
            "Shayantani Kar",
            "B. Shresth Bhimrajka",
            "Aditya Kumar",
            "Sahil Gupta",
            "Sourav Ghosh",
            "Subhamita Mukherjee",
            "Shauvik Paul"
        ],
        "title": "An Independent Discriminant Network Towards Identification of Counterfeit Images and Videos",
        "abstract": "arXiv:2506.05377v1 Announce Type: new  Abstract: Rapid spread of false images and videos on online platforms is an emerging problem. Anyone may add, delete, clone or modify people and entities from an image using various editing software which are readily available. This generates false and misleading proof to hide the crime. Now-a-days, these false and counterfeit images and videos are flooding on the internet. These spread false information. Many methods are available in literature for detecting those counterfeit contents but new methods of counterfeiting are also evolving. Generative Adversarial Networks (GAN) are observed to be one effective method as it modifies the context and definition of images producing plausible results via image-to-image translation. This work uses an independent discriminant network that can identify GAN generated image or video. A discriminant network has been created using a convolutional neural network based on InceptionResNetV2. The article also proposes a platform where users can detect forged images and videos. This proposed work has the potential to help the forensics domain to detect counterfeit videos and hidden criminal evidence towards the identification of criminal activities.",
        "arxiv_id": "2506.05377",
        "ARXIVID": "2506.05377",
        "COMMENT": "Does not match any specific criteria. Focuses on counterfeit image detection using GANs, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.05363": {
        "authors": [
            "Yui Tatsumi",
            "Ziyue Zeng",
            "Hiroshi Watanabe"
        ],
        "title": "Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion",
        "abstract": "arXiv:2506.05363v1 Announce Type: new  Abstract: Conventional methods for scalable image coding for humans and machines require the transmission of additional information to achieve scalability. A recent diffusion-based method avoids this by generating human-oriented images from machine-oriented images without extra bitrate. This method, however, uses a single random seed, which may lead to suboptimal image quality. In this paper, we propose a seed selection method that identifies the optimal seed from multiple candidates to improve image quality without increasing the bitrate. To reduce computational cost, the selection is performed based on intermediate outputs obtained from early steps of the reverse diffusion process. Experimental results demonstrate that our method outperforms the baseline across multiple metrics.",
        "arxiv_id": "2506.05363",
        "ARXIVID": "2506.05363",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in image reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.05815": {
        "authors": [
            "Pierluigi Zama Ramirez",
            "Fabio Tosi",
            "Luigi Di Stefano",
            "Radu Timofte",
            "Alex Costanzino",
            "Matteo Poggi",
            "Samuele Salti",
            "Stefano Mattoccia",
            "Zhe Zhang",
            "Yang Yang",
            "Wu Chen",
            "Anlong Ming",
            "Mingshuai Zhao",
            "Mengying Yu",
            "Shida Gao",
            "Xiangfeng Wang",
            "Feng Xue",
            "Jun Shi",
            "Yong Yang",
            "Yong A",
            "Yixiang Jin",
            "Dingzhe Li",
            "Aryan Shukla",
            "Liam Frija-Altarac",
            "Matthew Toews",
            "Hui Geng",
            "Tianjiao Wan",
            "Zijian Gao",
            "Qisheng Xu",
            "Kele Xu",
            "Zijian Zang",
            "Jameer Babu Pinjari",
            "Kuldeep Purohit",
            "Mykola Lavreniuk",
            "Jing Cao",
            "Shenyi Li",
            "Kui Jiang",
            "Junjun Jiang",
            "Yong Huang"
        ],
        "title": "NTIRE 2025 Challenge on HR Depth from Images of Specular and Transparent Surfaces",
        "abstract": "arXiv:2506.05815v1 Announce Type: new  Abstract: This paper reports on the NTIRE 2025 challenge on HR Depth From images of Specular and Transparent surfaces, held in conjunction with the New Trends in Image Restoration and Enhancement (NTIRE) workshop at CVPR 2025. This challenge aims to advance the research on depth estimation, specifically to address two of the main open issues in the field: high-resolution and non-Lambertian surfaces. The challenge proposes two tracks on stereo and single-image depth estimation, attracting about 177 registered participants. In the final testing stage, 4 and 4 participating teams submitted their models and fact sheets for the two tracks.",
        "arxiv_id": "2506.05815",
        "ARXIVID": "2506.05815",
        "COMMENT": "Does not match any specific criteria but focuses on depth estimation challenges, which may be of general interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.05982": {
        "authors": [
            "Zonglin Wu",
            "Yule Xue",
            "Xin Wei",
            "Yiren Song"
        ],
        "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks",
        "abstract": "arXiv:2506.05982v1 Announce Type: new  Abstract: As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.",
        "arxiv_id": "2506.05982",
        "ARXIVID": "2506.05982",
        "COMMENT": "Does not match any specific criteria. Focuses on CAPTCHA robustness and multimodal evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}