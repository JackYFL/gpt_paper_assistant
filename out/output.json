{
    "2602.10809": {
        "authors": [
            "Chenlong Deng",
            "Mengjie Deng",
            "Junjie Wu",
            "Dun Zeng",
            "Teng Wang",
            "Qingsong Xie",
            "Jiadeng Huang",
            "Shengjie Ma",
            "Changwang Zhang",
            "Zhaoxiang Wang",
            "Jun Wang",
            "Yutao Zhu",
            "Zhicheng Dou"
        ],
        "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
        "abstract": "arXiv:2602.10809v1 Announce Type: new  Abstract: Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
        "arxiv_id": "2602.10809",
        "ARXIVID": "2602.10809",
        "COMMENT": "Matches criterion 3 (embodied AI papers on new benchmarks and methods) by introducing a new agentic paradigm and benchmark (DISBench) for context-aware image retrieval in visual histories, focusing on agentic reasoning and spatiotemporal context, which is a novel angle. Also relevant to criterion 1 (spatial intelligence in agents) due to the focus on spatiotemporal reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.10698": {
        "authors": [
            "Zhifeng Rao",
            "Wenlong Chen",
            "Lei Xie",
            "Xia Hua",
            "Dongfu Yin",
            "Zhen Tian",
            "F. Richard Yu"
        ],
        "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
        "abstract": "arXiv:2602.10698v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.",
        "arxiv_id": "2602.10698",
        "ARXIVID": "2602.10698",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI with a novel angle: depth-driven augmentation for 3D spatial understanding in VLA models).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.11073": {
        "authors": [
            "Junfei Wu",
            "Jian Guan",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang",
            "Wei Wu",
            "Tienie Tan"
        ],
        "title": "Chatting with Images for Introspective Visual Thinking",
        "abstract": "arXiv:2602.11073v1 Announce Type: new  Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.",
        "arxiv_id": "2602.11073",
        "ARXIVID": "2602.11073",
        "COMMENT": "Matches criterion 2 (new VLLM/MLLM) and criterion 1 (spatial reasoning in vision-language models). Proposes ViLaVT, a novel LVLM for interactive visual reasoning with dynamic vision encoding, showing strong improvements on spatial reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.10146": {
        "authors": [
            "Rongcan Pei",
            "Huan Li",
            "Fang Guo",
            "Qi Zhu"
        ],
        "title": "VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding",
        "abstract": "arXiv:2602.10146v1 Announce Type: new  Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.",
        "arxiv_id": "2602.10146",
        "ARXIVID": "2602.10146",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and provides insightful analysis of VLM internals for long-context understanding, with a novel training-free augmentation method (VERA) that leverages attention head analysis for improved performance.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.10624": {
        "authors": [
            "Siyuan Yan",
            "Xieji Li",
            "Dan Mo",
            "Philipp Tschandl",
            "Yiwen Jiang",
            "Zhonghua Wang",
            "Ming Hu",
            "Lie Ju",
            "Cristina Vico-Alonso",
            "Yizhen Zheng",
            "Jiahe Liu",
            "Juexiao Zhou",
            "Camilla Chello",
            "Jen G. Cheung",
            "Julien Anriot",
            "Luc Thomas",
            "Clare Primiero",
            "Gin Tan",
            "Aik Beng Ng",
            "Simon See",
            "Xiaoying Tang",
            "Albert Ip",
            "Xiaoyang Liao",
            "Adrian Bowling",
            "Martin Haskett",
            "Shuang Zhao",
            "Monika Janda",
            "H. Peter Soyer",
            "Victoria Mar",
            "Harald Kittler",
            "Zongyuan Ge"
        ],
        "title": "A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology",
        "abstract": "arXiv:2602.10624v1 Announce Type: new  Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.",
        "arxiv_id": "2602.10624",
        "ARXIVID": "2602.10624",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new vision-language foundation model for dermatology, with strong zero-shot and concept discovery capabilities, and surprising empirical results in clinical settings.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.11024": {
        "authors": [
            "Rishikesh Bhyri",
            "Brian R Quaranto",
            "Philip J Seger",
            "Kaity Tung",
            "Brendan Fox",
            "Gene Yang",
            "Steven D. Schwaitzberg",
            "Junsong Yuan",
            "Nan Xi",
            "Peter C W Kim"
        ],
        "title": "Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting",
        "abstract": "arXiv:2602.11024v1 Announce Type: new  Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.",
        "arxiv_id": "2602.11024",
        "ARXIVID": "2602.11024",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) due to the introduction of a novel visual reasoning framework (Chain-of-Look) for spatial counting, and criterion 4 (vision foundation models and applications) as it benchmarks against VLLMs/MLLMs. Also introduces a new dataset (SurgCount-HD), which is relevant to criterion 3 (new benchmarks for embodied AI).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.10999": {
        "authors": [
            "Yusong Lin",
            "Haiyang Wang",
            "Shuzhe Wu",
            "Lue Fan",
            "Feiyang Pan",
            "Sanyuan Zhao",
            "Dandan Tu"
        ],
        "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
        "abstract": "arXiv:2602.10999v1 Announce Type: new  Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
        "arxiv_id": "2602.10999",
        "ARXIVID": "2602.10999",
        "COMMENT": "Matches criterion 3 (new embodied AI benchmark/simulator). Introduces CLI-Gym, a new large-scale benchmark for agentic coding in command-line environments, with a novel pipeline for scalable task derivation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.10639": {
        "authors": [
            "Yuxin Cao",
            "Wei Song",
            "Shangzhi Xu",
            "Jingling Xue",
            "Jin Song Dong"
        ],
        "title": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
        "abstract": "arXiv:2602.10639v1 Announce Type: new  Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.",
        "arxiv_id": "2602.10639",
        "ARXIVID": "2602.10639",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new benchmark and evaluation framework for VideoLLMs, focusing on a previously ignored failure mode (output repetition) and providing empirical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.10884": {
        "authors": [
            "Jinqing Zhang",
            "Zehua Fu",
            "Zelin Xu",
            "Wenying Dai",
            "Qingjie Liu",
            "Yunhong Wang"
        ],
        "title": "ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving",
        "abstract": "arXiv:2602.10884v1 Announce Type: new  Abstract: The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.",
        "arxiv_id": "2602.10884",
        "ARXIVID": "2602.10884",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel method for embodied AI with new angle: temporal residuals for dynamic object modeling in autonomous driving). Proposes a new world model and trajectory refinement for spatial-temporal prediction.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.10425": {
        "authors": [
            "Yilin Yang",
            "Zhenghui Guo",
            "Yuke Wang",
            "Omprakash Gnawali",
            "Sheng Di",
            "Chengming Zhang"
        ],
        "title": "HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images",
        "abstract": "arXiv:2602.10425v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.",
        "arxiv_id": "2602.10425",
        "ARXIVID": "2602.10425",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) as it focuses on hallucination in large vision-language models and introduces a new pipeline for generating hallucination-inducing images, as well as a new benchmark (Masked-Object-Hallucination), which is also relevant to criterion 3 (new benchmarks for vision-language models).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10687": {
        "authors": [
            "Jinjie Shen",
            "Jing Wu",
            "Yaxiong Wang",
            "Lechao Cheng",
            "Shengeng Tang",
            "Tianrui Hui",
            "Nan Pu",
            "Zhun Zhong"
        ],
        "title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
        "abstract": "arXiv:2602.10687v1 Announce Type: new  Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \\textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.",
        "arxiv_id": "2602.10687",
        "ARXIVID": "2602.10687",
        "COMMENT": "Matches criterion 2 (new VLLM/MLLM). Proposes OmniVL-Guard, a unified vision-language model for forgery detection and grounding, with a novel balanced RL framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10943": {
        "authors": [
            "Martin Gromniak",
            "Jan-Gerrit Habekost",
            "Sebastian Kamp",
            "Sven Magg",
            "Stefan Wermter"
        ],
        "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
        "abstract": "arXiv:2602.10943v1 Announce Type: new  Abstract: We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
        "arxiv_id": "2602.10943",
        "ARXIVID": "2602.10943",
        "COMMENT": "Matches criterion 1 (spatial understanding for embodied agents). Proposes a generalizable neural radiance field for 3D scene representation from egocentric robot observations, directly applicable to robotic manipulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10994": {
        "authors": [
            "Vasileios Arampatzakis",
            "George Pavlidis",
            "Nikolaos Mitianoudis",
            "Nikos Papamarkos"
        ],
        "title": "Interpretable Vision Transformers in Image Classification via SVDA",
        "abstract": "arXiv:2602.10994v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.",
        "arxiv_id": "2602.10994",
        "ARXIVID": "2602.10994",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) by improving interpretability of Vision Transformers (ViTs) with a new attention mechanism (SVDA).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10659": {
        "authors": [
            "Yin Wang",
            "Ziyao Zhang",
            "Zhiying Leng",
            "Haitian Liu",
            "Frederick W. B. Li",
            "Mu Li",
            "Xiaohui Liang"
        ],
        "title": "Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation",
        "abstract": "arXiv:2602.10659v1 Announce Type: new  Abstract: We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.",
        "arxiv_id": "2602.10659",
        "ARXIVID": "2602.10659",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a multimodal-priors-augmented framework for text-driven 3D human-object interaction generation, using large multimodal models and diffusion. Relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.11146": {
        "authors": [
            "Gongye Liu",
            "Bo Yang",
            "Yida Zhi",
            "Zhizhou Zhong",
            "Lei Ke",
            "Didan Deng",
            "Han Gao",
            "Yongxiang Huang",
            "Kaihao Zhang",
            "Hongbo Fu",
            "Wenhan Luo"
        ],
        "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
        "abstract": "arXiv:2602.11146v1 Announce Type: new  Abstract: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.",
        "arxiv_id": "2602.11146",
        "ARXIVID": "2602.11146",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications) and is also relevant to generative modeling in multi-modal learning. Proposes a new diffusion-native latent reward model for preference optimization, offering a novel statistical trick (noise-calibrated Thurstone likelihood) and empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10704": {
        "authors": [
            "Minglei Li",
            "Mengfan He",
            "Chao Chen",
            "Ziyang Meng"
        ],
        "title": "(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization",
        "abstract": "arXiv:2602.10704v1 Announce Type: new  Abstract: Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\\% on University-1652 and 97.02\\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \\href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.",
        "arxiv_id": "2602.10704",
        "ARXIVID": "2602.10704",
        "COMMENT": "Matches criterion 1 (spatial understanding, spatial intelligence). Proposes a new geometry-grounded framework for cross-view geo-localization, addressing 3D geometry and spatial misalignment with novel modules.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.10516": {
        "authors": [
            "Zhongju Wang",
            "Zhenhong Sun",
            "Beier Wang",
            "Yifu Wang",
            "Daoyi Dong",
            "Huadong Mo",
            "Hongdong Li"
        ],
        "title": "3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars",
        "abstract": "arXiv:2602.10516v1 Announce Type: new  Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.",
        "arxiv_id": "2602.10516",
        "ARXIVID": "2602.10516",
        "COMMENT": "Matches criterion 1 (spatial dynamics in embodied agents) and criterion 4 (vision foundation model application to avatars). The paper proposes a unified framework for expressive 3D talking avatars, with explicit modeling of spatial dynamics and identity, and introduces a flow-matching-based transformer for coherent facial dynamics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.11005": {
        "authors": [
            "Vasileios Arampatzakis",
            "George Pavlidis",
            "Nikolaos Mitianoudis",
            "Nikos Papamarkos"
        ],
        "title": "Interpretable Vision Transformers in Monocular Depth Estimation via SVDA",
        "abstract": "arXiv:2602.11005v1 Announce Type: new  Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.",
        "arxiv_id": "2602.11005",
        "ARXIVID": "2602.11005",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to interpretability in vision transformers for dense prediction. Introduces a new spectrally structured attention mechanism (SVDA) for monocular depth estimation, providing new insights into transformer interpretability.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.10635": {
        "authors": [
            "Keane Ong",
            "Sabri Boughorbel",
            "Luwei Xiao",
            "Chanakya Ekbote",
            "Wei Dai",
            "Ao Qu",
            "Jingyao Wu",
            "Rui Mao",
            "Ehsan Hoque",
            "Erik Cambria",
            "Gianmarco Mengaldo",
            "Paul Pu Liang"
        ],
        "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
        "abstract": "arXiv:2602.10635v1 Announce Type: new  Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.",
        "arxiv_id": "2602.10635",
        "ARXIVID": "2602.10635",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications, though focused on social behavior). Introduces Omnisapiens-7B 2.0, a foundation model for social behavior processing, with a new RL method for heterogeneous data.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.10720": {
        "authors": [
            "Craig Mahlasi",
            "Gciniwe S. Baloyi",
            "Zaheed Gaffoor",
            "Levente Klein",
            "Anne Jones",
            "Etienne Vos",
            "Michal Muszynski",
            "Geoffrey Dawson",
            "Campbell Watson"
        ],
        "title": "Ecological mapping with geospatial foundation models",
        "abstract": "arXiv:2602.10720v1 Announce Type: new  Abstract: Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.",
        "arxiv_id": "2602.10720",
        "ARXIVID": "2602.10720",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Applies geospatial foundation models to ecological mapping, comparing them to baselines and exploring new use cases.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.10546": {
        "authors": [
            "Hanzhe Yu",
            "Yun Ye",
            "Jintao Rong",
            "Qi Xuan",
            "Chen Ma"
        ],
        "title": "RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images",
        "abstract": "arXiv:2602.10546v1 Announce Type: new  Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.",
        "arxiv_id": "2602.10546",
        "ARXIVID": "2602.10546",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) by providing a new large-scale dataset and a baseline for robust detection of AI-generated images, relevant for evaluating and improving vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.10491": {
        "authors": [
            "Zhenyu Yang",
            "Gensheng Pei",
            "Yazhou Yao",
            "Tianfei Zhou",
            "Lizhong Ding",
            "Fumin Shen"
        ],
        "title": "Towards Remote Sensing Change Detection with Neural Memory",
        "abstract": "arXiv:2602.10491v1 Announce Type: new  Abstract: Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \\textbf{84.36\\%} IoU and \\textbf{91.52\\%} F1-score on LEVIR-CD, while remaining computationally competitive.",
        "arxiv_id": "2602.10491",
        "ARXIVID": "2602.10491",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new vision backbone (VTitans) for remote sensing change detection, integrating neural memory and attention for improved spatial-temporal modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.11105": {
        "authors": [
            "Divya Jyoti Bajpai",
            "Dhruv Bhardwaj",
            "Soumya Roy",
            "Tejas Duseja",
            "Harsh Agarwal",
            "Aashay Sandansing",
            "Manjesh Kumar Hanawal"
        ],
        "title": "FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference",
        "abstract": "arXiv:2602.11105v1 Announce Type: new  Abstract: Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.",
        "arxiv_id": "2602.11105",
        "ARXIVID": "2602.11105",
        "COMMENT": "Relevant to general interest in generative modeling and vision, but does not directly match any specific criterion. Focuses on accelerating generative flow matching models with a bandit-based inference method, which is a clever statistical trick but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.10978": {
        "authors": [
            "Ruiqi Song",
            "Lei Liu",
            "Ya-Nan Zhang",
            "Chao Wang",
            "Xiaoning Li",
            "Nan Mu"
        ],
        "title": "VFGS-Net: Frequency-Guided State-Space Learning for Topology-Preserving Retinal Vessel Segmentation",
        "abstract": "arXiv:2602.10978v1 Announce Type: new  Abstract: Accurate retinal vessel segmentation is a critical prerequisite for quantitative analysis of retinal images and computer-aided diagnosis of vascular diseases such as diabetic retinopathy. However, the elongated morphology, wide scale variation, and low contrast of retinal vessels pose significant challenges for existing methods, making it difficult to simultaneously preserve fine capillaries and maintain global topological continuity. To address these challenges, we propose the Vessel-aware Frequency-domain and Global Spatial modeling Network (VFGS-Net), an end-to-end segmentation framework that seamlessly integrates frequency-aware feature enhancement, dual-path convolutional representation learning, and bidirectional asymmetric spatial state-space modeling within a unified architecture. Specifically, VFGS-Net employs a dual-path feature convolution module to jointly capture fine-grained local textures and multi-scale contextual semantics. A novel vessel-aware frequency-domain channel attention mechanism is introduced to adaptively reweight spectral components, thereby enhancing vessel-relevant responses in high-level features. Furthermore, at the network bottleneck, we propose a bidirectional asymmetric Mamba2-based spatial modeling block to efficiently capture long-range spatial dependencies and strengthen the global continuity of vascular structures. Extensive experiments on four publicly available retinal vessel datasets demonstrate that VFGS-Net achieves competitive or superior performance compared to state-of-the-art methods. Notably, our model consistently improves segmentation accuracy for fine vessels, complex branching patterns, and low-contrast regions, highlighting its robustness and clinical potential.",
        "arxiv_id": "2602.10978",
        "ARXIVID": "2602.10978",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation model application), but is a specific application to retinal vessel segmentation with a new network architecture. Not a foundation model or multi-modal model.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.11154": {
        "authors": [
            "Yue Gao",
            "Hong-Xing Yu",
            "Sanghyeon Chang",
            "Qianxi Fu",
            "Bo Zhu",
            "Yoonjin Won",
            "Juan Carlos Niebles",
            "Jiajun Wu"
        ],
        "title": "SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos",
        "abstract": "arXiv:2602.11154v1 Announce Type: new  Abstract: Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.",
        "arxiv_id": "2602.11154",
        "ARXIVID": "2602.11154",
        "COMMENT": "Presents a novel model for reconstructing 3D interfacial dynamics from sparse videos, integrating neural rendering and video diffusion. While innovative, it does not directly match any specific criterion (not spatial intelligence for embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.10764": {
        "authors": [
            "Linwei Dong",
            "Ruoyu Guo",
            "Ge Bai",
            "Zehuan Yuan",
            "Yawei Luo",
            "Changqing Zou"
        ],
        "title": "Dual-End Consistency Model",
        "abstract": "arXiv:2602.10764v1 Announce Type: new  Abstract: The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.",
        "arxiv_id": "2602.10764",
        "ARXIVID": "2602.10764",
        "COMMENT": "Relevant to generative modeling and computer vision, but does not directly match any specific criterion (focuses on diffusion/consistency models, not spatial intelligence, VLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.11117": {
        "authors": [
            "Di Chang",
            "Ji Hou",
            "Aljaz Bozic",
            "Assaf Neuberger",
            "Felix Juefei-Xu",
            "Olivier Maury",
            "Gene Wei-Chin Lin",
            "Tuur Stuyck",
            "Doug Roble",
            "Mohammad Soleymani",
            "Stephane Grabli"
        ],
        "title": "HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion",
        "abstract": "arXiv:2602.11117v1 Announce Type: new  Abstract: We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.",
        "arxiv_id": "2602.11117",
        "ARXIVID": "2602.11117",
        "COMMENT": "Applies video diffusion to hair motion synthesis, which is a novel application in generative modeling, but does not directly match any specific criterion (not VLLMs, spatial intelligence, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.10825": {
        "authors": [
            "Yuexiao Ma",
            "Xuzhe Zheng",
            "Jing Xu",
            "Xiwei Xu",
            "Feng Ling",
            "Xiawu Zheng",
            "Huafeng Kuang",
            "Huixia Li",
            "Xing Wang",
            "Xuefeng Xiao",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "title": "Flow caching for autoregressive video generation",
        "abstract": "arXiv:2602.10825v1 Announce Type: new  Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.",
        "arxiv_id": "2602.10825",
        "ARXIVID": "2602.10825",
        "COMMENT": "Relevant to generative modeling in vision (autoregressive video generation), but does not directly match any specific criterion (not VLLMs, spatial intelligence, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.11136": {
        "authors": [
            "Jiayi Zhou",
            "Yang Sheng",
            "Hantao Lou",
            "Yaodong Yang",
            "Jie Fu"
        ],
        "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight",
        "abstract": "arXiv:2602.11136v1 Announce Type: new  Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.",
        "arxiv_id": "2602.11136",
        "ARXIVID": "2602.11136",
        "COMMENT": "Does not match any specific criteria. Focuses on neuro-symbolic oversight for LLM agents, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.10367": {
        "authors": [
            "Zhiling Yan",
            "Dingjie Song",
            "Zhe Fang",
            "Yisheng Ji",
            "Xiang Li",
            "Quanzheng Li",
            "Lichao Sun"
        ],
        "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
        "abstract": "arXiv:2602.10367v1 Announce Type: new  Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.",
        "arxiv_id": "2602.10367",
        "ARXIVID": "2602.10367",
        "COMMENT": "Does not match any specific criteria. Focuses on medical LLM benchmarks and evaluation, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.10485": {
        "authors": [
            "Zhenhe Cui",
            "Huaxiang Xia",
            "Hangjun Shen",
            "Kailun Luo",
            "Yong He",
            "Wei Liang"
        ],
        "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
        "abstract": "arXiv:2602.10485v1 Announce Type: new  Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.",
        "arxiv_id": "2602.10485",
        "ARXIVID": "2602.10485",
        "COMMENT": "Somewhat related to criterion 2 (use of LLMs for planning abstraction), but not directly about VLLMs or MLLMs. Focuses on LLMs for abstraction generation in planning, not vision or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10818": {
        "authors": [
            "Dongsik Yoon",
            "Jongeun Kim",
            "Dayeon Lee"
        ],
        "title": "Resource-Efficient RGB-Only Action Recognition for Edge Deployment",
        "abstract": "arXiv:2602.10818v1 Announce Type: new  Abstract: Action recognition on edge devices poses stringent constraints on latency, memory, storage, and power consumption. While auxiliary modalities such as skeleton and depth information can enhance recognition performance, they often require additional sensors or computationally expensive pose-estimation pipelines, limiting practicality for edge use. In this work, we propose a compact RGB-only network tailored for efficient on-device inference. Our approach builds upon an X3D-style backbone augmented with Temporal Shift, and further introduces selective temporal adaptation and parameter-free attention. Extensive experiments on the NTU RGB+D 60 and 120 benchmarks demonstrate a strong accuracy-efficiency balance. Moreover, deployment-level profiling on the Jetson Orin Nano verifies a smaller on-device footprint and practical resource utilization compared to existing RGB-based action recognition techniques.",
        "arxiv_id": "2602.10818",
        "ARXIVID": "2602.10818",
        "COMMENT": "Focuses on efficient action recognition for edge devices using RGB-only input; relevant to computer vision but does not directly match any specific criterion (not spatial intelligence, VLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10259": {
        "authors": [
            "Qingwu Liu",
            "Nicolas Saunier",
            "Guillaume-Alexandre Bilodeau"
        ],
        "title": "PMMA: The Polytechnique Montreal Mobility Aids Dataset",
        "abstract": "arXiv:2602.10259v1 Announce Type: new  Abstract: This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.",
        "arxiv_id": "2602.10259",
        "ARXIVID": "2602.10259",
        "COMMENT": "Presents a new dataset for object detection and tracking, but not focused on embodied AI benchmarks, spatial intelligence, or vision foundation models. General relevance to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10699": {
        "authors": [
            "Jie Jiang",
            "Yangru Huang",
            "Zeyu Wang",
            "Changping Wang",
            "Yuling Xiong",
            "Jun Zhang",
            "Huan Yu"
        ],
        "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
        "abstract": "arXiv:2602.10699v1 Announce Type: new  Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.",
        "arxiv_id": "2602.10699",
        "ARXIVID": "2602.10699",
        "COMMENT": "Does not match any specific criteria. Focuses on generative recommendation and RL optimization, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}