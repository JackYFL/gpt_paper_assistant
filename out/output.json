{
    "2507.12795": {
        "authors": [
            "Penglei Sun",
            "Yaoxian Song",
            "Xiangru Zhu",
            "Xiang Liu",
            "Qiang Wang",
            "Yue Liu",
            "Changqun Xia",
            "Tiefeng Li",
            "Yang Yang",
            "Xiaowen Chu"
        ],
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "abstract": "arXiv:2507.12795v1 Announce Type: new  Abstract: Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.",
        "arxiv_id": "2507.12795",
        "ARXIVID": "2507.12795",
        "COMMENT": "Matches criteria 2 and 3: Introduces City-VLM, a new LVLM for multidomain outdoor scene understanding, and builds a new large-scale multimodal dataset (SVM-City) for benchmarking. Focuses on novel multimodal fusion and outdoor scene understanding, which is a new angle for LVLMs and benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.13152": {
        "authors": [
            "Xiangyu Dong",
            "Haoran Zhao",
            "Jiang Gao",
            "Haozhou Li",
            "Xiaoguang Ma",
            "Yaoming Zhou",
            "Fuhai Chen",
            "Juan Liu"
        ],
        "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
        "abstract": "arXiv:2507.13152v1 Announce Type: new  Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
        "arxiv_id": "2507.13152",
        "ARXIVID": "2507.13152",
        "COMMENT": "Matches criterion 1, 2, and 3: Proposes a self-evolving vision-language navigation (VLN) framework based on multimodal LLMs, with hierarchical memory, retrieval-augmented reasoning, and continual evolution. Demonstrates large improvements on VLN benchmarks and introduces continual learning in embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.12916": {
        "authors": [
            "Yifan Xu",
            "Chao Zhang",
            "Hanqi Jiang",
            "Xiaoyan Wang",
            "Ruifei Ma",
            "Yiwei Li",
            "Zihao Wu",
            "Zeju Li",
            "Xiangde Liu"
        ],
        "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
        "abstract": "arXiv:2507.12916v1 Announce Type: new  Abstract: Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.",
        "arxiv_id": "2507.12916",
        "ARXIVID": "2507.12916",
        "COMMENT": "Matches criterion 1 (spatial intelligence and 3D scene understanding for embodied agents), criterion 2 (3D-LMM, a new multimodal large language model for 3D tasks), criterion 3 (novel method for 3D scene understanding with multi-view images and LLMs), and criterion 4 (vision foundation models for 3D scene understanding). This is a strong match for all criteria.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.12821": {
        "authors": [
            "Lance Ying",
            "Katherine M. Collins",
            "Prafull Sharma",
            "Cedric Colas",
            "Kaiya Ivy Zhao",
            "Adrian Weller",
            "Zenna Tavares",
            "Phillip Isola",
            "Samuel J. Gershman",
            "Jacob D. Andreas",
            "Thomas L. Griffiths",
            "Francois Chollet",
            "Kelsey R. Allen",
            "Joshua B. Tenenbaum"
        ],
        "title": "Assessing adaptive world models in machines with novel games",
        "abstract": "arXiv:2507.12821v1 Announce Type: new  Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.",
        "arxiv_id": "2507.12821",
        "ARXIVID": "2507.12821",
        "COMMENT": "Matches criterion 3: proposes a new benchmarking paradigm for evaluating adaptive world models in embodied AI, focusing on rapid adaptation and generalization in novel games. This is a novel angle for embodied AI evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.13113": {
        "authors": [
            "Pranav Singh",
            "Pravendra Singh"
        ],
        "title": "Leveraging Language Prior for Infrared Small Target Detection",
        "abstract": "arXiv:2507.13113v1 Announce Type: new  Abstract: IRSTD (InfraRed Small Target Detection) detects small targets in infrared blurry backgrounds and is essential for various applications. The detection task is challenging due to the small size of the targets and their sparse distribution in infrared small target datasets. Although existing IRSTD methods and datasets have led to significant advancements, they are limited by their reliance solely on the image modality. Recent advances in deep learning and large vision-language models have shown remarkable performance in various visual recognition tasks. In this work, we propose a novel multimodal IRSTD framework that incorporates language priors to guide small target detection. We leverage language-guided attention weights derived from the language prior to enhance the model's ability for IRSTD, presenting a novel approach that combines textual information with image data to improve IRSTD capabilities. Utilizing the state-of-the-art GPT-4 vision model, we generate text descriptions that provide the locations of small targets in infrared images, employing careful prompt engineering to ensure improved accuracy. Due to the absence of multimodal IR datasets, existing IRSTD methods rely solely on image data. To address this shortcoming, we have curated a multimodal infrared dataset that includes both image and text modalities for small target detection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We validate the effectiveness of our approach through extensive experiments and comprehensive ablation studies. The results demonstrate significant improvements over the state-of-the-art method, with relative percentage differences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the NUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset of the LangIR dataset, respectively.",
        "arxiv_id": "2507.13113",
        "ARXIVID": "2507.13113",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel multimodal IRSTD framework leveraging language priors and GPT-4 vision, and curates a new multimodal dataset for IR small target detection.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.12768": {
        "authors": [
            "Hengkai Tan",
            "Yao Feng",
            "Xinyi Mao",
            "Shuhe Huang",
            "Guodong Liu",
            "Zhongkai Hao",
            "Hang Su",
            "Jun Zhu"
        ],
        "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
        "abstract": "arXiv:2507.12768v1 Announce Type: new  Abstract: Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over $ 30\\times $ compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation. Project Page: https://embodiedfoundation.github.io/vidar_anypos",
        "arxiv_id": "2507.12768",
        "ARXIVID": "2507.12768",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for bimanual manipulation with a novel task-agnostic action paradigm and scalable self-supervised data collection). Also relevant to criterion 1 (spatial intelligence in embodied agents).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.13347": {
        "authors": [
            "Yifan Wang",
            "Jianjun Zhou",
            "Haoyi Zhu",
            "Wenzheng Chang",
            "Yang Zhou",
            "Zizun Li",
            "Junyi Chen",
            "Jiangmiao Pang",
            "Chunhua Shen",
            "Tong He"
        ],
        "title": "$\\pi^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
        "abstract": "arXiv:2507.13347v1 Announce Type: new  Abstract: We introduce $\\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.",
        "arxiv_id": "2507.13347",
        "ARXIVID": "2507.13347",
        "COMMENT": "Matches criterion 4: introduces a permutation-equivariant neural network for visual geometry learning, breaking the reliance on reference views and achieving state-of-the-art results in multiple vision tasks. Strong foundation model contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.12832": {
        "authors": [
            "Yuki Kondo",
            "Norimichi Ukita",
            "Riku Kanayama",
            "Yuki Yoshida",
            "Takayuki Yamaguchi",
            "Xiang Yu",
            "Guang Liang",
            "Xinyao Liu",
            "Guan-Zhang Wang",
            "Wei-Ta Chu",
            "Bing-Cheng Chuang",
            "Jia-Hua Lee",
            "Pin-Tseng Kuo",
            "I-Hsuan Chu",
            "Yi-Shein Hsiao",
            "Cheng-Han Wu",
            "Po-Yi Wu",
            "Jui-Chien Tsou",
            "Hsuan-Chi Liu",
            "Chun-Yi Lee",
            "Yuan-Fu Yang",
            "Kosuke Shigematsu",
            "Asuka Shin",
            "Ba Tran"
        ],
        "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
        "abstract": "arXiv:2507.12832v1 Announce Type: new  Abstract: Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
        "arxiv_id": "2507.12832",
        "ARXIVID": "2507.12832",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (SMOT4SB dataset) and a novel metric (SO-HOTA) for small multi-object tracking in UAV videos, focusing on a novel angle (motion entanglement, small object tracking).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.13353": {
        "authors": [
            "Shihao Wang",
            "Guo Chen",
            "De-an Huang",
            "Zhiqi Li",
            "Minghan Li",
            "Guilin Li",
            "Jose M. Alvarez",
            "Lei Zhang",
            "Zhiding Yu"
        ],
        "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
        "abstract": "arXiv:2507.13353v1 Announce Type: new  Abstract: Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.",
        "arxiv_id": "2507.13353",
        "ARXIVID": "2507.13353",
        "COMMENT": "Matches criterion 2: Proposes VideoITG, a new method and dataset for multimodal video understanding with instructed temporal grounding, leveraging Video-LLMs and a novel annotation pipeline. Strongly relevant to VLLMs/MLLMs and their empirical performance.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.12841": {
        "authors": [
            "Yiming Ren",
            "Zhiqiang Lin",
            "Yu Li",
            "Gao Meng",
            "Weiyun Wang",
            "Junjie Wang",
            "Zicheng Lin",
            "Jifeng Dai",
            "Yujiu Yang",
            "Wenhai Wang",
            "Ruihang Chu"
        ],
        "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning",
        "abstract": "arXiv:2507.12841v1 Announce Type: new  Abstract: Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.",
        "arxiv_id": "2507.12841",
        "ARXIVID": "2507.12841",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a unified framework, dataset, and benchmark for controllable omni-modal captioning, with a plug-and-play model for foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.13061": {
        "authors": [
            "Jingyao Wang",
            "Yiming Chen",
            "Lingyu Si",
            "Changwen Zheng"
        ],
        "title": "Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection",
        "abstract": "arXiv:2507.13061v1 Announce Type: new  Abstract: Scene understanding is one of the core tasks in computer vision, aiming to extract semantic information from images to identify objects, scene categories, and their interrelationships. Although advancements in Vision-Language Models (VLMs) have driven progress in this field, existing VLMs still face challenges in adaptation to unseen complex wide-area scenes. To address the challenges, this paper proposes a Hierarchical Coresets Selection (HCS) mechanism to advance the adaptation of VLMs in complex wide-area scene understanding. It progressively refines the selected regions based on the proposed theoretically guaranteed importance function, which considers utility, representativeness, robustness, and synergy. Without requiring additional fine-tuning, HCS enables VLMs to achieve rapid understandings of unseen scenes at any scale using minimal interpretable regions while mitigating insufficient feature density. HCS is a plug-and-play method that is compatible with any VLM. Experiments demonstrate that HCS achieves superior performance and universality in various tasks.",
        "arxiv_id": "2507.13061",
        "ARXIVID": "2507.13061",
        "COMMENT": "Matches criterion 4: proposes a plug-and-play method (Hierarchical Coresets Selection) to improve adaptation of vision-language models (VLMs) to complex wide-area scenes, with a clever statistical trick for region selection. Also relevant to criterion 1 for spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12675": {
        "authors": [
            "Christina Thrainer",
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi",
            "Christian Guetl",
            "Steven Sloan",
            "Kendall N. Niles",
            "Ken Pathak"
        ],
        "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks",
        "abstract": "arXiv:2507.12675v1 Announce Type: new  Abstract: Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: https://github.com/faeyelab/fortress-paper-code.",
        "arxiv_id": "2507.12675",
        "ARXIVID": "2507.12675",
        "COMMENT": "Matches criterion 1: introduces a new spatial attention network (Kolmogorov-Arnold Enhanced Spatial Attention) for real-time structural segmentation, with methodological improvements in spatial understanding for embodied agents or robotics. Also relevant to criterion 4 as it is a vision foundation model for segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12819": {
        "authors": [
            "Jeong-Woo Park",
            "Seong-Whan Lee"
        ],
        "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval",
        "abstract": "arXiv:2507.12819v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a gallery using a composed query consisting of a reference image and a modification text. Among various CIR approaches, training-free zero-shot methods based on pre-trained models are cost-effective but still face notable limitations. For example, sequential VLM-LLM pipelines process each modality independently, which often results in information loss and limits cross-modal interaction. In contrast, methods based on multimodal large language models (MLLMs) often focus exclusively on applying changes indicated by the text, without fully utilizing the contextual visual information from the reference image. To address these issues, we propose multi-faceted Chain-of-Thought with re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes multi-faceted Chain-of-Thought to guide the MLLM to balance explicit modifications and contextual visual cues, generating two distinct captions: one focused on modification and the other integrating comprehensive visual-textual context. The first caption is used to filter candidate images. Subsequently, we combine these two captions and the reference image to perform multi-grained re-ranking. This two-stage approach facilitates precise retrieval by aligning with the textual modification instructions while preserving the visual context of the reference image. Through extensive experiments, MCoT-RE achieves state-of-the-art results among training-free methods, yielding improvements of up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
        "arxiv_id": "2507.12819",
        "ARXIVID": "2507.12819",
        "COMMENT": "Matches criterion 2: Proposes MCoT-RE, a training-free zero-shot composed image retrieval framework using MLLMs and chain-of-thought reasoning, with strong empirical results. Directly relevant to VLLMs/MLLMs and clever statistical tricks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12998": {
        "authors": [
            "Zihua Zhao",
            "Feng Hong",
            "Mengxi Chen",
            "Pengyi Chen",
            "Benyuan Liu",
            "Jiangchao Yao",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "title": "Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning",
        "abstract": "arXiv:2507.12998v1 Announce Type: new  Abstract: The remarkable success of contrastive-learning-based multimodal models has been greatly driven by training on ever-larger datasets with expensive compute consumption. Sample selection as an alternative efficient paradigm plays an important direction to accelerate the training process. However, recent advances on sample selection either mostly rely on an oracle model to offline select a high-quality coreset, which is limited in the cold-start scenarios, or focus on online selection based on real-time model predictions, which has not sufficiently or efficiently considered the noisy correspondence. To address this dilemma, we propose a novel Differential-Informed Sample Selection (DISSect) method, which accurately and efficiently discriminates the noisy correspondence for training acceleration. Specifically, we rethink the impact of noisy correspondence on contrastive learning and propose that the differential between the predicted correlation of the current model and that of a historical model is more informative to characterize sample quality. Based on this, we construct a robust differential-based sample selection and analyze its theoretical insights. Extensive experiments on three benchmark datasets and various downstream tasks demonstrate the consistent superiority of DISSect over current state-of-the-art methods. Source code is available at: https://github.com/MediaBrain-SJTU/DISSect.",
        "arxiv_id": "2507.12998",
        "ARXIVID": "2507.12998",
        "COMMENT": "Matches criterion 2 (shows new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new sample selection method for accelerating multimodal contrastive learning, with theoretical and empirical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13032": {
        "authors": [
            "Yi Xin",
            "Le Zhuo",
            "Qi Qin",
            "Siqi Luo",
            "Yuewen Cao",
            "Bin Fu",
            "Yangfan He",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Peng Gao"
        ],
        "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation",
        "abstract": "arXiv:2507.13032v1 Announce Type: new  Abstract: AutoRegressive (AR) models have made notable progress in image generation, with Masked AutoRegressive (MAR) models gaining attention for their efficient parallel decoding. However, MAR models have traditionally underperformed when compared to standard AR models. This study refines the MAR architecture to improve image generation quality. We begin by evaluating various image tokenizers to identify the most effective one. Subsequently, we introduce an improved Bidirectional LLaMA architecture by replacing causal attention with bidirectional attention and incorporating 2D RoPE, which together form our advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves a FID score of 3.71, matching state-of-the-art AR models in the ImageNet 256x256 benchmark, while requiring only 8 inference steps compared to the 256 steps of AR models. Furthermore, we develop a text-driven MaskGIL model with 775M parameters for generating images from text at various resolutions. Beyond image generation, MaskGIL extends to accelerate AR-based generation and enable real-time speech-to-image conversion. Our codes and models are available at https://github.com/synbol/MaskGIL.",
        "arxiv_id": "2507.13032",
        "ARXIVID": "2507.13032",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes MaskGIL, a new Masked AutoRegressive model for efficient and scalable image generation, with strong empirical results and applications to text-to-image and speech-to-image.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12666": {
        "authors": [
            "Alex Zook",
            "Josef Spjut",
            "Jonathan Tremblay"
        ],
        "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models",
        "abstract": "arXiv:2507.12666v1 Announce Type: new  Abstract: Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.",
        "arxiv_id": "2507.12666",
        "ARXIVID": "2507.12666",
        "COMMENT": "Matches criterion 3: Proposes an iterative game repair framework combining RL agents and large multimodal models to automatically refine game mechanics based on behavioral traces. This is a novel method for embodied AI and agent-based simulation, with a new angle on AI-assisted game design.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12566": {
        "authors": [
            "Gen Luo",
            "Wenhan Dou",
            "Wenhao Li",
            "Zhaokai Wang",
            "Xue Yang",
            "Changyao Tian",
            "Hao Li",
            "Weiyun Wang",
            "Wenhai Wang",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models",
        "abstract": "arXiv:2507.12566v1 Announce Type: new  Abstract: This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.",
        "arxiv_id": "2507.12566",
        "ARXIVID": "2507.12566",
        "COMMENT": "Matches criterion 2: Introduces Mono-InternVL-1.5, a new monolithic MLLM with improved pretraining and inference efficiency, and demonstrates strong results on multiple benchmarks. The focus on monolithic architecture and efficient training is notable.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13162": {
        "authors": [
            "Arian Mousakhan",
            "Sudhanshu Mittal",
            "Silvio Galesso",
            "Karim Farid",
            "Thomas Brox"
        ],
        "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
        "abstract": "arXiv:2507.13162v1 Announce Type: new  Abstract: Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.",
        "arxiv_id": "2507.13162",
        "ARXIVID": "2507.13162",
        "COMMENT": "Matches criterion 1 and 3: Presents a new world model for long-horizon prediction in autonomous driving, with a careful empirical comparison of discrete vs. continuous token models, and demonstrates state-of-the-art results in challenging driving scenarios. The hybrid tokenizer and side-by-side comparison are methodologically interesting.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12857": {
        "authors": [
            "Shiqi Huang",
            "Shuting He",
            "Huaiyuan Qin",
            "Bihan Wen"
        ],
        "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
        "abstract": "arXiv:2507.12857v1 Announce Type: new  Abstract: Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$ ($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary $\\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE.",
        "arxiv_id": "2507.12857",
        "ARXIVID": "2507.12857",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding) and criterion 4 (vision foundation models and applications). Proposes a new framework (SCORE) for open-vocabulary remote sensing instance segmentation, integrating multi-granularity scene context, and establishes new benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12628": {
        "authors": [
            "Sandipan Sarma",
            "Agney Talwarr",
            "Arijit Sur"
        ],
        "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection",
        "abstract": "arXiv:2507.12628v1 Announce Type: new  Abstract: Human-object interaction detection (HOID) refers to localizing interactive human-object pairs in images and identifying the interactions. Since there could be an exponential number of object-action combinations, labeled data is limited - leading to a long-tail distribution problem. Recently, zero-shot learning emerged as a solution, with end-to-end transformer-based object detectors adapted for HOID becoming successful frameworks. However, their primary focus is designing improved decoders for learning entangled or disentangled interpretations of interactions. We advocate that HOI-specific cues must be anticipated at the encoder stage itself to obtain a stronger scene interpretation. Consequently, we build a top-down framework named Funnel-HOI inspired by the human tendency to grasp well-defined concepts first and then associate them with abstract concepts during scene understanding. We first probe an image for the presence of objects (well-defined concepts) and then probe for actions (abstract concepts) associated with them. A novel asymmetric co-attention mechanism mines these cues utilizing multimodal information (incorporating zero-shot capabilities) and yields stronger interaction representations at the encoder level. Furthermore, a novel loss is devised that considers objectaction relatedness and regulates misclassification penalty better than existing loss functions for guiding the interaction classifier. Extensive experiments on the HICO-DET and V-COCO datasets across fully-supervised and six zero-shot settings reveal our state-of-the-art performance, with up to 12.4% and 8.4% gains for unseen and rare HOI categories, respectively.",
        "arxiv_id": "2507.12628",
        "ARXIVID": "2507.12628",
        "COMMENT": "Matches criterion 1 (spatial understanding in HOI detection, with new top-down perception and encoder-level improvements) and criterion 4 (vision foundation models, as it leverages transformer-based detectors). Also relevant to criterion 3 (novel loss and encoder design for HOI detection, a new method for embodied scene understanding).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12956": {
        "authors": [
            "Qiang Wang",
            "Mengchao Wang",
            "Fan Jiang",
            "Yaqi Fan",
            "Yonggang Qi",
            "Mu Xu"
        ],
        "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
        "abstract": "arXiv:2507.12956v1 Announce Type: new  Abstract: Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/.",
        "arxiv_id": "2507.12956",
        "ARXIVID": "2507.12956",
        "COMMENT": "Matches criterion 4: Proposes FantasyPortrait, a diffusion transformer-based framework for multi-character portrait animation, with new datasets and benchmarks. Applies generative vision foundation models to a novel animation task.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.12646": {
        "authors": [
            "Kaihua Chen",
            "Tarasha Khurana",
            "Deva Ramanan"
        ],
        "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos",
        "abstract": "arXiv:2507.12646v1 Announce Type: new  Abstract: We explore novel-view synthesis for dynamic scenes from monocular videos. Prior approaches rely on costly test-time optimization of 4D representations or do not preserve scene geometry when trained in a feed-forward manner. Our approach is based on three key insights: (1) covisible pixels (that are visible in both the input and target views) can be rendered by first reconstructing the dynamic 3D scene and rendering the reconstruction from the novel-views and (2) hidden pixels in novel views can be \"inpainted\" with feed-forward 2D video diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be self-supervised from 2D videos, allowing us to train it on a large corpus of in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot to novel test videos via test-time finetuning. We empirically verify that CogNVS outperforms almost all prior art for novel-view synthesis of dynamic scenes from monocular videos.",
        "arxiv_id": "2507.12646",
        "ARXIVID": "2507.12646",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes CogNVS, a video inpainting diffusion model for novel-view synthesis from monocular videos, trained self-supervised on large-scale video data.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.13344": {
        "authors": [
            "Yudong Jin",
            "Sida Peng",
            "Xuan Wang",
            "Tao Xie",
            "Zhen Xu",
            "Yifan Yang",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models",
        "abstract": "arXiv:2507.13344v1 Announce Type: new  Abstract: This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .",
        "arxiv_id": "2507.13344",
        "ARXIVID": "2507.13344",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel 4D diffusion model for human view synthesis with a new sliding iterative denoising process for spatio-temporal consistency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.12952": {
        "authors": [
            "Jiaxiu Jiang",
            "Wenbo Li",
            "Jingjing Ren",
            "Yuping Qiu",
            "Yong Guo",
            "Xiaogang Xu",
            "Han Wu",
            "Wangmeng Zuo"
        ],
        "title": "LoViC: Efficient Long Video Generation with Context Compression",
        "abstract": "arXiv:2507.12952v1 Announce Type: new  Abstract: Despite recent advances in diffusion transformers (DiTs) for text-to-video generation, scaling to long-duration content remains challenging due to the quadratic complexity of self-attention. While prior efforts -- such as sparse attention and temporally autoregressive models -- offer partial relief, they often compromise temporal coherence or scalability. We introduce LoViC, a DiT-based framework trained on million-scale open-domain videos, designed to produce long, coherent videos through a segment-wise generation process. At the core of our approach is FlexFormer, an expressive autoencoder that jointly compresses video and text into unified latent representations. It supports variable-length inputs with linearly adjustable compression rates, enabled by a single query token design based on the Q-Former architecture. Additionally, by encoding temporal context through position-aware mechanisms, our model seamlessly supports prediction, retradiction, interpolation, and multi-shot generation within a unified paradigm. Extensive experiments across diverse tasks validate the effectiveness and versatility of our approach.",
        "arxiv_id": "2507.12952",
        "ARXIVID": "2507.12952",
        "COMMENT": "Matches criterion 4: Introduces LoViC, a DiT-based framework for long video generation with a new autoencoder (FlexFormer) for joint video-text compression, supporting variable-length and multi-shot generation. The context compression and unified paradigm are relevant for vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.13107": {
        "authors": [
            "Xiaohan Guo",
            "Yusong Cai",
            "Zejia Liu",
            "Zhengning Wang",
            "Lili Pan",
            "Hongliang Li"
        ],
        "title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept Learning",
        "abstract": "arXiv:2507.13107v1 Announce Type: new  Abstract: Enabling large-scale generative models to continuously learn new visual concepts is essential for personalizing pre-trained models to meet individual user preferences. Existing approaches for continual visual concept learning are constrained by two fundamental challenges: catastrophic forgetting and parameter expansion. In this paper, we propose Redundancy-Removal Mixture of Experts (R^2MoE), a parameter-efficient framework for lifelong visual concept learning that effectively learns new concepts while incurring minimal parameter overhead. Our framework includes three key innovative contributions: First, we propose a mixture-of-experts framework with a routing distillation mechanism that enables experts to acquire concept-specific knowledge while preserving the gating network's routing capability, thereby effectively mitigating catastrophic forgetting. Second, we propose a strategy for eliminating redundant layer-wise experts that reduces the number of expert parameters by fully utilizing previously learned experts. Third, we employ a hierarchical local attention-guided inference approach to mitigate interference between generated visual concepts. Extensive experiments have demonstrated that our method generates images with superior conceptual fidelity compared to the state-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in forgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset. Our code is available at {https://github.com/learninginvision/R2MoE}",
        "arxiv_id": "2507.13107",
        "ARXIVID": "2507.13107",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a parameter-efficient mixture-of-experts framework for lifelong visual concept learning in generative models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.13145": {
        "authors": [
            "Maulana Bisyir Azhari",
            "David Hyunchul Shim"
        ],
        "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
        "abstract": "arXiv:2507.13145v1 Announce Type: new  Abstract: Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.",
        "arxiv_id": "2507.13145",
        "ARXIVID": "2507.13145",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is tangentially related to criterion 1 (spatial understanding in embodied agents). Proposes DINO-VO, a visual odometry system leveraging DINOv2 visual foundation model for robust feature matching in robotics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.12883": {
        "authors": [
            "Weihuang Lin",
            "Yiwei Ma",
            "Xiaoshuai Sun",
            "Shuting He",
            "Jiayi Ji",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning Segmentation",
        "abstract": "arXiv:2507.12883v1 Announce Type: new  Abstract: The reasoning segmentation task involves segmenting objects within an image by interpreting implicit user instructions, which may encompass subtleties such as contextual cues and open-world knowledge. Despite significant advancements made by existing approaches, they remain constrained by low perceptual resolution, as visual encoders are typically pre-trained at lower resolutions. Furthermore, simply interpolating the positional embeddings of visual encoders to enhance perceptual resolution yields only marginal performance improvements while incurring substantial computational costs. To address this, we propose HRSeg, an efficient model with high-resolution fine-grained perception. It features two key innovations: High-Resolution Perception (HRP) and High-Resolution Enhancement (HRE). The HRP module processes high-resolution images through cropping, integrating local and global features for multi-granularity quality. The HRE module enhances mask features by integrating fine-grained information from high-resolution images, refining their alignment with text features for precise segmentation. Extensive ablation studies validate the effectiveness of our modules, while comprehensive experiments on multiple benchmark datasets demonstrate HRSeg's superior performance.",
        "arxiv_id": "2507.12883",
        "ARXIVID": "2507.12883",
        "COMMENT": "Matches criterion 4: introduces HRSeg, a new model for high-resolution visual perception and reasoning segmentation, with innovations in high-resolution enhancement and perception. Relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12771": {
        "authors": [
            "Min-Jeong Lee",
            "Hee-Dong Kim",
            "Seong-Whan Lee"
        ],
        "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
        "abstract": "arXiv:2507.12771v1 Announce Type: new  Abstract: Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.",
        "arxiv_id": "2507.12771",
        "ARXIVID": "2507.12771",
        "COMMENT": "Matches criterion 4: Proposes a novel token merging strategy (ReToM) for attention mechanisms in text-to-image generation, improving efficiency and quality in diffusion models, which are vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13345": {
        "authors": [
            "Yukai Shi",
            "Jiarong Ou",
            "Rui Chen",
            "Haotian Yang",
            "Jiahao Wang",
            "Xin Tao",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
        "abstract": "arXiv:2507.13345v1 Announce Type: new  Abstract: In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes.",
        "arxiv_id": "2507.13345",
        "ARXIVID": "2507.13345",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes an online concept balancing loss for visual generation models, with a new benchmark and improved concept response.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13314": {
        "authors": [
            "Junsu Kim",
            "Naeun Kim",
            "Jaeho Lee",
            "Incheol Park",
            "Dongyoon Han",
            "Seungryul Baek"
        ],
        "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
        "abstract": "arXiv:2507.13314v1 Announce Type: new  Abstract: The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.",
        "arxiv_id": "2507.13314",
        "ARXIVID": "2507.13314",
        "COMMENT": "Matches criterion 3 (embodied AI papers on new benchmarks or methods). Revisits and refines the reasoning-based pose estimation benchmark for MLLMs, addressing reproducibility and benchmark quality issues.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12761": {
        "authors": [
            "Hanlei Shi",
            "Leyuan Qu",
            "Yu Liu",
            "Di Gao",
            "Yuhua Zheng",
            "Taihao Li"
        ],
        "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation",
        "abstract": "arXiv:2507.12761v1 Announce Type: new  Abstract: Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a \"global emotion localization--local muscle control\" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.",
        "arxiv_id": "2507.12761",
        "ARXIVID": "2507.12761",
        "COMMENT": "Matches criterion 2: Proposes a novel framework for text-driven emotional talking-head generation using chain-of-thought and progressive denoising, leveraging advances in multimodal large language models. The fine-grained control and semantic decomposition are notable.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12851": {
        "authors": [
            "Ziyi Wang",
            "Zhi Gao",
            "Jin Chen",
            "Qingjie Zhao",
            "Xinxiao Wu",
            "Jiebo Luo"
        ],
        "title": "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization",
        "abstract": "arXiv:2507.12851v1 Announce Type: new  Abstract: Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods. The code is available at: https://github.com/bitPrincy/SRE-DG.",
        "arxiv_id": "2507.12851",
        "ARXIVID": "2507.12851",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes an attention-refocusing scheme for domain generalization using CLIP, a vision foundation model.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12945": {
        "authors": [
            "Yucheng Tang",
            "Yunguan Fu",
            "Weixi Yi",
            "Yipei Wang",
            "Daniel C. Alexander",
            "Rhodri Davies",
            "Yipeng Hu"
        ],
        "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications",
        "abstract": "arXiv:2507.12945v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) can process and integrate information from multimodality sources, such as text and images. However, interrelationship among input modalities, uncertainties due to individual uni-modal data and potential clinical applications following such an uncertainty decomposition are yet fully understood in the context of large-scale MLLMs. In this work, we propose a multimodal uncertainty propagation model (MUPM) based on uncertainty propagation, to characterise the relationship among the uncertainties arising from image-only, text-only, and joint image-text variations in MLLM inputs. Using real clinical data consisting of cardiac MR scans and digital health records, we describe that MUPMs can be optimised robustly with a few samples. We then show that the fitted MUPMs are generalisable across different input data distributions and, perhaps surprisingly, across different downstream tasks. Such a transferability may be explained by the shared pretraining, comparatively light MLLM fine-tuning, along with the low-dimensional nature of the MUPMs. More importantly, this learned transferability, quantifying the relationship between these uncertainties, led to direct clinical applications in which uncertainties may be estimated and thus analysed robustly for varying data or even a novel set of cardiac disease prediction tasks. In addition, we show experimentally the efficiency in multimodal data required for estimating the overall uncertainty and its ability to identify redundant factors, both of which are considered practical yet clinically useful applications with the proposed MUPMs. Codes are available at https://github.com/yucheng722/MUPM.",
        "arxiv_id": "2507.12945",
        "ARXIVID": "2507.12945",
        "COMMENT": "Matches criterion 2 (MLLMs, with a focus on uncertainty propagation in multimodal large language models) and criterion 4 (vision foundation models applied to medical imaging). The paper provides new insights into uncertainty in MLLMs for clinical applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12816": {
        "authors": [
            "Ju-Young Oh",
            "Ho-Joong Kim",
            "Seong-Whan Lee"
        ],
        "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering",
        "abstract": "arXiv:2507.12816v1 Announce Type: new  Abstract: Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.",
        "arxiv_id": "2507.12816",
        "ARXIVID": "2507.12816",
        "COMMENT": "Matches criterion 1 (spatial understanding in video QA, with new method for integrating spatial/scene information) and criterion 2 (multimodal VQA, with new question embedding integration). The method enhances spatial and contextual understanding in video QA, which is a spatial intelligence task for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13229": {
        "authors": [
            "Junhong Min",
            "Youngpil Jeon",
            "Jimin Kim",
            "Minyong Choi"
        ],
        "title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation",
        "abstract": "arXiv:2507.13229v1 Announce Type: new  Abstract: The pursuit of a generalizable stereo matching model, capable of performing across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. On the other hand, global matching architectures, while theoretically more robust, have been historically rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with $S^2M^2$: a global matching architecture that achieves both state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods across most metrics while reconstructing high-quality details with competitive efficiency.",
        "arxiv_id": "2507.13229",
        "ARXIVID": "2507.13229",
        "COMMENT": "Related to criterion 4: introduces a scalable stereo matching model using a multi-resolution transformer, which is a vision foundation model for depth estimation. Not directly about VLLMs or embodied AI, but relevant to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.13311": {
        "authors": [
            "Chuancheng Shi",
            "Yixiang Chen",
            "Burong Lei",
            "Jichao Chen"
        ],
        "title": "FashionPose: Text to Pose to Relight Image Generation for Personalized Fashion Visualization",
        "abstract": "arXiv:2507.13311v1 Announce Type: new  Abstract: Realistic and controllable garment visualization is critical for fashion e-commerce, where users expect personalized previews under diverse poses and lighting conditions. Existing methods often rely on predefined poses, limiting semantic flexibility and illumination adaptability. To address this, we introduce FashionPose, the first unified text-to-pose-to-relighting generation framework. Given a natural language description, our method first predicts a 2D human pose, then employs a diffusion model to generate high-fidelity person images, and finally applies a lightweight relighting module, all guided by the same textual input. By replacing explicit pose annotations with text-driven conditioning, FashionPose enables accurate pose alignment, faithful garment rendering, and flexible lighting control. Experiments demonstrate fine-grained pose synthesis and efficient, consistent relighting, providing a practical solution for personalized virtual fashion display.",
        "arxiv_id": "2507.13311",
        "ARXIVID": "2507.13311",
        "COMMENT": "Somewhat related to criterion 4: proposes a text-to-pose-to-relighting generation framework for fashion visualization, using diffusion models and text-driven conditioning. Interesting application of generative vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.13120": {
        "authors": [
            "Xiaozheng Jiang",
            "Wei Zhang",
            "Xuerui Mao"
        ],
        "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
        "abstract": "arXiv:2507.13120v1 Announce Type: new  Abstract: Detecting tiny objects in remote sensing (RS) imagery has been a long-standing challenge due to their extremely limited spatial information, weak feature representations, and dense distributions across complex backgrounds. Despite numerous efforts devoted, mainstream detectors still underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a multi-stage feature fusion and enhancement model explicitly tailored for RS tiny object detection in various RS scenarios. RS-TinyNet comes with two novel designs: tiny object saliency modeling and feature integrity reconstruction. Guided by these principles, we design three step-wise feature enhancement modules. Among them, the multi-dimensional collaborative attention (MDCA) module employs multi-dimensional attention to enhance the saliency of tiny objects. Additionally, the auxiliary reversible branch (ARB) and a progressive fusion detection head (PFDH) module are introduced to preserve information flow and fuse multi-level features to bridge semantic gaps and retain structural detail. Comprehensive experiments on public RS dataset AI-TOD show that our RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and 6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior detection performance in diverse RS scenarios. These results demonstrate that the proposed multi-stage feature fusion strategy offers an effective and practical solution for tiny object detection in complex RS environments.",
        "arxiv_id": "2507.13120",
        "ARXIVID": "2507.13120",
        "COMMENT": "Matches criterion 4: Proposes RS-TinyNet, a new model for tiny object detection in remote sensing images, with novel feature fusion and attention modules. Applies vision foundation model concepts to a challenging application.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12823": {
        "authors": [
            "Jeong-Woo Park",
            "Young-Eun Kim",
            "Seong-Whan Lee"
        ],
        "title": "FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval",
        "abstract": "arXiv:2507.12823v1 Announce Type: new  Abstract: Composed image retrieval (CIR) is a vision language task that retrieves a target image using a reference image and modification text, enabling intuitive specification of desired changes. While effectively fusing visual and textual modalities is crucial, existing methods typically adopt either early or late fusion. Early fusion tends to excessively focus on explicitly mentioned textual details and neglect visual context, whereas late fusion struggles to capture fine-grained semantic alignments between image regions and textual tokens. To address these issues, we propose FAR-Net, a multi-stage fusion framework designed with enhanced semantic alignment and adaptive reconciliation, integrating two complementary modules. The enhanced semantic alignment module (ESAM) employs late fusion with cross-attention to capture fine-grained semantic relationships, while the adaptive reconciliation module (ARM) applies early fusion with uncertainty embeddings to enhance robustness and adaptability. Experiments on CIRR and FashionIQ show consistent performance gains, improving Recall@1 by up to 2.4% and Recall@50 by 1.04% over existing state-of-the-art methods, empirically demonstrating that FAR Net provides a robust and scalable solution to CIR tasks.",
        "arxiv_id": "2507.12823",
        "ARXIVID": "2507.12823",
        "COMMENT": "Matches criterion 4: Proposes FAR-Net, a multi-stage fusion network for composed image retrieval, focusing on improved semantic alignment and robustness in vision-language tasks. The method is relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.13260": {
        "authors": [
            "Yiting Yang",
            "Hao Luo",
            "Yuan Sun",
            "Qingsen Yan",
            "Haokui Zhang",
            "Wei Dong",
            "Guoqing Wang",
            "Peng Wang",
            "Yang Yang",
            "Hengtao Shen"
        ],
        "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
        "abstract": "arXiv:2507.13260v1 Announce Type: new  Abstract: A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.",
        "arxiv_id": "2507.13260",
        "ARXIVID": "2507.13260",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new parameter-efficient fine-tuning strategy for Vision Transformers, focusing on orthogonality in adaptation matrices.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12889": {
        "authors": [
            "Mengke Song",
            "Yuge Xie",
            "Qi Cui",
            "Luming Li",
            "Xinyu Liu",
            "Guotao Wang",
            "Chenglizhao Chen",
            "Shanchen Pang"
        ],
        "title": "Camera-based implicit mind reading by capturing higher-order semantic dynamics of human gaze within environmental context",
        "abstract": "arXiv:2507.12889v1 Announce Type: new  Abstract: Emotion recognition,as a step toward mind reading,seeks to infer internal states from external cues.Most existing methods rely on explicit signals-such as facial expressions,speech,or gestures-that reflect only bodily responses and overlook the influence of environmental context.These cues are often voluntary,easy to mask,and insufficient for capturing deeper,implicit emotions. Physiological signal-based approaches offer more direct access to internal states but require complex sensors that compromise natural behavior and limit scalability.Gaze-based methods typically rely on static fixation analysis and fail to capture the rich,dynamic interactions between gaze and the environment,and thus cannot uncover the deep connection between emotion and implicit behavior.To address these limitations,we propose a novel camera-based,user-unaware emotion recognition approach that integrates gaze fixation patterns with environmental semantics and temporal dynamics.Leveraging standard HD cameras,our method unobtrusively captures users'eye appearance and head movements in natural settings-without the need for specialized hardware or active user participation.From these visual cues,the system estimates gaze trajectories over time and space, providing the basis for modeling the spatial, semantic,and temporal dimensions of gaze behavior. This allows us to capture the dynamic interplay between visual attention and the surrounding environment,revealing that emotions are not merely physiological responses but complex outcomes of human-environment interactions.The proposed approach enables user-unaware,real-time,and continuous emotion recognition,offering high generalizability and low deployment cost.",
        "arxiv_id": "2507.12889",
        "ARXIVID": "2507.12889",
        "COMMENT": "Matches criterion 1 (spatial understanding, as it models gaze behavior in environmental context for emotion recognition) and criterion 4 (vision foundation models, as it leverages camera-based vision for implicit mind reading).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12591": {
        "authors": [
            "Trong-Thang Pham",
            "Akash Awasthi",
            "Saba Khan",
            "Esteban Duran Marti",
            "Tien-Phat Nguyen",
            "Khoa Vo",
            "Minh Tran",
            "Ngoc Son Nguyen",
            "Cuong Tran Van",
            "Yuki Ikebe",
            "Anh Totti Nguyen",
            "Anh Nguyen",
            "Zhigang Deng",
            "Carol C. Wu",
            "Hien Van Nguyen",
            "Ngan Le"
        ],
        "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
        "abstract": "arXiv:2507.12591v1 Announce Type: new  Abstract: Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging.",
        "arxiv_id": "2507.12591",
        "ARXIVID": "2507.12591",
        "COMMENT": "Somewhat related to criterion 3: Presents a new dataset and baseline for 3D scanpath modeling in CT, which is a new benchmark for spatial attention in medical imaging, but not directly about embodied agents or simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.12796": {
        "authors": [
            "Junjie Gao",
            "Runze Liu",
            "Yingzhe Peng",
            "Shujian Yang",
            "Jin Zhang",
            "Kai Yang",
            "Zhiyuan You"
        ],
        "title": "DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment",
        "abstract": "arXiv:2507.12796v1 Announce Type: new  Abstract: Document quality assessment is critical for a wide range of applications including document digitization, OCR, and archival. However, existing approaches often struggle to provide accurate and robust quality scores, limiting their applicability in practical scenarios. With the rapid progress in Multi-modal Large Language Models (MLLMs), recent MLLM-based methods have achieved remarkable performance in image quality assessment. In this work, we extend this success to the document domain by adapting DeQA-Score, a state-of-the-art MLLM-based image quality scorer, for document quality assessment. We propose DeQA-Doc, a framework that leverages the visual language capabilities of MLLMs and a soft label strategy to regress continuous document quality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary solutions to construct soft labels without the variance information. Also, we relax the resolution constrains to support the large resolution of document images. Finally, we introduce ensemble methods to further enhance the performance. Extensive experiments demonstrate that DeQA-Doc significantly outperforms existing baselines, offering accurate and generalizable document quality assessment across diverse degradation types. Codes and model weights are available in https://github.com/Junjie-Gao19/DeQA-Doc.",
        "arxiv_id": "2507.12796",
        "ARXIVID": "2507.12796",
        "COMMENT": "Matches criterion 2 and 4: Adapts a state-of-the-art MLLM-based image quality scorer to document images, leveraging visual-language models for document quality assessment. The adaptation to document domain and use of MLLMs is relevant.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.12845": {
        "authors": [
            "Khang Truong",
            "Lam Pham",
            "Hieu Tang",
            "Jasmin Lampert",
            "Martin Boyer",
            "Son Phan",
            "Truong Nguyen"
        ],
        "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning",
        "abstract": "arXiv:2507.12845v1 Announce Type: new  Abstract: Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.",
        "arxiv_id": "2507.12845",
        "ARXIVID": "2507.12845",
        "COMMENT": "Somewhat related to criterion 4: proposes a new transformer-based architecture for remote sensing image captioning, which is an application of vision-language models, but does not introduce a new VLLM or MLLM.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2507.12739": {
        "authors": [
            "Ijazul Haq",
            "Muhammad Saqib",
            "Yingjie Zhang"
        ],
        "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
        "abstract": "arXiv:2507.12739v1 Announce Type: new  Abstract: Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.",
        "arxiv_id": "2507.12739",
        "ARXIVID": "2507.12739",
        "COMMENT": "Matches criterion 1: Survey of transformer-based spatial grounding, which is directly related to spatial understanding in embodied agents. Also provides insights into methodological trends.",
        "RELEVANCE": 6,
        "NOVELTY": 3
    },
    "2507.12590": {
        "authors": [
            "Judy Long",
            "Tao Liu",
            "Sean Alexander Woznicki",
            "Miljana Markovi\\'c",
            "Oskar Marko",
            "Molly Sears"
        ],
        "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows",
        "abstract": "arXiv:2507.12590v1 Announce Type: new  Abstract: Crop mapping involves identifying and classifying crop types using spatial data, primarily derived from remote sensing imagery. This study presents the first comprehensive review of large-scale, pixel-wise crop mapping workflows, encompassing both conventional supervised methods and emerging transfer learning approaches. To identify the optimal supervised crop mapping workflows, we conducted systematic experiments, comparing six widely adopted satellite image-based preprocessing methods, alongside eleven supervised pixel-wise classification models. Additionally, we assessed the synergistic impact of varied training sample sizes and variable combinations. Moreover, we identified optimal transfer learning techniques for different magnitudes of domain shift. The evaluation of best methods was conducted across five diverse agricultural sites. Landsat 8 served as the primary satellite data source. Labels come from CDL trusted pixels and field surveys.   Our findings reveal three key insights. First, fine-scale interval preprocessing paired with Transformer models consistently delivered optimal performance for both supervised and transferable workflows. RF offered rapid training and competitive performance in conventional supervised learning and direct transfer to similar domains. Second, transfer learning techniques enhanced workflow adaptability, with UDA being effective for homogeneous crop classes while fine-tuning remains robust across diverse scenarios. Finally, workflow choice depends heavily on the availability of labeled samples. With a sufficient sample size, supervised training typically delivers more accurate and generalizable results. Below a certain threshold, transfer learning that matches the level of domain shift is a viable alternative to achieve crop mapping. Repository: Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
        "arxiv_id": "2507.12590",
        "ARXIVID": "2507.12590",
        "COMMENT": "Somewhat relevant to criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications) as it discusses pixel-wise crop mapping with transformers and transfer learning, but is more remote sensing focused.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.12988": {
        "authors": [
            "Uranik Berisha",
            "Jens Mehnert",
            "Alexandru Paul Condurache"
        ],
        "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
        "abstract": "arXiv:2507.12988v1 Announce Type: new  Abstract: Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44x.",
        "arxiv_id": "2507.12988",
        "ARXIVID": "2507.12988",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it proposes a new pruning method for vision transformers, but not a direct match to any criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.12806": {
        "authors": [
            "Zhiwei Liu",
            "Jielin Qiu",
            "Shiyu Wang",
            "Jianguo Zhang",
            "Zuxin Liu",
            "Roshan Ram",
            "Haolin Chen",
            "Weiran Yao",
            "Huan Wang",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
        "abstract": "arXiv:2507.12806v1 Announce Type: new  Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \\oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.",
        "arxiv_id": "2507.12806",
        "ARXIVID": "2507.12806",
        "COMMENT": "Somewhat related to criterion 3: Proposes an automatic evaluation framework for LLM-based agents, but focuses on evaluation protocols rather than embodied AI or spatial intelligence. More about agent evaluation than spatial/embodied methods.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.13346": {
        "authors": [
            "Minghao Chen",
            "Jianyuan Wang",
            "Roman Shapovalov",
            "Tom Monnier",
            "Hyunyoung Jung",
            "Dilin Wang",
            "Rakesh Ranjan",
            "Iro Laina",
            "Andrea Vedaldi"
        ],
        "title": "AutoPartGen: Autogressive 3D Part Generation and Discovery",
        "abstract": "arXiv:2507.13346v1 Announce Type: new  Abstract: We introduce AutoPartGen, a model that generates objects composed of 3D parts in an autoregressive manner. This model can take as input an image of an object, 2D masks of the object's parts, or an existing 3D object, and generate a corresponding compositional 3D reconstruction. Our approach builds upon 3DShape2VecSet, a recent latent 3D representation with powerful geometric expressiveness. We observe that this latent space exhibits strong compositional properties, making it particularly well-suited for part-based generation tasks. Specifically, AutoPartGen generates object parts autoregressively, predicting one part at a time while conditioning on previously generated parts and additional inputs, such as 2D images, masks, or 3D objects. This process continues until the model decides that all parts have been generated, thus determining automatically the type and number of parts. The resulting parts can be seamlessly assembled into coherent objects or scenes without requiring additional optimization. We evaluate both the overall 3D generation capabilities and the part-level generation quality of AutoPartGen, demonstrating that it achieves state-of-the-art performance in 3D part generation.",
        "arxiv_id": "2507.13346",
        "ARXIVID": "2507.13346",
        "COMMENT": "Relevant to general computer vision and generative modeling, but does not directly match any specific criterion. Focuses on 3D part-based generation, which is interesting for generative modeling but not directly about spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.12884": {
        "authors": [
            "Mengxi Liu",
            "Lala Shakti Swarup Ray",
            "Sizhen Bian",
            "Ko Watanabe",
            "Ankur Bhatt",
            "Joanna Sorysz",
            "Russel Torah",
            "Bo Zhou",
            "Paul Lukowicz"
        ],
        "title": "From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation",
        "abstract": "arXiv:2507.12884v1 Announce Type: new  Abstract: We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validate NeckSense on 7 participants using the current SOTA pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact, line-of-sight-free bio-impedance wearable can deliver head-tracking performance comparable to SOTA vision-based methods.",
        "arxiv_id": "2507.12884",
        "ARXIVID": "2507.12884",
        "COMMENT": "Does not match any specific criteria. Presents a wearable system for head pose estimation using bio-impedance sensing, not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13018": {
        "authors": [
            "Songlin Li",
            "Guofeng Yu",
            "Zhiqing Guo",
            "Yunfeng Diao",
            "Dan Ma",
            "Gaobo Yang",
            "Liejun Wang"
        ],
        "title": "Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization",
        "abstract": "arXiv:2507.13018v1 Announce Type: new  Abstract: Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotated mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential tampered regions. Finally, we propose a confidence-aware entropy minimization loss (${\\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.",
        "arxiv_id": "2507.13018",
        "ARXIVID": "2507.13018",
        "COMMENT": "Relevant to computer vision and weakly-supervised learning, but does not directly match any specific criterion. Focuses on image manipulation localization with scribble-based weak supervision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12953": {
        "authors": [
            "Sidaty El Hadramy",
            "Oumeymah Cherkaoui",
            "Philippe C. Cattin"
        ],
        "title": "cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration",
        "abstract": "arXiv:2507.12953v1 Announce Type: new  Abstract: Regularization is essential in deformable image registration (DIR) to ensure that the estimated Deformation Vector Field (DVF) remains smooth, physically plausible, and anatomically consistent. However, fine-tuning regularization parameters in learning-based DIR frameworks is computationally expensive, often requiring multiple training iterations. To address this, we propose cIDI, a novel DIR framework based on Implicit Neural Representations (INRs) that conditions the registration process on regularization hyperparameters. Unlike conventional methods that require retraining for each regularization hyperparameter setting, cIDIR is trained over a prior distribution of these hyperparameters, then optimized over the regularization hyperparameters by using the segmentations masks as an observation. Additionally, cIDIR models a continuous and differentiable DVF, enabling seamless integration of advanced regularization techniques via automatic differentiation. Evaluated on the DIR-LAB dataset, $\\operatorname{cIDIR}$ achieves high accuracy and robustness across the dataset.",
        "arxiv_id": "2507.12953",
        "ARXIVID": "2507.12953",
        "COMMENT": "Does not match any specific criterion. Focuses on deformable image registration with implicit neural representations, which is related to computer vision but not spatial intelligence for embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12804": {
        "authors": [
            "Hoang-Son Vo",
            "Quang-Vinh Nguyen",
            "Seungwon Kim",
            "Hyung-Jeong Yang",
            "Soonja Yeom",
            "Soo-Hyung Kim"
        ],
        "title": "ATL-Diff: Audio-Driven Talking Head Generation with Early Landmarks-Guide Noise Diffusion",
        "abstract": "arXiv:2507.12804v1 Announce Type: new  Abstract: Audio-driven talking head generation requires precise synchronization between facial animations and audio signals. This paper introduces ATL-Diff, a novel approach addressing synchronization limitations while reducing noise and computational costs. Our framework features three key components: a Landmark Generation Module converting audio to facial landmarks, a Landmarks-Guide Noise approach that decouples audio by distributing noise according to landmarks, and a 3D Identity Diffusion network preserving identity characteristics. Experiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms state-of-the-art methods across all metrics. Our approach achieves near real-time processing with high-quality animations, computational efficiency, and exceptional preservation of facial nuances. This advancement offers promising applications for virtual assistants, education, medical communication, and digital platforms. The source code is available at: \\href{https://github.com/sonvth/ATL-Diff}{https://github.com/sonvth/ATL-Diff}",
        "arxiv_id": "2507.12804",
        "ARXIVID": "2507.12804",
        "COMMENT": "Does not match any specific criterion. Focuses on audio-driven talking head generation, which is generative modeling but not spatial intelligence, VLLM/MLLM, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12942": {
        "authors": [
            "Yafei Zhang",
            "Lingqi Kong",
            "Huafeng Li",
            "Jie Wen"
        ],
        "title": "Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning",
        "abstract": "arXiv:2507.12942v1 Announce Type: new  Abstract: To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method.",
        "arxiv_id": "2507.12942",
        "ARXIVID": "2507.12942",
        "COMMENT": "Does not match any specific criteria. Focuses on person re-identification across visible and infrared modalities, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}