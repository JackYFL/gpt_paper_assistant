{
    "2508.10104": {
        "authors": [
            "Oriane Sim\\'eoni",
            "Huy V. Vo",
            "Maximilian Seitzer",
            "Federico Baldassarre",
            "Maxime Oquab",
            "Cijo Jose",
            "Vasil Khalidov",
            "Marc Szafraniec",
            "Seungeun Yi",
            "Micha\\\"el Ramamonjisoa",
            "Francisco Massa",
            "Daniel Haziza",
            "Luca Wehrstedt",
            "Jianyuan Wang",
            "Timoth\\'ee Darcet",
            "Th\\'eo Moutakanni",
            "Leonel Sentana",
            "Claire Roberts",
            "Andrea Vedaldi",
            "Jamie Tolan",
            "John Brandt",
            "Camille Couprie",
            "Julien Mairal",
            "Herv\\'e J\\'egou",
            "Patrick Labatut",
            "Piotr Bojanowski"
        ],
        "title": "DINOv3",
        "abstract": "arXiv:2508.10104v1 Announce Type: new  Abstract: Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.",
        "arxiv_id": "2508.10104",
        "ARXIVID": "2508.10104",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. DINOv3 is a new vision foundation model with methodological improvements (Gram anchoring, post-hoc strategies) and strong empirical results across a wide range of vision tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2508.10833": {
        "authors": [
            "Zhangxuan Gu",
            "Zhengwen Zeng",
            "Zhenyu Xu",
            "Xingran Zhou",
            "Shuheng Shen",
            "Yunfei Liu",
            "Beitong Zhou",
            "Changhua Meng",
            "Tianyu Xia",
            "Weizhi Chen",
            "Yue Wen",
            "Jingya Dou",
            "Fei Tang",
            "Jinzhen Lin",
            "Yulin Liu",
            "Zhenlin Guo",
            "Yichen Gong",
            "Heng Jia",
            "Changlong Gao",
            "Yuan Guo",
            "Yong Deng",
            "Zhenyu Guo",
            "Liang Chen",
            "Weiqiang Wang"
        ],
        "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
        "abstract": "arXiv:2508.10833v1 Announce Type: new  Abstract: We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.",
        "arxiv_id": "2508.10833",
        "ARXIVID": "2508.10833",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) and criterion 3 (embodied AI, new methods for UI agents). UI-Venus is a multimodal large language model for UI grounding and navigation, with novel reinforcement finetuning and self-evolving trajectory alignment. It also provides a new open-source agent and data cleaning protocols, which is a significant methodological contribution.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.10711": {
        "authors": [
            "NextStep Team",
            "Chunrui Han",
            "Guopeng Li",
            "Jingwei Wu",
            "Quan Sun",
            "Yan Cai",
            "Yuang Peng",
            "Zheng Ge",
            "Deyu Zhou",
            "Haomiao Tang",
            "Hongyu Zhou",
            "Kenkun Liu",
            "Ailin Huang",
            "Bin Wang",
            "Changxin Miao",
            "Deshan Sun",
            "En Yu",
            "Fukun Yin",
            "Gang Yu",
            "Hao Nie",
            "Haoran Lv",
            "Hanpeng Hu",
            "Jia Wang",
            "Jian Zhou",
            "Jianjian Sun",
            "Kaijun Tan",
            "Kang An",
            "Kangheng Lin",
            "Liang Zhao",
            "Mei Chen",
            "Peng Xing",
            "Rui Wang",
            "Shiyu Liu",
            "Shutao Xia",
            "Tianhao You",
            "Wei Ji",
            "Xianfang Zeng",
            "Xin Han",
            "Xuelin Zhang",
            "Yana Wei",
            "Yanming Xu",
            "Yimin Jiang",
            "Yingming Wang",
            "Yu Zhou",
            "Yucheng Han",
            "Ziyang Meng",
            "Binxing Jiao",
            "Daxin Jiang",
            "Xiangyu Zhang",
            "Yibo Zhu"
        ],
        "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "abstract": "arXiv:2508.10711v1 Announce Type: new  Abstract: Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
        "arxiv_id": "2508.10711",
        "ARXIVID": "2508.10711",
        "COMMENT": "This paper presents NextStep-1, a large-scale autoregressive model for text-to-image generation using continuous image tokens, achieving state-of-the-art results. It is directly relevant to criterion 4 (vision foundation models and applications) and also touches on generative modeling in multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.10681": {
        "authors": [
            "Mengyang Zhao",
            "Teng Fu",
            "Haiyang Yu",
            "Ke Niu",
            "Bin Li"
        ],
        "title": "IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning",
        "abstract": "arXiv:2508.10681v1 Announce Type: new  Abstract: Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in automating industrial quality inspection. Recently, some FS-IAD methods based on Large Vision-Language Models (LVLMs) have been proposed with some achievements through prompt learning or fine-tuning. However, existing LVLMs focus on general tasks but lack basic industrial knowledge and reasoning capabilities related to FS-IAD, making these methods far from specialized human quality inspectors. To address these challenges, we propose a unified framework, IADGPT, designed to perform FS-IAD in a human-like manner, while also handling associated localization and reasoning tasks, even for diverse and novel industrial products. To this end, we introduce a three-stage progressive training strategy inspired by humans. Specifically, the first two stages gradually guide IADGPT in acquiring fundamental industrial knowledge and discrepancy awareness. In the third stage, we design an in-context learning-based training paradigm, enabling IADGPT to leverage a few-shot image as the exemplars for improved generalization to novel products. In addition, we design a strategy that enables IADGPT to output image-level and pixel-level anomaly scores using the logits output and the attention map, respectively, in conjunction with the language output to accomplish anomaly reasoning. To support our training, we present a new dataset comprising 100K images across 400 diverse industrial product categories with extensive attribute-level textual annotations. Experiments indicate IADGPT achieves considerable performance gains in anomaly detection and demonstrates competitiveness in anomaly localization and reasoning. We will release our dataset in camera-ready.",
        "arxiv_id": "2508.10681",
        "ARXIVID": "2508.10681",
        "COMMENT": "This paper introduces IADGPT, a unified Large Vision-Language Model (LVLM) for few-shot industrial anomaly detection, localization, and reasoning, with a new dataset and in-context learning. This directly matches criterion 2 (new VLLMs/MLLMs) and also touches on criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.10771": {
        "authors": [
            "Jieyu Li",
            "Xin Zhang",
            "Joey Tianyi Zhou"
        ],
        "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences",
        "abstract": "arXiv:2508.10771v1 Announce Type: new  Abstract: Recent advances in AI-generated content have fueled the rise of highly realistic synthetic videos, posing severe risks to societal trust and digital integrity. Existing benchmarks for video authenticity detection typically suffer from limited realism, insufficient scale, and inadequate complexity, failing to effectively evaluate modern vision-language models against sophisticated forgeries. To address this critical gap, we introduce AEGIS, a novel large-scale benchmark explicitly targeting the detection of hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises over 10,000 rigorously curated real and synthetic videos generated by diverse, state-of-the-art generative models, including Stable Video Diffusion, CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary architectures. In particular, AEGIS features specially constructed challenging subsets enhanced with robustness evaluation. Furthermore, we provide multimodal annotations spanning Semantic-Authenticity Descriptions, Motion Features, and Low-level Visual Features, facilitating authenticity detection and supporting downstream tasks such as multimodal fusion and forgery localization. Extensive experiments using advanced vision-language models demonstrate limited detection capabilities on the most challenging subsets of AEGIS, highlighting the dataset's unique complexity and realism beyond the current generalization capabilities of existing models. In essence, AEGIS establishes an indispensable evaluation benchmark, fundamentally advancing research toward developing genuinely robust, reliable, broadly generalizable video authenticity detection methodologies capable of addressing real-world forgery threats. Our dataset is available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.",
        "arxiv_id": "2508.10771",
        "ARXIVID": "2508.10771",
        "COMMENT": "Matches criterion 3 and 4: This paper introduces AEGIS, a large-scale benchmark for evaluating the authenticity of AI-generated videos, with multimodal annotations and challenging subsets. It is highly relevant for vision foundation models and embodied AI benchmarks, especially for evaluating VLMs on real-world forgery detection.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.10893": {
        "authors": [
            "Yushi Lan",
            "Yihang Luo",
            "Fangzhou Hong",
            "Shangchen Zhou",
            "Honghua Chen",
            "Zhaoyang Lyu",
            "Shuai Yang",
            "Bo Dai",
            "Chen Change Loy",
            "Xingang Pan"
        ],
        "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
        "abstract": "arXiv:2508.10893v1 Announce Type: new  Abstract: We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.",
        "arxiv_id": "2508.10893",
        "ARXIVID": "2508.10893",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods): STream3R introduces a causal Transformer for scalable sequential 3D reconstruction, enabling online 3D perception and real-time understanding, which is a novel angle for embodied AI and 3D vision.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.10667": {
        "authors": [
            "Shixiong Xu",
            "Chenghao Zhang",
            "Lubin Fan",
            "Yuan Zhou",
            "Bin Fan",
            "Shiming Xiang",
            "Gaofeng Meng",
            "Jieping Ye"
        ],
        "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models",
        "abstract": "arXiv:2508.10667v1 Announce Type: new  Abstract: Large visual language models (LVLMs) have demonstrated impressive performance in coarse-grained geo-localization at the country or city level, but they struggle with fine-grained street-level localization within urban areas. In this paper, we explore integrating city-wide address localization capabilities into LVLMs, facilitating flexible address-related question answering using street-view images. A key challenge is that the street-view visual question-and-answer (VQA) data provides only microscopic visual cues, leading to subpar performance in fine-tuned models. To tackle this issue, we incorporate perspective-invariant satellite images as macro cues and propose cross-view alignment tuning including a satellite-view and street-view image grafting mechanism, along with an automatic label generation mechanism. Then LVLM's global understanding of street distribution is enhanced through cross-view matching. Our proposed model, named AddressVLM, consists of two-stage training protocols: cross-view alignment tuning and address localization tuning. Furthermore, we have constructed two street-view VQA datasets based on image address localization datasets from Pittsburgh and San Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on these two datasets, respectively.",
        "arxiv_id": "2508.10667",
        "ARXIVID": "2508.10667",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): AddressVLM is a large vision-language model for fine-grained address localization, introducing cross-view alignment tuning and new datasets. The focus on cross-view alignment and fine-grained localization is a novel methodological improvement in VLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.10770": {
        "authors": [
            "Tiancheng Han",
            "Yunfei Gao",
            "Yong Li",
            "Wuzhou Yu",
            "Qiaosheng Zhang",
            "Wenqi Shao"
        ],
        "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models",
        "abstract": "arXiv:2508.10770v1 Announce Type: new  Abstract: Spatio-physical reasoning, a foundation capability for understanding the real physics world, is a critical step towards building robust world models. While recent vision language models (VLMs) have shown remarkable progress in specialized domains like multimodal mathematics and pure spatial understanding, their capability for spatio-physical reasoning remains largely unexplored. This paper provides a comprehensive diagnostic analysis of mainstream VLMs, revealing that current models perform inadequately on this crucial task. Further detailed analysis shows that this underperformance is largely attributable to biases caused by human-like prior and a lack of deep reasoning. To address these challenges, we apply supervised fine-tuning followed by rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant improvements in spatio-physical reasoning capabilities and surpassing leading proprietary models. Nevertheless, despite this success, the model's generalization to new physics scenarios remains limited -- underscoring the pressing need for new approaches in spatio-physical reasoning.",
        "arxiv_id": "2508.10770",
        "ARXIVID": "2508.10770",
        "COMMENT": "Matches criterion 1 and 2: Probes and improves spatio-physical reasoning in VLMs (spatial understanding in VLLMs), and introduces a new method for improving this capability. Also provides a comprehensive diagnostic analysis and new training approach.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.10433": {
        "authors": [
            "Runqi Qiao",
            "Qiuna Tan",
            "Peiqing Yang",
            "Yanzi Wang",
            "Xiaowan Wang",
            "Enhui Wan",
            "Sitong Zhou",
            "Guanting Dong",
            "Yuchen Zeng",
            "Yida Xu",
            "Jie Wang",
            "Chong Sun",
            "Chen Li",
            "Honggang Zhang"
        ],
        "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning",
        "abstract": "arXiv:2508.10433v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.",
        "arxiv_id": "2508.10433",
        "ARXIVID": "2508.10433",
        "COMMENT": "This paper introduces We-Math 2.0, a system for enhancing mathematical reasoning in MLLMs, including a new knowledge system, datasets, RL-based training, and a benchmark. It directly matches criterion 2 (new MLLMs) and also provides a new benchmark (criterion 3).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10133": {
        "authors": [
            "Thanh-Dat Truong",
            "Christophe Bobda",
            "Nitin Agarwal",
            "Khoa Luu"
        ],
        "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning",
        "abstract": "arXiv:2508.10133v1 Announce Type: new  Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.",
        "arxiv_id": "2508.10133",
        "ARXIVID": "2508.10133",
        "COMMENT": "This paper introduces MANGO, a multimodal attention-based normalizing flow approach for fusion learning, with a new invertible cross-attention layer and several new cross-attention mechanisms. This is a methodological improvement in multi-modal learning and fusion, relevant to criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10897": {
        "authors": [
            "Mengyuan Liu",
            "Xinshun Wang",
            "Zhongbin Fang",
            "Deheng Ye",
            "Xia Li",
            "Tao Tang",
            "Songtao Wu",
            "Xiangtai Li",
            "Ming-Hsuan Yang"
        ],
        "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning",
        "abstract": "arXiv:2508.10897v1 Announce Type: new  Abstract: This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context.",
        "arxiv_id": "2508.10897",
        "ARXIVID": "2508.10897",
        "COMMENT": "This paper introduces Human-in-Context, a unified cross-domain 3D human motion model using in-context learning, handling multiple modalities, tasks, and datasets. It is relevant to spatial understanding and intelligence in embodied agents (criterion 1), and proposes a new methodological improvement for cross-domain modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10655": {
        "authors": [
            "Zhangyong Tang",
            "Tianyang Xu",
            "Xuefeng Zhu",
            "Chunyang Cheng",
            "Tao Zhou",
            "Xiaojun Wu",
            "Josef Kittler"
        ],
        "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking",
        "abstract": "arXiv:2508.10655v1 Announce Type: new  Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \\textit{inconsistency} between training and testing, thus leading to performance \\textit{degradation}. To address these issues, this work advances in two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\\%. \\ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.",
        "arxiv_id": "2508.10655",
        "ARXIVID": "2508.10655",
        "COMMENT": "Matches criterion 3: This paper introduces a new unified benchmark (UniBench300) for multi-modal visual object tracking and proposes a continual learning-based unification process, addressing issues of evaluation inconsistency and knowledge forgetting. The benchmark and continual learning angle are novel and relevant for embodied AI and multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10264": {
        "authors": [
            "Haonan Ge",
            "Yiwei Wang",
            "Ming-Hsuan Yang",
            "Yujun Cai"
        ],
        "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs",
        "abstract": "arXiv:2508.10264v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.",
        "arxiv_id": "2508.10264",
        "ARXIVID": "2508.10264",
        "COMMENT": "Matches criterion 2: Proposes a new decoding method (MRFD) for LVLMs (Large Vision-Language Models) to mitigate hallucinations, with a clever statistical trick (JSD-based reliability weights) and region-aware prompts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10631": {
        "authors": [
            "Nicola Dall'Asen",
            "Xiaofeng Zhang",
            "Reyhane Askari Hemmat",
            "Melissa Hall",
            "Jakob Verbeek",
            "Adriana Romero-Soriano",
            "Michal Drozdzal"
        ],
        "title": "Increasing the Utility of Synthetic Images through Chamfer Guidance",
        "abstract": "arXiv:2508.10631v1 Announce Type: new  Abstract: Conditional image generative models hold considerable promise to produce infinite amounts of synthetic training data. Yet, recent progress in generation quality has come at the expense of generation diversity, limiting the utility of these models as a source of synthetic training data. Although guidance-based approaches have been introduced to improve the utility of generated data by focusing on quality or diversity, the (implicit or explicit) utility functions oftentimes disregard the potential distribution shift between synthetic and real data. In this work, we introduce Chamfer Guidance: a training-free guidance approach which leverages a handful of real exemplar images to characterize the quality and diversity of synthetic data. We show that by leveraging the proposed Chamfer Guidance, we can boost the diversity of the generations w.r.t. a dataset of real images while maintaining or improving the generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our approach achieves state-of-the-art few-shot performance with as little as 2 exemplar real images, obtaining 96.4\\% in terms of precision, and 86.4\\% in terms of distributional coverage, which increase to 97.5\\% and 92.7\\%, respectively, when using 32 real images. We showcase the benefits of the Chamfer Guidance generation by training downstream image classifiers on synthetic data, achieving accuracy boost of up to 15\\% for in-distribution over the baselines, and up to 16\\% in out-of-distribution. Furthermore, our approach does not require using the unconditional model, and thus obtains a 31\\% reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling time.",
        "arxiv_id": "2508.10631",
        "ARXIVID": "2508.10631",
        "COMMENT": "Matches criterion 4: Proposes a new guidance method (Chamfer Guidance) to improve the utility of synthetic images from generative models, with strong empirical results for vision foundation models and downstream applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10522": {
        "authors": [
            "Quang Nguyen",
            "Nhat Le",
            "Baoru Huang",
            "Minh Nhat Vu",
            "Chengcheng Tang",
            "Van Nguyen",
            "Ngan Le",
            "Thieu Vo",
            "Anh Nguyen"
        ],
        "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba",
        "abstract": "arXiv:2508.10522v1 Announce Type: new  Abstract: Estimating human dance motion is a challenging task with various industrial applications. Recently, many efforts have focused on predicting human dance motion using either egocentric video or music as input. However, the task of jointly estimating human motion from both egocentric video and music remains largely unexplored. In this paper, we aim to develop a new method that predicts human dance motion from both egocentric video and music. In practice, the egocentric view often obscures much of the body, making accurate full-pose estimation challenging. Additionally, incorporating music requires the generated head and body movements to align well with both visual and musical inputs. We first introduce EgoAIST++, a new large-scale dataset that combines both egocentric views and music with more than 36 hours of dancing motion. Drawing on the success of diffusion models and Mamba on modeling sequences, we develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body. We illustrate that our approach is theoretically supportive. Intensive experiments show that our method clearly outperforms state-of-the-art approaches and generalizes effectively to real-world data.",
        "arxiv_id": "2508.10522",
        "ARXIVID": "2508.10522",
        "COMMENT": "Matches criterion 3: Embodied AI with a new benchmark (EgoAIST++) and a novel method (Skeleton Mamba) for joint egocentric video and music-driven human motion estimation. Also leverages diffusion models and Mamba for sequence modeling, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10868": {
        "authors": [
            "Yibo Zhang",
            "Li Zhang",
            "Rui Ma",
            "Nan Cao"
        ],
        "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
        "abstract": "arXiv:2508.10868v1 Announce Type: new  Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.",
        "arxiv_id": "2508.10868",
        "ARXIVID": "2508.10868",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): TexVerse is a large-scale, high-resolution 3D dataset with textures, which will be valuable for training and evaluating vision foundation models and generative models in 3D vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10769": {
        "authors": [
            "Zhiqi Shen",
            "Shaojing Fan",
            "Danni Xu",
            "Terence Sim",
            "Mohan Kankanhalli"
        ],
        "title": "Modeling Human Responses to Multimodal AI Content",
        "abstract": "arXiv:2508.10769v1 Announce Type: new  Abstract: As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.",
        "arxiv_id": "2508.10769",
        "ARXIVID": "2508.10769",
        "COMMENT": "This paper introduces a large dataset and a new LLM-based agent system (T-Lens) for modeling human responses to multimodal AI content, with new metrics for trustworthiness, impact, and openness. It is relevant to multi-modal learning and VLLM/MLLM applications (criterion 2), and provides empirical insights into human-AI interaction.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.10838": {
        "authors": [
            "Peng Xu",
            "Zhiyu Xiang",
            "Jingyun Fu",
            "Tianyu Pu",
            "Kai Wang",
            "Chaojie Ji",
            "Tingming Bai",
            "Eryun Liu"
        ],
        "title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning",
        "abstract": "arXiv:2508.10838v1 Announce Type: new  Abstract: Current self-supervised stereo matching relies on the photometric consistency assumption, which breaks down in occluded regions due to ill-posed correspondences. To address this issue, we propose BaCon-Stereo, a simple yet effective contrastive learning framework for self-supervised stereo network training in both non-occluded and occluded regions. We adopt a teacher-student paradigm with multi-baseline inputs, in which the stereo pairs fed into the teacher and student share the same reference view but differ in target views. Geometrically, regions occluded in the student's target view are often visible in the teacher's, making it easier for the teacher to predict in these regions. The teacher's prediction is rescaled to match the student's baseline and then used to supervise the student. We also introduce an occlusion-aware attention map to better guide the student in learning occlusion completion. To support training, we synthesize a multi-baseline dataset BaCon-20k. Extensive experiments demonstrate that BaCon-Stereo improves prediction in both occluded and non-occluded regions, achieves strong generalization and robustness, and outperforms state-of-the-art self-supervised methods on both KITTI 2015 and 2012 benchmarks. Our code and dataset will be released upon paper acceptance.",
        "arxiv_id": "2508.10838",
        "ARXIVID": "2508.10838",
        "COMMENT": "Matches criterion 1: Presents a new self-supervised stereo matching method with multi-baseline contrastive learning, improving spatial understanding in vision systems. Also introduces a new dataset (BaCon-20k), which is relevant to spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.10869": {
        "authors": [
            "Sushant Gautam",
            "Vajira Thambawita",
            "Michael Riegler",
            "P{\\aa}l Halvorsen",
            "Steven Hicks"
        ],
        "title": "Medico 2025: Visual Question Answering for Gastrointestinal Imaging",
        "abstract": "arXiv:2508.10869v1 Announce Type: new  Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025",
        "arxiv_id": "2508.10869",
        "ARXIVID": "2508.10869",
        "COMMENT": "This paper introduces a new VQA benchmark for gastrointestinal imaging, focusing on explainable AI and multimodal explanations. It matches criterion 3 (new benchmark for embodied AI or vision-language tasks) and is also relevant to criterion 2 (VLLMs/MLLMs) due to the VQA focus.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10554": {
        "authors": [
            "Marc J. Fischer",
            "Jeffrey Potts",
            "Gabriel Urreola",
            "Dax Jones",
            "Paolo Palmisciano",
            "E. Bradley Strong",
            "Branden Cord",
            "Andrew D. Hernandez",
            "Julia D. Sharma",
            "E. Brandon Strong"
        ],
        "title": "AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications",
        "abstract": "arXiv:2508.10554v1 Announce Type: new  Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next generation of intraoperative surgical guidance, promising to overcome limitations of traditional navigation systems. However, known issues with AR depth perception due to vergence-accommodation conflict and occlusion handling limitations of the currently commercially available display technology present acute challenges in surgical settings where precision is paramount. This study presents a novel methodology for utilizing AR guidance to register anatomical targets and provide real-time instrument navigation using placement of simulated external ventricular drain catheters on a phantom model as the clinical scenario. The system registers target positions to the patient through a novel surface tracing method and uses real-time infrared tool tracking to aid in catheter placement, relying only on the onboard sensors of the Microsoft HoloLens 2. A group of intended users performed the procedure of simulated insertions under two AR guidance conditions: static in-situ visualization, where planned trajectories are overlaid directly onto the patient anatomy, and real-time tool-tracking guidance, where live feedback of the catheter's pose is provided relative to the plan. Following the insertion tests, computed tomography scans of the phantom models were acquired, allowing for evaluation of insertion accuracy, target deviation, angular error, and depth precision. System Usability Scale surveys assessed user experience and cognitive workload. Tool-tracking guidance improved performance metrics across all accuracy measures and was preferred by users in subjective evaluations. A free copy of this paper and all supplemental materials are available at https://bit.ly/45l89Hq.",
        "arxiv_id": "2508.10554",
        "ARXIVID": "2508.10554",
        "COMMENT": "This paper presents a novel AR surgical navigation system using surface tracing and real-time tool tracking for neurosurgical applications. It involves spatial understanding and intelligence in an embodied (surgical) setting, matching criterion 1. The method leverages AR and sensor fusion for spatial registration and navigation, which is a methodological improvement in spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10794": {
        "authors": [
            "De-Xing Huang",
            "Xiao-Hu Zhou",
            "Mei-Jiang Gui",
            "Xiao-Liang Xie",
            "Shi-Qi Liu",
            "Shuang-Yi Wang",
            "Tian-Yu Xiang",
            "Rui-Ze Ma",
            "Nu-Fang Xiao",
            "Zeng-Guang Hou"
        ],
        "title": "VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation",
        "abstract": "arXiv:2508.10794v1 Announce Type: new  Abstract: Accurate vessel segmentation in X-ray angiograms is crucial for numerous clinical applications. However, the scarcity of annotated data presents a significant challenge, which has driven the adoption of self-supervised learning (SSL) methods such as masked image modeling (MIM) to leverage large-scale unlabeled data for learning transferable representations. Unfortunately, conventional MIM often fails to capture vascular anatomy because of the severe class imbalance between vessel and background pixels, leading to weak vascular representations. To address this, we introduce Vascular anatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored for X-ray angiograms that explicitly integrates anatomical knowledge into the pre-training process. Specifically, it comprises two complementary components: anatomy-guided masking strategy and anatomical consistency loss. The former preferentially masks vessel-containing patches to focus the model on reconstructing vessel-relevant regions. The latter enforces consistency in vascular semantics between the original and reconstructed images, thereby improving the discriminability of vascular representations. Empirically, VasoMIM achieves state-of-the-art performance across three datasets. These findings highlight its potential to facilitate X-ray angiogram analysis.",
        "arxiv_id": "2508.10794",
        "ARXIVID": "2508.10794",
        "COMMENT": "This paper proposes VasoMIM, a vascular anatomy-aware masked image modeling framework for vessel segmentation in X-ray angiograms. It is a vision foundation model application (criterion 4), with a novel anatomy-guided masking strategy and anatomical consistency loss.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10113": {
        "authors": [
            "Kaixin Peng",
            "Mengyang Zhao",
            "Haiyang Yu",
            "Teng Fu",
            "Bin Li"
        ],
        "title": "Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs",
        "abstract": "arXiv:2508.10113v1 Announce Type: new  Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed significant challenges for archaeological decipherment due to its rarity, abstractness, and pictographic diversity. Current deep learning-based methods have made exciting progress on the OBS decipherment task, but existing approaches often ignore the intricate connections between glyphs and the semantics of OBS. This results in limited generalization and interpretability, especially when addressing zero-shot settings and undeciphered OBS. To this end, we propose an interpretable OBS decipherment method based on Large Vision-Language Models, which synergistically combines radical analysis and pictograph-semantic understanding to bridge the gap between glyphs and meanings of OBS. Specifically, we propose a progressive training strategy that guides the model from radical recognition and analysis to pictographic analysis and mutual analysis, thus enabling reasoning from glyph to meaning. We also design a Radical-Pictographic Dual Matching mechanism informed by the analysis results, significantly enhancing the model's zero-shot decipherment performance. To facilitate model training, we propose the Pictographic Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated with OBS images and pictographic analysis texts. Experimental results on public benchmarks demonstrate that our approach achieves state-of-the-art Top-10 accuracy and superior zero-shot decipherment capabilities. More importantly, our model delivers logical analysis processes, possibly providing archaeologically valuable reference results for undeciphered OBS, and thus has potential applications in digital humanities and historical research. The dataset and code will be released in https://github.com/PKXX1943/PD-OBS.",
        "arxiv_id": "2508.10113",
        "ARXIVID": "2508.10113",
        "COMMENT": "This paper uses LVLMs for interpretable Oracle Bone Script decipherment, combining radical and pictographic analysis. It is relevant to VLLMs (criterion 2), and demonstrates a novel application of vision-language models to historical script analysis.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.10710": {
        "authors": [
            "Joohyeon Lee",
            "Jin-Seop Lee",
            "Jee-Hyong Lee"
        ],
        "title": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation",
        "abstract": "arXiv:2508.10710v1 Announce Type: new  Abstract: Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \\textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .",
        "arxiv_id": "2508.10710",
        "ARXIVID": "2508.10710",
        "COMMENT": "This paper proposes CountCluster, a training-free method for controlling object quantity in text-to-image diffusion models by clustering cross-attention maps. It is a clever generative modeling trick for vision foundation models (criterion 4), and addresses a known limitation in T2I models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.10110": {
        "authors": [
            "Sushrut Patwardhan",
            "Raghavendra Ramachandra",
            "Sushma Venkatesh"
        ],
        "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model",
        "abstract": "arXiv:2508.10110v1 Announce Type: new  Abstract: Morphing attack detection has become an essential component of face recognition systems for ensuring a reliable verification scenario. In this paper, we present a multimodal learning approach that can provide a textual description of morphing attack detection. We first show that zero-shot evaluation of the proposed framework using Contrastive Language-Image Pretraining (CLIP) can yield not only generalizable morphing attack detection, but also predict the most relevant text snippet. We present an extensive analysis of ten different textual prompts that include both short and long textual prompts. These prompts are engineered by considering the human understandable textual snippet. Extensive experiments were performed on a face morphing dataset that was developed using a publicly available face biometric dataset. We present an evaluation of SOTA pre-trained neural networks together with the proposed framework in the zero-shot evaluation of five different morphing generation techniques that are captured in three different mediums.",
        "arxiv_id": "2508.10110",
        "ARXIVID": "2508.10110",
        "COMMENT": "This paper uses CLIP (a vision-language foundation model) for morphing attack detection and interpretable image-text explanations. It matches criterion 4 (vision foundation models and applications), with a focus on security and interpretability.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10900": {
        "authors": [
            "Shuteng Wang",
            "Christian Theobalt",
            "Vladislav Golyanik"
        ],
        "title": "Quantum Visual Fields with Neural Amplitude Encoding",
        "abstract": "arXiv:2508.10900v1 Announce Type: new  Abstract: Quantum Implicit Neural Representations (QINRs) include components for learning and execution on gate-based quantum computers. While QINRs recently emerged as a promising new paradigm, many challenges concerning their architecture and ansatz design, the utility of quantum-mechanical properties, training efficiency and the interplay with classical modules remain. This paper advances the field by introducing a new type of QINR for 2D image and 3D geometric field learning, which we collectively refer to as Quantum Visual Field (QVF). QVF encodes classical data into quantum statevectors using neural amplitude encoding grounded in a learnable energy manifold, ensuring meaningful Hilbert space embeddings. Our ansatz follows a fully entangled design of learnable parametrised quantum circuits, with quantum (unitary) operations performed in the real Hilbert space, resulting in numerically stable training with fast convergence. QVF does not rely on classical post-processing -- in contrast to the previous QINR learning approach -- and directly employs projective measurement to extract learned signals encoded in the ansatz. Experiments on a quantum hardware simulator demonstrate that QVF outperforms the existing quantum approach and widely used classical foundational baselines in terms of visual representation accuracy across various metrics and model characteristics, such as learning of high-frequency details. We also show applications of QVF in 2D and 3D field completion and 3D shape interpolation, highlighting its practical potential.",
        "arxiv_id": "2508.10900",
        "ARXIVID": "2508.10900",
        "COMMENT": "This paper introduces a quantum neural representation for 2D/3D visual fields. While it is a novel approach in representation learning and touches on vision foundation models, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or embodied AI benchmarks. It is more foundational in quantum/classical hybrid models for vision.",
        "RELEVANCE": 5,
        "NOVELTY": 8
    },
    "2508.10858": {
        "authors": [
            "Harold Haodong Chen",
            "Haojian Huang",
            "Qifeng Chen",
            "Harry Yang",
            "Ser-Nam Lim"
        ],
        "title": "Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation",
        "abstract": "arXiv:2508.10858v1 Announce Type: new  Abstract: Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize \"good data\" from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.",
        "arxiv_id": "2508.10858",
        "ARXIVID": "2508.10858",
        "COMMENT": "This paper introduces PhysHPO, a hierarchical preference optimization framework for physically plausible video generation. It addresses video generation with a focus on physical realism and introduces a novel data selection pipeline. While it is relevant to generative modeling in vision, it does not directly match the four criteria, but is close to criterion 4 (vision foundation models and applications) due to its focus on video generation quality and alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.10498": {
        "authors": [
            "Jianda Mao",
            "Kaibo Wang",
            "Yang Xiang",
            "Kani Chen"
        ],
        "title": "TweezeEdit: Consistent and Efficient Image Editing with Path Regularization",
        "abstract": "arXiv:2508.10498v1 Announce Type: new  Abstract: Large-scale pre-trained diffusion models empower users to edit images through text guidance. However, existing methods often over-align with target prompts while inadequately preserving source image semantics. Such approaches generate target images explicitly or implicitly from the inversion noise of the source images, termed the inversion anchors. We identify this strategy as suboptimal for semantic preservation and inefficient due to elongated editing paths. We propose TweezeEdit, a tuning- and inversion-free framework for consistent and efficient image editing. Our method addresses these limitations by regularizing the entire denoising path rather than relying solely on the inversion anchors, ensuring source semantic retention and shortening editing paths. Guided by gradient-driven regularization, we efficiently inject target prompt semantics along a direct path using a consistency model. Extensive experiments demonstrate TweezeEdit's superior performance in semantic preservation and target alignment, outperforming existing methods. Remarkably, it requires only 12 steps (1.6 seconds per edit), underscoring its potential for real-time applications.",
        "arxiv_id": "2508.10498",
        "ARXIVID": "2508.10498",
        "COMMENT": "This paper presents TweezeEdit, a new method for efficient and consistent image editing using diffusion models, with a focus on semantic preservation and editing path regularization. It is relevant to vision foundation models and their applications (criterion 4), especially in generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.10743": {
        "authors": [
            "Ziwei Zou",
            "Bei Zou",
            "Xiaoyan Kui",
            "Wenqi Lu",
            "Haoran Dou",
            "Arezoo Zakeri",
            "Timothy Cootes",
            "Alejandro F Frangi",
            "Jinming Duan"
        ],
        "title": "An Efficient Model-Driven Groupwise Approach for Atlas Construction",
        "abstract": "arXiv:2508.10743v1 Announce Type: new  Abstract: Atlas construction is fundamental to medical image analysis, offering a standardized spatial reference for tasks such as population-level anatomical modeling. While data-driven registration methods have recently shown promise in pairwise settings, their reliance on large training datasets, limited generalizability, and lack of true inference phases in groupwise contexts hinder their practical use. In contrast, model-driven methods offer training-free, theoretically grounded, and data-efficient alternatives, though they often face scalability and optimization challenges when applied to large 3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration via Coordinate descent), a novel model-driven groupwise registration framework for atlas construction. DARC supports a broad range of image dissimilarity metrics and efficiently handles arbitrary numbers of 3D images without incurring GPU memory issues. Through a coordinate descent strategy and a centrality-enforcing activation function, DARC produces unbiased, diffeomorphic atlases with high anatomical fidelity. Beyond atlas construction, we demonstrate two key applications: (1) One-shot segmentation, where labels annotated only on the atlas are propagated to subjects via inverse deformations, outperforming state-of-the-art few-shot methods; and (2) shape synthesis, where new anatomical variants are generated by warping the atlas mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a flexible, generalizable, and resource-efficient framework for atlas construction and applications.",
        "arxiv_id": "2508.10743",
        "ARXIVID": "2508.10743",
        "COMMENT": "This paper introduces DARC, a model-driven groupwise registration framework for atlas construction in medical imaging. It is a methodological improvement in spatial understanding, but not on embodied agents. Closest to criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.10453": {
        "authors": [
            "Qiang Zhu",
            "Xiandong Meng",
            "Yuxian Jiang",
            "Fan Zhang",
            "David Bull",
            "Shuyuan Zhu",
            "Bing Zeng"
        ],
        "title": "Trajectory-aware Shifted State Space Models for Online Video Super-Resolution",
        "abstract": "arXiv:2508.10453v1 Announce Type: new  Abstract: Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com.",
        "arxiv_id": "2508.10453",
        "ARXIVID": "2508.10453",
        "COMMENT": "This paper proposes a new method for online video super-resolution using trajectory-aware shifted state space models, which involves spatio-temporal information aggregation. This is a methodological improvement in spatial understanding for video, but not directly on embodied agents. Closest to criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.10407": {
        "authors": [
            "Eunseo Koh",
            "Seunghoo Hong",
            "Tae-Young Kim",
            "Simon S. Woo",
            "Jae-Pil Heo"
        ],
        "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models",
        "abstract": "arXiv:2508.10407v1 Announce Type: new  Abstract: Text-to-Image (T2I) diffusion models have made significant progress in generating diverse high-quality images from textual prompts. However, these models still face challenges in suppressing content that is strongly entangled with specific words. For example, when generating an image of ``Charlie Chaplin\", a ``mustache\" consistently appears even if explicitly instructed not to include it, as the concept of ``mustache\" is strongly entangled with ``Charlie Chaplin\". To address this issue, we propose a novel approach to directly suppress such entangled content within the text embedding space of diffusion models. Our method introduces a delta vector that modifies the text embedding to weaken the influence of undesired content in the generated image, and we further demonstrate that this delta vector can be easily obtained through a zero-shot approach. Furthermore, we propose a Selective Suppression with Delta Vector (SSDV) method to adapt delta vector into the cross-attention mechanism, enabling more effective suppression of unwanted content in regions where it would otherwise be generated. Additionally, we enabled more precise suppression in personalized T2I models by optimizing delta vector, which previous baselines were unable to achieve. Extensive experimental results demonstrate that our approach significantly outperforms existing methods, both in terms of quantitative and qualitative metrics.",
        "arxiv_id": "2508.10407",
        "ARXIVID": "2508.10407",
        "COMMENT": "This paper proposes a new method for suppressing entangled content in text-to-image diffusion models by manipulating text embeddings with a delta vector. While it is relevant to generative modeling and multi-modal learning, it does not directly introduce a new VLLM/MLLM or a vision foundation model, nor is it about embodied AI or spatial intelligence. It is a clever statistical trick for generative models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.10704": {
        "authors": [
            "Zhanwen Liu",
            "Yujing Sun",
            "Yang Wang",
            "Nan Yang",
            "Shengbo Eben Li",
            "Xiangmo Zhao"
        ],
        "title": "Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios",
        "abstract": "arXiv:2508.10704v1 Announce Type: new  Abstract: The dynamic range limitation of conventional RGB cameras reduces global contrast and causes loss of high-frequency details such as textures and edges in complex traffic environments (e.g., nighttime driving, tunnels), hindering discriminative feature extraction and degrading frame-based object detection. To address this, we integrate a bio-inspired event camera with an RGB camera to provide high dynamic range information and propose a motion cue fusion network (MCFNet), which achieves optimal spatiotemporal alignment and adaptive cross-modal feature fusion under challenging lighting. Specifically, an event correction module (ECM) temporally aligns asynchronous event streams with image frames via optical-flow-based warping, jointly optimized with the detection network to learn task-aware event representations. The event dynamic upsampling module (EDUM) enhances spatial resolution of event frames to match image structures, ensuring precise spatiotemporal alignment. The cross-modal mamba fusion module (CMM) uses adaptive feature fusion with a novel interlaced scanning mechanism, effectively integrating complementary information for robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that MCFNet significantly outperforms existing methods in various poor lighting and fast moving traffic scenarios. Notably, on the DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The code is available at https://github.com/Charm11492/MCFNet.",
        "arxiv_id": "2508.10704",
        "ARXIVID": "2508.10704",
        "COMMENT": "Related to vision foundation models and multi-modal fusion (RGB + event camera), but does not introduce a new VLLM/MLLM or embodied AI benchmark. Focuses on robust object detection in dynamic scenarios with a novel fusion network.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.10779": {
        "authors": [
            "Zhenning Shi",
            "Zizheng Yan",
            "Yuhang Yu",
            "Clara Xue",
            "Jingyu Zhuang",
            "Qi Zhang",
            "Jinwei Chen",
            "Tao Li",
            "Qingnan Fan"
        ],
        "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior",
        "abstract": "arXiv:2508.10779v1 Announce Type: new  Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.",
        "arxiv_id": "2508.10779",
        "ARXIVID": "2508.10779",
        "COMMENT": "This paper proposes a diffusion-based reference super-resolution method and introduces a new UHD dataset. While it is a strong vision paper with generative modeling, it does not directly match the criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models, but is relevant to generative modeling in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.10898": {
        "authors": [
            "Chaoyue Song",
            "Xiu Li",
            "Fan Yang",
            "Zhongcong Xu",
            "Jiacheng Wei",
            "Fayao Liu",
            "Jiashi Feng",
            "Guosheng Lin",
            "Jianfeng Zhang"
        ],
        "title": "Puppeteer: Rig and Animate Your 3D Models",
        "abstract": "arXiv:2508.10898v1 Announce Type: new  Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.",
        "arxiv_id": "2508.10898",
        "ARXIVID": "2508.10898",
        "COMMENT": "Relevant to computer vision and generative modeling for 3D content, but does not directly match any of the four criteria. The paper introduces a new framework for automatic rigging and animation of 3D models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.10774": {
        "authors": [
            "Youping Gu",
            "Xiaolong Li",
            "Yuhao Hu",
            "Bohan Zhuang"
        ],
        "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
        "abstract": "arXiv:2508.10774v1 Announce Type: new  Abstract: Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
        "arxiv_id": "2508.10774",
        "ARXIVID": "2508.10774",
        "COMMENT": "Relevant to generative modeling in multi-modal learning (video generation), but does not directly match any of the four criteria. The paper proposes a new efficient framework for video diffusion transformers, which is of general interest for generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.10719": {
        "authors": [
            "Longxiang Tang",
            "Ruihang Chu",
            "Xiang Wang",
            "Yujin Han",
            "Pingyu Wu",
            "Chunming He",
            "Yingya Zhang",
            "Shiwei Zhang",
            "Jiaya Jia"
        ],
        "title": "Exploiting Discriminative Codebook Prior for Autoregressive Image Generation",
        "abstract": "arXiv:2508.10719v1 Announce Type: new  Abstract: Advanced discrete token-based autoregressive image generation systems first tokenize images into sequences of token indices with a codebook, and then model these sequences in an autoregressive paradigm. While autoregressive generative models are trained only on index values, the prior encoded in the codebook, which contains rich token similarity information, is not exploited. Recent studies have attempted to incorporate this prior by performing naive k-means clustering on the tokens, helping to facilitate the training of generative models with a reduced codebook. However, we reveal that k-means clustering performs poorly in the codebook feature space due to inherent issues, including token space disparity and centroid distance inaccuracy. In this work, we propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to k-means clustering for more effectively mining and utilizing the token similarity information embedded in the codebook. DCPE replaces the commonly used centroid-based distance, which is found to be unsuitable and inaccurate for the token feature space, with a more reasonable instance-based distance. Using an agglomerative merging technique, it further addresses the token space disparity issue by avoiding splitting high-density regions and aggregating low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play and integrates seamlessly with existing codebook prior-based paradigms. With the discriminative prior extracted, DCPE accelerates the training of autoregressive models by 42% on LlamaGen-B and improves final FID and IS performance.",
        "arxiv_id": "2508.10719",
        "ARXIVID": "2508.10719",
        "COMMENT": "Related to generative modeling and codebook priors for autoregressive image generation, but does not directly match any of the four criteria. No new VLLM/MLLM, spatial intelligence, or embodied AI benchmark.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.10391": {
        "authors": [
            "Yaoze Zhang",
            "Rong Wu",
            "Pinlong Cai",
            "Xiaoman Wang",
            "Guohang Yan",
            "Song Mao",
            "Ding Wang",
            "Botian Shi"
        ],
        "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval",
        "abstract": "arXiv:2508.10391v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large Language Models by leveraging external knowledge, whereas the effectiveness is often compromised by the retrieval of contextually flawed or incomplete information. To address this, knowledge graph-based RAG methods have evolved towards hierarchical structures, organizing knowledge into multi-level summaries. However, these approaches still suffer from two critical, unaddressed challenges: high-level conceptual summaries exist as disconnected ``semantic islands'', lacking the explicit relations needed for cross-community reasoning; and the retrieval process itself remains structurally unaware, often degenerating into an inefficient flat search that fails to exploit the graph's rich topology. To overcome these limitations, we introduce LeanRAG, a framework that features a deeply collaborative design combining knowledge aggregation and retrieval strategies. LeanRAG first employs a novel semantic aggregation algorithm that forms entity clusters and constructs new explicit relations among aggregation-level summaries, creating a fully navigable semantic network. Then, a bottom-up, structure-guided retrieval strategy anchors queries to the most relevant fine-grained entities and then systematically traverses the graph's semantic pathways to gather concise yet contextually comprehensive evidence sets. The LeanRAG can mitigate the substantial overhead associated with path retrieval on graphs and minimizes redundant information retrieval. Extensive experiments on four challenging QA benchmarks with different domains demonstrate that LeanRAG significantly outperforming existing methods in response quality while reducing 46\\% retrieval redundancy. Code is available at: https://github.com/RaZzzyz/LeanRAG",
        "arxiv_id": "2508.10391",
        "ARXIVID": "2508.10391",
        "COMMENT": "This paper proposes LeanRAG, a knowledge-graph-based retrieval-augmented generation framework with semantic aggregation and hierarchical retrieval. While it is an interesting RAG method, it is not focused on vision, spatial intelligence, or embodied AI. Not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10731": {
        "authors": [
            "Luyao Tang",
            "Kunze Huang",
            "Chaoqi Chen",
            "Yuxuan Yuan",
            "Chenxin Li",
            "Xiaotong Tu",
            "Xinghao Ding",
            "Yue Huang"
        ],
        "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction",
        "abstract": "arXiv:2508.10731v1 Announce Type: new  Abstract: Human perceptual systems excel at inducing and recognizing objects across both known and novel categories, a capability far beyond current machine learning frameworks. While generalized category discovery (GCD) aims to bridge this gap, existing methods predominantly focus on optimizing objective functions. We present an orthogonal solution, inspired by the human cognitive process for novel object understanding: decomposing objects into visual primitives and establishing cross-knowledge comparisons. We propose ConGCD, which establishes primitive-oriented representations through high-level semantic reconstruction, binding intra-class shared attributes via deconstruction. Mirroring human preference diversity in visual processing, where distinct individuals leverage dominant or contextual cues, we implement dominant and contextual consensus units to capture class-discriminative patterns and inherent distributional invariants, respectively. A consensus scheduler dynamically optimizes activation pathways, with final predictions emerging through multiplex consensus integration. Extensive evaluations across coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a consensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.",
        "arxiv_id": "2508.10731",
        "ARXIVID": "2508.10731",
        "COMMENT": "This paper proposes a new method for generalized category discovery by decomposing objects into visual primitives and using consensus mechanisms inspired by human cognition. While it is a novel approach to category discovery, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about unsupervised category learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10542": {
        "authors": [
            "Mengyu Ren",
            "Yutong Li",
            "Hua Li",
            "Runmin Cong",
            "Sam Kwong"
        ],
        "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images",
        "abstract": "arXiv:2508.10542v1 Announce Type: new  Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.",
        "arxiv_id": "2508.10542",
        "ARXIVID": "2508.10542",
        "COMMENT": "Applies Mamba architecture and graph attention to salient object detection in remote sensing images. While it uses a novel architecture, it does not directly match any of the four criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.10566": {
        "authors": [
            "Shiyu Liu",
            "Kui Jiang",
            "Xianming Liu",
            "Hongxun Yao",
            "Xiaocheng Feng"
        ],
        "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis",
        "abstract": "arXiv:2508.10566v1 Announce Type: new  Abstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.",
        "arxiv_id": "2508.10566",
        "ARXIVID": "2508.10566",
        "COMMENT": "Related to generative modeling in multi-modal learning (audio-to-video), but does not directly match any of the four criteria. No new VLLM/MLLM, spatial intelligence, or embodied AI benchmark.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.10669": {
        "authors": [
            "Zhenye Yang",
            "Jinpeng Chen",
            "Huan Li",
            "Xiongnan Jin",
            "Xuanyang Li",
            "Junwei Zhang",
            "Hongbo Gao",
            "Kaimin Wei",
            "Senzhang Wang"
        ],
        "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation",
        "abstract": "arXiv:2508.10669v1 Announce Type: new  Abstract: Conversational recommender systems (CRSs) aim to proactively capture user preferences through natural language dialogue and recommend high-quality items. To achieve this, CRS gathers user preferences via a dialog module and builds user profiles through a recommendation module to generate appropriate recommendations. However, existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.   To address these challenges, we introduce STEP, a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates. This dual-prompt scheme allows the model to share cross-task semantics while respecting the distinct objectives of dialogue and recommendation. Experimental results show that STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.",
        "arxiv_id": "2508.10669",
        "ARXIVID": "2508.10669",
        "COMMENT": "This paper proposes STEP, a curriculum learning approach for conversational recommendation systems, focusing on context-knowledge fusion. While it uses pre-trained language models and knowledge graphs, it does not directly address any of the four criteria, especially not in vision, spatial, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10539": {
        "authors": [
            "Zetian Sun",
            "Dongfang Li",
            "Baotian Hu",
            "Min Zhang"
        ],
        "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
        "abstract": "arXiv:2508.10539v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte \\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.",
        "arxiv_id": "2508.10539",
        "ARXIVID": "2508.10539",
        "COMMENT": "This paper proposes a variance reduction method for value-based process verifiers in LLMs, focusing on mathematical reasoning. While it introduces a clever statistical trick, it does not directly address any of the four criteria, especially not in vision, spatial, or multi-modal settings.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10164": {
        "authors": [
            "Bin Hong",
            "Jiayu Liu",
            "Zhenya Huang",
            "Kai Zhang",
            "Mengdi Zhang"
        ],
        "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization",
        "abstract": "arXiv:2508.10164v1 Announce Type: new  Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong performance on complex tasks through long Chain-of-Thought (CoT) reasoning. However, their lengthy outputs increase computational costs and may lead to overthinking, raising challenges in balancing reasoning effectiveness and efficiency. Current methods for efficient reasoning often compromise reasoning quality or require extensive resources. This paper investigates efficient methods to reduce the generation length of LRMs. We analyze generation path distributions and filter generated trajectories through difficulty estimation. Subsequently, we analyze the convergence behaviors of the objectives of various preference optimization methods under a Bradley-Terry loss based framework. Based on the analysis, we propose Length Controlled Preference Optimization (LCPO) that directly balances the implicit reward related to NLL loss. LCPO can effectively learn length preference with limited data and training. Extensive experiments demonstrate that our approach significantly reduces the average output length by over 50\\% across multiple benchmarks while maintaining the reasoning performance. Our work highlights the potential for computationally efficient approaches in guiding LRMs toward efficient reasoning.",
        "arxiv_id": "2508.10164",
        "ARXIVID": "2508.10164",
        "COMMENT": "This paper proposes a method for pruning long chain-of-thought in large reasoning models via small-scale preference optimization. It is about language models and reasoning, but not vision, spatial intelligence, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10703": {
        "authors": [
            "Yiping Song",
            "Jiaoyan Chen",
            "Renate A. Schmidt"
        ],
        "title": "GenOM: Ontology Matching with Description Generation and Large Language Model",
        "abstract": "arXiv:2508.10703v1 Announce Type: new  Abstract: Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.",
        "arxiv_id": "2508.10703",
        "ARXIVID": "2508.10703",
        "COMMENT": "This paper presents GenOM, an LLM-based ontology alignment framework for biomedical ontologies. While it uses LLMs, it is not a VLLM or MLLM, nor is it focused on vision or spatial intelligence. Not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10786": {
        "authors": [
            "Artem Sokolov",
            "Mikhail Nikitin",
            "Anton Konushin"
        ],
        "title": "Cooperative Face Liveness Detection from Optical Flow",
        "abstract": "arXiv:2508.10786v1 Announce Type: new  Abstract: In this work, we proposed a novel cooperative video-based face liveness detection method based on a new user interaction scenario where participants are instructed to slowly move their frontal-oriented face closer to the camera. This controlled approaching face protocol, combined with optical flow analysis, represents the core innovation of our approach. By designing a system where users follow this specific movement pattern, we enable robust extraction of facial volume information through neural optical flow estimation, significantly improving discrimination between genuine faces and various presentation attacks (including printed photos, screen displays, masks, and video replays). Our method processes both the predicted optical flows and RGB frames through a neural classifier, effectively leveraging spatial-temporal features for more reliable liveness detection compared to passive methods.",
        "arxiv_id": "2508.10786",
        "ARXIVID": "2508.10786",
        "COMMENT": "This paper proposes a new cooperative face liveness detection method using optical flow and a controlled user interaction protocol. While it is a novel approach to liveness detection, it does not directly match any of the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10616": {
        "authors": [
            "Daejune Choi",
            "Youchan No",
            "Jinhyung Lee",
            "Duksu Kim"
        ],
        "title": "Fourier-Guided Attention Upsampling for Image Super-Resolution",
        "abstract": "arXiv:2508.10616v1 Announce Type: new  Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.",
        "arxiv_id": "2508.10616",
        "ARXIVID": "2508.10616",
        "COMMENT": "Proposes a new upsampling module for image super-resolution using frequency-guided attention. Relevant to vision models, but does not match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10600": {
        "authors": [
            "Yuxin Cao",
            "Yedi Zhang",
            "Wentao He",
            "Yifan Liao",
            "Yan Xiao",
            "Chang Li",
            "Zhiyong Huang",
            "Jin Song Dong"
        ],
        "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving",
        "abstract": "arXiv:2508.10600v1 Announce Type: new  Abstract: Learning-based autonomous driving systems remain critically vulnerable to adversarial patches, posing serious safety and security risks in their real-world deployment. Black-box attacks, notable for their high attack success rate without model knowledge, are especially concerning, with their transferability extensively studied to reduce computational costs compared to query-based attacks. Previous transferability-based black-box attacks typically adopt mean Average Precision (mAP) as the evaluation metric and design training loss accordingly. However, due to the presence of multiple detected bounding boxes and the relatively lenient Intersection over Union (IoU) thresholds, the attack effectiveness of these approaches is often overestimated, resulting in reduced success rates in practical attacking scenarios. Furthermore, patches trained on low-resolution data often fail to maintain effectiveness on high-resolution images, limiting their transferability to autonomous driving datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch Attack framework for 2D object detection in autonomous driving, specifically optimized for high-resolution datasets. First, we introduce a novel metric, Practical Attack Success Rate (PASR), to more accurately quantify attack effectiveness with greater relevance for pedestrian safety. Second, we present a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack transferability under PASR. Finally, to maintain the transferability for high-resolution datasets, we further incorporate the Probabilistic Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data preprocessing step. Extensive experiments show that P$^3$A outperforms state-of-the-art attacks on unseen models and unseen high-resolution datasets, both under the proposed practical IoU-based evaluation metric and the previous mAP-based metrics.",
        "arxiv_id": "2508.10600",
        "ARXIVID": "2508.10600",
        "COMMENT": "Focuses on adversarial attacks for object detection in autonomous driving. While relevant to computer vision, it does not match any of the four criteria directly.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10806": {
        "authors": [
            "Maria J. P. Peixoto",
            "Akriti Pandey",
            "Ahsan Zaman",
            "Peter R. Lewis"
        ],
        "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems",
        "abstract": "arXiv:2508.10806v1 Announce Type: new  Abstract: As AI systems are increasingly deployed to support decision-making in critical domains, explainability has become a means to enhance the understandability of these outputs and enable users to make more informed and conscious choices. However, despite growing interest in the usability of eXplainable AI (XAI), the accessibility of these methods, particularly for users with vision impairments, remains underexplored. This paper investigates accessibility gaps in XAI through a two-pronged approach. First, a literature review of 79 studies reveals that evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats. Second, we present a four-part methodological proof of concept that operationalizes inclusive XAI design: (1) categorization of AI systems, (2) persona definition and contextualization, (3) prototype design and implementation, and (4) expert and user assessment of XAI techniques for accessibility. Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability.",
        "arxiv_id": "2508.10806",
        "ARXIVID": "2508.10806",
        "COMMENT": "This paper discusses explainable AI (XAI) accessibility, especially for users with vision impairments. While it is relevant to interpretability and multimodal presentation, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10066": {
        "authors": [
            "Javier Rodenas",
            "Eduardo Aguilar",
            "Petia Radeva"
        ],
        "title": "Stochastic-based Patch Filtering for Few-Shot Learning",
        "abstract": "arXiv:2508.10066v1 Announce Type: new  Abstract: Food images present unique challenges for few-shot learning models due to their visual complexity and variability. For instance, a pasta dish might appear with various garnishes on different plates and in diverse lighting conditions and camera perspectives. This problem leads to losing focus on the most important elements when comparing the query with support images, resulting in misclassification. To address this issue, we propose Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that show greater correlation with the class representation. The key concept of SPFF involves the stochastic filtering of patch embeddings, where patches less similar to the class-aware embedding are more likely to be discarded. With patch embedding filtered according to the probability of appearance, we use a similarity matrix that quantifies the relationship between the query image and its respective support images. Through a qualitative analysis, we demonstrate that SPFF effectively focuses on patches where class-specific food features are most prominent while successfully filtering out non-relevant patches. We validate our approach through extensive experiments on few-shot classification benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing SoA methods.",
        "arxiv_id": "2508.10066",
        "ARXIVID": "2508.10066",
        "COMMENT": "This paper proposes a stochastic patch filtering method for few-shot learning, focusing on food image classification. While it is a clever statistical trick for vision, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10509": {
        "authors": [
            "Yangjie Xiao",
            "Ke Zhang",
            "Jiacun Wang",
            "Xin Sheng",
            "Yurong Guo",
            "Meijuan Chen",
            "Zehua Ren",
            "Zhaoye Zheng",
            "Zhenbing Zhao"
        ],
        "title": "A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection",
        "abstract": "arXiv:2508.10509v1 Announce Type: new  Abstract: Bolt defect detection is critical to ensure the safety of transmission lines. However, the scarcity of defect images and imbalanced data distributions significantly limit detection performance. To address this problem, we propose a segmentationdriven bolt defect editing method (SBDE) to augment the dataset. First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which enhances the segmentation of complex bolt attributes through the CLAHE-FFT Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality masks for subsequent editing tasks. Second, a mask optimization module (MOD) is designed and integrated with the image inpainting model (LaMa) to construct the bolt defect attribute editing model (MOD-LaMa), which converts normal bolts into defective ones through attribute editing. Finally, an editing recovery augmentation (ERA) strategy is proposed to recover and put the edited defect bolts back into the original inspection scenes and expand the defect detection dataset. We constructed multiple bolt datasets and conducted extensive experiments. Experimental results demonstrate that the bolt defect images generated by SBDE significantly outperform state-of-the-art image editing models, and effectively improve the performance of bolt defect detection, which fully verifies the effectiveness and application potential of the proposed method. The code of the project is available at https://github.com/Jay-xyj/SBDE.",
        "arxiv_id": "2508.10509",
        "ARXIVID": "2508.10509",
        "COMMENT": "This paper introduces a segmentation-driven editing method for bolt defect augmentation and detection, with a new segmentation model and data augmentation pipeline. It is an application paper, not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.10712": {
        "authors": [
            "Fabian Kresse",
            "Georgios Pilikos",
            "Mario Azcueta",
            "Nicolas Floury"
        ],
        "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and Classification",
        "abstract": "arXiv:2508.10712v1 Announce Type: new  Abstract: Synthetic Aperture Radar (SAR) data enables large-scale surveillance of maritime vessels. However, near-real-time monitoring is currently constrained by the need to downlink all raw data, perform image focusing, and subsequently analyze it on the ground. On-board processing to generate higher-level products could reduce the data volume that needs to be downlinked, alleviating bandwidth constraints and minimizing latency. However, traditional image focusing and processing algorithms face challenges due to the satellite's limited memory, processing power, and computational resources. This work proposes and evaluates neural networks designed for real-time inference on unfocused SAR data acquired in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our results demonstrate the feasibility of using one of our models for on-board processing and deployment on an FPGA. Additionally, by investigating a binary classification task between ships and windmills, we demonstrate that target classification is possible.",
        "arxiv_id": "2508.10712",
        "ARXIVID": "2508.10712",
        "COMMENT": "This paper proposes lightweight CNNs for embedded SAR ship detection and classification, focusing on real-time inference on unfocused SAR data. While it is relevant to computer vision, it does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.10108": {
        "authors": [
            "Sattvik Sahai",
            "Prasoon Goyal",
            "Michael Johnston",
            "Anna Gottardi",
            "Yao Lu",
            "Lucy Hu",
            "Luke Dai",
            "Shaohua Liu",
            "Samyuth Sagi",
            "Hangjie Shi",
            "Desheng Zhang",
            "Lavina Vaz",
            "Leslie Ball",
            "Maureen Murray",
            "Rahul Gupta",
            "Shankar Ananthakrishna"
        ],
        "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development",
        "abstract": "arXiv:2508.10108v1 Announce Type: new  Abstract: AI systems for software development are rapidly gaining prominence, yet significant challenges remain in ensuring their safety. To address this, Amazon launched the Trusted AI track of the Amazon Nova AI Challenge, a global competition among 10 university teams to drive advances in secure AI. In the challenge, five teams focus on developing automated red teaming bots, while the other five create safe AI assistants. This challenge provides teams with a unique platform to evaluate automated red-teaming and safety alignment methods through head-to-head adversarial tournaments where red teams have multi-turn conversations with the competing AI coding assistants to test their safety alignment. Along with this, the challenge provides teams with a feed of high quality annotated data to fuel iterative improvement. Throughout the challenge, teams developed state-of-the-art techniques, introducing novel approaches in reasoning-based safety alignment, robust model guardrails, multi-turn jail-breaking, and efficient probing of large language models (LLMs). To support these efforts, the Amazon Nova AI Challenge team made substantial scientific and engineering investments, including building a custom baseline coding specialist model for the challenge from scratch, developing a tournament orchestration service, and creating an evaluation harness. This paper outlines the advancements made by university teams and the Amazon Nova AI Challenge team in addressing the safety challenges of AI for software development, highlighting this collaborative effort to raise the bar for AI safety.",
        "arxiv_id": "2508.10108",
        "ARXIVID": "2508.10108",
        "COMMENT": "Does not match any specific criteria. Focuses on AI safety and secure software development, not spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.10351": {
        "authors": [
            "Zhentai Zhang",
            "Danyi Weng",
            "Guibin Zhang",
            "Xiang Chen",
            "Kaixing Long",
            "Jian Geng",
            "Yanmeng Lu",
            "Lei Zhang",
            "Zhitao Zhou",
            "Lei Cao"
        ],
        "title": "Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images",
        "abstract": "arXiv:2508.10351v1 Announce Type: new  Abstract: Complex and diverse ultrastructural features can indicate the type, progression, and prognosis of kidney diseases. Recently, computational pathology combined with deep learning methods has shown tremendous potential in advancing automatic morphological analysis of glomerular ultrastructure. However, current research predominantly focuses on the recognition of individual ultrastructure, which makes it challenging to meet practical diagnostic needs. In this study, we propose the glomerular morphometry framework of ultrastructural characterization (Glo-DMU), which is grounded on three deep models: the ultrastructure segmentation model, the glomerular filtration barrier region classification model, and the electron-dense deposits detection model. Following the conventional protocol of renal biopsy diagnosis, this framework simultaneously quantifies the three most widely used ultrastructural features: the thickness of glomerular basement membrane, the degree of foot process effacement, and the location of electron-dense deposits. We evaluated the 115 patients with 9 renal pathological types in real-world diagnostic scenarios, demonstrating good consistency between automatic quantification results and morphological descriptions in the pathological reports. Glo-DMU possesses the characteristics of full automation, high precision, and high throughput, quantifying multiple ultrastructural features simultaneously, and providing an efficient tool for assisting renal pathologists.",
        "arxiv_id": "2508.10351",
        "ARXIVID": "2508.10351",
        "COMMENT": "This paper presents a deep learning framework for ultrastructural characterization in glomerular electron microscopic images. While it uses deep models for segmentation and classification, it does not address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more focused on medical imaging applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}