{
    "2506.14512": {
        "authors": [
            "Zijian Song",
            "Xiaoxin Lin",
            "Qiuming Huang",
            "Guangrun Wang",
            "Liang Lin"
        ],
        "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks",
        "abstract": "arXiv:2506.14512v1 Announce Type: new  Abstract: Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.",
        "arxiv_id": "2506.14512",
        "ARXIVID": "2506.14512",
        "COMMENT": "Matches criteria 1 and 3. Introduces SIRI-Bench, a benchmark for evaluating VLMs' spatial intelligence and reasoning, which directly relates to spatial understanding and embodied AI benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.14238": {
        "authors": [
            "Yinuo Zheng",
            "Lipeng Gu",
            "Honghua Chen",
            "Liangliang Nan",
            "Mingqiang Wei"
        ],
        "title": "Unified Representation Space for 3D Visual Grounding",
        "abstract": "arXiv:2506.14238v1 Announce Type: new  Abstract: 3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.",
        "arxiv_id": "2506.14238",
        "ARXIVID": "2506.14238",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a new unified representation space for 3D visual grounding, which is a methodological improvement in spatial understanding for embodied agents, and focuses on a novel angle by addressing the modality gap in 3DVG.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.14356": {
        "authors": [
            "Xiaoqi Wang",
            "Yi Wang",
            "Lap-Pui Chau"
        ],
        "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization",
        "abstract": "arXiv:2506.14356v1 Announce Type: new  Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .",
        "arxiv_id": "2506.14356",
        "ARXIVID": "2506.14356",
        "COMMENT": "Matches criterion 4 as it discusses a vision-language foundation model (EVA02-AT) tailored for egocentric video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.14189": {
        "authors": [
            "Kunyuan Deng",
            "Yi Wang",
            "Lap-Pui Chau"
        ],
        "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method",
        "abstract": "arXiv:2506.14189v1 Announce Type: new  Abstract: Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/",
        "arxiv_id": "2506.14189",
        "ARXIVID": "2506.14189",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Ego-HOIBench) and a novel method (HGIR scheme) for egocentric human-object interaction detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.14229": {
        "authors": [
            "Changbai Li",
            "Haodong Zhu",
            "Hanlin Chen",
            "Juan Zhang",
            "Tongfei Chen",
            "Shuo Yang",
            "Shuwei Shao",
            "Wenhao Dong",
            "Baochang Zhang"
        ],
        "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction",
        "abstract": "arXiv:2506.14229v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
        "arxiv_id": "2506.14229",
        "ARXIVID": "2506.14229",
        "COMMENT": "Matches criterion 3 as it introduces a novel hierarchical framework (HRGS) for memory-efficient high-resolution 3D reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.14495": {
        "authors": [
            "Yu Qi",
            "Lipeng Gu",
            "Honghua Chen",
            "Liangliang Nan",
            "Mingqiang Wei"
        ],
        "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs",
        "abstract": "arXiv:2506.14495v1 Announce Type: new  Abstract: Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.",
        "arxiv_id": "2506.14495",
        "ARXIVID": "2506.14495",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework (SpeechRefer) for 3D visual grounding with noisy speech inputs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.14271": {
        "authors": [
            "Weiming Zhang",
            "Dingwen Xiao",
            "Aobotao Dai",
            "Yexin Liu",
            "Tianbo Pan",
            "Shiqi Wen",
            "Lei Chen",
            "Lin Wang"
        ],
        "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment",
        "abstract": "arXiv:2506.14271v1 Announce Type: new  Abstract: 360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.",
        "arxiv_id": "2506.14271",
        "ARXIVID": "2506.14271",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for 360 video segmentation and tracking, with a novel annotation pipeline.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.14765": {
        "authors": [
            "Nikolaos Dionelis",
            "Jente Bosmans",
            "Riccardo Musto",
            "Giancarlo Paoletti",
            "Simone Sarti",
            "Giacomo Cascarano",
            "Casper Fibaek",
            "Luke Camilleri",
            "Bertrand Le Saux",
            "Nicolas Long\\'ep\\'e"
        ],
        "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset",
        "abstract": "arXiv:2506.14765v1 Announce Type: new  Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).",
        "arxiv_id": "2506.14765",
        "ARXIVID": "2506.14765",
        "COMMENT": "Matches criterion 4 as it focuses on scaling Earth Observation foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.14435": {
        "authors": [
            "Hongyu Wang",
            "Jiayu Xu",
            "Ruiping Wang",
            "Yan Feng",
            "Yitao Zhai",
            "Peng Pei",
            "Xunliang Cai",
            "Xilin Chen"
        ],
        "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models",
        "abstract": "arXiv:2506.14435v1 Announce Type: new  Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.",
        "arxiv_id": "2506.14435",
        "ARXIVID": "2506.14435",
        "COMMENT": "Matches criterion 2 as it proposes a memory-efficient approach for large multimodal models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.14015": {
        "authors": [
            "Nick Yiwen Huang",
            "Akin Caliskan",
            "Berkay Kicanaoglu",
            "James Tompkin",
            "Hyeongwoo Kim"
        ],
        "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation",
        "abstract": "arXiv:2506.14015v1 Announce Type: new  Abstract: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.",
        "arxiv_id": "2506.14015",
        "ARXIVID": "2506.14015",
        "COMMENT": "Matches criterion 2 as it disentangles 3D from large vision-language models for controlled portrait generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.14243": {
        "authors": [
            "Xiaohui Jiang",
            "Haijiang Zhu",
            "Chadei Li",
            "Fulin Tang",
            "Ning An"
        ],
        "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition",
        "abstract": "arXiv:2506.14243v1 Announce Type: new  Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.",
        "arxiv_id": "2506.14243",
        "ARXIVID": "2506.14243",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for spatial understanding in 3D place recognition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.13841": {
        "authors": [
            "Miho Koda",
            "Yu Zheng",
            "Ruixian Ma",
            "Mingyang Sun",
            "Devesh Pansare",
            "Fabio Duarte",
            "Paolo Santi"
        ],
        "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning",
        "abstract": "arXiv:2506.13841v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.",
        "arxiv_id": "2506.13841",
        "ARXIVID": "2506.13841",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (LocationReasoner) for evaluating LLMs' reasoning in real-world site selection tasks, which involves spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.13902": {
        "authors": [
            "Raymond Yu",
            "Paul Han",
            "Josh Myers-Dean",
            "Piper Wolters",
            "Favyen Bastani"
        ],
        "title": "OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data",
        "abstract": "arXiv:2506.13902v1 Announce Type: new  Abstract: In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.",
        "arxiv_id": "2506.13902",
        "ARXIVID": "2506.13902",
        "COMMENT": "Matches criterion 3 as it introduces a self-supervised learning method (OPTIMUS) for detecting changes in satellite imagery.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.14142": {
        "authors": [
            "Wenting Chen",
            "Yi Dong",
            "Zhaojun Ding",
            "Yucheng Shi",
            "Yifan Zhou",
            "Fang Zeng",
            "Yijun Luo",
            "Tianyu Lin",
            "Yihang Su",
            "Yichen Wu",
            "Kai Zhang",
            "Zhen Xiang",
            "Tianming Liu",
            "Ninghao Liu",
            "Lichao Sun",
            "Yixuan Yuan",
            "Xiang Li"
        ],
        "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology",
        "abstract": "arXiv:2506.14142v1 Announce Type: new  Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.",
        "arxiv_id": "2506.14142",
        "ARXIVID": "2506.14142",
        "COMMENT": "Matches criterion 2 as it discusses a multi-agent, multimodal reasoning framework (RadFabric) for radiology.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.14384": {
        "authors": [
            "Huan Kang",
            "Hui Li",
            "Xiao-Jun Wu",
            "Tianyang Xu",
            "Rui Wang",
            "Chunyang Cheng",
            "Josef Kittler"
        ],
        "title": "GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion",
        "abstract": "arXiv:2506.14384v1 Announce Type: new  Abstract: In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.   However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot   encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic   similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.   While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To   address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible   image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the   Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into   high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic   fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on   a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high   correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both   qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.",
        "arxiv_id": "2506.14384",
        "ARXIVID": "2506.14384",
        "COMMENT": "Does not match any specific criteria but proposes a novel transformer for image fusion, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14096": {
        "authors": [
            "Sanjeda Akter",
            "Ibne Farabi Shihab",
            "Anuj Sharma"
        ],
        "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems",
        "abstract": "arXiv:2506.14096v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.",
        "arxiv_id": "2506.14096",
        "ARXIVID": "2506.14096",
        "COMMENT": "Matches criterion 2. Surveys the integration of LLMs with image segmentation, which aligns with visual large language models (VLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.14451": {
        "authors": [
            "Aditya Shourya",
            "Michel Dumontier",
            "Chang Sun"
        ],
        "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering",
        "abstract": "arXiv:2506.14451v1 Announce Type: new  Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.",
        "arxiv_id": "2506.14451",
        "ARXIVID": "2506.14451",
        "COMMENT": "Matches criterion 2 as it discusses fine-tuning a lightweight vision-language model for Radiological VQA.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.14382": {
        "authors": [
            "Ning Zhou",
            "Shanxiong Chen",
            "Mingting Zhou",
            "Haigang Sui",
            "Lieyun Hu",
            "Han Li",
            "Li Hua",
            "Qiming Zhou"
        ],
        "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation",
        "abstract": "arXiv:2506.14382v1 Announce Type: new  Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.",
        "arxiv_id": "2506.14382",
        "ARXIVID": "2506.14382",
        "COMMENT": "Does not match any specific criteria but is tangentially related to vision foundation models (criterion 4) due to its use of a vision transformer encoder and its application in remote sensing semantic segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.13846": {
        "authors": [
            "Runtao Liu",
            "Jiahao Zhan",
            "Yingqing He",
            "Chen Wei",
            "Alan Yuille",
            "Qifeng Chen"
        ],
        "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction",
        "abstract": "arXiv:2506.13846v1 Announce Type: new  Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).",
        "arxiv_id": "2506.13846",
        "ARXIVID": "2506.13846",
        "COMMENT": "Does not match any specific criteria but explores reward modeling for visual generative models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14473": {
        "authors": [
            "Zhijing Wan",
            "Zhixiang Wang",
            "Zheng Wang",
            "Xin Xu",
            "Shin'ichi Satoh"
        ],
        "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection",
        "abstract": "arXiv:2506.14473v1 Announce Type: new  Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.",
        "arxiv_id": "2506.14473",
        "ARXIVID": "2506.14473",
        "COMMENT": "Does not match any specific criteria but explores foundation models and subset selection, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14549": {
        "authors": [
            "Yong Liu",
            "Wenpeng Xiao",
            "Qianqian Wang",
            "Junlin Chen",
            "Shiyin Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Yansong Tang"
        ],
        "title": "DreamLight: Towards Harmonious and Consistent Image Relighting",
        "abstract": "arXiv:2506.14549v1 Announce Type: new  Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.",
        "arxiv_id": "2506.14549",
        "ARXIVID": "2506.14549",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14603": {
        "authors": [
            "Amirmojtaba Sabour",
            "Sanja Fidler",
            "Karsten Kreis"
        ],
        "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
        "abstract": "arXiv:2506.14603v1 Announce Type: new  Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.",
        "arxiv_id": "2506.14603",
        "ARXIVID": "2506.14603",
        "COMMENT": "Does not match any specific criteria. Focuses on flow map distillation for generative modeling, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14245": {
        "authors": [
            "Xumeng Wen",
            "Zihan Liu",
            "Shun Zheng",
            "Zhijian Xu",
            "Shengyu Ye",
            "Zhirong Wu",
            "Xiao Liang",
            "Yang Wang",
            "Junjie Li",
            "Ziming Miao",
            "Jiang Bian",
            "Mao Yang"
        ],
        "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs",
        "abstract": "arXiv:2506.14245v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.",
        "arxiv_id": "2506.14245",
        "ARXIVID": "2506.14245",
        "COMMENT": "Does not match any specific criteria. Focuses on reinforcement learning with verifiable rewards for reasoning in LLMs, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14373": {
        "authors": [
            "Junyeob Baek",
            "Hosung Lee",
            "Christopher Hoang",
            "Mengye Ren",
            "Sungjin Ahn"
        ],
        "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction",
        "abstract": "arXiv:2506.14373v1 Announce Type: new  Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.",
        "arxiv_id": "2506.14373",
        "ARXIVID": "2506.14373",
        "COMMENT": "Does not match any specific criteria. Focuses on symbolic reasoning and tokenization, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14168": {
        "authors": [
            "Hu Yu",
            "Biao Gong",
            "Hangjie Yuan",
            "DanDan Zheng",
            "Weilong Chai",
            "Jingdong Chen",
            "Kecheng Zheng",
            "Feng Zhao"
        ],
        "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
        "abstract": "arXiv:2506.14168v1 Announce Type: new  Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).",
        "arxiv_id": "2506.14168",
        "ARXIVID": "2506.14168",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14755": {
        "authors": [
            "Zhengxiang Cheng",
            "Dongping Chen",
            "Mingyang Fu",
            "Tianyi Zhou"
        ],
        "title": "Optimizing Length Compression in Large Reasoning Models",
        "abstract": "arXiv:2506.14755v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.",
        "arxiv_id": "2506.14755",
        "ARXIVID": "2506.14755",
        "COMMENT": "Does not match any specific criteria but focuses on optimizing reasoning models, which is tangentially related to large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.14428": {
        "authors": [
            "Ruihao Xi",
            "Xuekuan Wang",
            "Yongcheng Li",
            "Shuhua Li",
            "Zichen Wang",
            "Yiwei Wang",
            "Feng Wei",
            "Cairong Zhao"
        ],
        "title": "Toward Rich Video Human-Motion2D Generation",
        "abstract": "arXiv:2506.14428v1 Announce Type: new  Abstract: Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.",
        "arxiv_id": "2506.14428",
        "ARXIVID": "2506.14428",
        "COMMENT": "Does not match any specific criteria. Focuses on human motion generation and a new dataset, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.14130": {
        "authors": [
            "Chunyu Cao",
            "Jintao Cheng",
            "Zeyu Chen",
            "Linfan Zhan",
            "Rui Fan",
            "Zhijian He",
            "Xiaoyu Tang"
        ],
        "title": "KDMOS:Knowledge Distillation for Motion Segmentation",
        "abstract": "arXiv:2506.14130v1 Announce Type: new  Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.",
        "arxiv_id": "2506.14130",
        "ARXIVID": "2506.14130",
        "COMMENT": "Does not match any specific criteria. Focuses on motion segmentation for autonomous driving, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.14642": {
        "authors": [
            "Yuke Xing",
            "Jiarui Wang",
            "Peizhi Niu",
            "Wenjie Huang",
            "Guangtao Zhai",
            "Yiling Xu"
        ],
        "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting",
        "abstract": "arXiv:2506.14642v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.",
        "arxiv_id": "2506.14642",
        "ARXIVID": "2506.14642",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D Gaussian Splatting and image quality assessment, which is not directly related to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.14560": {
        "authors": [
            "David Butler",
            "Adrian Hilton",
            "Gustavo Carneiro"
        ],
        "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images",
        "abstract": "arXiv:2506.14560v1 Announce Type: new  Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.",
        "arxiv_id": "2506.14560",
        "ARXIVID": "2506.14560",
        "COMMENT": "Does not match any specific criterion but is relevant to predictive modeling and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.14092": {
        "authors": [
            "Haonan Yin",
            "Shai Vardi",
            "Vidyanand Choudhary"
        ],
        "title": "Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models",
        "abstract": "arXiv:2506.14092v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.",
        "arxiv_id": "2506.14092",
        "ARXIVID": "2506.14092",
        "COMMENT": "Does not match any specific criterion but is relevant to understanding biases in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}