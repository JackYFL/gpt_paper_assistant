{
    "2601.10781": {
        "authors": [
            "Kanchana Ranasinghe",
            "Honglu Zhou",
            "Yu Fang",
            "Luyu Yang",
            "Le Xue",
            "Ran Xu",
            "Caiming Xiong",
            "Silvio Savarese",
            "Michael S Ryoo",
            "Juan Carlos Niebles"
        ],
        "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
        "abstract": "arXiv:2601.10781v1 Announce Type: new  Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.",
        "arxiv_id": "2601.10781",
        "ARXIVID": "2601.10781",
        "COMMENT": "Matches criteria 1 and 2. Presents a novel language-conditioned optical flow forecasting model (FOFPred) with a unified Vision-Language Model (VLM) and Diffusion architecture, applied to robot control and video generation. This is a new methodological improvement for spatial understanding in embodied agents and introduces a new VLM for multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.11442": {
        "authors": [
            "Xiangjun Gao",
            "Zhensong Zhang",
            "Dave Zhenyu Chen",
            "Songcen Xu",
            "Long Quan",
            "Eduardo P\\'erez-Pellitero",
            "Youngkyoon Jang"
        ],
        "title": "Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps",
        "abstract": "arXiv:2601.11442v1 Announce Type: new  Abstract: We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.",
        "arxiv_id": "2601.11442",
        "ARXIVID": "2601.11442",
        "COMMENT": "Matches criterion 1: Introduces Map2Thought, a framework for explicit and interpretable 3D spatial reasoning in VLMs, with a novel metric cognitive map and chain-of-thought reasoning for spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.11475": {
        "authors": [
            "Rajeev Yasarla",
            "Deepti Hegde",
            "Shizhong Han",
            "Hsin-Pai Cheng",
            "Yunxiao Shi",
            "Meysam Sadeghigooghari",
            "Shweta Mahajan",
            "Apratim Bhattacharyya",
            "Litian Liu",
            "Risheek Garrepalli",
            "Thomas Svantesson",
            "Fatih Porikli",
            "Hong Cai"
        ],
        "title": "Generative Scenario Rollouts for End-to-End Autonomous Driving",
        "abstract": "arXiv:2601.11475v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.",
        "arxiv_id": "2601.11475",
        "ARXIVID": "2601.11475",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs: Vision-Language-Action models for autonomous driving) and criterion 3 (novel generative scenario rollout method for embodied AI, with new empirical results and a plug-and-play framework).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.11254": {
        "authors": [
            "Cheng-Zhuang Liu",
            "Si-Bao Chen",
            "Qing-Ling Shu",
            "Chris Ding",
            "Jin Tang",
            "Bin Luo"
        ],
        "title": "FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection",
        "abstract": "arXiv:2601.11254v1 Announce Type: new  Abstract: Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.",
        "arxiv_id": "2601.11254",
        "ARXIVID": "2601.11254",
        "COMMENT": "Matches criterion 1 and 3: Proposes a new method (FTDMamba) for spatial-temporal understanding in UAV video anomaly detection, with a novel frequency-assisted module and a new large-scale moving UAV VAD benchmark (MUVAD).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.11087": {
        "authors": [
            "Qiyuan Zhang",
            "Biao Gong",
            "Shuai Tan",
            "Zheng Zhang",
            "Yujun Shen",
            "Xing Zhu",
            "Yuyuan Li",
            "Kelu Yao",
            "Chunhua Shen",
            "Changqing Zou"
        ],
        "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
        "abstract": "arXiv:2601.11087v1 Announce Type: new  Abstract: Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.",
        "arxiv_id": "2601.11087",
        "ARXIVID": "2601.11087",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in generative video models for embodied agents) and criterion 3 (introduces a new benchmark PhysRVGBench for physics-aware video generation). The paper proposes a physics-aware RL paradigm for video generation, directly enforcing physical collision rules, and introduces a new benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.11183": {
        "authors": [
            "Shuang Chen",
            "Jie Wang",
            "Shuai Yuan",
            "Jiayang Li",
            "Yu Xia",
            "Yuanhong Liao",
            "Junbo Wei",
            "Jincheng Yuan",
            "Xiaoqing Xu",
            "Xiaolin Zhu",
            "Peng Zhu",
            "Hongsheng Zhang",
            "Yuyu Zhou",
            "Haohuan Fu",
            "Huabing Huang",
            "Bin Chen",
            "Fan Dai",
            "Peng Gong"
        ],
        "title": "Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring",
        "abstract": "arXiv:2601.11183v1 Announce Type: new  Abstract: The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.",
        "arxiv_id": "2601.11183",
        "ARXIVID": "2601.11183",
        "COMMENT": "Matches criterion 4. Introduces a new vision foundation model (ESDNet) for global land monitoring, with a novel ultra-lightweight Earth embedding database. The work is highly relevant to vision foundation models and their applications in geospatial AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11522": {
        "authors": [
            "Ruiheng Zhang",
            "Jingfeng Yao",
            "Huangxuan Zhao",
            "Hao Yan",
            "Xiao He",
            "Lei Chen",
            "Zhou Wei",
            "Yong Luo",
            "Zengmao Wang",
            "Lefei Zhang",
            "Dacheng Tao",
            "Bo Du"
        ],
        "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
        "abstract": "arXiv:2601.11522v1 Announce Type: new  Abstract: Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
        "arxiv_id": "2601.11522",
        "ARXIVID": "2601.11522",
        "COMMENT": "Matches criterion 4: Proposes UniX, a unified medical vision foundation model for both understanding and generation, combining autoregressive and diffusion branches with cross-modal attention.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11400": {
        "authors": [
            "Shuai Yuan",
            "Tianwu Lin",
            "Shuang Chen",
            "Yu Xia",
            "Peng Qin",
            "Xiangyu Liu",
            "Xiaoqing Xu",
            "Nan Xu",
            "Hongsheng Zhang",
            "Jie Wang",
            "Peng Gong"
        ],
        "title": "Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model",
        "abstract": "arXiv:2601.11400v1 Announce Type: new  Abstract: Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.",
        "arxiv_id": "2601.11400",
        "ARXIVID": "2601.11400",
        "COMMENT": "Matches criterion 4 (applies vision foundation model SAM to satellite image time series for wetland mapping, with a novel temporal-aware extension and dual-branch design).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11044": {
        "authors": [
            "Keyu Li",
            "Junhao Shi",
            "Yang Xiao",
            "Mohan Jiang",
            "Jie Sun",
            "Yunze Wu",
            "Shijie Xia",
            "Xiaojie Cai",
            "Tianze Xu",
            "Weiye Si",
            "Wenjie Li",
            "Dequan Wang",
            "Pengfei Liu"
        ],
        "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
        "abstract": "arXiv:2601.11044v1 Announce Type: new  Abstract: Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.",
        "arxiv_id": "2601.11044",
        "ARXIVID": "2601.11044",
        "COMMENT": "Matches criterion 3 (introduces AgencyBench, a new large-scale benchmark for autonomous agents, focusing on long-horizon, real-world scenarios and automated evaluation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11354": {
        "authors": [
            "Weiyi Wang",
            "Xinchi Chen",
            "Jingjing Gong",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
        "abstract": "arXiv:2601.11354v1 Announce Type: new  Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
        "arxiv_id": "2601.11354",
        "ARXIVID": "2601.11354",
        "COMMENT": "Matches criterion 3 (introduces a new benchmark, AstroReason-Bench, for agentic planning in physics-constrained real-world domains, focusing on long-horizon, physically realistic tasks that previous benchmarks ignored).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10909": {
        "authors": [
            "Chuqiao Li",
            "Xianghui Xie",
            "Yong Cao",
            "Andreas Geiger",
            "Gerard Pons-Moll"
        ],
        "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
        "abstract": "arXiv:2601.10909v1 Announce Type: new  Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.",
        "arxiv_id": "2601.10909",
        "ARXIVID": "2601.10909",
        "COMMENT": "Somewhat matches criterion 1: Proposes a new dataset and method for part-level human motion generation with fine-grained spatial and temporal control, relevant to spatial understanding in generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2601.11035": {
        "authors": [
            "Long Ma",
            "Zihao Xue",
            "Yan Wang",
            "Zhiyuan Yan",
            "Jin Xu",
            "Xiaorui Jiang",
            "Haiyang Yu",
            "Yong Liao",
            "Zhen Bi"
        ],
        "title": "Your One-Stop Solution for AI-Generated Video Detection",
        "abstract": "arXiv:2601.11035v1 Announce Type: new  Abstract: Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones and necessitating reliable detection methods.   However, two key limitations hinder the development of this field.   \\textbf{From the dataset perspective}, existing datasets are often limited in scale and constructed using outdated or narrowly scoped generative models, making it difficult to capture the diversity and rapid evolution of modern generative techniques. Moreover, the dataset construction process frequently prioritizes quantity over quality, neglecting essential aspects such as semantic diversity, scenario coverage, and technological representativeness.   \\textbf{From the benchmark perspective}, current benchmarks largely remain at the stage of dataset creation, leaving many fundamental issues and in-depth analysis yet to be systematically explored.   Addressing this gap, we propose AIGVDBench, a benchmark designed to be comprehensive and representative, covering \\textbf{31} state-of-the-art generation models and over \\textbf{440,000} videos. By executing more than \\textbf{1,500} evaluations on \\textbf{33} existing detectors belonging to four distinct categories. This work presents \\textbf{8 in-depth analyses} from multiple perspectives and identifies \\textbf{4 novel findings} that offer valuable insights for future research. We hope this work provides a solid foundation for advancing the field of AI-generated video detection.   Our benchmark is open-sourced at https://github.com/LongMa-2025/AIGVDBench.",
        "arxiv_id": "2601.11035",
        "ARXIVID": "2601.11035",
        "COMMENT": "Matches criterion 4: Proposes a new large-scale benchmark (AIGVDBench) for AI-generated video detection, with extensive analysis and coverage of many generative models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.10802": {
        "authors": [
            "Gerhard Krumpl",
            "Henning Avenhaus",
            "Horst Possegger"
        ],
        "title": "ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research",
        "abstract": "arXiv:2601.10802v1 Announce Type: new  Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.",
        "arxiv_id": "2601.10802",
        "ARXIVID": "2601.10802",
        "COMMENT": "Matches criterion 4: Introduces a large-scale dataset (ICONIC-444) for OOD detection, supporting vision foundation model evaluation and benchmarking.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.11178": {
        "authors": [
            "Girish A. Koushik",
            "Helen Treharne",
            "Diptesh Kanojia"
        ],
        "title": "TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech",
        "abstract": "arXiv:2601.11178v1 Announce Type: new  Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as \"black boxes\" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.",
        "arxiv_id": "2601.11178",
        "ARXIVID": "2601.11178",
        "COMMENT": "Matches criterion 2 (uses vision-language and audio-language models in a novel reinforcement learning framework for multimodal hate speech detection). The structured reasoning and interpretable alignment are also of interest for multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.11243": {
        "authors": [
            "Zhiqi Pang",
            "Lingling Zhao",
            "Yang Liu",
            "Chunyu Wang",
            "Gaurav Sharma"
        ],
        "title": "Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification",
        "abstract": "arXiv:2601.11243v1 Announce Type: new  Abstract: We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.",
        "arxiv_id": "2601.11243",
        "ARXIVID": "2601.11243",
        "COMMENT": "Matches criterion 2 (leverages vision-language models, specifically CLIP, for unsupervised multi-scenario person re-identification with novel image-text knowledge modeling).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.11096": {
        "authors": [
            "Shuai Tan",
            "Biao Gong",
            "Ke Ma",
            "Yutong Feng",
            "Qiyuan Zhang",
            "Yan Wang",
            "Yujun Shen",
            "Hengshuang Zhao"
        ],
        "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
        "abstract": "arXiv:2601.11096v1 Announce Type: new  Abstract: Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.",
        "arxiv_id": "2601.11096",
        "ARXIVID": "2601.11096",
        "COMMENT": "Somewhat matches criterion 1: Proposes a new paradigm for multi-subject animation with spatial misalignment handling, but focus is more on animation than embodied spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.11393": {
        "authors": [
            "Haomiao Tang",
            "Jinpeng Wang",
            "Minyi Zhao",
            "Guanghao Meng",
            "Ruisheng Luo",
            "Long Chen",
            "Shu-Tao Xia"
        ],
        "title": "Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning",
        "abstract": "arXiv:2601.11393v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.",
        "arxiv_id": "2601.11393",
        "ARXIVID": "2601.11393",
        "COMMENT": "Somewhat matches criterion 2: Proposes a new probabilistic learning framework for composed image retrieval with multi-modal queries, but not a new VLLM/MLLM.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.11492": {
        "authors": [
            "Kaiwen Wang",
            "Kaili Zheng",
            "Rongrong Deng",
            "Qingmin Fan",
            "Milin Zhang",
            "Zongrui Li",
            "Xuesi Zhou",
            "Bo Han",
            "Liren Chen",
            "Chenyi Guo",
            "Ji Wu"
        ],
        "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics",
        "abstract": "arXiv:2601.11492v1 Announce Type: new  Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.",
        "arxiv_id": "2601.11492",
        "ARXIVID": "2601.11492",
        "COMMENT": "Somewhat matches criterion 4: Applies computer vision and graph-based models to sports analytics, but not a vision foundation model or new VLLM/MLLM.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.11252": {
        "authors": [
            "Qianyue Wang",
            "Jinwu Hu",
            "Yufeng Wang",
            "Huanxiang Lin",
            "Bolin Chen",
            "Zhiquan Wen",
            "Yaofo Chen",
            "Mingkui Tan"
        ],
        "title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning",
        "abstract": "arXiv:2601.11252v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.",
        "arxiv_id": "2601.11252",
        "ARXIVID": "2601.11252",
        "COMMENT": "Does not directly match any of the four criteria. Focuses on efficient reasoning in large reasoning models with a novel test-time intervention paradigm, but not specifically on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.11451": {
        "authors": [
            "Oishee Bintey Hoque",
            "Nibir Chandra Mandal",
            "Kyle Luong",
            "Amanda Wilson",
            "Samarth Swarup",
            "Madhav Marathe",
            "Abhijin Adiga"
        ],
        "title": "PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs",
        "abstract": "arXiv:2601.11451v1 Announce Type: new  Abstract: Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho",
        "arxiv_id": "2601.11451",
        "ARXIVID": "2601.11451",
        "COMMENT": "Somewhat matches criterion 4: Uses vision models for remote sensing and infrastructure mapping, but not a general-purpose vision foundation model.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.11102": {
        "authors": [
            "Shangbo Yuan",
            "Jie Xu",
            "Ping Hu",
            "Xiaofeng Zhu",
            "Na Zhao"
        ],
        "title": "Graph Smoothing for Enhanced Local Geometry Learning in Point Cloud Analysis",
        "abstract": "arXiv:2601.11102v1 Announce Type: new  Abstract: Graph-based methods have proven to be effective in capturing relationships among points for 3D point cloud analysis. However, these methods often suffer from suboptimal graph structures, particularly due to sparse connections at boundary points and noisy connections in junction areas. To address these challenges, we propose a novel method that integrates a graph smoothing module with an enhanced local geometry learning module. Specifically, we identify the limitations of conventional graph structures, particularly in handling boundary points and junction areas. In response, we introduce a graph smoothing module designed to optimize the graph structure and minimize the negative impact of unreliable sparse and noisy connections. Based on the optimized graph structure, we improve the feature extract function with local geometry information. These include shape features derived from adaptive geometric descriptors based on eigenvectors and distribution features obtained through cylindrical coordinate transformation. Experimental results on real-world datasets validate the effectiveness of our method in various point cloud learning tasks, i.e., classification, part segmentation, and semantic segmentation.",
        "arxiv_id": "2601.11102",
        "ARXIVID": "2601.11102",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1) via improved local geometry learning in point cloud analysis, but not directly about embodied agents or multi-modal models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.11037": {
        "authors": [
            "Shiyu Liu",
            "Yongjing Yin",
            "Jianhao Yan",
            "Yunbo Tang",
            "Qinggang Zhang",
            "Bei Li",
            "Xin Chen",
            "Jingang Wang",
            "Xunliang Cai",
            "Jinsong Su"
        ],
        "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
        "abstract": "arXiv:2601.11037v1 Announce Type: new  Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.",
        "arxiv_id": "2601.11037",
        "ARXIVID": "2601.11037",
        "COMMENT": "Does not directly match any of the four criteria. Proposes a boundary-aware RL framework for agentic search in LLMs, but not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11147": {
        "authors": [
            "Zixu Wang",
            "Bingbing Xu",
            "Yige Yuan",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "title": "Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems",
        "abstract": "arXiv:2601.11147v1 Announce Type: new  Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \\textbf{SCALE}, which means \\underline{\\textbf{S}}elf prediction of the optimizer with few shot \\underline{\\textbf{CAL}}ibration for \\underline{\\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \\textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\\%.",
        "arxiv_id": "2601.11147",
        "ARXIVID": "2601.11147",
        "COMMENT": "Does not directly match any of the four criteria. Discusses workflow generation in multi-agent systems using LLMs, but not specifically about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11286": {
        "authors": [
            "Weihong Qi",
            "Fan Huang",
            "Rasika Muralidharan",
            "Jisun An",
            "Haewoon Kwak"
        ],
        "title": "XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making",
        "abstract": "arXiv:2601.11286v1 Announce Type: new  Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.",
        "arxiv_id": "2601.11286",
        "ARXIVID": "2601.11286",
        "COMMENT": "Does not directly match any of the four criteria. Presents a framework for explainable evaluation of AI-human alignment in LLM-based decision making, but not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10836": {
        "authors": [
            "Gerhard Krumpl",
            "Henning Avenhaus",
            "Horst Possegger"
        ],
        "title": "One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection",
        "abstract": "arXiv:2601.10836v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.",
        "arxiv_id": "2601.10836",
        "ARXIVID": "2601.10836",
        "COMMENT": "Related to OOD detection and training effects in vision models, but does not match any specific criterion (no spatial intelligence, VLLMs, embodied AI, or vision foundation model application).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11235": {
        "authors": [
            "Ana Davila",
            "Jacinto Colan",
            "Yasuhisa Hasegawa"
        ],
        "title": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "abstract": "arXiv:2601.11235v1 Announce Type: new  Abstract: Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.",
        "arxiv_id": "2601.11235",
        "ARXIVID": "2601.11235",
        "COMMENT": "Related to transfer learning and image classification, but does not match any specific criterion (no spatial intelligence, VLLMs, embodied AI, or vision foundation model application).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}