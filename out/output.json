{
    "2507.23734": {
        "authors": [
            "Dongming Wu",
            "Yanping Fu",
            "Saike Huang",
            "Yingfei Liu",
            "Fan Jia",
            "Nian Liu",
            "Feng Dai",
            "Tiancai Wang",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Jianbing Shen"
        ],
        "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping",
        "abstract": "arXiv:2507.23734v1 Announce Type: new  Abstract: General robotic grasping systems require accurate object affordance perception in diverse open-world scenarios following human instructions. However, current studies suffer from the problem of lacking reasoning-based large-scale affordance prediction data, leading to considerable concern about open-world effectiveness. To address this limitation, we build a large-scale grasping-oriented affordance segmentation benchmark with human-like instructions, named RAGNet. It contains 273k images, 180 categories, and 26k reasoning instructions. The images cover diverse embodied data domains, such as wild, robot, ego-centric, and even simulation data. They are carefully annotated with an affordance map, while the difficulty of language instructions is largely increased by removing their category name and only providing functional descriptions. Furthermore, we propose a comprehensive affordance-based grasping framework, named AffordanceNet, which consists of a VLM pre-trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the target. Extensive experiments on affordance segmentation benchmarks and real-robot manipulation tasks show that our model has a powerful open-world generalization ability. Our data and code is available at https://github.com/wudongming97/AffordanceNet.",
        "arxiv_id": "2507.23734",
        "ARXIVID": "2507.23734",
        "COMMENT": "This paper introduces RAGNet, a large-scale reasoning-based affordance segmentation benchmark for general robotic grasping, and proposes a VLM-based affordance framework. It matches criterion 3 (new embodied AI benchmark and method) and also touches on criterion 2 (VLMs for affordance segmentation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.23478": {
        "authors": [
            "Ting Huang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
        "abstract": "arXiv:2507.23478v1 Announce Type: new  Abstract: Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.",
        "arxiv_id": "2507.23478",
        "ARXIVID": "2507.23478",
        "COMMENT": "This paper introduces 3D-R1, a foundation model for 3D vision-language reasoning, with a new synthetic dataset, RLHF training, and dynamic view selection. It matches criterion 1 (spatial understanding in embodied agents), criterion 2 (new VLLMs/MLLMs), and criterion 4 (vision foundation models and applications), and is highly relevant for embodied AI and spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.23278": {
        "authors": [
            "Hao Tang",
            "Chenwei Xie",
            "Xiaoyi Bao",
            "Tingyu Weng",
            "Pandeng Li",
            "Yun Zheng",
            "Liwei Wang"
        ],
        "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing",
        "abstract": "arXiv:2507.23278v1 Announce Type: new  Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.",
        "arxiv_id": "2507.23278",
        "ARXIVID": "2507.23278",
        "COMMENT": "This paper proposes UniLiP, a unified model extending CLIP for multimodal understanding, generation, and editing. It matches criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications), and introduces a new architecture and training scheme.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.23782": {
        "authors": [
            "Zihan Wang",
            "Jeff Tan",
            "Tarasha Khurana",
            "Neehar Peri",
            "Deva Ramanan"
        ],
        "title": "MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion",
        "abstract": "arXiv:2507.23782v1 Announce Type: new  Abstract: We address the problem of dynamic scene reconstruction from sparse-view videos. Prior work often requires dense multi-view captures with hundreds of calibrated cameras (e.g. Panoptic Studio). Such multi-view setups are prohibitively expensive to build and cannot capture diverse scenes in-the-wild. In contrast, we aim to reconstruct dynamic human behaviors, such as repairing a bike or dancing, from a small set of sparse-view cameras with complete scene coverage (e.g. four equidistant inward-facing static cameras). We find that dense multi-view reconstruction methods struggle to adapt to this sparse-view setup due to limited overlap between viewpoints. To address these limitations, we carefully align independent monocular reconstructions of each camera to produce time- and view-consistent dynamic scene reconstructions. Extensive experiments on PanopticStudio and Ego-Exo4D demonstrate that our method achieves higher quality reconstructions than prior art, particularly when rendering novel views. Code, data, and data-processing scripts are available on https://github.com/ImNotPrepared/MonoFusion.",
        "arxiv_id": "2507.23782",
        "ARXIVID": "2507.23782",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/methods, novel angle). Proposes MonoFusion for sparse-view 4D dynamic scene reconstruction, aligning monocular reconstructions for time- and view-consistent results, addressing a previously ignored sparse-view setting.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.23779": {
        "authors": [
            "Miaosen Zhang",
            "Ziqiang Xu",
            "Jialiang Zhu",
            "Qi Dai",
            "Kai Qiu",
            "Yifan Yang",
            "Chong Luo",
            "Tianyi Chen",
            "Justin Wagle",
            "Tim Franklin",
            "Baining Guo"
        ],
        "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
        "abstract": "arXiv:2507.23779v1 Announce Type: new  Abstract: With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the \\textbf{Phi-Ground} model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under $10B$ parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on ScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: \\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}",
        "arxiv_id": "2507.23779",
        "ARXIVID": "2507.23779",
        "COMMENT": "This paper presents Phi-Ground, a new model family for GUI grounding in computer use agents, achieving SOTA on multiple benchmarks. It is relevant to multi-modal large language models (criterion 2) and vision foundation models (criterion 4), as GUI grounding is a key perception task for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23340": {
        "authors": [
            "Xingyue Peng",
            "Yuandong Lyu",
            "Lang Zhang",
            "Jian Zhu",
            "Songtao Wang",
            "Jiaxin Deng",
            "Songxin Lu",
            "Weiliang Ma",
            "Dangen She",
            "Peng Jia",
            "XianPeng Lang"
        ],
        "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting",
        "abstract": "arXiv:2507.23340v1 Announce Type: new  Abstract: Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.",
        "arxiv_id": "2507.23340",
        "ARXIVID": "2507.23340",
        "COMMENT": "This paper presents MagicRoad, a semantic-aware 3D road surface reconstruction framework using obstacle inpainting and segmentation-guided video inpainting. It is relevant to spatial understanding and vision foundation models (criteria 1 and 4), especially for embodied agents in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23473": {
        "authors": [
            "Bin Xie",
            "Congxuan Zhang",
            "Fagan Wang",
            "Peng Liu",
            "Feng Lu",
            "Zhen Chen",
            "Weiming Hu"
        ],
        "title": "CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes",
        "abstract": "arXiv:2507.23473v1 Announce Type: new  Abstract: The widespread application of Unmanned Aerial Vehicles (UAVs) has raised serious public safety and privacy concerns, making UAV perception crucial for anti-UAV tasks. However, existing UAV tracking datasets predominantly feature conspicuous objects and lack diversity in scene complexity and attribute representation, limiting their applicability to real-world scenarios. To overcome these limitations, we present the CST Anti-UAV, a new thermal infrared dataset specifically designed for Single Object Tracking (SOT) in Complex Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k high-quality bounding box annotations, highlighting two key properties: a significant number of tiny-sized UAV targets and the diverse and complex scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to incorporate complete manual frame-level attribute annotations, enabling precise evaluations under varied challenges. To conduct an in-depth performance analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed dataset. Experimental results demonstrate that tracking tiny UAVs in complex environments remains a challenge, as the state-of-the-art method achieves only 35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410 dataset. These findings underscore the limitations of existing benchmarks and the need for further advancements in UAV tracking research. The CST Anti-UAV benchmark is about to be publicly released, which not only fosters the development of more robust SOT methods but also drives innovation in anti-UAV systems.",
        "arxiv_id": "2507.23473",
        "ARXIVID": "2507.23473",
        "COMMENT": "This paper introduces a new benchmark dataset for tiny UAV tracking in complex scenes using thermal infrared imagery. This matches criterion 3 (new embodied AI benchmarks with a novel angle), as it addresses limitations in existing datasets and provides new challenges for tracking in real-world scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23277": {
        "authors": [
            "Gyeongjin Kang",
            "Seungtae Nam",
            "Xiangyu Sun",
            "Sameh Khamis",
            "Abdelrahman Mohamed",
            "Eunbyung Park"
        ],
        "title": "iLRM: An Iterative Large 3D Reconstruction Model",
        "abstract": "arXiv:2507.23277v1 Announce Type: new  Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.",
        "arxiv_id": "2507.23277",
        "ARXIVID": "2507.23277",
        "COMMENT": "This paper introduces iLRM, a scalable and efficient feed-forward 3D reconstruction model using iterative refinement and attention mechanisms. It is highly relevant to vision foundation models and their applications (criterion 4), and also touches on spatial understanding (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23070": {
        "authors": [
            "Dmitry Demidov",
            "Zaigham Zaheer",
            "Omkar Thawakar",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model",
        "abstract": "arXiv:2507.23070v1 Announce Type: new  Abstract: Fine-grained image classification, the task of distinguishing between visually similar subcategories within a broader category (e.g., bird species, car models, flower types), is a challenging computer vision problem. Traditional approaches rely heavily on fixed vocabularies and closed-set classification paradigms, limiting their scalability and adaptability in real-world settings where novel classes frequently emerge. Recent research has demonstrated that combining large language models (LLMs) with vision-language models (VLMs) makes open-set recognition possible without the need for predefined class labels. However, the existing methods are often limited in harnessing the power of LLMs at the classification phase, and also rely heavily on the guessed class names provided by an LLM without thorough analysis and refinement. To address these bottlenecks, we propose our training-free method, Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art results in fine-grained visual recognition while also offering greater interpretability, highlighting its strong potential in real-world scenarios and new domains where expert annotations are difficult to obtain. Additionally, we demonstrate the application of our proposed approach to zero-shot and few-shot classification, where it demonstrated performance on par with the existing SOTA while being training-free and not requiring human interventions. Overall, our vocabulary-free framework supports the shift in image classification from rigid label prediction to flexible, language-driven understanding, enabling scalable and generalizable systems for real-world applications. Well-documented code is available on https://github.com/demidovd98/e-finer.",
        "arxiv_id": "2507.23070",
        "ARXIVID": "2507.23070",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a vocabulary-free, training-free fine-grained visual recognition method using enriched contextually grounded vision-language models, with strong zero/few-shot results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23374": {
        "authors": [
            "Shuangkang Fang",
            "I-Chao Shen",
            "Takeo Igarashi",
            "Yufeng Wang",
            "ZeSheng Wang",
            "Yi Yang",
            "Wenrui Ding",
            "Shuchang Zhou"
        ],
        "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
        "abstract": "arXiv:2507.23374v1 Announce Type: new  Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.",
        "arxiv_id": "2507.23374",
        "ARXIVID": "2507.23374",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a hybrid NeRF-GS framework that leverages NeRF's spatial representation to improve 3D Gaussian Splatting, with new insights into hybrid 3D scene representation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23372": {
        "authors": [
            "Yijie Zhu",
            "Lingsen Zhang",
            "Zitong Yu",
            "Rui Shao",
            "Tao Tan",
            "Liqiang Nie"
        ],
        "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries",
        "abstract": "arXiv:2507.23372v1 Announce Type: new  Abstract: Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the model's understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at https://github.com/JiuTian-VL/UniEmo.",
        "arxiv_id": "2507.23372",
        "ARXIVID": "2507.23372",
        "COMMENT": "This paper proposes UniEmo, a unified framework for emotional understanding and generation using learnable expert queries and diffusion models. It is relevant to vision foundation models and their applications (criterion 4), and introduces a novel approach to unifying understanding and generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23300": {
        "authors": [
            "Hanshen Zhu",
            "Zhen Zhu",
            "Kaile Zhang",
            "Yiming Gong",
            "Yuliang Liu",
            "Xiang Bai"
        ],
        "title": "Training-free Geometric Image Editing on Diffusion Models",
        "abstract": "arXiv:2507.23300v1 Announce Type: new  Abstract: We tackle the task of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations. Code and benchmark are available at: https://github.com/CIawevy/FreeFine",
        "arxiv_id": "2507.23300",
        "ARXIVID": "2507.23300",
        "COMMENT": "This paper proposes a new training-free geometric image editing method using diffusion models and introduces a new benchmark (GeoBench) for 2D and 3D editing. The method is relevant to vision foundation models and their applications (criterion 4), and the new benchmark is of interest for generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23715": {
        "authors": [
            "Emery Pierson",
            "Lei Li",
            "Angela Dai",
            "Maks Ovsjanikov"
        ],
        "title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching",
        "abstract": "arXiv:2507.23715v1 Announce Type: new  Abstract: Deep functional maps have recently emerged as a powerful tool for solving non-rigid shape correspondence tasks. Methods that use this approach combine the power and flexibility of the functional map framework, with data-driven learning for improved accuracy and generality. However, most existing methods in this area restrict the learning aspect only to the feature functions and still rely on axiomatic modeling for formulating the training loss or for functional map regularization inside the networks. This limits both the accuracy and the applicability of the resulting approaches only to scenarios where assumptions of the axiomatic models hold. In this work, we show, for the first time, that both in-network regularization and functional map training can be replaced with data-driven methods. For this, we first train a generative model of functional maps in the spectral domain using score-based generative modeling, built from a large collection of high-quality maps. We then exploit the resulting model to promote the structural properties of ground truth functional maps on new shape collections. Remarkably, we demonstrate that the learned models are category-agnostic, and can fully replace commonly used strategies such as enforcing Laplacian commutativity or orthogonality of functional maps. Our key technical contribution is a novel distillation strategy from diffusion models in the spectral domain. Experiments demonstrate that our learned regularization leads to better results than axiomatic approaches for zero-shot non-rigid shape matching. Our code is available at: https://github.com/daidedou/diffumatch/",
        "arxiv_id": "2507.23715",
        "ARXIVID": "2507.23715",
        "COMMENT": "This paper introduces DiffuMatch, a generative model for non-rigid shape matching using diffusion models in the spectral domain. It is relevant to vision foundation models and generative modeling in multi-modal learning (criterion 4), and offers a novel regularization approach.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23597": {
        "authors": [
            "Zijian Dong",
            "Longteng Duan",
            "Jie Song",
            "Michael J. Black",
            "Andreas Geiger"
        ],
        "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction",
        "abstract": "arXiv:2507.23597v1 Announce Type: new  Abstract: We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to the limited amount of 3D training data such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as a model inversion process by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides a meaningful initialization for model fitting, enforces 3D regularization, and helps in refining pose estimation. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable",
        "arxiv_id": "2507.23597",
        "ARXIVID": "2507.23597",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling. Introduces a generative 3D avatar prior for monocular Gaussian avatar reconstruction, combining 2D diffusion and 3D generative models for improved 3D consistency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23268": {
        "authors": [
            "Shuai Wang",
            "Ziteng Gao",
            "Chenhui Zhu",
            "Weilin Huang",
            "Limin Wang"
        ],
        "title": "PixNerd: Pixel Neural Field Diffusion",
        "abstract": "arXiv:2507.23268v1 Announce Type: new  Abstract: The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID on ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.",
        "arxiv_id": "2507.23268",
        "ARXIVID": "2507.23268",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Proposes a new pixel neural field diffusion model that avoids VAE/cascade pipelines, with strong empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23755": {
        "authors": [
            "Rongzhen Zhao",
            "Yi Zhao",
            "Juho Kannala",
            "Joni Pajarinen"
        ],
        "title": "Slot Attention with Re-Initialization and Self-Distillation",
        "abstract": "arXiv:2507.23755v1 Announce Type: new  Abstract: Unlike popular solutions based on dense feature maps, Object-Centric Learning (OCL) represents visual scenes as sub-symbolic object-level feature vectors, termed slots, which are highly versatile for tasks involving visual modalities. OCL typically aggregates object superpixels into slots by iteratively applying competitive cross attention, known as Slot Attention, with the slots as the query. However, once initialized, these slots are reused naively, causing redundant slots to compete with informative ones for representing objects. This often results in objects being erroneously segmented into parts. Additionally, mainstream methods derive supervision signals solely from decoding slots into the input's reconstruction, overlooking potential supervision based on internal information. To address these issues, we propose Slot Attention with re-Initialization and self-Distillation (DIAS): $\\emph{i)}$ We reduce redundancy in the aggregated slots and re-initialize extra aggregation to update the remaining slots; $\\emph{ii)}$ We drive the bad attention map at the first aggregation iteration to approximate the good at the last iteration to enable self-distillation. Experiments demonstrate that DIAS achieves state-of-the-art on OCL tasks like object discovery and recognition, while also improving advanced visual prediction and reasoning. Our code is available on https://github.com/Genera1Z/DIAS.",
        "arxiv_id": "2507.23755",
        "ARXIVID": "2507.23755",
        "COMMENT": "This paper proposes improvements to Slot Attention for object-centric learning, with new self-distillation and re-initialization strategies. It is relevant to spatial intelligence and spatial understanding in embodied agents (criterion 1), as it improves object-centric scene representation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.23569": {
        "authors": [
            "Maxime Pietrantoni",
            "Gabriela Csurka",
            "Torsten Sattler"
        ],
        "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization",
        "abstract": "arXiv:2507.23569v1 Announce Type: new  Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.",
        "arxiv_id": "2507.23569",
        "ARXIVID": "2507.23569",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to spatial understanding (criterion 1). Proposes Gaussian Splatting Feature Fields for privacy-preserving visual localization, combining explicit 3D geometry with implicit feature fields and contrastive learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.23313": {
        "authors": [
            "Alfio Ferrara",
            "Sergio Picascia",
            "Elisabetta Rocchetti"
        ],
        "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models",
        "abstract": "arXiv:2507.23313v1 Announce Type: new  Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.",
        "arxiv_id": "2507.23313",
        "ARXIVID": "2507.23313",
        "COMMENT": "This paper analyzes how text-to-image diffusion models interpret artistic prompts, focusing on the internal representation of content and style. It is relevant to vision foundation models and their interpretability (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.23785": {
        "authors": [
            "Bowen Zhang",
            "Sicheng Xu",
            "Chuxin Wang",
            "Jiaolong Yang",
            "Feng Zhao",
            "Dong Chen",
            "Baining Guo"
        ],
        "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis",
        "abstract": "arXiv:2507.23785v1 Announce Type: new  Abstract: In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
        "arxiv_id": "2507.23785",
        "ARXIVID": "2507.23785",
        "COMMENT": "This paper presents a novel framework for high-fidelity video-to-4D synthesis using a new diffusion model and a compact latent representation for dynamic 3D content. It is relevant to vision foundation models and generative modeling (criterion 4), but does not directly address spatial intelligence in embodied agents or benchmarks for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.23143": {
        "authors": [
            "Xiaochen Zhao",
            "Hongyi Xu",
            "Guoxian Song",
            "You Xie",
            "Chenxu Zhang",
            "Xiu Li",
            "Linjie Luo",
            "Jinli Suo",
            "Yebin Liu"
        ],
        "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention",
        "abstract": "arXiv:2507.23143v1 Announce Type: new  Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.",
        "arxiv_id": "2507.23143",
        "ARXIVID": "2507.23143",
        "COMMENT": "This paper proposes a new zero-shot diffusion-based portrait animation pipeline with a novel disentangled latent attention mechanism for expressive neural motion reenactment. It introduces a new method for controlling motion in image generation, which is relevant to generative modeling in multi-modal learning. However, it does not directly address spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.23202": {
        "authors": [
            "Chengwei Xia",
            "Fan Ma",
            "Ruijie Quan",
            "Kun Zhan",
            "Yi Yang"
        ],
        "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks",
        "abstract": "arXiv:2507.23202v1 Announce Type: new  Abstract: This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.",
        "arxiv_id": "2507.23202",
        "ARXIVID": "2507.23202",
        "COMMENT": "This paper proposes a new adversarial attack method for MLLMs using diffusion models. It matches criterion 2 (new MLLMs) and is relevant for empirical insights into model robustness and adversarial attacks in multi-modal models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.23371": {
        "authors": [
            "Ali Youssef"
        ],
        "title": "VMatcher: State-Space Semi-Dense Local Feature Matching",
        "abstract": "arXiv:2507.23371v1 Announce Type: new  Abstract: This paper introduces VMatcher, a hybrid Mamba-Transformer network for semi-dense feature matching between image pairs. Learning-based feature matching methods, whether detector-based or detector-free, achieve state-of-the-art performance but depend heavily on the Transformer's attention mechanism, which, while effective, incurs high computational costs due to its quadratic complexity. In contrast, Mamba introduces a Selective State-Space Model (SSM) that achieves comparable or superior performance with linear complexity, offering significant efficiency gains. VMatcher leverages a hybrid approach, integrating Mamba's highly efficient long-sequence processing with the Transformer's attention mechanism. Multiple VMatcher configurations are proposed, including hierarchical architectures, demonstrating their effectiveness in setting new benchmarks efficiently while ensuring robustness and practicality for real-time applications where rapid inference is crucial. Source Code is available at: https://github.com/ayoussf/VMatcher",
        "arxiv_id": "2507.23371",
        "ARXIVID": "2507.23371",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces VMatcher, a hybrid Mamba-Transformer for efficient semi-dense local feature matching, with new architectures and efficiency improvements.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.23673": {
        "authors": [
            "Alfie Roddan",
            "Tobias Czempiel",
            "Chi Xu",
            "Daniel S. Elson",
            "Stamatia Giannarou"
        ],
        "title": "SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation",
        "abstract": "arXiv:2507.23673v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) provides rich spectral information for medical imaging, yet encounters significant challenges due to data limitations and hardware variations. We introduce SAMSA, a novel interactive segmentation framework that combines an RGB foundation model with spectral analysis. SAMSA efficiently utilizes user clicks to guide both RGB segmentation and spectral similarity computations. The method addresses key limitations in HSI segmentation through a unique spectral feature fusion strategy that operates independently of spectral band count and resolution. Performance evaluation on publicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a neurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative porcine hyperspectral dataset. Experimental results demonstrate SAMSA's effectiveness in few-shot and zero-shot learning scenarios and using minimal training examples. Our approach enables seamless integration of datasets with different spectral characteristics, providing a flexible framework for hyperspectral medical image analysis.",
        "arxiv_id": "2507.23673",
        "ARXIVID": "2507.23673",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes SAMSA, which enhances the Segment Anything Model (SAM) for hyperspectral medical image segmentation using spectral angles and feature fusion, showing strong results in few/zero-shot learning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.23006": {
        "authors": [
            "Zhensheng Yuan",
            "Haozhi Huang",
            "Zhen Xiong",
            "Di Wang",
            "Guanghua Yang"
        ],
        "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction",
        "abstract": "arXiv:2507.23006v1 Announce Type: new  Abstract: We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS.",
        "arxiv_id": "2507.23006",
        "ARXIVID": "2507.23006",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) due to robust and efficient 3D Gaussian Splatting for urban scene reconstruction, with new methods for scene partitioning, LOD, and appearance transformation. Also relevant to spatial understanding (criterion 1) via explicit spatial partitioning and regularization.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.23307": {
        "authors": [
            "Xihang Hu",
            "Fuming Sun",
            "Jiazhe Liu",
            "Feilong Xu",
            "Xiaoli Zhang"
        ],
        "title": "ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection",
        "abstract": "arXiv:2507.23307v1 Announce Type: new  Abstract: Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance on costly pixel-level annotations by leveraging limited annotated data and abundant unlabeled data. However, existing SSCOD methods based on Teacher-Student frameworks suffer from severe prediction bias and error propagation under scarce supervision, while their multi-network architectures incur high computational overhead and limited scalability. To overcome these limitations, we propose ST-SAM, a highly annotation-efficient yet concise framework that breaks away from conventional SSCOD constraints. Specifically, ST-SAM employs Self-Training strategy that dynamically filters and expands high-confidence pseudo-labels to enhance a single-model architecture, thereby fundamentally circumventing inter-model prediction bias. Furthermore, by transforming pseudo-labels into hybrid prompts containing domain-specific knowledge, ST-SAM effectively harnesses the Segment Anything Model's potential for specialized tasks to mitigate error accumulation in self-training. Experiments on COD benchmark datasets demonstrate that ST-SAM achieves state-of-the-art performance with only 1\\% labeled data, outperforming existing SSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM requires training only a single network, without relying on specific models or loss functions. This work establishes a new paradigm for annotation-efficient SSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.",
        "arxiv_id": "2507.23307",
        "ARXIVID": "2507.23307",
        "COMMENT": "This paper proposes ST-SAM, a self-training framework for semi-supervised camouflaged object detection leveraging the Segment Anything Model (SAM). It is relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.23726": {
        "authors": [
            "Luoxin Chen",
            "Jinming Gu",
            "Liankai Huang",
            "Wenhao Huang",
            "Zhicheng Jiang",
            "Allan Jie",
            "Xiaoran Jin",
            "Xing Jin",
            "Chenggang Li",
            "Kaijing Ma",
            "Cheng Ren",
            "Jiawei Shen",
            "Wenlei Shi",
            "Tong Sun",
            "He Sun",
            "Jiahui Wang",
            "Siran Wang",
            "Zhihong Wang",
            "Chenrui Wei",
            "Shufa Wei",
            "Yonghui Wu",
            "Yuchen Wu",
            "Yihang Xia",
            "Huajian Xin",
            "Fan Yang",
            "Huaiyuan Ying",
            "Hongyi Yuan",
            "Zheng Yuan",
            "Tianyang Zhan",
            "Chi Zhang",
            "Yue Zhang",
            "Ge Zhang",
            "Tianyun Zhao",
            "Jianqiu Zhao",
            "Yichi Zhou",
            "Thomas Hanwen Zhu"
        ],
        "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
        "abstract": "arXiv:2507.23726v1 Announce Type: new  Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
        "arxiv_id": "2507.23726",
        "ARXIVID": "2507.23726",
        "COMMENT": "This paper introduces Seed-Prover, a new model for automated theorem proving using LLMs and formal verification. While it is a significant advancement in mathematical reasoning and LLMs, it does not match the specific criteria related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 8
    },
    "2507.23554": {
        "authors": [
            "Ruoyu Wang",
            "Junda Wu",
            "Yu Xia",
            "Tong Yu",
            "Ryan A. Rossi",
            "Julian McAuley",
            "Lina Yao"
        ],
        "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer",
        "abstract": "arXiv:2507.23554v1 Announce Type: new  Abstract: Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.",
        "arxiv_id": "2507.23554",
        "ARXIVID": "2507.23554",
        "COMMENT": "This paper introduces DICE, a principled method for dynamic in-context example selection in LLM agents. While it is a methodological improvement for LLM agents, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2507.23751": {
        "authors": [
            "Ping Yu",
            "Jack Lanchantin",
            "Tianlu Wang",
            "Weizhe Yuan",
            "Olga Golovneva",
            "Ilia Kulikov",
            "Sainbayar Sukhbaatar",
            "Jason Weston",
            "Jing Xu"
        ],
        "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks",
        "abstract": "arXiv:2507.23751v1 Announce Type: new  Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.",
        "arxiv_id": "2507.23751",
        "ARXIVID": "2507.23751",
        "COMMENT": "This paper proposes CoT-Self-Instruct, a synthetic data generation method for LLMs using chain-of-thought reasoning. It is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23601": {
        "authors": [
            "Xin Li",
            "Keren Fu",
            "Qijun Zhao"
        ],
        "title": "Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection",
        "abstract": "arXiv:2507.23601v1 Announce Type: new  Abstract: Existing video camouflaged object detection (VCOD) methods primarily rely on spatial appearance features to perceive motion cues for breaking camouflage. However, the high similarity between foreground and background in VCOD results in limited discriminability of spatial appearance features (e.g., color and texture), restricting detection accuracy and completeness. Recent studies demonstrate that frequency features can not only enhance feature representation to compensate for appearance limitations but also perceive motion through dynamic variations in frequency energy. Furthermore, the emerging state space model called Mamba, enables efficient perception of motion cues in frame sequences due to its linear-time long-sequence modeling capability. Motivated by this, we propose a novel visual camouflage Mamba (Vcamba) based on spatio-frequency motion perception that integrates frequency and spatial features for efficient and accurate VCOD. Specifically, we propose a receptive field visual state space (RFVSS) module to extract multi-scale spatial features after sequence modeling. For frequency learning, we introduce an adaptive frequency component enhancement (AFE) module with a novel frequency-domain sequential scanning strategy to maintain semantic consistency. Then we propose a space-based long-range motion perception (SLMP) module and a frequency-based long-range motion perception (FLMP) module to model spatio-temporal and frequency-temporal sequences in spatial and frequency phase domains. Finally, the space and frequency motion fusion module (SFMF) integrates dual-domain features for unified motion representation. Experimental results show that our Vcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2 datasets with lower computation cost, confirming the superiority of Vcamba. Our code is available at: https://github.com/BoydeLi/Vcamba.",
        "arxiv_id": "2507.23601",
        "ARXIVID": "2507.23601",
        "COMMENT": "This paper proposes a Mamba-based model for video camouflaged object detection, integrating spatio-frequency motion perception. It is a new method in computer vision, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23253": {
        "authors": [
            "Mingyang Yu",
            "Xiahui Guo",
            "Peng chen",
            "Zhenkai Li",
            "Yang Shu"
        ],
        "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality",
        "abstract": "arXiv:2507.23253v1 Announce Type: new  Abstract: Time Series forecasting is critical in diverse domains such as weather forecasting, financial investment, and traffic management. While traditional numerical metrics like mean squared error (MSE) can quantify point-wise accuracy, they fail to evaluate the geometric structure of time series data, which is essential to understand temporal dynamics. To address this issue, we propose the time series Geometric Structure Index (TGSI), a novel evaluation metric that transforms time series into images to leverage their inherent two-dimensional geometric representations. However, since the image transformation process is non-differentiable, TGSI cannot be directly integrated as a training loss. We further introduce the Shape-Aware Temporal Loss (SATL), a multi-component loss function operating in the time series modality to bridge this gap and enhance structure modeling during training. SATL combines three components: a first-order difference loss that measures structural consistency through the MSE between first-order differences, a frequency domain loss that captures essential periodic patterns using the Fast Fourier Transform while minimizing noise, and a perceptual feature loss that measures geometric structure difference in time-series by aligning temporal features with geometric structure features through a pre-trained temporal feature extractor and time-series image autoencoder. Experiments across multiple datasets demonstrate that models trained with SATL achieve superior performance in both MSE and the proposed TGSI metrics compared to baseline methods, without additional computational cost during inference.",
        "arxiv_id": "2507.23253",
        "ARXIVID": "2507.23253",
        "COMMENT": "This paper proposes a new metric and loss for time series forecasting by leveraging image modality, but it is not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23701": {
        "authors": [
            "Long Phan",
            "Mantas Mazeika",
            "Andy Zou",
            "Dan Hendrycks"
        ],
        "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
        "abstract": "arXiv:2507.23701v1 Announce Type: new  Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.",
        "arxiv_id": "2507.23701",
        "ARXIVID": "2507.23701",
        "COMMENT": "This paper introduces TextQuests, a benchmark for evaluating LLM agents in text-based video games. While it is a new benchmark for agent evaluation, it is not focused on spatial intelligence, embodied AI in the physical world, or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23336": {
        "authors": [
            "Ram Mohan Rao Kadiyala",
            "Siddhant Gupta",
            "Jebish Purbey",
            "Giulio Martini",
            "Suman Debnath",
            "Hamza Farooq"
        ],
        "title": "DSBC : Data Science task Benchmarking with Context engineering",
        "abstract": "arXiv:2507.23336v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.",
        "arxiv_id": "2507.23336",
        "ARXIVID": "2507.23336",
        "COMMENT": "This paper introduces a benchmark for data science agents using LLMs, but it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23709": {
        "authors": [
            "Alfie Roddan",
            "Chi Xu",
            "Serine Ajlouni",
            "Irini Kakaletri",
            "Patra Charalampaki",
            "Stamatia Giannarou"
        ],
        "title": "Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation",
        "abstract": "arXiv:2507.23709v1 Announce Type: new  Abstract: The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For image classification models, pixel attribution methods are popular to infer explainability. However, overconfidence in deep learning model's predictions translates to overconfidence in pixel attribution. In this paper, we propose the first approach which incorporates risk estimation into a pixel attribution method for improved image classification explainability. The proposed method iteratively applies a classification model with a pixel attribution method to create a volume of PA maps. This volume is used for the first time, to generate a pixel-wise distribution of PA values. We introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data and ImageNet verifies that our improved explainability method outperforms the state-of-the-art.",
        "arxiv_id": "2507.23709",
        "ARXIVID": "2507.23709",
        "COMMENT": "This paper proposes a new explainability method for image classification with risk estimation, but it is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23343": {
        "authors": [
            "Yingjie Zhou",
            "Jiezhang Cao",
            "Zicheng Zhang",
            "Farong Wen",
            "Yanwei Jiang",
            "Jun Jia",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads",
        "abstract": "arXiv:2507.23343v1 Announce Type: new  Abstract: Speech-driven methods for portraits are figuratively known as \"Talkers\" because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at https://github.com/zyj-2000/Talker.",
        "arxiv_id": "2507.23343",
        "ARXIVID": "2507.23343",
        "COMMENT": "This paper presents a large-scale quality assessment dataset for AI-generated talking heads and proposes a new objective quality assessment method. While it is related to generative modeling and vision-language models, it does not directly match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23608": {
        "authors": [
            "Linmin Pei",
            "Granger Sutton",
            "Michael Rutherford",
            "Ulrike Wagner",
            "Tracy Nolan",
            "Kirk Smith",
            "Phillip Farmer",
            "Peter Gu",
            "Ambar Rana",
            "Kailing Chen",
            "Thomas Ferleman",
            "Brian Park",
            "Ye Wu",
            "Jordan Kojouharov",
            "Gargi Singh",
            "Jon Lemon",
            "Tyler Willis",
            "Milos Vukadinovic",
            "Grant Duffy",
            "Bryan He",
            "David Ouyang",
            "Marco Pereanez",
            "Daniel Samber",
            "Derek A. Smith",
            "Christopher Cannistraci",
            "Zahi Fayad",
            "David S. Mendelson",
            "Michele Bufano",
            "Elmar Kotter",
            "Hamideh Haghiri",
            "Rajesh Baidya",
            "Stefan Dvoretskii",
            "Klaus H. Maier-Hein",
            "Marco Nolden",
            "Christopher Ablett",
            "Silvia Siggillino",
            "Sandeep Kaushik",
            "Hongzhu Jiang",
            "Sihan Xie",
            "Zhiyu Wan",
            "Alex Michie",
            "Simon J Doran",
            "Angeline Aurelia Waly",
            "Felix A. Nathaniel Liang",
            "Humam Arshad Mustagfirin",
            "Michelle Grace Felicia",
            "Kuo Po Chih",
            "Rahul Krish",
            "Ghulam Rasool",
            "Nidhal Bouaynaya",
            "Nikolas Koutsoubis",
            "Kyle Naddeo",
            "Kartik Pandit",
            "Tony O'Sullivan",
            "Raj Krish",
            "Qinyan Pan",
            "Scott Gustafson",
            "Benjamin Kopchick",
            "Laura Opsahl-Ong",
            "Andrea Olvera-Morales",
            "Jonathan Pinney",
            "Kathryn Johnson",
            "Theresa Do",
            "Juergen Klenk",
            "Maria Diaz",
            "Arti Singh",
            "Rong Chai",
            "David A. Clunie",
            "Fred Prior",
            "Keyvan Farahani"
        ],
        "title": "Medical Image De-Identification Benchmark Challenge",
        "abstract": "arXiv:2507.23608v1 Announce Type: new  Abstract: The de-identification (deID) of protected health information (PHI) and personally identifiable information (PII) is a fundamental requirement for sharing medical images, particularly through public repositories, to ensure compliance with patient privacy laws. In addition, preservation of non-PHI metadata to inform and enable downstream development of imaging artificial intelligence (AI) is an important consideration in biomedical research. The goal of MIDI-B was to provide a standardized platform for benchmarking of DICOM image deID tools based on a set of rules conformant to the HIPAA Safe Harbor regulation, the DICOM Attribute Confidentiality Profiles, and best practices in preservation of research-critical metadata, as defined by The Cancer Imaging Archive (TCIA). The challenge employed a large, diverse, multi-center, and multi-modality set of real de-identified radiology images with synthetic PHI/PII inserted.   The MIDI-B Challenge consisted of three phases: training, validation, and test. Eighty individuals registered for the challenge. In the training phase, we encouraged participants to tune their algorithms using their in-house or public data. The validation and test phases utilized the DICOM images containing synthetic identifiers (of 216 and 322 subjects, respectively). Ten teams successfully completed the test phase of the challenge. To measure success of a rule-based approach to image deID, scores were computed as the percentage of correct actions from the total number of required actions. The scores ranged from 97.91% to 99.93%. Participants employed a variety of open-source and proprietary tools with customized configurations, large language models, and optical character recognition (OCR). In this paper we provide a comprehensive report on the MIDI-B Challenge's design, implementation, results, and lessons learned.",
        "arxiv_id": "2507.23608",
        "ARXIVID": "2507.23608",
        "COMMENT": "This paper presents a benchmark challenge for medical image de-identification. While it is a new benchmark, it is not focused on embodied AI, spatial intelligence, or vision foundation models, but rather on privacy in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23206": {
        "authors": [
            "Xiaoyu Ji",
            "Ali Shakouri",
            "Fengqing Zhu"
        ],
        "title": "Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images",
        "abstract": "arXiv:2507.23206v1 Announce Type: new  Abstract: Food crystal agglomeration is a phenomenon occurs during crystallization which traps water between crystals and affects food product quality. Manual annotation of agglomeration in 2D microscopic images is particularly difficult due to the transparency of water bonding and the limited perspective focusing on a single slide of the imaged sample. To address this challenge, we first propose a supervised baseline model to generate segmentation pseudo-labels for the coarsely labeled classification dataset. Next, an instance classification model that simultaneously performs pixel-wise segmentation is trained. Both models are used in the inference stage to combine their respective strengths in classification and segmentation. To preserve crystal properties, a post processing module is designed and included to both steps. Our method improves true positive agglomeration classification accuracy and size distribution predictions compared to other existing methods. Given the variability in confidence levels of manual annotations, our proposed method is evaluated under two confidence levels and successfully classifies potential agglomerated instances.",
        "arxiv_id": "2507.23206",
        "ARXIVID": "2507.23206",
        "COMMENT": "This paper proposes a confidence-aware method for classification and segmentation of food crystal agglomeration in microscopic images. It is a computer vision application but does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.23311": {
        "authors": [
            "Timm Hess",
            "Gido M van de Ven",
            "Tinne Tuytelaars"
        ],
        "title": "Forgetting of task-specific knowledge in model merging-based continual learning",
        "abstract": "arXiv:2507.23311v1 Announce Type: new  Abstract: This paper investigates the linear merging of models in the context of continual learning (CL). Using controlled visual cues in computer vision experiments, we demonstrate that merging largely preserves or enhances shared knowledge, while unshared task-specific knowledge rapidly degrades. We further find that merging models from an incremental training process consistently outperforms merging models trained in parallel.",
        "arxiv_id": "2507.23311",
        "ARXIVID": "2507.23311",
        "COMMENT": "This paper investigates model merging in continual learning for computer vision, but does not focus on spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about continual learning methodology.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}