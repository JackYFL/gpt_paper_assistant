{
    "2510.18054": {
        "authors": [
            "Ata \\c{C}elen",
            "Marc Pollefeys",
            "Daniel Barath",
            "Iro Armeni"
        ],
        "title": "HouseTour: A Virtual Real Estate A(I)gent",
        "abstract": "arXiv:2510.18054v1 Announce Type: new  Abstract: We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.",
        "arxiv_id": "2510.18054",
        "ARXIVID": "2510.18054",
        "COMMENT": "Matches criteria 1 and 3: Presents a new method for spatially-aware 3D camera trajectory and language summary generation for embodied agents, and introduces a new benchmark (HouseTour dataset) for this task. Also leverages 3D Gaussian splatting and joint metric for evaluation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18632": {
        "authors": [
            "Zhangquan Chen",
            "Manyuan Zhang",
            "Xinlei Yu",
            "Xufang Luo",
            "Mingze Sun",
            "Zihao Pan",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Ruqi Huang"
        ],
        "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "abstract": "arXiv:2510.18632v1 Announce Type: new  Abstract: Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.",
        "arxiv_id": "2510.18632",
        "ARXIVID": "2510.18632",
        "COMMENT": "Matches criterion 1: Proposes 3DThinker, a framework for geometric imagination and spatial reasoning from limited views, enabling 3D mental modeling in VLMs without explicit 3D data. Strongly relevant to spatial intelligence and spatial understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18135": {
        "authors": [
            "Jiahan Zhang",
            "Muqing Jiang",
            "Nanru Dai",
            "Taiming Lu",
            "Arda Uzunoglu",
            "Shunchi Zhang",
            "Yana Wei",
            "Jiahao Wang",
            "Vishal M. Patel",
            "Paul Pu Liang",
            "Daniel Khashabi",
            "Cheng Peng",
            "Rama Chellappa",
            "Tianmin Shu",
            "Alan Yuille",
            "Yilun Du",
            "Jieneng Chen"
        ],
        "title": "World-in-World: World Models in a Closed-Loop World",
        "abstract": "arXiv:2510.18135v1 Announce Type: new  Abstract: Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.",
        "arxiv_id": "2510.18135",
        "ARXIVID": "2510.18135",
        "COMMENT": "Directly matches criterion 3 (embodied AI, new benchmark/simulator, novel evaluation protocol). Introduces World-in-World, a closed-loop platform for benchmarking world models in embodied agent settings, with new insights into controllability and scaling laws.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18873": {
        "authors": [
            "Ziang Zhang",
            "Zehan Wang",
            "Guanghao Zhang",
            "Weilong Dai",
            "Yan Xia",
            "Ziang Yan",
            "Minjie Hong",
            "Zhou Zhao"
        ],
        "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
        "abstract": "arXiv:2510.18873v1 Announce Type: new  Abstract: Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.",
        "arxiv_id": "2510.18873",
        "ARXIVID": "2510.18873",
        "COMMENT": "Directly matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 3 (new benchmark for embodied AI with a novel angle: dynamic spatial intelligence in videos). Introduces DSI-Bench for evaluating dynamic spatial reasoning in VLMs and expert models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18726": {
        "authors": [
            "Shihao Li",
            "Yuanxing Zhang",
            "Jiangtao Wu",
            "Zhide Lei",
            "Yiwen He",
            "Runzhe Wen",
            "Chenxi Liao",
            "Chengkang Jiang",
            "An Ping",
            "Shuo Gao",
            "Suhan Wang",
            "Zhaozhou Bian",
            "Zijun Zhou",
            "Jingyi Xie",
            "Jiayi Zhou",
            "Jing Wang",
            "Yifan Yao",
            "Weihao Xie",
            "Yingshui Tan",
            "Yanghai Wang",
            "Qianqian Xie",
            "Zhaoxiang Zhang",
            "Jiaheng Liu"
        ],
        "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
        "abstract": "arXiv:2510.18726v1 Announce Type: new  Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.",
        "arxiv_id": "2510.18726",
        "ARXIVID": "2510.18726",
        "COMMENT": "Introduces IF-VidCap, a new benchmark for instruction-following in video captioning, and evaluates MLLMs on this task. Directly matches criterion 2 (new MLLMs/VLLMs) and criterion 3 (new benchmark for embodied AI, with a novel focus on instruction-following in video captioning).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18457": {
        "authors": [
            "Tianci Bi",
            "Xiaoyi Zhang",
            "Yan Lu",
            "Nanning Zheng"
        ],
        "title": "Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models",
        "abstract": "arXiv:2510.18457v1 Announce Type: new  Abstract: The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.",
        "arxiv_id": "2510.18457",
        "ARXIVID": "2510.18457",
        "COMMENT": "Directly addresses criterion 4 (Vision foundation models and applications) by proposing a new way to use Vision Foundation Models as tokenizers for Latent Diffusion Models, bypassing distillation and introducing new training strategies and metrics. Also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.18262": {
        "authors": [
            "Da Zhang",
            "Chenggang Rong",
            "Bingyu Li",
            "Feiyu Wang",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "title": "UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding",
        "abstract": "arXiv:2510.18262v1 Announce Type: new  Abstract: Large vision-language models (VLMs) have achieved remarkable success in natural scene understanding, yet their application to underwater environments remains largely unexplored. Underwater imagery presents unique challenges including severe light attenuation, color distortion, and suspended particle scattering, while requiring specialized knowledge of marine ecosystems and organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive benchmark specifically designed for underwater vision-language understanding. UWBench comprises 15,003 high-resolution underwater images captured across diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea habitats. Each image is enriched with human-verified annotations including 15,281 object referring expressions that precisely describe marine organisms and underwater structures, and 124,983 question-answer pairs covering diverse reasoning capabilities from object recognition to ecological relationship understanding. The dataset captures rich variations in visibility, lighting conditions, and water turbidity, providing a realistic testbed for model evaluation. Based on UWBench, we establish three comprehensive benchmarks: detailed image captioning for generating ecologically informed scene descriptions, visual grounding for precise localization of marine organisms, and visual question answering for multimodal reasoning about underwater environments. Extensive experiments on state-of-the-art VLMs demonstrate that underwater understanding remains challenging, with substantial room for improvement. Our benchmark provides essential resources for advancing vision-language research in underwater contexts and supporting applications in marine science, ecological monitoring, and autonomous underwater exploration. Our code and benchmark will be available.",
        "arxiv_id": "2510.18262",
        "ARXIVID": "2510.18262",
        "COMMENT": "Matches criterion 3 (new embodied AI benchmark/simulator) and criterion 2 (VLLMs/MLLMs): introduces a comprehensive vision-language benchmark for underwater understanding, with new tasks and empirical results showing VLMs struggle in this domain.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.18619": {
        "authors": [
            "Wei Cai",
            "Jian Zhao",
            "Yuchen Yuan",
            "Tianle Zhang",
            "Ming Zhu",
            "Haichuan Tang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "VAR: Visual Attention Reasoning via Structured Search and Backtracking",
        "abstract": "arXiv:2510.18619v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are hindered by their high hallucination tendency and heavy reliance on brittle, linear reasoning processes, leading to failures in complex tasks. To address these limitations, we introduce Visual Attention Reasoning (VAR), a novel framework that recasts grounded reasoning as a structured search over a reasoning trajectory space. VAR decomposes the reasoning process into two key stages: traceable evidence grounding and search-based chain-of-thought (CoT) generation, which incorporates a backtracking mechanism for self-correction. The search is guided by a multi-faceted reward function with semantic and geometric self-verification components, which penalize outputs that are not faithfully grounded in the visual input. We provide a theoretical analysis for our search strategy, validating its capability to find the correct solution with high probability. Experimental results show that our 7B model, VAR-7B, sets a new state-of-the-art on a comprehensive suite of hallucination and safety benchmarks, significantly outperforming existing open-source models and demonstrating competitive performance against leading proprietary systems.",
        "arxiv_id": "2510.18619",
        "ARXIVID": "2510.18619",
        "COMMENT": "Matches criterion 2: Introduces a new MLLM (VAR-7B) with a novel structured search and backtracking mechanism for visual attention reasoning, addressing hallucination and reasoning in MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.18313": {
        "authors": [
            "Bohan Li",
            "Zhuang Ma",
            "Dalong Du",
            "Baorui Peng",
            "Zhujin Liang",
            "Zhenqiang Liu",
            "Chao Ma",
            "Yueming Jin",
            "Hao Zhao",
            "Wenjun Zeng",
            "Xin Jin"
        ],
        "title": "OmniNWM: Omniscient Driving Navigation World Models",
        "abstract": "arXiv:2510.18313v1 Announce Type: new  Abstract: Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.",
        "arxiv_id": "2510.18313",
        "ARXIVID": "2510.18313",
        "COMMENT": "OmniNWM introduces a unified world model for autonomous driving, generating panoramic videos and 3D occupancy, and integrating state, action, and reward. Closely matches criterion 1 (spatial understanding in embodied agents) and criterion 3 (novel methods for embodied AI, especially with unified modeling and occupancy-grounded rewards).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.18714": {
        "authors": [
            "Changkun Liu",
            "Bin Tan",
            "Zeran Ke",
            "Shangzhan Zhang",
            "Jiachen Liu",
            "Ming Qian",
            "Nan Xue",
            "Yujun Shen",
            "Tristan Braud"
        ],
        "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting",
        "abstract": "arXiv:2510.18714v1 Announce Type: new  Abstract: This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at https://lck666666.github.io/plana3r",
        "arxiv_id": "2510.18714",
        "ARXIVID": "2510.18714",
        "COMMENT": "Proposes PLANA3R, a new pose-free, feed-forward method for metric planar 3D reconstruction from unposed images using vision transformers and planar splatting. Closely matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI, especially with new angles like planar primitives and no explicit plane supervision).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.18822": {
        "authors": [
            "Jiaming Zhang",
            "Cheng Liang",
            "Yichun Yang",
            "Chenkai Zeng",
            "Yutao Cui",
            "Xinwen Zhang",
            "Xin Zhou",
            "Kai Ma",
            "Gangshan Wu",
            "Limin Wang"
        ],
        "title": "SAM 2++: Tracking Anything at Any Granularity",
        "abstract": "arXiv:2510.18822v1 Announce Type: new  Abstract: Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.",
        "arxiv_id": "2510.18822",
        "ARXIVID": "2510.18822",
        "COMMENT": "Matches criteria 4: Proposes SAM 2++, a unified video tracking model at any granularity, introduces a new large-scale dataset (Tracking-Any-Granularity), and achieves SOTA on multiple benchmarks. Relevant to vision foundation models and applications.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.18321": {
        "authors": [
            "Jinlin Li",
            "Yuran Wang",
            "Yifei Yuan",
            "Xiao Zhou",
            "Yingying Zhang",
            "Xixian Yong",
            "Yefeng Zheng",
            "Xian Wu"
        ],
        "title": "Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding",
        "abstract": "arXiv:2510.18321v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering. However, they remain prone to object hallucination -- generating descriptions of nonexistent or misidentified objects. Prior work has partially mitigated this via auxiliary training objectives or external modules, but challenges remain in terms of scalability, adaptability, and model independence. To address these limitations, we propose Adaptive Token Ensemble Decoding (ATED), a training-free, token-level ensemble framework that mitigates hallucination by aggregating predictions from multiple LVLMs during inference. ATED dynamically computes uncertainty-based weights for each model, reflecting their reliability at each decoding step. It also integrates diverse decoding paths to improve contextual grounding and semantic consistency. Experiments on standard hallucination detection benchmarks demonstrate that ATED significantly outperforms state-of-the-art methods, reducing hallucination without compromising fluency or relevance. Our findings highlight the benefits of adaptive ensembling and point to a promising direction for improving LVLM robustness in high-stakes applications. The code is available at https://github.com/jinlin2021/ATED.",
        "arxiv_id": "2510.18321",
        "ARXIVID": "2510.18321",
        "COMMENT": "Directly matches criterion 2 (new VLLMs/MLLMs): Proposes a novel, training-free ensemble decoding method to mitigate hallucinations in LVLMs, with strong empirical results. Also relevant to criterion 4 (vision foundation models).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.18795": {
        "authors": [
            "Xiaoxing Hu",
            "Kaicheng Yang",
            "Ziyong Feng",
            "Qi Ming",
            "Zonghao Guo",
            "Xiang An",
            "Ziyong Feng",
            "Junchi Yan",
            "Xue Yang"
        ],
        "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
        "abstract": "arXiv:2510.18795v1 Announce Type: new  Abstract: The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP",
        "arxiv_id": "2510.18795",
        "ARXIVID": "2510.18795",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): ProCLIP proposes a new method for aligning CLIP's image encoder with an LLM-based embedder, addressing limitations in text encoding and multilingual support. Also relevant to criterion 4 (vision foundation models).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.18244": {
        "authors": [
            "Ajinkya Khoche",
            "Gerg\\H{o} L\\'aszl\\'o Nagy",
            "Maciej Wozniak",
            "Thomas Gustafsson",
            "Patric Jensfelt"
        ],
        "title": "BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining",
        "abstract": "arXiv:2510.18244v1 Announce Type: new  Abstract: Zero-shot 3D object classification is crucial for real-world applications like autonomous driving, however it is often hindered by a significant domain gap between the synthetic data used for training and the sparse, noisy LiDAR scans encountered in the real-world. Current methods trained solely on synthetic data fail to generalize to outdoor scenes, while those trained only on real data lack the semantic diversity to recognize rare or unseen objects.   We introduce BlendCLIP, a multimodal pretraining framework that bridges this synthetic-to-real gap by strategically combining the strengths of both domains. We first propose a pipeline to generate a large-scale dataset of object-level triplets -- consisting of a point cloud, image, and text description -- mined directly from real-world driving data and human annotated 3D boxes. Our core contribution is a curriculum-based data mixing strategy that first grounds the model in the semantically rich synthetic CAD data before progressively adapting it to the specific characteristics of real-world scans.   Our experiments show that our approach is highly label-efficient: introducing as few as 1.5\\% real-world samples per batch into training boosts zero-shot accuracy on the nuScenes benchmark by 27\\%. Consequently, our final model achieves state-of-the-art performance on challenging outdoor datasets like nuScenes and TruckScenes, improving over the best prior method by 19.3\\% on nuScenes, while maintaining strong generalization on diverse synthetic benchmarks. Our findings demonstrate that effective domain adaptation, not full-scale real-world annotation, is the key to unlocking robust open-vocabulary 3D perception. Our code and dataset will be released upon acceptance on https://github.com/kesu1/BlendCLIP.",
        "arxiv_id": "2510.18244",
        "ARXIVID": "2510.18244",
        "COMMENT": "Matches criterion 2 (new MLLMs): introduces BlendCLIP, a multimodal pretraining framework for zero-shot 3D object classification, bridging synthetic and real domains with a curriculum-based strategy. Strong empirical results and relevant for multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.18837": {
        "authors": [
            "Yubin Zheng",
            "Pak-Hei Yeung",
            "Jing Xia",
            "Tianjie Ju",
            "Peng Tang",
            "Weidong Qiu",
            "Jagath C. Rajapakse"
        ],
        "title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning",
        "abstract": "arXiv:2510.18837v1 Announce Type: new  Abstract: Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.",
        "arxiv_id": "2510.18837",
        "ARXIVID": "2510.18837",
        "COMMENT": "Matches criterion 4: Proposes a new federated prompt tuning method for CLIP (a vision foundation model) to improve multi-domain generalization, with a dual-prompt design and semantic/domain disentanglement.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.18213": {
        "authors": [
            "Maryam Dialameh",
            "Hossein Rajabzadeh",
            "Jung Suk Sim",
            "Hyock Ju Kwon"
        ],
        "title": "EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation",
        "abstract": "arXiv:2510.18213v1 Announce Type: new  Abstract: Papillary thyroid microcarcinoma (PTMC) is increasingly managed with radio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasound videos remains difficult due to low contrast, probe-induced motion, and heat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizes well to static images, but its frame-independent design yields unstable predictions and temporal drift in interventional ultrasound. We introduce \\textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates a confidence-weighted exponential moving average pointer into the memory bank, providing a stable latent prototype of the tumour across frames. This design preserves temporal coherence through probe pressure and bubble occlusion while rapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset (124 minutes, 13 patients), EMA-SAM improves \\emph{maxDice} from 0.82 (SAM-2) to 0.86 and \\emph{maxIoU} from 0.72 to 0.76, while reducing false positives by 29\\%. On external benchmarks, including VTUS and colonoscopy video polyp datasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2. Importantly, the EMA pointer adds \\textless0.1\\% FLOPs, preserving real-time throughput of $\\sim$30\\,FPS on a single A100 GPU. These results establish EMA-SAM as a robust and efficient framework for stable tumour tracking, bridging the gap between foundation models and the stringent demands of interventional ultrasound. Codes are available here \\hyperref[code {https://github.com/mdialameh/EMA-SAM}.",
        "arxiv_id": "2510.18213",
        "ARXIVID": "2510.18213",
        "COMMENT": "Matches criterion 4: Presents a novel extension to the Segment Anything Model (SAM-2), a vision foundation model, for stable tumor tracking in ultrasound videos. Also relevant for clever statistical tricks (EMA pointer).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.18318": {
        "authors": [
            "Aaron Bell",
            "Amit Aides",
            "Amr Helmy",
            "Arbaaz Muslim",
            "Aviad Barzilai",
            "Aviv Slobodkin",
            "Bolous Jaber",
            "David Schottlander",
            "George Leifman",
            "Joydeep Paul",
            "Mimi Sun",
            "Nadav Sherman",
            "Natalie Williams",
            "Per Bjornsson",
            "Roy Lee",
            "Ruth Alcantara",
            "Thomas Turnbull",
            "Tomer Shekel",
            "Vered Silverman",
            "Yotam Gigi",
            "Adam Boulanger",
            "Alex Ottenwess",
            "Ali Ahmadalipour",
            "Anna Carter",
            "Charles Elliott",
            "David Andre",
            "Elad Aharoni",
            "Gia Jung",
            "Hassler Thurston",
            "Jacob Bien",
            "Jamie McPike",
            "Juliet Rothenberg",
            "Kartik Hegde",
            "Kel Markert",
            "Kim Philipp Jablonski",
            "Luc Houriez",
            "Monica Bharel",
            "Phing VanLee",
            "Reuven Sayag",
            "Sebastian Pilarski",
            "Shelley Cazares",
            "Shlomi Pasternak",
            "Siduo Jiang",
            "Stone Jiang",
            "Thomas Colthurst",
            "Yang Chen",
            "Yehonathan Refael",
            "Yochai Blau",
            "Yuval Carny",
            "Yael Maguire",
            "Avinatan Hassidim",
            "James Manyika",
            "Tim Thelin",
            "Genady Beryozkin",
            "Gautam Prasad",
            "Luke Barrington",
            "Yossi Matias",
            "Niv Efron",
            "Shravya Shetty"
        ],
        "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
        "abstract": "arXiv:2510.18318v1 Announce Type: new  Abstract: Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.",
        "arxiv_id": "2510.18318",
        "ARXIVID": "2510.18318",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications) and partially criterion 2 (cross-modal reasoning with foundation models). The paper introduces Earth AI, a family of geospatial foundation models and an agentic reasoning engine for geospatial inference, validated on new benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.18034": {
        "authors": [
            "Roberto Brusnicki",
            "David Pop",
            "Yuan Gao",
            "Mattia Piccinini",
            "Johannes Betz"
        ],
        "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection",
        "abstract": "arXiv:2510.18034v1 Announce Type: new  Abstract: Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.",
        "arxiv_id": "2510.18034",
        "ARXIVID": "2510.18034",
        "COMMENT": "Matches criteria 2: Proposes a structured reasoning framework (SAVANT) for semantic anomaly detection using VLMs, and demonstrates strong empirical results with open-source VLMs. Also relevant to vision-language models and their practical deployment.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.18083": {
        "authors": [
            "Shivam Singh",
            "Yiming Chen",
            "Agneet Chatterjee",
            "Amit Raj",
            "James Hays",
            "Yezhou Yang",
            "Chitra Baral"
        ],
        "title": "Chimera: Compositional Image Generation using Part-based Concepting",
        "abstract": "arXiv:2510.18083v1 Announce Type: new  Abstract: Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.",
        "arxiv_id": "2510.18083",
        "ARXIVID": "2510.18083",
        "COMMENT": "Matches criteria 4: Proposes a new compositional image generation model (Chimera) with part-based concepting, introduces a new dataset and metric (PartEval), and shows improved compositional accuracy. Relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18091": {
        "authors": [
            "Rohan Choudhury",
            "JungEun Kim",
            "Jinhyung Park",
            "Eunho Yang",
            "L\\'aszl\\'o A. Jeni",
            "Kris M. Kitani"
        ],
        "title": "Accelerating Vision Transformers with Adaptive Patch Sizes",
        "abstract": "arXiv:2510.18091v1 Announce Type: new  Abstract: Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\\% faster training and inference in visual QA, object detection, and semantic segmentation.",
        "arxiv_id": "2510.18091",
        "ARXIVID": "2510.18091",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): proposes adaptive patch sizes for Vision Transformers, improving efficiency for high-res images and dense tasks. Relevant for vision foundation models and statistical tricks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18775": {
        "authors": [
            "Teng Hu",
            "Jiangning Zhang",
            "Zihan Su",
            "Ran Yi"
        ],
        "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
        "abstract": "arXiv:2510.18775v1 Announce Type: new  Abstract: Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
        "arxiv_id": "2510.18775",
        "ARXIVID": "2510.18775",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): introduces a new hierarchical attention architecture for high-resolution video generation, scaling up video diffusion transformers to 4K. Relevant for generative modeling and vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18229": {
        "authors": [
            "Xinhao Cai",
            "Liulei Li",
            "Gensheng Pei",
            "Tao Chen",
            "Jinshan Pan",
            "Yazhou Yao",
            "Wenguan Wang"
        ],
        "title": "Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis",
        "abstract": "arXiv:2510.18229v1 Announce Type: new  Abstract: This paper presents a generation-based debiasing framework for object detection. Prior debiasing methods are often limited by the representation diversity of samples, while naive generative augmentation often preserves the biases it aims to solve. Moreover, our analysis reveals that simply generating more data for rare classes is suboptimal due to two core issues: i) instance frequency is an incomplete proxy for the true data needs of a model, and ii) current layout-to-image synthesis lacks the fidelity and control to generate high-quality, complex scenes. To overcome this, we introduce the representation score (RS) to diagnose representational gaps beyond mere frequency, guiding the creation of new, unbiased layouts. To ensure high-quality synthesis, we replace ambiguous text prompts with a precise visual blueprint and employ a generative alignment strategy, which fosters communication between the detector and generator. Our method significantly narrows the performance gap for underrepresented object groups, \\eg, improving large/rare instances by 4.4/3.6 mAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP for layout accuracy in generated images.",
        "arxiv_id": "2510.18229",
        "ARXIVID": "2510.18229",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and shows a clever generative debiasing method for object detection using blueprint-prompted image synthesis. Also of interest for generative modeling and statistical tricks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18705": {
        "authors": [
            "Peiqin Zhuang",
            "Lei Bai",
            "Yichao Wu",
            "Ding Liang",
            "Luping Zhou",
            "Yali Wang",
            "Wanli Ouyang"
        ],
        "title": "A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition",
        "abstract": "arXiv:2510.18705v1 Announce Type: new  Abstract: Recently, action recognition has been dominated by transformer-based methods, thanks to their spatiotemporal contextual aggregation capacities. However, despite the significant progress achieved on scene-related datasets, they do not perform well on motion-sensitive datasets due to the lack of elaborate motion modeling designs. Meanwhile, we observe that the widely-used cost volume in traditional action recognition is highly similar to the affinity matrix defined in self-attention, but equipped with powerful motion modeling capacities. In light of this, we propose to integrate those effective motion modeling properties into the existing transformer in a unified and neat way, with the proposal of the Explicit Motion Information Mining module (EMIM). In EMIM, we propose to construct the desirable affinity matrix in a cost volume style, where the set of key candidate tokens is sampled from the query-based neighboring area in the next frame in a sliding-window manner. Then, the constructed affinity matrix is used to aggregate contextual information for appearance modeling and is converted into motion features for motion modeling as well. We validate the motion modeling capacities of our method on four widely-used datasets, and our method performs better than existing state-of-the-art approaches, especially on motion-sensitive datasets, i.e., Something-Something V1 & V2.",
        "arxiv_id": "2510.18705",
        "ARXIVID": "2510.18705",
        "COMMENT": "Matches criterion 4: Proposes a new module for transformer-based action recognition, explicitly mining motion information, which is a novel application and extension of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18407": {
        "authors": [
            "Manjie Xu",
            "Xinyi Yang",
            "Jiayu Zhan",
            "Wei Liang",
            "Chi Zhang",
            "Yixin Zhu"
        ],
        "title": "Heterogeneous Adversarial Play in Interactive Environments",
        "abstract": "arXiv:2510.18407v1 Announce Type: new  Abstract: Self-play constitutes a fundamental paradigm for autonomous skill acquisition, whereby agents iteratively enhance their capabilities through self-directed environmental exploration. Conventional self-play frameworks exploit agent symmetry within zero-sum competitive settings, yet this approach proves inadequate for open-ended learning scenarios characterized by inherent asymmetry. Human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners' developmental trajectories. The principal challenge resides in operationalizing these asymmetric, adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies. Here we present Heterogeneous Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework that formalizes teacher-student interactions as a minimax optimization wherein task-generating instructor and problem-solving learner co-evolve through adversarial dynamics. In contrast to prevailing ACL methodologies that employ static curricula or unidirectional task selection mechanisms, HAP establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to real-time learner performance metrics. Experimental validation across multi-task learning domains demonstrates that our framework achieves performance parity with SOTA baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects.",
        "arxiv_id": "2510.18407",
        "ARXIVID": "2510.18407",
        "COMMENT": "Matches criterion 3: Proposes a novel adversarial curriculum learning framework (HAP) for embodied agents, focusing on teacher-student dynamics and open-ended learning, which is a new angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18304": {
        "authors": [
            "Liangyu Chen",
            "James Burgess",
            "Jeffrey J Nirschl",
            "Orr Zohar",
            "Serena Yeung-Levy"
        ],
        "title": "The Impact of Image Resolution on Biomedical Multimodal Large Language Models",
        "abstract": "arXiv:2510.18304v1 Announce Type: new  Abstract: Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.",
        "arxiv_id": "2510.18304",
        "ARXIVID": "2510.18304",
        "COMMENT": "Analyzes the impact of image resolution on biomedical MLLMs, providing empirical insights and recommendations for training and inference. Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications), with a focus on practical implications for biomedical imaging.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.18291": {
        "authors": [
            "Tuan Pham",
            "Thanh-Tung Le",
            "Xiaohui Xie",
            "Stephan Mandt"
        ],
        "title": "GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation",
        "abstract": "arXiv:2510.18291v1 Announce Type: new  Abstract: We introduce a novel framework for metric depth estimation that enhances pretrained diffusion-based monocular depth estimation (DB-MDE) models with stereo vision guidance. While existing DB-MDE methods excel at predicting relative depth, estimating absolute metric depth remains challenging due to scale ambiguities in single-image scenarios. To address this, we reframe depth estimation as an inverse problem, leveraging pretrained latent diffusion models (LDMs) conditioned on RGB images, combined with stereo-based geometric constraints, to learn scale and shift for accurate depth recovery. Our training-free solution seamlessly integrates into existing DB-MDE frameworks and generalizes across indoor, outdoor, and complex environments. Extensive experiments demonstrate that our approach matches or surpasses state-of-the-art methods, particularly in challenging scenarios involving translucent and specular surfaces, all without requiring retraining.",
        "arxiv_id": "2510.18291",
        "ARXIVID": "2510.18291",
        "COMMENT": "Presents a new geometry-guided diffusion framework for metric depth estimation, integrating stereo constraints into pretrained diffusion models. This is a methodological improvement for spatial understanding (criterion 1) and uses generative modeling (diffusion), which is of interest.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.18489": {
        "authors": [
            "Jinfeng Liu",
            "Lingtong Kong",
            "Mi Zhou",
            "Jinwen Chen",
            "Dan Xu"
        ],
        "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos",
        "abstract": "arXiv:2510.18489v1 Announce Type: new  Abstract: We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
        "arxiv_id": "2510.18489",
        "ARXIVID": "2510.18489",
        "COMMENT": "Matches criteria 4: Presents a new method for 4D HDR scene reconstruction using Gaussian Splatting from monocular videos, and introduces a new benchmark for HDR video reconstruction. Relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.18739": {
        "authors": [
            "Hao Wang",
            "Ying Zhou",
            "Haoyu Zhao",
            "Rui Wang",
            "Qiang Hu",
            "Xing Zhang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting",
        "abstract": "arXiv:2510.18739v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
        "arxiv_id": "2510.18739",
        "ARXIVID": "2510.18739",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents): proposes illumination-aware 3D Gaussian Splatting for colonoscopy, addressing photometric variation in real-time 3D reconstruction. Relevant for spatial intelligence and clever modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.18660": {
        "authors": [
            "Hichem Sahbi"
        ],
        "title": "Image augmentation with invertible networks in interactive satellite image change detection",
        "abstract": "arXiv:2510.18660v1 Announce Type: new  Abstract: This paper devises a novel interactive satellite image change detection algorithm based on active learning. Our framework employs an iterative process that leverages a question-and-answer model. This model queries the oracle (user) about the labels of a small subset of images (dubbed as display), and based on the oracle's responses, change detection model is dynamically updated. The main contribution of our framework resides in a novel invertible network that allows augmenting displays, by mapping them from highly nonlinear input spaces to latent ones, where augmentation transformations become linear and more tractable. The resulting augmented data are afterwards mapped back to the input space, and used to retrain more effective change detection criteria in the subsequent iterations of active learning. Experimental results demonstrate superior performance of our proposed method compared to the related work.",
        "arxiv_id": "2510.18660",
        "ARXIVID": "2510.18660",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents): proposes a novel invertible network for image augmentation in interactive satellite change detection, with an active learning loop. Interesting for spatial reasoning and clever statistical tricks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.18781": {
        "authors": [
            "Wenping Jin",
            "Yuyang Tang",
            "Li Zhu",
            "Fei Guo"
        ],
        "title": "Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection",
        "abstract": "arXiv:2510.18781v1 Announce Type: new  Abstract: A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel \"Rebellious Student\" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at https://github.com/xjpp2016/FERS.",
        "arxiv_id": "2510.18781",
        "ARXIVID": "2510.18781",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding) via a novel teacher-student framework for spatial-spectral feature learning in hyperspectral anomaly detection. The 'rebellious student' approach is a clever twist on standard distillation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.18267": {
        "authors": [
            "Xiang Zhang",
            "Suping Wu",
            "Sheng Yang"
        ],
        "title": "Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization",
        "abstract": "arXiv:2510.18267v1 Announce Type: new  Abstract: Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.",
        "arxiv_id": "2510.18267",
        "ARXIVID": "2510.18267",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1), as it proposes a new method for 3D human mesh recovery using latent information and low-dimensional learning, but not directly about embodied agents or spatial intelligence in agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.18377": {
        "authors": [
            "Yuqing Luo",
            "Yixiao Li",
            "Jiang Liu",
            "Jun Fu",
            "Hadi Amirpour",
            "Guanghui Yue",
            "Baoquan Zhao",
            "Padraig Corcoran",
            "Hantao Liu",
            "Wei Zhou"
        ],
        "title": "Cross-Modal Scene Semantic Alignment for Image Complexity Assessment",
        "abstract": "arXiv:2510.18377v1 Announce Type: new  Abstract: Image complexity assessment (ICA) is a challenging task in perceptual evaluation due to the subjective nature of human perception and the inherent semantic diversity in real-world images. Existing ICA methods predominantly rely on hand-crafted or shallow convolutional neural network-based features of a single visual modality, which are insufficient to fully capture the perceived representations closely related to image complexity. Recently, cross-modal scene semantic information has been shown to play a crucial role in various computer vision tasks, particularly those involving perceptual understanding. However, the exploration of cross-modal scene semantic information in the context of ICA remains unaddressed. Therefore, in this paper, we propose a novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which leverages scene semantic alignment from a cross-modal perspective to enhance ICA performance, enabling complexity predictions to be more consistent with subjective human perception. Specifically, the proposed CM-SSA consists of a complexity regression branch and a scene semantic alignment branch. The complexity regression branch estimates image complexity levels under the guidance of the scene semantic alignment branch, while the scene semantic alignment branch is used to align images with corresponding text prompts that convey rich scene semantic information by pair-wise learning. Extensive experiments on several ICA datasets demonstrate that the proposed CM-SSA significantly outperforms state-of-the-art approaches. Codes are available at https://github.com/XQ2K/First-Cross-Model-ICA.",
        "arxiv_id": "2510.18377",
        "ARXIVID": "2510.18377",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications), as it uses cross-modal scene semantic alignment for image complexity assessment, but not a direct match to the main criteria.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2510.18256": {
        "authors": [
            "Xiang Zhang",
            "Suping Wu",
            "Weibin Qiu",
            "Zhaocheng Jin",
            "Sheng Yang"
        ],
        "title": "Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery",
        "abstract": "arXiv:2510.18256v1 Announce Type: new  Abstract: 3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.",
        "arxiv_id": "2510.18256",
        "ARXIVID": "2510.18256",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1), as it proposes a hyperbolic space learning method for 3D human mesh recovery, leveraging temporal motion priors. However, it is not about embodied agents or spatial intelligence in agents, but about mesh recovery.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.18362": {
        "authors": [
            "Duoxun Tang",
            "Xi Xiao",
            "Guangwu Hu",
            "Kangkang Sun",
            "Xiao Yang",
            "Dongyang Chen",
            "Qing Li",
            "Yongjie Yin",
            "Jiyao Wang"
        ],
        "title": "FeatureFool: Zero-Query Fooling of Video Models via Feature Map",
        "abstract": "arXiv:2510.18362v1 Announce Type: new  Abstract: The vulnerability of deep neural networks (DNNs) has been preliminarily verified. Existing black-box adversarial attacks usually require multi-round interaction with the model and consume numerous queries, which is impractical in the real-world and hard to scale to recently emerged Video-LLMs. Moreover, no attack in the video domain directly leverages feature maps to shift the clean-video feature space. We therefore propose FeatureFool, a stealthy, video-domain, zero-query black-box attack that utilizes information extracted from a DNN to alter the feature space of clean videos. Unlike query-based methods that rely on iterative interaction, FeatureFool performs a zero-query attack by directly exploiting DNN-extracted information. This efficient approach is unprecedented in the video domain. Experiments show that FeatureFool achieves an attack success rate above 70\\% against traditional video classifiers without any queries. Benefiting from the transferability of the feature map, it can also craft harmful content and bypass Video-LLM recognition. Additionally, adversarial videos generated by FeatureFool exhibit high quality in terms of SSIM, PSNR, and Temporal-Inconsistency, making the attack barely perceptible. This paper may contain violent or explicit content.",
        "arxiv_id": "2510.18362",
        "ARXIVID": "2510.18362",
        "COMMENT": "Somewhat relevant to general interest in vision-language models, but focuses on adversarial attacks on video models and Video-LLMs. Does not match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.18554": {
        "authors": [
            "Federico Barbero",
            "Xiangming Gu",
            "Christopher A. Choquette-Choo",
            "Chawin Sitawarin",
            "Matthew Jagielski",
            "Itay Yona",
            "Petar Veli\\v{c}kovi\\'c",
            "Ilia Shumailov",
            "Jamie Hayes"
        ],
        "title": "Extracting alignment data in open models",
        "abstract": "arXiv:2510.18554v1 Announce Type: new  Abstract: In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of $10\\times$) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset.",
        "arxiv_id": "2510.18554",
        "ARXIVID": "2510.18554",
        "COMMENT": "Does not match any specific criterion. Focuses on alignment data extraction from LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.18428": {
        "authors": [
            "Minwei Kong",
            "Ao Qu",
            "Xiaotong Guo",
            "Wenbin Ouyang",
            "Chonghe Jiang",
            "Han Zheng",
            "Yining Ma",
            "Dingyi Zhuang",
            "Yuhan Tang",
            "Junyi Li",
            "Hai Wang",
            "Cathy Wu",
            "Jinhua Zhao"
        ],
        "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library",
        "abstract": "arXiv:2510.18428v1 Announce Type: new  Abstract: Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.",
        "arxiv_id": "2510.18428",
        "ARXIVID": "2510.18428",
        "COMMENT": "Does not match any specific criterion. Focuses on optimization program formulation with LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.18342": {
        "authors": [
            "Peng Tang",
            "Xiaoxiao Yan",
            "Xiaobin Hu",
            "Yuning Cui",
            "Donghao Luo",
            "Jiangning Zhang",
            "Pengcheng Xu",
            "Jinlong Peng",
            "Qingdong He",
            "Feiyue Huang",
            "Song Xue",
            "Tobias Lasser"
        ],
        "title": "ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection",
        "abstract": "arXiv:2510.18342v1 Announce Type: new  Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing research interest, as it seeks to develop a unified model for anomaly detection across multiple classes, i.e., eliminating the need to train separate models for distinct objects and thereby saving substantial computational resources. Under the MUAD setting, while advanced Transformer-based architectures have brought significant performance improvements, identity shortcuts persist: they directly copy inputs to outputs, narrowing the gap in reconstruction errors between normal and abnormal cases, and thereby making the two harder to distinguish. Therefore, we propose ShortcutBreaker, a novel unified feature-reconstruction framework for MUAD tasks, featuring two key innovations to address the issue of shortcuts. First, drawing on matrix rank inequality, we design a low-rank noisy bottleneck (LRNB) to project highdimensional features into a low-rank latent space, and theoretically demonstrate its capacity to prevent trivial identity reproduction. Second, leveraging ViTs global modeling capability instead of merely focusing on local features, we incorporate a global perturbation attention to prevent information shortcuts in the decoders. Extensive experiments are performed on four widely used anomaly detection benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD) and one medical dataset (Universal Medical). The proposed method achieves a remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four datasets, respectively, consistently outperforming previous MUAD methods across different scenarios.",
        "arxiv_id": "2510.18342",
        "ARXIVID": "2510.18342",
        "COMMENT": "Presents a new method for multi-class unsupervised anomaly detection using a low-rank noisy bottleneck and global perturbation attention. While it uses vision transformers and clever statistical tricks, it does not directly match any of the four criteria, but is relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.18101": {
        "authors": [
            "Vitor Pereira Matias",
            "Daniel Perazzo",
            "Vinicius Silva",
            "Alberto Raposo",
            "Luiz Velho",
            "Afonso Paiva",
            "Tiago Novello"
        ],
        "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications",
        "abstract": "arXiv:2510.18101v1 Announce Type: new  Abstract: The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.",
        "arxiv_id": "2510.18101",
        "ARXIVID": "2510.18101",
        "COMMENT": "Covers 3D Gaussian Splatting, a new method for 3D reconstruction and rendering, with applications in surface reconstruction and content generation. Closely matches criterion 4 (Vision foundation models and applications) and is tangentially relevant to criterion 1 (spatial understanding in embodied agents) due to its focus on spatial representations, but is primarily a tutorial/survey.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2510.17995": {
        "authors": [
            "Abhigya Verma",
            "Seganrasan Subramanian",
            "Nandhakumar Kandasamy",
            "Naman Gupta"
        ],
        "title": "FABRIC: Framework for Agent-Based Realistic Intelligence Creation",
        "abstract": "arXiv:2510.17995v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data- structured interaction records that couple user intents with tool specifications, argument-grounded calls, and verifiable execution traces. However, collecting such data from human annotators is costly, time-consuming, and difficult to scale.   We present a unified framework for synthesizing agentic data using only LLMs, without any human-in-the-loop supervision. This framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications, tool definitions, policy pseudocode, natural language exchanges, and execution traces. Records conform to strict syntactic and semantic constraints, ensuring machine-parseability and faithful alignment across inputs, outputs, and tool calls.   Beyond single tasks, there is support for both multi-task and multi-turn agent interactions, enabling the construction of datasets that reflect the full spectrum of tool-use competencies. To ensure quality and consistency, the framework integrates constrained generation formats, JSON-schema validation, and judge-based filtering.   This paper formalizes the schema for agentic records, details the prompt design principles that guide generation, and introduces scalable pipelines for high-quality synthetic data. By providing a reproducible, LLM-only alternative to manual collection, hence advancing the development of agentic LLMs capable of robust tool use.",
        "arxiv_id": "2510.17995",
        "ARXIVID": "2510.17995",
        "COMMENT": "Does not directly match any criterion. Focuses on LLMs as agents and synthetic data generation for agentic tool use, but not specifically spatial, vision, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.18659": {
        "authors": [
            "Dong Yun",
            "Marco Schouten",
            "Dim Papadopoulos"
        ],
        "title": "Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval",
        "abstract": "arXiv:2510.18659v1 Announce Type: new  Abstract: User queries in information retrieval are often ambiguous, making it challenging for systems to identify a user's target from a single query. While recent dialogue-based interactive retrieval systems can clarify user intent, they are inefficient as they often lack an explicit strategy to ask the most informative questions. To address this limitation, we propose SherlockLLM, a dialogue-driven retrieval framework that learns an optimal questioning strategy via Reinforcement Learning (RL) and avoids the need for large-scale annotated dialogue data. In our framework, an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space. To validate our approach, we introduce a benchmark with both structured and unstructured tasks. Experimental results show that SherlockLLM is a robust and efficient solution. On the structured tasks, its performance matches strong baselines and approaches the theoretical optimal defined by binary search. On the challenging unstructured task, our agent significantly outperforms these baselines, showcasing its ability to learn a highly effective information-seeking dialogue policy.",
        "arxiv_id": "2510.18659",
        "ARXIVID": "2510.18659",
        "COMMENT": "Does not match any specific criterion. Focuses on dialogue-based retrieval and question generation, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.18437": {
        "authors": [
            "Ji Du",
            "Xin Wang",
            "Fangwei Hao",
            "Mingyang Yu",
            "Chunyuan Chen",
            "Jiesheng Wu",
            "Bin Wang",
            "Jing Xu",
            "Ping Li"
        ],
        "title": "Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection",
        "abstract": "arXiv:2510.18437v1 Announce Type: new  Abstract: At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings. Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization. Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented paradigm that exploits the entire training dataset to generate pseudo-labels for single images, which could be used to train COD models. RISE begins by constructing prototype libraries for environments and camouflaged objects using training images (without ground truth), followed by K-Nearest Neighbor (KNN) retrieval to generate pseudo-masks for each image based on these libraries. It is important to recognize that using only training images without annotations exerts a pronounced challenge in crafting high-quality prototype libraries. In this light, we introduce a Clustering-then-Retrieval (CR) strategy, where coarse masks are first generated through clustering, facilitating subsequent histogram-based image filtering and cross-category retrieval to produce high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which integrates retrieval results from diverse views to produce more robust and precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms state-of-the-art unsupervised and prompt-based methods. Code is available at https://github.com/xiaohainku/RISE.",
        "arxiv_id": "2510.18437",
        "ARXIVID": "2510.18437",
        "COMMENT": "Does not match any specific criterion. Focuses on unsupervised camouflaged object detection using retrieval-based self-augmentation, not on spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.18633": {
        "authors": [
            "Roxana Petcu",
            "Kenton Murray",
            "Daniel Khashabi",
            "Evangelos Kanoulas",
            "Maarten de Rijke",
            "Dawn Lawrie",
            "Kevin Duh"
        ],
        "title": "Query Decomposition for RAG: Balancing Exploration-Exploitation",
        "abstract": "arXiv:2510.18633v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by decomposing them into subqueries, retrieving potentially relevant documents for each, and then aggregating them to generate an answer. Efficiently selecting informative documents requires balancing a key trade-off: (i) retrieving broadly enough to capture all the relevant material, and (ii) limiting retrieval to avoid excessive noise and computational cost. We formulate query decomposition and document retrieval in an exploitation-exploration setting, where retrieving one document at a time builds a belief about the utility of a given sub-query and informs the decision to continue exploiting or exploring an alternative. We experiment with a variety of bandit learning methods and demonstrate their effectiveness in dynamically selecting the most informative sub-queries. Our main finding is that estimating document relevance using rank information and human judgments yields a 35% gain in document-level precision, 15% increase in {\\alpha}-nDCG, and better performance on the downstream task of long-form generation.",
        "arxiv_id": "2510.18633",
        "ARXIVID": "2510.18633",
        "COMMENT": "Focuses on retrieval-augmented generation (RAG) and query decomposition, which is not directly related to any of the four criteria, though it is relevant to general machine learning and language modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}