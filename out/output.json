{
    "2507.17801": {
        "authors": [
            "Yi Xin",
            "Juncheng Yan",
            "Qi Qin",
            "Zhen Li",
            "Dongyang Liu",
            "Shicheng Li",
            "Victor Shea-Jay Huang",
            "Yupeng Zhou",
            "Renrui Zhang",
            "Le Zhuo",
            "Tiancheng Han",
            "Xiaoqing Sun",
            "Siqi Luo",
            "Mengmeng Wang",
            "Bin Fu",
            "Yuewen Cao",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Yu Qiao",
            "Peng Gao"
        ],
        "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling",
        "abstract": "arXiv:2507.17801v1 Announce Type: new  Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.",
        "arxiv_id": "2507.17801",
        "ARXIVID": "2507.17801",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Lumina-mGPT 2.0 is a new stand-alone autoregressive image model, trained from scratch, with strong multi-modal and multi-task capabilities, and is positioned as a foundation model.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.18473": {
        "authors": [
            "Haoran Xu",
            "Saining Zhang",
            "Peishuo Li",
            "Baijun Ye",
            "Xiaoxue Chen",
            "Huan-ang Gao",
            "Jv Zheng",
            "Xiaowei Song",
            "Ziqiao Peng",
            "Run Miao",
            "Jinrang Jia",
            "Yifeng Shi",
            "Guangqi Yi",
            "Hang Zhao",
            "Hao Tang",
            "Hongyang Li",
            "Kaicheng Yu",
            "Hao Zhao"
        ],
        "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting",
        "abstract": "arXiv:2507.18473v1 Announce Type: new  Abstract: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.",
        "arxiv_id": "2507.18473",
        "ARXIVID": "2507.18473",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator-related methods). Introduces a new V2X reconstruction and editing framework for data augmentation and scene editing in autonomous driving, with a novel use of Gaussian Splatting.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.18576": {
        "authors": [
            "Shanghai AI Lab",
            ":",
            "Yicheng Bao",
            "Guanxu Chen",
            "Mingkang Chen",
            "Yunhao Chen",
            "Chiyu Chen",
            "Lingjie Chen",
            "Sirui Chen",
            "Xinquan Chen",
            "Jie Cheng",
            "Yu Cheng",
            "Dengke Deng",
            "Yizhuo Ding",
            "Dan Ding",
            "Xiaoshan Ding",
            "Yi Ding",
            "Zhichen Dong",
            "Lingxiao Du",
            "Yuyu Fan",
            "Xinshun Feng",
            "Yanwei Fu",
            "Yuxuan Gao",
            "Ruijun Ge",
            "Tianle Gu",
            "Lujun Gui",
            "Jiaxuan Guo",
            "Qianxi He",
            "Yuenan Hou",
            "Xuhao Hu",
            "Hong Huang",
            "Kaichen Huang",
            "Shiyang Huang",
            "Yuxian Jiang",
            "Shanzhe Lei",
            "Jie Li",
            "Lijun Li",
            "Hao Li",
            "Juncheng Li",
            "Xiangtian Li",
            "Yafu Li",
            "Lingyu Li",
            "Xueyan Li",
            "Haotian Liang",
            "Dongrui Liu",
            "Qihua Liu",
            "Zhixuan Liu",
            "Bangwei Liu",
            "Huacan Liu",
            "Yuexiao Liu",
            "Zongkai Liu",
            "Chaochao Lu",
            "Yudong Lu",
            "Xiaoya Lu",
            "Zhenghao Lu",
            "Qitan Lv",
            "Caoyuan Ma",
            "Jiachen Ma",
            "Xiaoya Ma",
            "Zhongtian Ma",
            "Lingyu Meng",
            "Ziqi Miao",
            "Yazhe Niu",
            "Yuezhang Peng",
            "Yuan Pu",
            "Han Qi",
            "Chen Qian",
            "Xingge Qiao",
            "Jingjing Qu",
            "Jiashu Qu",
            "Wanying Qu",
            "Wenwen Qu",
            "Xiaoye Qu",
            "Qihan Ren",
            "Qingnan Ren",
            "Qingyu Ren",
            "Jing Shao",
            "Wenqi Shao",
            "Shuai Shao",
            "Dongxing Shi",
            "Xin Song",
            "Xinhao Song",
            "Yan Teng",
            "Xuan Tong",
            "Yingchun Wang",
            "Xuhong Wang",
            "Shujie Wang",
            "Xin Wang",
            "Yige Wang",
            "Yixu Wang",
            "Yuanfu Wang",
            "Futing Wang",
            "Ruofan Wang",
            "Wenjie Wang",
            "Yajie Wang",
            "Muhao Wei",
            "Xiaoyu Wen",
            "Fenghua Weng",
            "Yuqi Wu",
            "Yingtong Xiong",
            "Xingcheng Xu",
            "Chao Yang",
            "Yue Yang",
            "Yang Yao",
            "Yulei Ye",
            "Zhenyun Yin",
            "Yi Yu",
            "Bo Zhang",
            "Qiaosheng Zhang",
            "Jinxuan Zhang",
            "Yexin Zhang",
            "Yinqiang Zheng",
            "Hefeng Zhou",
            "Zhanhui Zhou",
            "Pengyu Zhu",
            "Qingzi Zhu",
            "Yubo Zhu",
            "Bowen Zhou"
        ],
        "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
        "abstract": "arXiv:2507.18576v1 Announce Type: new  Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.",
        "arxiv_id": "2507.18576",
        "ARXIVID": "2507.18576",
        "COMMENT": "Matches criterion 2: introduces SafeWork-R1, a new multimodal reasoning model (VLLM/MLLM) with a novel safety-oriented training framework. Also relevant to criterion 4 as it discusses general-purpose AI and foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.18634": {
        "authors": [
            "Junfei Xiao",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Shengqu Cai",
            "Yang Zhao",
            "Yuwei Guo",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Alan Yuille",
            "Lu Jiang"
        ],
        "title": "Captain Cinema: Towards Short Movie Generation",
        "abstract": "arXiv:2507.18634v1 Announce Type: new  Abstract: We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai",
        "arxiv_id": "2507.18634",
        "ARXIVID": "2507.18634",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a multimodal diffusion transformer for long-form video generation from text, a novel generative modeling approach.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.18498": {
        "authors": [
            "Zongzheng Zhang",
            "Xuchong Qiu",
            "Boran Zhang",
            "Guantian Zheng",
            "Xunjiang Gu",
            "Guoxuan Chi",
            "Huan-ang Gao",
            "Leichen Wang",
            "Ziming Liu",
            "Xinrun Li",
            "Igor Gilitschenski",
            "Hongyang Li",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction",
        "abstract": "arXiv:2507.18498v1 Announce Type: new  Abstract: Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.",
        "arxiv_id": "2507.18498",
        "ARXIVID": "2507.18498",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel method for mapless trajectory prediction in autonomous driving, with a focus on uncertainty and kinematic state, a previously overlooked factor).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.18074": {
        "authors": [
            "Yixiu Liu",
            "Yang Nan",
            "Weixian Xu",
            "Xiangkun Hu",
            "Lyumanshan Ye",
            "Zhen Qin",
            "Pengfei Liu"
        ],
        "title": "AlphaGo Moment for Model Architecture Discovery",
        "abstract": "arXiv:2507.18074v1 Announce Type: new  Abstract: While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.",
        "arxiv_id": "2507.18074",
        "ARXIVID": "2507.18074",
        "COMMENT": "Matches criterion 1 (methodological improvements to model architecture discovery, which is foundational for spatial intelligence in embodied agents) and criterion 4 (vision foundation models and their applications). The paper presents a fully autonomous system for neural architecture discovery, which is a significant methodological advance.",
        "RELEVANCE": 7,
        "NOVELTY": 9
    },
    "2507.18531": {
        "authors": [
            "Tianheng Qiu",
            "Jingchun Gao",
            "Jingyu Li",
            "Huiyi Leong",
            "Xuan Huang",
            "Xi Wang",
            "Xiaocheng Zhang",
            "Kele Xu",
            "Lan Zhang"
        ],
        "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning",
        "abstract": "arXiv:2507.18531v1 Announce Type: new  Abstract: Intent-oriented controlled video captioning aims to generate targeted descriptions for specific targets in a video based on customized user intent. Current Large Visual Language Models (LVLMs) have gained strong instruction following and visual comprehension capabilities. Although the LVLMs demonstrated proficiency in spatial and temporal understanding respectively, it was not able to perform fine-grained spatial control in time sequences in direct response to instructions. This substantial spatio-temporal gap complicates efforts to achieve fine-grained intention-oriented control in video. Towards this end, we propose a novel IntentVCNet that unifies the temporal and spatial understanding knowledge inherent in LVLMs to bridge the spatio-temporal gap from both prompting and model perspectives. Specifically, we first propose a prompt combination strategy designed to enable LLM to model the implicit relationship between prompts that characterize user intent and video sequences. We then propose a parameter efficient box adapter that augments the object semantic information in the global visual context so that the visual token has a priori information about the user intent. The final experiment proves that the combination of the two strategies can further enhance the LVLM's ability to model spatial details in video sequences, and facilitate the LVLMs to accurately generate controlled intent-oriented captions. Our proposed method achieved state-of-the-art results in several open source LVLMs and was the runner-up in the IntentVC challenge. Our code is available on https://github.com/thqiu0419/IntentVCNet.",
        "arxiv_id": "2507.18531",
        "ARXIVID": "2507.18531",
        "COMMENT": "Matches criterion 2: proposes IntentVCNet, a new method to enhance spatio-temporal understanding and controllability in LVLMs for video captioning, and demonstrates improvements on open-source LVLMs. Also relevant to criterion 1 for spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17998": {
        "authors": [
            "Jaeho Shin",
            "Hyeonjae Gil",
            "Junwoo Jang",
            "Maani Ghaffari",
            "Ayoung Kim"
        ],
        "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold",
        "abstract": "arXiv:2507.17998v1 Announce Type: new  Abstract: Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation ($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on https://github.com/joomeok/GrassmannRegistration.",
        "arxiv_id": "2507.17998",
        "ARXIVID": "2507.17998",
        "COMMENT": "Matches criterion 1: proposes a new optimizable cost function for affine subspace alignment using geodesic distance on the Grassmann manifold, which is a methodological improvement for spatial understanding in computer vision.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2507.17859": {
        "authors": [
            "Muayad Abujabal",
            "Lyes Saad Saoud",
            "Irfan Hussain"
        ],
        "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains",
        "abstract": "arXiv:2507.17859v1 Announce Type: new  Abstract: Accurate fish detection in underwater imagery is essential for ecological monitoring, aquaculture automation, and robotic perception. However, practical deployment remains limited by fragmented datasets, heterogeneous imaging conditions, and inconsistent evaluation protocols. To address these gaps, we present \\textit{FishDet-M}, the largest unified benchmark for fish detection, comprising 13 publicly available datasets spanning diverse aquatic environments including marine, brackish, occluded, and aquarium scenes. All data are harmonized using COCO-style annotations with both bounding boxes and segmentation masks, enabling consistent and scalable cross-domain evaluation. We systematically benchmark 28 contemporary object detection models, covering the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models. Evaluations are conducted using standard metrics including mAP, mAP@50, and mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and inference profiling in terms of latency and parameter count. The results highlight the varying detection performance across models trained on FishDet-M, as well as the trade-off between accuracy and efficiency across models of different architectures. To support adaptive deployment, we introduce a CLIP-based model selection framework that leverages vision-language alignment to dynamically identify the most semantically appropriate detector for each input image. This zero-shot selection strategy achieves high performance without requiring ensemble computation, offering a scalable solution for real-time applications. FishDet-M establishes a standardized and reproducible platform for evaluating object detection in complex aquatic scenes. All datasets, pretrained models, and evaluation tools are publicly available to facilitate future research in underwater computer vision and intelligent marine systems.",
        "arxiv_id": "2507.17859",
        "ARXIVID": "2507.17859",
        "COMMENT": "Matches criterion 3: introduces FishDet-M, a new large-scale unified benchmark for fish detection in diverse aquatic domains, with a novel CLIP-based model selection framework for adaptive deployment. Also relevant to criterion 4 due to CLIP-guided model selection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18255": {
        "authors": [
            "Zhuoguang Chen",
            "Minghui Qin",
            "Tianyuan Yuan",
            "Zhe Liu",
            "Hang Zhao"
        ],
        "title": "LONG3R: Long Sequence Streaming 3D Reconstruction",
        "abstract": "arXiv:2507.18255v1 Announce Type: new  Abstract: Recent advancements in multi-view scene reconstruction have been significant, yet existing methods face limitations when processing streams of input images. These methods either rely on time-consuming offline optimization or are restricted to shorter sequences, hindering their applicability in real-time scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D Reconstruction), a novel model designed for streaming multi-view 3D scene reconstruction over longer sequences. Our model achieves real-time processing by operating recurrently, maintaining and updating memory with each new observation. We first employ a memory gating mechanism to filter relevant memory, which, together with a new observation, is fed into a dual-source refined decoder for coarse-to-fine interaction. To effectively capture long-sequence memory, we propose a 3D spatio-temporal memory that dynamically prunes redundant spatial information while adaptively adjusting resolution along the scene. To enhance our model's performance on long sequences while maintaining training efficiency, we employ a two-stage curriculum training strategy, each stage targeting specific capabilities. Experiments demonstrate that LONG3R outperforms state-of-the-art streaming methods, particularly for longer sequences, while maintaining real-time inference speed. Project page: https://zgchen33.github.io/LONG3R/.",
        "arxiv_id": "2507.18255",
        "ARXIVID": "2507.18255",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for simulators/benchmarks). Proposes a novel streaming 3D reconstruction model for long sequences, with real-time capability and memory mechanisms, relevant for embodied AI and simulators.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18331": {
        "authors": [
            "Runmin Zhang",
            "Zhu Yu",
            "Si-Yuan Cao",
            "Lingyu Zhu",
            "Guangyi Zhang",
            "Xiaokai Bai",
            "Hui-Liang Shen"
        ],
        "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction",
        "abstract": "arXiv:2507.18331v1 Announce Type: new  Abstract: This work presents SGCDet, a novel multi-view indoor 3D object detection framework based on adaptive 3D volume construction. Unlike previous approaches that restrict the receptive field of voxels to fixed locations on images, we introduce a geometry and context aware aggregation module to integrate geometric and contextual information within adaptive regions in each image and dynamically adjust the contributions from different views, enhancing the representation capability of voxel features. Furthermore, we propose a sparse volume construction strategy that adaptively identifies and selects voxels with high occupancy probabilities for feature refinement, minimizing redundant computation in free space. Benefiting from the above designs, our framework achieves effective and efficient volume construction in an adaptive way. Better still, our network can be supervised using only 3D bounding boxes, eliminating the dependence on ground-truth scene geometry. Experimental results demonstrate that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200 and ARKitScenes datasets. The source code is available at https://github.com/RM-Zhang/SGCDet.",
        "arxiv_id": "2507.18331",
        "ARXIVID": "2507.18331",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding) and criterion 3 (new method for multi-view indoor 3D object detection with adaptive 3D volume construction, eliminating dependence on ground-truth scene geometry).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18481": {
        "authors": [
            "Francesco Dalmonte",
            "Emirhan Bayar",
            "Emre Akbas",
            "Mariana-Iuliana Georgescu"
        ],
        "title": "Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection",
        "abstract": "arXiv:2507.18481v1 Announce Type: new  Abstract: Anomaly detection in medical images is an important yet challenging task due to the diversity of possible anomalies and the practical impossibility of collecting comprehensively annotated data sets. In this work, we tackle unsupervised medical anomaly detection proposing a modernized autoencoder-based framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead of training encoders from scratch, we directly utilize frozen vision foundation models as feature extractors, enabling rich, multi-stage, high-level representations without domain-specific fine-tuning. We propose the usage of the Q-Former architecture as the bottleneck, which enables the control of the length of the reconstruction sequence, while efficiently aggregating multiscale features. Additionally, we incorporate a perceptual loss computed using features from a pretrained Masked Autoencoder, guiding the reconstruction towards semantically meaningful structures. Our framework is evaluated on four diverse medical anomaly detection benchmarks, achieving state-of-the-art results on BraTS2021, RESC, and RSNA. Our results highlight the potential of vision foundation model encoders, pretrained on natural images, to generalize effectively to medical image analysis tasks without further fine-tuning. We release the code and models at https://github.com/emirhanbayar/QFAE.",
        "arxiv_id": "2507.18481",
        "ARXIVID": "2507.18481",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications), as it leverages DINO, DINOv2, and Masked Autoencoder for medical anomaly detection, showing their generalization to medical images.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18537": {
        "authors": [
            "Zhekai Chen",
            "Ruihang Chu",
            "Yukang Chen",
            "Shiwei Zhang",
            "Yujie Wei",
            "Yingya Zhang",
            "Xihui Liu"
        ],
        "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation",
        "abstract": "arXiv:2507.18537v1 Announce Type: new  Abstract: Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR.",
        "arxiv_id": "2507.18537",
        "ARXIVID": "2507.18537",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling. Proposes a new test-time scaling framework for visual auto-regressive models, with clever statistical tricks for improving sample quality and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17853": {
        "authors": [
            "Lifeng Chen",
            "Jiner Wang",
            "Zihao Pan",
            "Beier Zhu",
            "Xiaofeng Yang",
            "Chi Zhang"
        ],
        "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models",
        "abstract": "arXiv:2507.17853v1 Announce Type: new  Abstract: Recent advances in text-to-image (T2I) generation have led to impressive visual results. However, these models still face significant challenges when handling complex prompt, particularly those involving multiple subjects with distinct attributes. Inspired by the human drawing process, which first outlines the composition and then incrementally adds details, we propose Detail++, a training-free framework that introduces a novel Progressive Detail Injection (PDI) strategy to address this limitation. Specifically, we decompose a complex prompt into a sequence of simplified sub-prompts, guiding the generation process in stages. This staged generation leverages the inherent layout-controlling capacity of self-attention to first ensure global composition, followed by precise refinement. To achieve accurate binding between attributes and corresponding subjects, we exploit cross-attention mechanisms and further introduce a Centroid Alignment Loss at test time to reduce binding noise and enhance attribute consistency. Extensive experiments on T2I-CompBench and a newly constructed style composition benchmark demonstrate that Detail++ significantly outperforms existing methods, particularly in scenarios involving multiple objects and complex stylistic conditions.",
        "arxiv_id": "2507.17853",
        "ARXIVID": "2507.17853",
        "COMMENT": "Matches criterion 4: introduces Detail++, a training-free detail enhancer for text-to-image diffusion models, with a novel progressive detail injection strategy. Directly relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18243": {
        "authors": [
            "Longjian Zeng",
            "Zunjie Zhu",
            "Rongfeng Lu",
            "Ming Lu",
            "Bolun Zheng",
            "Chenggang Yan",
            "Anke Xue"
        ],
        "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments",
        "abstract": "arXiv:2507.18243v1 Announce Type: new  Abstract: In recent years, foundation models for monocular depth estimation have received increasing attention. Current methods mainly address typical daylight conditions, but their effectiveness notably decreases in low-light environments. There is a lack of robust foundational models for monocular depth estimation specifically designed for low-light scenarios. This largely stems from the absence of large-scale, high-quality paired depth datasets for low-light conditions and the effective parameter-efficient fine-tuning (PEFT) strategy. To address these challenges, we propose DepthDark, a robust foundation model for low-light monocular depth estimation. We first introduce a flare-simulation module and a noise-simulation module to accurately simulate the imaging process under nighttime conditions, producing high-quality paired depth datasets for low-light conditions. Additionally, we present an effective low-light PEFT strategy that utilizes illumination guidance and multiscale feature fusion to enhance the model's capability in low-light environments. Our method achieves state-of-the-art depth estimation performance on the challenging nuScenes-Night and RobotCar-Night datasets, validating its effectiveness using limited training data and computing resources.",
        "arxiv_id": "2507.18243",
        "ARXIVID": "2507.18243",
        "COMMENT": "Matches criterion 4: introduces DepthDark, a robust foundation model for monocular depth estimation in low-light, with new data simulation and fine-tuning strategies. Directly relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18569": {
        "authors": [
            "Yanzuo Lu",
            "Yuxi Ren",
            "Xin Xia",
            "Shanchuan Lin",
            "Xing Wang",
            "Xuefeng Xiao",
            "Andy J. Ma",
            "Xiaohua Xie",
            "Jian-Huang Lai"
        ],
        "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
        "abstract": "arXiv:2507.18569v1 Announce Type: new  Abstract: Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.",
        "arxiv_id": "2507.18569",
        "ARXIVID": "2507.18569",
        "COMMENT": "Matches criterion 4: proposes a new adversarial distillation method for diffusion models, improving efficiency for image and video synthesis. Relevant for vision foundation models and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18382": {
        "authors": [
            "Yayuan Li",
            "Filippos Bellos",
            "Jason Corso"
        ],
        "title": "Towards Consistent Long-Term Pose Generation",
        "abstract": "arXiv:2507.18382v1 Announce Type: new  Abstract: Current approaches to pose generation rely heavily on intermediate representations, either through two-stage pipelines with quantization or autoregressive models that accumulate errors during inference. This fundamental limitation leads to degraded performance, particularly in long-term pose generation where maintaining temporal coherence is crucial. We propose a novel one-stage architecture that directly generates poses in continuous coordinate space from minimal context - a single RGB image and text description - while maintaining consistent distributions between training and inference. Our key innovation is eliminating the need for intermediate representations or token-based generation by operating directly on pose coordinates through a relative movement prediction mechanism that preserves spatial relationships, and a unified placeholder token approach that enables single-forward generation with identical behavior during training and inference. Through extensive experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB) datasets, we demonstrate that our approach significantly outperforms existing quantization-based and autoregressive methods, especially in long-term generation scenarios.",
        "arxiv_id": "2507.18382",
        "ARXIVID": "2507.18382",
        "COMMENT": "Matches criterion 1: proposes a new method for long-term pose generation that directly predicts pose coordinates from image and text, improving spatial/temporal coherence. This is a methodological improvement for spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18031": {
        "authors": [
            "Ahmad ALBarqawi",
            "Mahmoud Nazzal",
            "Issa Khalil",
            "Abdallah Khreishah",
            "NhatHai Phan"
        ],
        "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks",
        "abstract": "arXiv:2507.18031v1 Announce Type: new  Abstract: The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.",
        "arxiv_id": "2507.18031",
        "ARXIVID": "2507.18031",
        "COMMENT": "Matches criterion 2: introduces ViGText, which uses Vision Large Language Model (VLLM) explanations and GNNs for deepfake detection, integrating vision-language model outputs in a novel way.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.18064": {
        "authors": [
            "Xiaoran Sun",
            "Liyan Wang",
            "Cong Wang",
            "Yeying Jin",
            "Kin-man Lam",
            "Zhixun Su",
            "Yang Yang",
            "Jinshan Pan"
        ],
        "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement",
        "abstract": "arXiv:2507.18064v1 Announce Type: new  Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained model priors, low-light inputs, or both, while neglecting the semantic guidance available from normal-light images. This limitation hinders their effectiveness in complex lighting conditions. In this paper, we propose VLM-IMI, a novel framework that leverages large vision-language models (VLMs) with iterative and manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions of the desired normal-light content as enhancement cues, enabling semantically informed restoration. To effectively integrate cross-modal priors, we introduce an instruction prior fusion module, which dynamically aligns and fuses image and text features, promoting the generation of detailed and semantically coherent outputs. During inference, we adopt an iterative and manual instruction strategy to refine textual instructions, progressively improving visual quality. This refinement enhances structural fidelity, semantic alignment, and the recovery of fine details under extremely low-light conditions. Extensive experiments across diverse scenarios demonstrate that VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and perceptual quality. The source code is available at https://github.com/sunxiaoran01/VLM-IMI.",
        "arxiv_id": "2507.18064",
        "ARXIVID": "2507.18064",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a VLM-based framework for low-light image enhancement using iterative and manual instructions, with cross-modal fusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.18176": {
        "authors": [
            "Abhishek Kaushik",
            "Norbert Haala",
            "Uwe Soergel"
        ],
        "title": "Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling",
        "abstract": "arXiv:2507.18176v1 Announce Type: new  Abstract: Addressing performance degradation in 3D LiDAR semantic segmentation due to domain shifts (e.g., sensor type, geographical location) is crucial for autonomous systems, yet manual annotation of target data is prohibitive. This study addresses the challenge using Unsupervised Domain Adaptation (UDA) and introduces a novel two-stage framework to tackle it. Initially, unsupervised contrastive learning at the segment level is used to pre-train a backbone network, enabling it to learn robust, domain-invariant features without labels. Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing an ensemble of diverse state-of-the-art architectures (including projection, voxel, hybrid, and cylinder-based methods). Predictions from these models are aggregated via hard voting to generate high-quality, refined pseudo-labels for the unlabeled target domain, mitigating single-model biases. The contrastively pre-trained network is then fine-tuned using these robust pseudo-labels. Experiments adapting from SemanticKITTI to unlabeled target datasets (SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in segmentation accuracy compared to direct transfer and single-model UDA approaches. These results highlight the effectiveness of combining contrastive pre-training with refined ensemble pseudo-labeling for bridging complex domain gaps without requiring target domain annotations.",
        "arxiv_id": "2507.18176",
        "ARXIVID": "2507.18176",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for 3D LiDAR semantic segmentation with a novel UDA framework using contrastive learning and multi-model pseudo labeling). The approach is novel in combining ensemble pseudo-labeling and contrastive pretraining for domain adaptation in 3D perception.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.18004": {
        "authors": [
            "Yusen Peng",
            "Shuhua Mao"
        ],
        "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI",
        "abstract": "arXiv:2507.18004v1 Announce Type: new  Abstract: How can AI move beyond imitation toward genuine creativity? This paper proposes the E.A.R.T.H. framework, a five-stage generative pipeline that transforms model-generated errors into creative assets through Error generation, Amplification, Refine selection, Transform, and Harness feedback. Drawing on cognitive science and generative modeling, we posit that \"creative potential hides in failure\" and operationalize this via structured prompts, semantic scoring, and human-in-the-loop evaluation. Implemented using LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline employs a composite reward function based on novelty, surprise, and relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to 1.898, t = -5.56, p = 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones (3.99). Feedback highlights stylistic precision and emotional resonance. These results demonstrate that error-centered, feedback-driven generation enhances creativity, offering a scalable path toward self-evolving, human-aligned creative AI.",
        "arxiv_id": "2507.18004",
        "ARXIVID": "2507.18004",
        "COMMENT": "Somewhat relevant to criterion 4: proposes a novel generative modeling pipeline (E.A.R.T.H.) that leverages vision-language models (CLIP, BLIP-2, Stable Diffusion) for creativity, but not a new foundation model itself. Interesting for generative modeling and creativity.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.18348": {
        "authors": [
            "Ioannis Sarridis",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Christos Diou"
        ],
        "title": "VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation",
        "abstract": "arXiv:2507.18348v1 Announce Type: new  Abstract: Bias in computer vision models remains a significant challenge, often resulting in unfair, unreliable, and non-generalizable AI systems. Although research into bias mitigation has intensified, progress continues to be hindered by fragmented implementations and inconsistent evaluation practices. Disparate datasets and metrics used across studies complicate reproducibility, making it difficult to fairly assess and compare the effectiveness of various approaches. To overcome these limitations, we introduce the Visual Bias Mitigator (VB-Mitigator), an open-source framework designed to streamline the development, evaluation, and comparative analysis of visual bias mitigation techniques. VB-Mitigator offers a unified research environment encompassing 12 established mitigation methods, 7 diverse benchmark datasets. A key strength of VB-Mitigator is its extensibility, allowing for seamless integration of additional methods, datasets, metrics, and models. VB-Mitigator aims to accelerate research toward fairness-aware computer vision models by serving as a foundational codebase for the research community to develop and assess their approaches. To this end, we also recommend best evaluation practices and provide a comprehensive performance comparison among state-of-the-art methodologies.",
        "arxiv_id": "2507.18348",
        "ARXIVID": "2507.18348",
        "COMMENT": "Matches criterion 3: introduces a new open-source framework (VB-Mitigator) for evaluating and advancing visual bias mitigation, with unified benchmarks and extensibility. This is a new benchmark/codebase for vision fairness, which is a novel angle in evaluation practices.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.18225": {
        "authors": [
            "Xin Wei",
            "Qin Yang",
            "Yijie Fang",
            "Mingrui Zhu",
            "Nannan Wang"
        ],
        "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift",
        "abstract": "arXiv:2507.18225v1 Announce Type: new  Abstract: While test-time adaptation (TTA) methods effectively address domain shifts by dynamically adapting pre-trained models to target domain data during online inference, their application to 3D point clouds is hindered by their irregular and unordered structure. Current 3D TTA methods often rely on computationally expensive spatial-domain optimizations and may require additional training data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation (GSDTTA), a novel approach for 3D point cloud classification that shifts adaptation to the graph spectral domain, enabling more efficient adaptation by capturing global structural properties with fewer parameters. Point clouds in target domain are represented as outlier-aware graphs and transformed into graph spectral domain by Graph Fourier Transform (GFT). For efficiency, adaptation is performed by optimizing only the lowest 10% of frequency components, which capture the majority of the point cloud's energy. An inverse GFT (IGFT) is then applied to reconstruct the adapted point cloud with the graph spectral-driven point shift. This process is enhanced by an eigenmap-guided self-training strategy that iteratively refines both the spectral adjustments and the model parameters. Experimental results and ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA, outperforming existing TTA methods for 3D point cloud classification.",
        "arxiv_id": "2507.18225",
        "ARXIVID": "2507.18225",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding in embodied agents) as it proposes a novel test-time adaptation method for 3D point clouds, but not directly focused on embodied agents or spatial intelligence in that context.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.18192": {
        "authors": [
            "Minghao Fu",
            "Guo-Hua Wang",
            "Xiaohao Chen",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
        "abstract": "arXiv:2507.18192v1 Announce Type: new  Abstract: Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt \\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6$\\times$ faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at \\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.",
        "arxiv_id": "2507.18192",
        "ARXIVID": "2507.18192",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to generative modeling and efficient guidance in text-to-image diffusion models (general interest area).",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2507.17995": {
        "authors": [
            "Huy Nguyen",
            "Kien Nguyen",
            "Akila Pemasiri",
            "Akmal Jahan",
            "Clinton Fookes",
            "Sridha Sridharan"
        ],
        "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID",
        "abstract": "arXiv:2507.17995v1 Announce Type: new  Abstract: Person re-identification (Re-ID) across visible and infrared modalities is crucial for 24-hour surveillance systems, but existing datasets primarily focus on ground-level perspectives. While ground-based IR systems offer nighttime capabilities, they suffer from occlusions, limited coverage, and vulnerability to obstructions--problems that aerial perspectives uniquely solve. To address these limitations, we introduce AG-VPReID.VIR, the first aerial-ground cross-modality video-based person Re-ID dataset. This dataset captures 1,837 identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents unique challenges including cross-viewpoint variations, modality discrepancies, and temporal dynamics. Additionally, we propose TCC-VPReID, a novel three-stream architecture designed to address the joint challenges of cross-platform and cross-modality person Re-ID. Our approach bridges the domain gaps between aerial-ground perspectives and RGB-IR modalities, through style-robust feature learning, memory-based cross-view adaptation, and intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR presents distinctive challenges compared to existing datasets, with our TCC-VPReID framework achieving significant performance gains across multiple evaluation protocols. Dataset and code are available at https://github.com/agvpreid25/AG-VPReID.VIR.",
        "arxiv_id": "2507.17995",
        "ARXIVID": "2507.17995",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it introduces a new dataset and method for cross-modality person re-ID, but not a direct match to the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.18534": {
        "authors": [
            "Xingyu Qiu",
            "Mengying Yang",
            "Xinghua Ma",
            "Dong Liang",
            "Yuzhen Li",
            "Fanding Li",
            "Gongning Luo",
            "Wei Wang",
            "Kuanquan Wang",
            "Shuo Li"
        ],
        "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models",
        "abstract": "arXiv:2507.18534v1 Announce Type: new  Abstract: EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.",
        "arxiv_id": "2507.18534",
        "ARXIVID": "2507.18534",
        "COMMENT": "Relevant to generative modeling and diffusion models, but focuses on arbitrary-noise-based diffusion for image restoration, not directly on vision foundation models or VLLMs/MLLMs. Some relevance to criterion 4.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.18060": {
        "authors": [
            "Chengxuan Zhu",
            "Qingnan Fan",
            "Qi Zhang",
            "Jinwei Chen",
            "Huaqi Zhang",
            "Chao Xu",
            "Boxin Shi"
        ],
        "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion",
        "abstract": "arXiv:2507.18060v1 Announce Type: new  Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity.",
        "arxiv_id": "2507.18060",
        "ARXIVID": "2507.18060",
        "COMMENT": "Presents a novel diffusion-based method for lens blur rendering, which is a generative vision model but not a foundation model or VLLM/MLLM. Some relevance to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.18391": {
        "authors": [
            "Shiye Lei",
            "Zhihao Cheng",
            "Kai Jia",
            "Dacheng Tao"
        ],
        "title": "Revisiting LLM Reasoning via Information Bottleneck",
        "abstract": "arXiv:2507.18391v1 Announce Type: new  Abstract: Large language models (LLMs) have recently demonstrated remarkable progress in reasoning capabilities through reinforcement learning with verifiable rewards (RLVR). By leveraging simple rule-based rewards, RL effectively incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning trajectories, progressively guiding them toward correct answers. However, existing approaches remain largely heuristic and intuition-driven, limiting the development of principled methodologies. In this paper, we present a theoretical characterization of LLM reasoning grounded in information bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO), a framework that encourages reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts. We derive a practical token-level surrogate objective and propose an efficient approximation, resulting in the lightweight IB regularization method. This technique integrates seamlessly into existing RL-based post-training frameworks without additional computational overhead, requiring only a one-line code modification. Empirically, we validate IB regularization across multiple mathematical reasoning benchmarks and RL algorithms, demonstrating consistent improvements in LLM reasoning performance.",
        "arxiv_id": "2507.18391",
        "ARXIVID": "2507.18391",
        "COMMENT": "This paper proposes a new theoretical and practical framework (IBRO) for improving LLM reasoning using the information bottleneck principle. While it is a methodological improvement for LLMs, it does not focus on spatial understanding, embodied agents, VLLMs/MLLMs, or vision foundation models. It is most relevant to general LLM reasoning and optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18059": {
        "authors": [
            "Yueheng Li",
            "Guangming Xie",
            "Zongqing Lu"
        ],
        "title": "Multi-Agent Guided Policy Optimization",
        "abstract": "arXiv:2507.18059v1 Announce Type: new  Abstract: Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.",
        "arxiv_id": "2507.18059",
        "ARXIVID": "2507.18059",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to multi-agent reinforcement learning (general interest area).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18237": {
        "authors": [
            "Chengchang Tian",
            "Jianwei Ma",
            "Yan Huang",
            "Zhanye Chen",
            "Honghao Wei",
            "Hui Zhang",
            "Wei Hong"
        ],
        "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception",
        "abstract": "arXiv:2507.18237v1 Announce Type: new  Abstract: Feature-level fusion shows promise in collaborative perception (CP) through balanced performance and communication bandwidth trade-off. However, its effectiveness critically relies on input feature quality. The acquisition of high-quality features faces domain gaps from hardware diversity and deployment conditions, alongside temporal misalignment from transmission delays. These challenges degrade feature quality with cumulative effects throughout the collaborative network. In this paper, we present the Domain-And-Time Alignment (DATA) network, designed to systematically align features while maximizing their semantic representations for fusion. Specifically, we propose a Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps through proximal-region hierarchical downsampling and observability-constrained discriminator. We further propose a Progressive Temporal Alignment Module (PTAM) to handle transmission delays via multi-scale motion modeling and two-stage compensation. Building upon the aligned features, an Instance-focused Feature Aggregation Module (IFAM) is developed to enhance semantic representations. Extensive experiments demonstrate that DATA achieves state-of-the-art performance on three typical datasets, maintaining robustness with severe communication delays and pose errors. The code will be released at https://github.com/ChengchangTian/DATA.",
        "arxiv_id": "2507.18237",
        "ARXIVID": "2507.18237",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to multi-agent perception and feature fusion (general interest area).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18633": {
        "authors": [
            "Grace Su",
            "Sheng-Yu Wang",
            "Aaron Hertzmann",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Richard Zhang"
        ],
        "title": "Identifying Prompted Artist Names from Generated Images",
        "abstract": "arXiv:2507.18633v1 Announce Type: new  Abstract: A common and controversial use of text-to-image models is to generate pictures by explicitly naming artists, such as \"in the style of Greg Rutkowski\". We introduce a benchmark for prompted-artist recognition: predicting which artist names were invoked in the prompt from the image alone. The dataset contains 1.95M images covering 110 artists and spans four generalization settings: held-out artists, increasing prompt complexity, multiple-artist prompts, and different text-to-image models. We evaluate feature similarity baselines, contrastive style descriptors, data attribution methods, supervised classifiers, and few-shot prototypical networks. Generalization patterns vary: supervised and few-shot models excel on seen artists and complex prompts, whereas style descriptors transfer better when the artist's style is pronounced; multi-artist prompts remain the most challenging. Our benchmark reveals substantial headroom and provides a public testbed to advance the responsible moderation of text-to-image models. We release the dataset and benchmark to foster further research: https://graceduansu.github.io/IdentifyingPromptedArtists/",
        "arxiv_id": "2507.18633",
        "ARXIVID": "2507.18633",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to vision-language models and their evaluation (general interest area).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18144": {
        "authors": [
            "Jinhong He",
            "Minglong Xue",
            "Zhipu Liu",
            "Mingliang Zhou",
            "Aoxiang Ning",
            "Palaiahnakote Shivakumara"
        ],
        "title": "Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement",
        "abstract": "arXiv:2507.18144v1 Announce Type: new  Abstract: Low-light image enhancement aims to improve the visibility of degraded images to better align with human visual perception. While diffusion-based methods have shown promising performance due to their strong generative capabilities. However, their unidirectional modelling of degradation often struggles to capture the complexity of real-world degradation patterns, leading to structural inconsistencies and pixel misalignments. To address these challenges, we propose a bidirectional diffusion optimization mechanism that jointly models the degradation processes of both low-light and normal-light images, enabling more precise degradation parameter matching and enhancing generation quality. Specifically, we perform bidirectional diffusion-from low-to-normal light and from normal-to-low light during training and introduce an adaptive feature interaction block (AFI) to refine feature representation. By leveraging the complementarity between these two paths, our approach imposes an implicit symmetry constraint on illumination attenuation and noise distribution, facilitating consistent degradation learning and improving the models ability to perceive illumination and detail degradation. Additionally, we design a reflection-aware correction module (RACM) to guide color restoration post-denoising and suppress overexposed regions, ensuring content consistency and generating high-quality images that align with human visual perception. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art methods in both quantitative and qualitative evaluations while generalizing effectively to diverse degradation scenarios. Code at https://github.com/hejh8/BidDiff",
        "arxiv_id": "2507.18144",
        "ARXIVID": "2507.18144",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to generative modeling in vision (general interest area).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18513": {
        "authors": [
            "Adhemar de Senneville",
            "Xavier Bou",
            "Thibaud Ehret",
            "Rafael Grompone",
            "Jean Louis Bonne",
            "Nicolas Dumelie",
            "Thomas Lauvaux",
            "Gabriele Facciolo"
        ],
        "title": "Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection",
        "abstract": "arXiv:2507.18513v1 Announce Type: new  Abstract: Object detection is one of the main applications of computer vision in remote sensing imagery. Despite its increasing availability, the sheer volume of remote sensing data poses a challenge when detecting rare objects across large geographic areas. Paradoxically, this common challenge is crucial to many applications, such as estimating environmental impact of certain human activities at scale. In this paper, we propose to address the problem by investigating the methane production and emissions of bio-digesters in France. We first introduce a novel dataset containing bio-digesters, with small training and validation sets, and a large test set with a high imbalance towards observations without objects since such sites are rare. We develop a part-based method that considers essential bio-digester sub-elements to boost initial detections. To this end, we apply our method to new, unseen regions to build an inventory of bio-digesters. We then compute geostatistical estimates of the quantity of methane produced that can be attributed to these infrastructures in a given area at a given time.",
        "arxiv_id": "2507.18513",
        "ARXIVID": "2507.18513",
        "COMMENT": "Applies part-based object detection to remote sensing for methane monitoring. While it is an application of computer vision, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18184": {
        "authors": [
            "Hoang Hai Nam Nguyen",
            "Phan Nguyen Duc Hieu",
            "Ho Won Lee"
        ],
        "title": "MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation",
        "abstract": "arXiv:2507.18184v1 Announce Type: new  Abstract: MatSSL is a streamlined self-supervised learning (SSL) architecture that employs Gated Feature Fusion at each stage of the backbone to integrate multi-level representations effectively. Current micrograph analysis of metallic materials relies on supervised methods, which require retraining for each new dataset and often perform inconsistently with only a few labeled samples. While SSL offers a promising alternative by leveraging unlabeled data, most existing methods still depend on large-scale datasets to be effective. MatSSL is designed to overcome this limitation. We first perform self-supervised pretraining on a small-scale, unlabeled dataset and then fine-tune the model on multiple benchmark datasets. The resulting segmentation models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an ImageNet-pretrained encoder, and delivers consistently up to nearly 40% improvement in average mIoU on the Environmental Barrier Coating benchmark dataset (EBC) compared to models pretrained with MicroNet. This suggests that MatSSL enables effective adaptation to the metallographic domain using only a small amount of unlabeled data, while preserving the rich and transferable features learned from large-scale pretraining on natural images.",
        "arxiv_id": "2507.18184",
        "ARXIVID": "2507.18184",
        "COMMENT": "Presents a new self-supervised learning architecture for metallographic image segmentation. While relevant to computer vision and representation learning, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18106": {
        "authors": [
            "JinYoung Kim",
            "DaeUng Jo",
            "Kimin Yun",
            "Jeonghyo Song",
            "Youngjoon Yoo"
        ],
        "title": "Distributional Uncertainty for Out-of-Distribution Detection",
        "abstract": "arXiv:2507.18106v1 Announce Type: new  Abstract: Estimating uncertainty from deep neural networks is a widely used approach for detecting out-of-distribution (OoD) samples, which typically exhibit high predictive uncertainty. However, conventional methods such as Monte Carlo (MC) Dropout often focus solely on either model or data uncertainty, failing to align with the semantic objective of OoD detection. To address this, we propose the Free-Energy Posterior Network, a novel framework that jointly models distributional uncertainty and identifying OoD and misclassified regions using free energy. Our method introduces two key contributions: (1) a free-energy-based density estimator parameterized by a Beta distribution, which enables fine-grained uncertainty estimation near ambiguous or unseen regions; and (2) a loss integrated within a posterior network, allowing direct uncertainty estimation from learned parameters without requiring stochastic sampling. By integrating our approach with the residual prediction branch (RPL) framework, the proposed method goes beyond post-hoc energy thresholding and enables the network to learn OoD regions by leveraging the variance of the Beta distribution, resulting in a semantically meaningful and computationally efficient solution for uncertainty-aware segmentation. We validate the effectiveness of our method on challenging real-world benchmarks, including Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.",
        "arxiv_id": "2507.18106",
        "ARXIVID": "2507.18106",
        "COMMENT": "Relevant to uncertainty estimation in segmentation, but does not directly match any of the four criteria. It is more about OoD detection and uncertainty modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18327": {
        "authors": [
            "Jiangjun Peng",
            "Yisi Luo",
            "Xiangyong Cao",
            "Shuang Xu",
            "Deyu Meng"
        ],
        "title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm",
        "abstract": "arXiv:2507.18327v1 Announce Type: new  Abstract: The nuclear norm (NN) has been widely explored in matrix recovery problems, such as Robust PCA and matrix completion, leveraging the inherent global low-rank structure of the data. In this study, we introduce a new modified nuclear norm (MNN) framework, where the MNN family norms are defined by adopting suitable transformations and performing the NN on the transformed matrix. The MNN framework offers two main advantages: (1) it jointly captures both local information and global low-rankness without requiring trade-off parameter tuning; (2) Under mild assumptions on the transformation, we provided exact theoretical recovery guarantees for both Robust PCA and MC tasks-an achievement not shared by existing methods that combine local and global information. Thanks to its general and flexible design, MNN can accommodate various proven transformations, enabling a unified and effective approach to structured low-rank recovery. Extensive experiments demonstrate the effectiveness of our method. Code and supplementary material are available at https://github.com/andrew-pengjj/modified_nuclear_norm.",
        "arxiv_id": "2507.18327",
        "ARXIVID": "2507.18327",
        "COMMENT": "Does not match any specific criterion. Proposes a new matrix recovery method, which is more general machine learning and not specific to vision, spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17777": {
        "authors": [
            "Theofanis Aravanis",
            "Grigorios Chrimatopoulos",
            "Mohammad Ferdows",
            "Michalis Xenos",
            "Efstratios Em Tzirtzilakis"
        ],
        "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics",
        "abstract": "arXiv:2507.17777v1 Announce Type: new  Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as \"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for revealing interpretable mathematical relationships in complex physical systems, requiring no a priori assumptions about models' structures. Motivated by the recognition that, in fluid mechanics, an understanding of the underlying flow physics is as crucial as accurate prediction, this study applies SR to model a fundamental three-dimensional (3D) incompressible flow in a rectangular channel, focusing on the (axial) velocity and pressure fields under laminar conditions. By employing the PySR library, compact symbolic equations were derived directly from numerical simulation data, revealing key characteristics of the flow dynamics. These equations not only approximate the parabolic velocity profile and pressure drop observed in the studied fluid flow, but also perfectly coincide with analytical solutions from the literature. Furthermore, we propose an innovative approach that integrates SR with the knowledge-representation framework of Answer Set Programming (ASP), combining the generative power of SR with the declarative reasoning strengths of ASP. The proposed hybrid SR/ASP framework ensures that the SR-generated symbolic expressions are not only statistically accurate, but also physically plausible, adhering to domain-specific principles. Overall, the study highlights two key contributions: SR's ability to simplify complex flow behaviours into concise, interpretable equations, and the potential of knowledge-representation approaches to improve the reliability and alignment of data-driven SR models with domain principles. Insights from the examined 3D channel flow pave the way for integrating such hybrid approaches into efficient frameworks, [...] where explainable predictions and real-time data analysis are crucial.",
        "arxiv_id": "2507.17777",
        "ARXIVID": "2507.17777",
        "COMMENT": "Does not match any specific criterion. Focuses on symbolic regression and knowledge representation for fluid mechanics, not directly related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18407": {
        "authors": [
            "Xun Ye",
            "Ruixiang Tang",
            "Mingda Zhang",
            "Jianglong Qin"
        ],
        "title": "DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation",
        "abstract": "arXiv:2507.18407v1 Announce Type: new  Abstract: Medical image segmentation leverages topological connectivity theory to enhance edge precision and regional consistency. However, existing deep networks integrating connectivity often forcibly inject it as an additional feature module, resulting in coupled feature spaces with no standardized mechanism to quantify different feature strengths. To address these issues, we propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It introduces an innovative feature space decoupling strategy. This strategy quantifies the relative strength between connectivity features and other features. It then builds a deep connectivity feature fusion-separation architecture. This architecture dynamically balances multi-scale feature expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by 1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice) and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU). The results demonstrate that DCFFSNet exceeds existing mainstream methods across all metrics. It effectively resolves segmentation fragmentation and achieves smooth edge transitions. This significantly enhances clinical usability.",
        "arxiv_id": "2507.18407",
        "ARXIVID": "2507.18407",
        "COMMENT": "Does not match any specific criterion. Focuses on medical image segmentation with a new feature fusion network, not directly related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}