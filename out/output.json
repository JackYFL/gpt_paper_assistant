{
    "2510.16410": {
        "authors": [
            "Changyue Shi",
            "Minghao Chen",
            "Yiping Mao",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Jiajun Ding",
            "Zhou Yu"
        ],
        "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
        "abstract": "arXiv:2510.16410v1 Announce Type: new  Abstract: Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.",
        "arxiv_id": "2510.16410",
        "ARXIVID": "2510.16410",
        "COMMENT": "Strong match to criteria 1 and 3: introduces a new MLLM-agent framework (REALM) for 3D spatial reasoning and segmentation, with a novel global-to-local spatial grounding strategy, and introduces a new benchmark (REALM3D). Also relevant to criterion 2 (MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16559": {
        "authors": [
            "Tian Xia",
            "Tianrun Gao",
            "Wenhao Deng",
            "Long Wei",
            "Xiaowei Qian",
            "Yixian Jiang",
            "Chenglei Yu",
            "Tailin Wu"
        ],
        "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction",
        "abstract": "arXiv:2510.16559v1 Announce Type: new  Abstract: Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.",
        "arxiv_id": "2510.16559",
        "ARXIVID": "2510.16559",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/simulator). Introduces BuildArena, a new interactive, physics-aligned benchmark for evaluating LLMs in engineering construction, with a 3D spatial geometric computation library and agentic workflow.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.15963": {
        "authors": [
            "Jiani Huang",
            "Amish Sethi",
            "Matthew Kuo",
            "Mayank Keoliya",
            "Neelay Velingker",
            "JungHo Jung",
            "Ser-Nam Lim",
            "Ziyang Li",
            "Mayur Naik"
        ],
        "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
        "abstract": "arXiv:2510.15963v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, current training pipelines primarily rely on high-level vision-sound-text pairs and lack fine-grained, structured alignment between pixel-level visual content and textual semantics. To overcome this challenge, we propose ESCA, a new framework for contextualizing embodied agents through structured spatial-temporal understanding. At its core is SGClip, a novel CLIP-based, open-domain, and promptable model for generating scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, which harnesses model-driven self-supervision from video-caption pairs and structured reasoning, thereby eliminating the need for human-labeled scene graph annotations. We demonstrate that SGClip supports both prompt-based inference and task-specific fine-tuning, excelling in scene graph generation and action localization benchmarks. ESCA with SGClip consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across two embodied environments. Notably, it significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines.",
        "arxiv_id": "2510.15963",
        "ARXIVID": "2510.15963",
        "COMMENT": "This paper introduces ESCA, a framework for contextualizing embodied agents via scene-graph generation, with a novel CLIP-based model (SGClip) trained on open-domain videos. It directly matches criterion 1 (spatial understanding for embodied agents) and criterion 2 (MLLMs), and is highly relevant for embodied AI with spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.17023": {
        "authors": [
            "Shraman Pramanick",
            "Effrosyni Mavroudi",
            "Yale Song",
            "Rama Chellappa",
            "Lorenzo Torresani",
            "Triantafyllos Afouras"
        ],
        "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
        "abstract": "arXiv:2510.17023v1 Announce Type: new  Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.",
        "arxiv_id": "2510.17023",
        "ARXIVID": "2510.17023",
        "COMMENT": "This paper proposes ED-VTG, a method for video temporal grounding using multi-modal large language models (MLLMs), with a two-stage process leveraging enriched queries and a multiple-instance-learning objective. It directly matches criterion 2 (new VLLMs/MLLMs) and is also relevant to surprising empirical results in vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16641": {
        "authors": [
            "Young-Jun Lee",
            "Byung-Kwan Lee",
            "Jianshu Zhang",
            "Yechan Hwang",
            "Byungsoo Ko",
            "Han-Gyu Kim",
            "Dongyu Yao",
            "Xuankun Rong",
            "Eojin Joo",
            "Seung-Ho Han",
            "Bowon Ko",
            "Ho-Jin Choi"
        ],
        "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
        "abstract": "arXiv:2510.16641v1 Announce Type: new  Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues - each averaging four turns - derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs.",
        "arxiv_id": "2510.16641",
        "ARXIVID": "2510.16641",
        "COMMENT": "This paper introduces MultiVerse, a new multi-turn conversation benchmark for evaluating large vision and language models (VLMs). This is a direct match to criterion 3 (embodied AI papers on new benchmarks) and criterion 2 (new VLLMs/MLLMs), as it provides a challenging new benchmark for VLMs and evaluates 18 models, revealing surprising empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16333": {
        "authors": [
            "Junha Song",
            "Sangdoo Yun",
            "Dongyoon Han",
            "Jaegul Choo",
            "Byeongho Heo"
        ],
        "title": "RL makes MLLMs see better than SFT",
        "abstract": "arXiv:2510.16333v1 Announce Type: new  Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/",
        "arxiv_id": "2510.16333",
        "ARXIVID": "2510.16333",
        "COMMENT": "This paper investigates the impact of RL vs SFT on MLLMs, showing that RL leads to better visual representations and proposing a new training recipe (PIVOT) for vision encoders in MLLMs. It provides surprising empirical results and new methodology for MLLMs (criterion 2), with direct relevance to vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16756": {
        "authors": [
            "Siyin Wang",
            "Wenyi Yu",
            "Xianzhao Chen",
            "Xiaohai Tian",
            "Jun Zhang",
            "Lu Lu",
            "Chao Zhang"
        ],
        "title": "End-to-end Listen, Look, Speak and Act",
        "abstract": "arXiv:2510.16756v1 Announce Type: new  Abstract: Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.",
        "arxiv_id": "2510.16756",
        "ARXIVID": "2510.16756",
        "COMMENT": "This paper introduces ELLSA, a full-duplex, end-to-end multimodal model that simultaneously perceives and generates across vision, text, speech, and action. It features a novel SA-MoE architecture for joint multimodal perception and concurrent generation, and is evaluated on speech-interaction and robot-manipulation benchmarks. This matches criterion 2 (new MLLMs/VLLMs) and criterion 3 (embodied AI with new methods for interaction and benchmarks).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.17482": {
        "authors": [
            "Chenxu Dang",
            "Haiyan Liu",
            "Guangjun Bao",
            "Pei An",
            "Xinyue Tang",
            "Jie Ma",
            "Bingchuan Sun",
            "Yan Wang"
        ],
        "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
        "abstract": "arXiv:2510.17482v1 Announce Type: new  Abstract: Semantic occupancy has emerged as a powerful representation in world models for its ability to capture rich spatial semantics. However, most existing occupancy world models rely on static and fixed embeddings or grids, which inherently limit the flexibility of perception. Moreover, their ``in-place classification\" over grids exhibits a potential misalignment with the dynamic and continuous nature of real scenarios.In this paper, we propose SparseWorld, a novel 4D occupancy world model that is flexible, adaptive, and efficient, powered by sparse and dynamic queries. We propose a Range-Adaptive Perception module, in which learnable queries are modulated by the ego vehicle states and enriched with temporal-spatial associations to enable extended-range perception. To effectively capture the dynamics of the scene, we design a State-Conditioned Forecasting module, which replaces classification-based forecasting with regression-guided formulation, precisely aligning the dynamic queries with the continuity of the 4D environment. In addition, We specifically devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and efficient training. Extensive experiments demonstrate that SparseWorld achieves state-of-the-art performance across perception, forecasting, and planning tasks. Comprehensive visualizations and ablation studies further validate the advantages of SparseWorld in terms of flexibility, adaptability, and efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.",
        "arxiv_id": "2510.17482",
        "ARXIVID": "2510.17482",
        "COMMENT": "Matches criterion 1 and 3: Proposes a new 4D occupancy world model (SparseWorld) for spatial understanding in embodied agents, with novel sparse and dynamic queries, and introduces new modules for perception and forecasting. Also provides a new training strategy and extensive benchmarking.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16036": {
        "authors": [
            "Zewen Li",
            "Zitong Yu",
            "Qilang Ye",
            "Weicheng Xie",
            "Wei Zhuo",
            "Linlin Shen"
        ],
        "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection",
        "abstract": "arXiv:2510.16036v1 Announce Type: new  Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold the potential of detecting defective objects in Industrial Anomaly Detection (IAD). However, most traditional IAD methods lack the ability to provide multi-turn human-machine dialogues and detailed descriptions, such as the color of objects, the shape of an anomaly, or specific types of anomalies. At the same time, methods based on large pre-trained models have not fully stimulated the ability of large models in anomaly detection tasks. In this paper, we explore the combination of rich text semantics with both image-level and pixel-level information from images and propose IAD-GPT, a novel paradigm based on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate detailed anomaly prompts for specific objects. These specific prompts from the large language model (LLM) are used to activate the detection and segmentation functions of the pre-trained visual-language model (i.e., CLIP). To enhance the visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein image features interact with normal and abnormal text prompts to dynamically select enhancement pathways, which enables language models to focus on specific aspects of visual data, enhancing their ability to accurately interpret and respond to anomalies within images. Moreover, we design a Multi-Mask Fusion module to incorporate mask as expert knowledge, which enhances the LLM's perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA datasets demonstrate our state-of-the-art performance on self-supervised and few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA datasets. The codes are available at \\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.",
        "arxiv_id": "2510.16036",
        "ARXIVID": "2510.16036",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multimodal large language models). The paper introduces IAD-GPT, a new MLLM-based paradigm for industrial anomaly detection, with novel modules for text-guided enhancement and multi-mask fusion, and demonstrates state-of-the-art results. This is directly relevant to your friend's interest in VLLMs/MLLMs and their applications.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.16714": {
        "authors": [
            "Xiongkun Linghu",
            "Jiangyong Huang",
            "Ziyu Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes",
        "abstract": "arXiv:2510.16714v1 Announce Type: new  Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to achieve grounded question-answering, primarily due to the under-exploration of the mech- anism of human-like scene-object grounded reasoning. This paper bridges the gap by presenting a novel framework. We first introduce a grounded Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a complex reasoning task into simpler and manageable problems, and building corresponding visual clues based on multimodal expert modules. To enable such a method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning dataset, consisting of 185K high-quality instances. Extensive experiments across various complex 3D scene reasoning benchmarks demonstrate that our new framework achieves strong performance with high grounding-QA coherence. To the best of our knowledge, this is the first successful application of CoT reasoning to 3D scene understanding, enabling step-by-step human-like reasoning and showing potential for extension to broader 3D scene understanding scenarios.",
        "arxiv_id": "2510.16714",
        "ARXIVID": "2510.16714",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 2 (VLLMs/MLLMs). Proposes a new grounded chain-of-thought reasoning framework for 3D scene understanding, with a large new dataset and step-by-step reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.17274": {
        "authors": [
            "Katie Luo",
            "Jingwei Ji",
            "Tong He",
            "Runsheng Xu",
            "Yichen Xie",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
        "abstract": "arXiv:2510.17274v1 Announce Type: new  Abstract: Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.",
        "arxiv_id": "2510.17274",
        "ARXIVID": "2510.17274",
        "COMMENT": "This paper proposes Plug-and-Forecast (PnF), a plug-and-play approach that augments motion forecasting models with multimodal large language models (MLLMs). This is a direct match to criterion 2 (new VLLMs/MLLMs) and also touches on criterion 1 (spatial understanding in embodied agents) as it applies to autonomous driving. The use of MLLMs for structured scene understanding in motion forecasting is novel.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.16416": {
        "authors": [
            "Xiaojun Guo",
            "Runyu Zhou",
            "Yifei Wang",
            "Qi Zhang",
            "Chenheng Zhang",
            "Stefanie Jegelka",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin",
            "Yisen Wang"
        ],
        "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
        "abstract": "arXiv:2510.16416v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.",
        "arxiv_id": "2510.16416",
        "ARXIVID": "2510.16416",
        "COMMENT": "Matches criterion 2: Proposes SSL4RL, a new framework for aligning vision-language models using self-supervised learning as intrinsic reward for RL-based fine-tuning, with empirical insights into design factors.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.17777": {
        "authors": [
            "Samir Khaki",
            "Junxian Guo",
            "Jiaming Tang",
            "Shang Yang",
            "Yukang Chen",
            "Konstantinos N. Plataniotis",
            "Yao Lu",
            "Song Han",
            "Zhijian Liu"
        ],
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "abstract": "arXiv:2510.17777v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.",
        "arxiv_id": "2510.17777",
        "ARXIVID": "2510.17777",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes SparseVILA, a new efficient inference paradigm for VLMs, with a novel decoupling of visual sparsity and significant empirical speedups.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.16926": {
        "authors": [
            "Chenxu Li",
            "Zhicai Wang",
            "Yuan Sheng",
            "Xingyu Zhu",
            "Yanbin Hao",
            "Xiang Wang"
        ],
        "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
        "abstract": "arXiv:2510.16926v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.",
        "arxiv_id": "2510.16926",
        "ARXIVID": "2510.16926",
        "COMMENT": "Matches criterion 3: Introduces Res-Bench, a new benchmark for evaluating the robustness of multimodal large language models (MLLMs) to dynamic resolution input, with novel robustness metrics and large-scale evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17681": {
        "authors": [
            "Yuandong Pu",
            "Le Zhuo",
            "Songhao Han",
            "Jinbo Xing",
            "Kaiwen Zhu",
            "Shuo Cao",
            "Bin Fu",
            "Si Liu",
            "Hongsheng Li",
            "Yu Qiao",
            "Wenlong Zhang",
            "Xi Chen",
            "Yihao Liu"
        ],
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "abstract": "arXiv:2510.17681v1 Announce Type: new  Abstract: Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism.",
        "arxiv_id": "2510.17681",
        "ARXIVID": "2510.17681",
        "COMMENT": "Matches criterion 3 (new benchmarks for vision/embodied AI): Introduces PICABench, a new benchmark for physically realistic image editing, and proposes a new evaluation protocol (PICAEval) using VLM-as-a-judge and human annotations. Also explores learning physics from videos for improved realism.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.16325": {
        "authors": [
            "Yuyao Zhang",
            "Yu-Wing Tai"
        ],
        "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention",
        "abstract": "arXiv:2510.16325v1 Announce Type: new  Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained texture synthesis and globally coherent structure, yet current diffusion models remain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive quadratic complexity of attention and the scarcity of native $4K$ training data. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces hierarchical local attention with low-resolution global guidance, enabling efficient, scalable, and semantically coherent image synthesis at ultra-high resolutions. Specifically, high-resolution latents are divided into fixed-size local windows to reduce attention complexity from quadratic to near-linear, while a low-resolution latent equipped with scaled positional anchors injects global semantics. A lightweight LoRA adaptation bridges global and local pathways during denoising, ensuring consistency across structure and detail. To maximize inference efficiency, we repermute token sequence in Hilbert curve order and implement a fused-kernel for skipping masked operations, resulting in a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT achieves more than $2\\times$ faster inference and lower memory usage compared to dense attention baselines, while reliably scaling to $4K \\times 4K$ resolution without requiring additional high-resolution training data. On both quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons, Scale-DiT delivers superior global coherence and sharper local detail, matching or outperforming state-of-the-art methods that rely on native 4K training. Taken together, these results highlight hierarchical local attention with guided low-resolution anchors as a promising and effective approach for advancing ultra-high-resolution image generation.",
        "arxiv_id": "2510.16325",
        "ARXIVID": "2510.16325",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes Scale-DiT, a new diffusion model for ultra-high-resolution image generation with hierarchical local attention and global guidance, with strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.16870": {
        "authors": [
            "Yudan Ren",
            "Xinlong Wang",
            "Kexin Wang",
            "Tian Xia",
            "Zihan Ma",
            "Zhaowei Li",
            "Xiangrong Bi",
            "Xiao Li",
            "Xiaowei He"
        ],
        "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding",
        "abstract": "arXiv:2510.16870v1 Announce Type: new  Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising results, current understanding of the parallels between artificial neural networks (ANNs) and human brain processing remains limited: (1) unimodal ANN studies fail to capture the brain's inherent multimodal processing capabilities, and (2) multimodal ANN research primarily focuses on high-level model outputs, neglecting the crucial role of individual neurons. To address these limitations, we propose a novel neuron-level analysis framework that investigates the multimodal information processing mechanisms in vision-language models (VLMs) through the lens of human brain activity. Our approach uniquely combines fine-grained artificial neuron (AN) analysis with fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP and METER. Our analysis reveals four key findings: (1) ANs successfully predict biological neurons (BNs) activities across multiple functional networks (including language, vision, attention, and default mode), demonstrating shared representational mechanisms; (2) Both ANs and BNs demonstrate functional redundancy through overlapping neural representations, mirroring the brain's fault-tolerant and collaborative information processing mechanisms; (3) ANs exhibit polarity patterns that parallel the BNs, with oppositely activated BNs showing mirrored activation trends across VLM layers, reflecting the complexity and bidirectional nature of neural information processing; (4) The architectures of CLIP and METER drive distinct BNs: CLIP's independent branches show modality-specific specialization, whereas METER's cross-modal design yields unified cross-modal activation, highlighting the architecture's influence on ANN brain-like properties. These results provide compelling evidence for brain-like hierarchical processing in VLMs at the neuronal level.",
        "arxiv_id": "2510.16870",
        "ARXIVID": "2510.16870",
        "COMMENT": "Matches criterion 2 (VLLMs) and criterion 4 (vision foundation models and applications). Provides a novel neuron-level analysis of VLMs (CLIP, METER) using fMRI data, uncovering brain-like hierarchical patterns.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.16598": {
        "authors": [
            "Jiaying Zhu",
            "Yurui Zhu",
            "Xin Lu",
            "Wenrui Yan",
            "Dong Li",
            "Kunlin Liu",
            "Xueyang Fu",
            "Zheng-Jun Zha"
        ],
        "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs",
        "abstract": "arXiv:2510.16598v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational and memory bottlenecks from the massive number of visual tokens generated by high-resolution images or multi-image inputs. Previous token compression techniques are often constrained by heuristic rules that risk discarding critical information. They may suffer from biases, such as attention sinks, that lead to sharp performance drops under aggressive compression ratios. To address these limitations, we reformulate token compression as a lightweight plug-and-play framework that reformulates token compression into an end-to-end learnable decision process. To be specific, we propose VisionSelector, a scorer module decoupled from the MLLM backbone that incorporates a differentiable Top-K mechanism and a curriculum annealing strategy to bridge the training-inference gap, enabling efficient and adaptive token selection various arbitrary compression rates. Remarkably lightweight with only 12.85M trainable parameters, VisionSelector demonstrates generalization across various compression rates and adaptively identifying critical tokens. This leads to superior performance across all compression budgets, evidenced by preserving 100% accuracy on MME with 30% retention budget, outperforming prior methods by 12.14% at 10% retention budget, and doubling prefill speed. Our code is available at https://github.com/JulietChoo/VisionSelector .",
        "arxiv_id": "2510.16598",
        "ARXIVID": "2510.16598",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes VisionSelector, a learnable visual token compression module for efficient MLLMs, with strong empirical results and a clever differentiable Top-K mechanism.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17568": {
        "authors": [
            "Kaichen Zhou",
            "Yuhan Wang",
            "Grace Chen",
            "Xinhai Chang",
            "Gaspard Beaudouin",
            "Fangneng Zhan",
            "Paul Pu Liang",
            "Mengyu Wang"
        ],
        "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
        "abstract": "arXiv:2510.17568v1 Announce Type: new  Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.",
        "arxiv_id": "2510.17568",
        "ARXIVID": "2510.17568",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications). Proposes PAGE-4D, a feedforward model for dynamic 4D scene understanding, with a novel disentanglement of static/dynamic information for pose and geometry estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17664": {
        "authors": [
            "Ling Liu",
            "Jun Tian",
            "Li Yi"
        ],
        "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
        "abstract": "arXiv:2510.17664v1 Announce Type: new  Abstract: 4D panoptic segmentation in a streaming setting is critical for highly dynamic environments, such as evacuating dense crowds and autonomous driving in complex scenarios, where real-time, fine-grained perception within a constrained time budget is essential. In this paper, we introduce 4DSegStreamer, a novel framework that employs a Dual-Thread System to efficiently process streaming frames. The framework is general and can be seamlessly integrated into existing 3D and 4D segmentation methods to enable real-time capability. It also demonstrates superior robustness compared to existing streaming perception approaches, particularly under high FPS conditions. The system consists of a predictive thread and an inference thread. The predictive thread leverages historical motion and geometric information to extract features and forecast future dynamics. The inference thread ensures timely prediction for incoming frames by aligning with the latest memory and compensating for ego-motion and dynamic object movements. We evaluate 4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of our approach, particularly in accurately predicting dynamic objects in complex scenes.",
        "arxiv_id": "2510.17664",
        "ARXIVID": "2510.17664",
        "COMMENT": "This paper presents 4DSegStreamer, a dual-thread system for streaming 4D panoptic segmentation, enabling real-time perception in dynamic environments. It is relevant to embodied AI and introduces a new method for streaming segmentation, matching criterion 3 (embodied AI, new methods/benchmarks, novel angles).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16989": {
        "authors": [
            "Luca Zanella",
            "Massimiliano Mancini",
            "Yiming Wang",
            "Alessio Tonioni",
            "Elisa Ricci"
        ],
        "title": "Training-free Online Video Step Grounding",
        "abstract": "arXiv:2510.16989v1 Announce Type: new  Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims to detect which steps are performed in a video. Standard approaches for this task require a labeled training set (e.g., with step-level annotations or narrations), which may be costly to collect. Moreover, they process the full video offline, limiting their applications for scenarios requiring online decisions. Thus, in this work, we explore how to perform VSG online and without training. We achieve this by exploiting the zero-shot capabilities of recent Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step associated with a restricted set of frames, without access to the whole video. We show that this online strategy without task-specific tuning outperforms offline and training-based models. Motivated by this finding, we develop Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting knowledge of past frames into the LMM-based predictions. BaGLM exploits Bayesian filtering principles, modeling step transitions via (i) a dependency matrix extracted through large language models and (ii) an estimation of step progress. Experiments on three datasets show superior performance of BaGLM over state-of-the-art training-based offline methods.",
        "arxiv_id": "2510.16989",
        "ARXIVID": "2510.16989",
        "COMMENT": "This paper proposes a training-free, online video step grounding method using large multimodal models (LMMs), and introduces a Bayesian filtering approach (BaGLM) for improved performance. It leverages zero-shot capabilities of LMMs for a new method in video understanding, matching criterion 2 (new MLLMs/VLLMs) and offering a clever statistical trick (Bayesian filtering).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17305": {
        "authors": [
            "ZhaoYang Han",
            "Qihan Lin",
            "Hao Liang",
            "Bowen Chen",
            "Zhou Liu",
            "Wentao Zhang"
        ],
        "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding",
        "abstract": "arXiv:2510.17305v1 Announce Type: new  Abstract: We introduce \\textbf{LongInsightBench}, the first benchmark designed to assess models' ability to understand long videos, with a focus on human language, viewpoints, actions, and other contextual elements, while integrating \\textbf{visual, audio, and text} modalities. Our benchmark excels in three key areas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select approximately 1,000 videos from open-source datasets FineVideo based on duration limit and the information density of both visual and audio modalities, focusing on content like lectures, interviews, and vlogs, which contain rich language elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have designed six challenging task scenarios, including both Intra-Event and Inter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance Pipelines:} We have developed a three-step, semi-automated data quality assurance pipeline to ensure the difficulty and validity of the synthesized questions and answer options. Based on LongInsightBench, we designed a series of experiments. Experimental results shows that Omni-modal models(OLMs) still face challenge in tasks requiring precise temporal localization (T-Loc) and long-range causal inference (CE-Caus). Extended experiments reveal the information loss and processing bias in multi-modal fusion of OLMs. Our dataset and code is available at https://anonymous.4open.science/r/LongInsightBench-910F/.",
        "arxiv_id": "2510.17305",
        "ARXIVID": "2510.17305",
        "COMMENT": "This paper introduces LongInsightBench, a comprehensive benchmark for evaluating omni-modal models on long-video understanding, integrating visual, audio, and text modalities. It is a new benchmark for multi-modal models (criterion 2 and 4), with a focus on long-range, human-centric video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16505": {
        "authors": [
            "Lukas Selch",
            "Yufang Hou",
            "M. Jehanzeb Mirza",
            "Sivan Doveh",
            "James Glass",
            "Rogerio Feris",
            "Wei Lin"
        ],
        "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies",
        "abstract": "arXiv:2510.16505v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants.",
        "arxiv_id": "2510.16505",
        "ARXIVID": "2510.16505",
        "COMMENT": "This paper introduces PRISMM-Bench, a new benchmark for evaluating LMMs on peer-review-grounded multimodal inconsistencies in scientific papers. It is a new benchmark for MLLMs/VLLMs (criterion 2), with a novel focus on real-world multimodal scientific reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17137": {
        "authors": [
            "WenBo Xu",
            "Liu Liu",
            "Li Zhang",
            "Ran Zhang",
            "Hao Wu",
            "Dan Guo",
            "Meng Wang"
        ],
        "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
        "abstract": "arXiv:2510.17137v1 Announce Type: new  Abstract: Articulated objects, such as laptops and drawers, exhibit significant challenges for 3D reconstruction and pose estimation due to their multi-part geometries and variable joint configurations, which introduce structural diversity across different states. To address these challenges, we propose KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation, a unified framework for reconstructing diverse articulated instances and pose estimation from single view input. Specifically, we first encode complete geometry (SDFs), joint angles, and part segmentation into a structured latent space via a novel Kinematic-Aware VAE (KA-VAE). In addition, we employ two conditional diffusion models: one for regressing global pose (SE(3)) and joint parameters, and another for generating the kinematic-aware latent code from partial observations. Finally, we produce an iterative optimization module that bidirectionally refines reconstruction accuracy and kinematic parameters via Chamfer-distance minimization while preserving articulation constraints. Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the effectiveness of our approach in accurately reconstructing articulated objects and estimating their kinematic properties.",
        "arxiv_id": "2510.17137",
        "ARXIVID": "2510.17137",
        "COMMENT": "Matches criterion 1: Proposes KineDiff3D, a kinematic-aware diffusion framework for articulated object shape reconstruction and generation, focusing on spatial understanding and generative modeling for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16888": {
        "authors": [
            "Zongjian Li",
            "Zheyuan Liu",
            "Qihui Zhang",
            "Bin Lin",
            "Shenghai Yuan",
            "Zhiyuan Yan",
            "Yang Ye",
            "Wangbo Yu",
            "Yuwei Niu",
            "Li Yuan"
        ],
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
        "abstract": "arXiv:2510.16888v1 Announce Type: new  Abstract: Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. UniWorld-V2, trained with this framework, achieves \\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available at https://github.com/PKU-YuanGroup/UniWorld-V2.",
        "arxiv_id": "2510.16888",
        "ARXIVID": "2510.16888",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Proposes a new post-training framework for instruction-based image editing using policy optimization and MLLM-based feedback, achieving SOTA results and demonstrating model-agnostic applicability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17771": {
        "authors": [
            "Zhining Liu",
            "Ziyi Chen",
            "Hui Liu",
            "Chen Luo",
            "Xianfeng Tang",
            "Suhang Wang",
            "Joy Zeng",
            "Zhenwei Dai",
            "Zhan Shi",
            "Tianxin Wei",
            "Benoit Dumoulin",
            "Hanghang Tong"
        ],
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
        "abstract": "arXiv:2510.17771v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.",
        "arxiv_id": "2510.17771",
        "ARXIVID": "2510.17771",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Investigates the disconnect between visual attention and answer correctness in VLMs, introduces a novel inference-time intervention to improve accuracy, and provides diagnostic insights into VLM reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16596": {
        "authors": [
            "Yiyang Huang",
            "Liang Shi",
            "Yitian Zhang",
            "Yi Xu",
            "Yun Fu"
        ],
        "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
        "abstract": "arXiv:2510.16596v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks. However, object hallucination, where models produce plausible but inaccurate object descriptions, remains a significant challenge. In contrast to previous work focusing on LLM components, this paper is the first to trace LVLM hallucinations to visual encoders and identifies three key issues: statistical bias, inherent bias, and vulnerability. To address these challenges, we propose SHIELD, a training-free framework that mitigates hallucinations through three strategies: re-weighting visual tokens to reduce statistical bias, introducing noise-derived tokens to counter inherent bias, and applying adversarial attacks with contrastive decoding to address vulnerability. Experiments demonstrate that SHIELD effectively mitigates object hallucinations across diverse benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on the general LVLM benchmark, highlighting its broad applicability. Code will be released.",
        "arxiv_id": "2510.16596",
        "ARXIVID": "2510.16596",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes SHIELD, a training-free framework to suppress hallucinations in LVLMs by targeting visual encoders, with new strategies for bias and vulnerability defense. Also provides broad empirical results across LVLM families.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16540": {
        "authors": [
            "Jihoon Kwon",
            "Kyle Min",
            "Jy-yong Sohn"
        ],
        "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
        "abstract": "arXiv:2510.16540v1 Announce Type: new  Abstract: Despite recent advances, vision-language models trained with standard contrastive objectives still struggle with compositional reasoning -- the ability to understand structured relationships between visual and linguistic elements. This shortcoming is largely due to the tendency of the text encoder to focus on individual words rather than their relations, a limitation reinforced by contrastive training that primarily aligns words with visual objects. In this paper, we introduce REconstruction and Alignment of text Descriptions (READ), a fine-tuning method designed to enhance compositional reasoning by adding two auxiliary objectives to the contrastive learning: (1) a token-level reconstruction objective, where a frozen pre-trained decoder reconstructs alternative captions based on the embedding of the original caption; and (2) a sentence-level alignment objective, which explicitly aligns paraphrased sentences in the embedding space. We show that READ-CLIP, a model derived by applying the READ method to the pre-trained CLIP model, achieves the state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest conventional fine-tuning baseline by up to 4.1%. Furthermore, applying the READ to existing CLIP variants (including NegCLIP and FSC-CLIP) also improves performance on these benchmarks. Quantitative and qualitative analyses reveal that our proposed objectives -- reconstruction and alignment -- offer complementary benefits: the former encourages the encoder to capture relationships between words within a caption, while the latter ensures consistent representations for paraphrases expressed with different wording.",
        "arxiv_id": "2510.16540",
        "ARXIVID": "2510.16540",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes READ, a fine-tuning method for CLIP to enhance compositional reasoning, with new objectives for reconstruction and alignment, and strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16442": {
        "authors": [
            "Haoran Sun",
            "Chen Cai",
            "Huiping Zhuang",
            "Kong Aik Lee",
            "Lap-Pui Chau",
            "Yi Wang"
        ],
        "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning",
        "abstract": "arXiv:2510.16442v1 Announce Type: new  Abstract: The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ benchmark dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The source code and dataset will be publicly available.",
        "arxiv_id": "2510.16442",
        "ARXIVID": "2510.16442",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes EDVD-LLaMA, a multimodal LLM for explainable deepfake video detection, with a novel spatio-temporal tokenization and chain-of-thought mechanism.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16753": {
        "authors": [
            "Wei Huang",
            "Peining Li",
            "Meiyu Liang",
            "Xu Hou",
            "Junping Du",
            "Yingxia Shao",
            "Guanhua Ye",
            "Wu Liu",
            "Kangkang Lu",
            "Yang Yu"
        ],
        "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion",
        "abstract": "arXiv:2510.16753v1 Announce Type: new  Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by incorporating visual and textual modalities, enabling richer and more expressive entity representations. However, existing MKGs often suffer from incompleteness, which hinder their effectiveness in downstream tasks. Therefore, multimodal knowledge graph completion (MKGC) task is receiving increasing attention. While large language models (LLMs) have shown promise for knowledge graph completion (KGC), their application to the multimodal setting remains underexplored. Moreover, applying Multimodal Large Language Models (MLLMs) to the task of MKGC introduces significant challenges: (1) the large number of image tokens per entity leads to semantic noise and modality conflicts, and (2) the high computational cost of processing large token inputs. To address these issues, we propose Efficient Lightweight Multimodal Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token Compressor (MVTC) based on multi-head attention mechanism, which adaptively compresses image tokens from both textual and visual views, thereby effectively reducing redundancy while retaining necessary information and avoiding modality conflicts. Additionally, we design an attention pruning strategy to remove redundant attention layers from MLLMs, thereby significantly reducing the inference cost. We further introduce a linear projection to compensate for the performance degradation caused by pruning. Extensive experiments on benchmark FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art performance while substantially improving computational efficiency, establishing a new paradigm for multimodal knowledge graph completion.",
        "arxiv_id": "2510.16753",
        "ARXIVID": "2510.16753",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes ELMM, a lightweight MLLM for multimodal knowledge graph completion, with a novel visual token compressor and attention pruning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16295": {
        "authors": [
            "Ryoto Miyamoto",
            "Xin Fan",
            "Fuyuko Kido",
            "Tsuneo Matsumoto",
            "Hayato Yamana"
        ],
        "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
        "abstract": "arXiv:2510.16295v1 Announce Type: new  Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in evaluating membership inference attacks (MIA) against large vision-language models (LVLMs). While prior work has reported high attack success rates, our analysis suggests that these results often arise from detecting distributional bias introduced during dataset construction rather than from identifying true membership status. To address this issue, we introduce a controlled benchmark of 6{,}000 images where the distributions of member and non-member samples are carefully balanced, and ground-truth membership labels are provided across three distinct training stages. Experiments using OpenLVLM-MIA demonstrated that the performance of state-of-the-art MIA methods converged to random chance under unbiased conditions. By offering a transparent and unbiased benchmark, OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and provides a solid foundation for developing stronger privacy-preserving techniques.",
        "arxiv_id": "2510.16295",
        "ARXIVID": "2510.16295",
        "COMMENT": "Matches criterion 3: Embodied AI papers on building new benchmarks (simulator related) or new methods, with a novel angle. The paper introduces OpenLVLM-MIA, a controlled benchmark for membership inference attacks on large vision-language models, revealing that prior high attack rates were due to dataset bias. The focus on unbiased evaluation and privacy in LVLMs is a novel and insightful contribution.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.16776": {
        "authors": [
            "Mingzheng Zhang",
            "Jinfeng Gao",
            "Dan Xu",
            "Jiangrui Yu",
            "Yuhan Qiao",
            "Lan Chen",
            "Jin Tang",
            "Xiao Wang"
        ],
        "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation",
        "abstract": "arXiv:2510.16776v1 Announce Type: new  Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.",
        "arxiv_id": "2510.16776",
        "ARXIVID": "2510.16776",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. The paper proposes a novel framework (EMRRG) for X-ray report generation using pre-trained Mamba networks (a non-Transformer vision backbone), and discusses parameter-efficient fine-tuning and cross-attention improvements. The focus on vision foundation models and their application to medical report generation is directly relevant.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.16444": {
        "authors": [
            "Kunyu Peng",
            "Di Wen",
            "Jia Fu",
            "Jiamin Wu",
            "Kailun Yang",
            "Junwei Zheng",
            "Ruiping Liu",
            "Yufan Chen",
            "Yuqian Fu",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Rainer Stiefelhagen"
        ],
        "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba",
        "abstract": "arXiv:2510.16444v1 Announce Type: new  Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.",
        "arxiv_id": "2510.16444",
        "ARXIVID": "2510.16444",
        "COMMENT": "Presents RefAtomNet++, a new framework for referring atomic video action recognition with a novel multi-hierarchical semantic-aligned cross-attention mechanism and a new dataset (RefAVA++). Relevant to criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.16769": {
        "authors": [
            "Shuo Han",
            "Yukun Cao",
            "Zezhong Ding",
            "Zengyi Gao",
            "S Kevin Zhou",
            "Xike Xie"
        ],
        "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models",
        "abstract": "arXiv:2510.16769v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but remain limited by input-token constraints, facing scalability bottlenecks and lacking effective mechanisms to coordinate textual and visual modalities. To address these challenges, we propose GraphVista, a unified framework that enhances both scalability and modality coordination in graph understanding. For scalability, GraphVista organizes graph information hierarchically into a lightweight GraphRAG base, which retrieves only task-relevant textual descriptions and high-resolution visual subgraphs, compressing redundant context while preserving key reasoning elements. For modality coordination, GraphVista introduces a planning agent that routes tasks to the most suitable modality-using the text modality for simple property reasoning and the visual modality for local and structurally complex reasoning grounded in explicit topology. Extensive experiments demonstrate that GraphVista scales to large graphs, up to $200\\times$ larger than those used in existing benchmarks, and consistently outperforms existing textual, visual, and fusion-based methods, achieving up to $4.4\\times$ quality improvement over the state-of-the-art baselines by fully exploiting the complementary strengths of both modalities.",
        "arxiv_id": "2510.16769",
        "ARXIVID": "2510.16769",
        "COMMENT": "Presents GraphVista, a unified framework for scalable graph understanding with vision-language models, with a planning agent for modality coordination. This is relevant to criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.17739": {
        "authors": [
            "Timur Ismagilov",
            "Shakaiba Majeed",
            "Michael Milford",
            "Tan Viet Tuyen Nguyen",
            "Sarvapali D. Ramchurn",
            "Shoaib Ehsan"
        ],
        "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition",
        "abstract": "arXiv:2510.17739v1 Announce Type: new  Abstract: We address multi-reference visual place recognition (VPR), where reference sets captured under varying conditions are used to improve localisation performance. While deep learning with large-scale training improves robustness, increasing data diversity and model complexity incur extensive computational cost during training and deployment. Descriptor-level fusion via voting or aggregation avoids training, but often targets multi-sensor setups or relies on heuristics with limited gains under appearance and viewpoint change. We propose a training-free, descriptor-agnostic approach that jointly models places using multiple reference descriptors via matrix decomposition into basis representations, enabling projection-based residual matching. We also introduce SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance data, our method improves Recall@1 by up to ~18% over single-reference and outperforms multi-reference baselines across appearance and viewpoint changes, with gains of ~5% on unstructured data, demonstrating strong generalisation while remaining lightweight.",
        "arxiv_id": "2510.17739",
        "ARXIVID": "2510.17739",
        "COMMENT": "Presents a new, training-free, descriptor-agnostic approach for visual place recognition and introduces a new benchmark (SotonMV), matching criterion 3 (new methods and benchmarks in embodied AI, simulator-related).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.16290": {
        "authors": [
            "Yue Zheng",
            "Xiufang Shi",
            "Jiming Chen",
            "Yuanchao Shu"
        ],
        "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
        "abstract": "arXiv:2510.16290v1 Announce Type: new  Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of Vision-Language Models (VLMs). While these models offer superior zero-shot detection capabilities, their immense computational cost and unstable visual grounding performance hinder real-time deployment. To overcome these challenges, we introduce Cerberus, a two-stage cascaded system designed for efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules offline, and combines lightweight filtering with fine-grained VLM reasoning during online inference. The performance gains of Cerberus come from two key innovations: motion mask prompting and rule-based deviation detection. The former directs the VLM's attention to regions relevant to motion, while the latter identifies anomalies as deviations from learned norms rather than enumerating possible anomalies. Extensive evaluations on four datasets show that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a 151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art VLM-based VAD methods, establishing it as a practical solution for real-time video analytics.",
        "arxiv_id": "2510.16290",
        "ARXIVID": "2510.16290",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes Cerberus, a cascaded VLM-based system for real-time video anomaly detection, with novel prompting and rule-based detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.17519": {
        "authors": [
            "Yongshun Zhang",
            "Zhongyi Fan",
            "Yonghang Zhang",
            "Zhangzikang Li",
            "Weifeng Chen",
            "Zhongwei Feng",
            "Chaoyue Wang",
            "Peng Hou",
            "Anxiang Zeng"
        ],
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
        "abstract": "arXiv:2510.17519v1 Announce Type: new  Abstract: In recent years, large-scale generative models for visual content (\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in \\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
        "arxiv_id": "2510.17519",
        "ARXIVID": "2510.17519",
        "COMMENT": "This paper presents MUG-V 10B, a large-scale video generation model with a high-efficiency training pipeline, and open-sources the full stack. It is relevant to criterion 4 (vision foundation models and applications), especially for generative modeling in multi-modal learning. The open-sourcing of Megatron-Core-based training for video generation is notable.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.16332": {
        "authors": [
            "Haiyue Sun",
            "Qingdong He",
            "Jinlong Peng",
            "Peng Tang",
            "Jiangning Zhang",
            "Junwei Zhu",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement",
        "abstract": "arXiv:2510.16332v1 Announce Type: new  Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image generation. However, these approaches for multiple reference generation struggle with decoupling different reference identities. In this work, we propose the TokenAR framework, specifically focused on a simple but effective token-level enhancement mechanism to address reference identity confusion problem. Such token-level enhancement consists of three parts, 1). Token Index Embedding clusters the tokens index for better representing the same reference images; 2). Instruct Token Injection plays as a role of extra visual feature container to inject detailed and complementary priors for reference tokens; 3). The identity-token disentanglement strategy (ITD) explicitly guides the token representations toward independently representing the features of each identity.This token-enhancement framework significantly augments the capabilities of existing AR based methods in conditional image generation, enabling good identity consistency while preserving high quality background reconstruction. Driven by the goal of high-quality and high-diversity in multi-subject generation, we introduce the InstructAR Dataset, the first open-source, large-scale, multi-reference input, open domain image generation dataset that includes 28K training pairs, each example has two reference subjects, a relative prompt and a background with mask annotation, curated for multiple reference image generation training and evaluating. Comprehensive experiments validate that our approach surpasses current state-of-the-art models in multiple reference image generation task. The implementation code and datasets will be made publicly. Codes are available, see https://github.com/lyrig/TokenAR",
        "arxiv_id": "2510.16332",
        "ARXIVID": "2510.16332",
        "COMMENT": "Matches criterion 2: Proposes TokenAR, a new autoregressive token-level enhancement framework for multi-subject image generation, and introduces a new dataset for multi-reference image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.17700": {
        "authors": [
            "Walter Simoncini",
            "Michael Dorkenwald",
            "Tijmen Blankevoort",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ],
        "title": "Elastic ViTs from Pretrained Models without Retraining",
        "abstract": "arXiv:2510.17700v1 Announce Type: new  Abstract: Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: Single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our approach efficiently combines gradient information with cross-network structure correlations, approximated via an evolutionary algorithm, does not require labeled data, generalizes to models without a classification head, and is retraining-free. Experiments on DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over state-of-the-art methods across various sparsities, requiring less than five minutes on a single A100 GPU to generate elastic models that can be adjusted to any computational budget. Our key contributions include an efficient pruning strategy for pretrained Vision Transformers, a novel evolutionary approximation of Hessian off-diagonal structures, and a self-supervised importance scoring mechanism that maintains strong performance without requiring retraining or labels. Code and pruned models are available at: https://elastic.ashita.nl/",
        "arxiv_id": "2510.17700",
        "ARXIVID": "2510.17700",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes SnapViT, a post-pretraining pruning method for Vision Transformers, enabling elastic inference without retraining or labels. Directly addresses deployment of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.16438": {
        "authors": [
            "Aidyn Ubingazhibov",
            "R\\'emi Pautrat",
            "Iago Su\\'arez",
            "Shaohui Liu",
            "Marc Pollefeys",
            "Viktor Larsson"
        ],
        "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching",
        "abstract": "arXiv:2510.16438v1 Announce Type: new  Abstract: Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion. The backbone of these pipelines are the local feature matchers, establishing correspondences across images. Traditionally, point and line matching have been treated as independent tasks. Recently, GlueStick proposed a GNN-based network that simultaneously operates on points and lines to establish matches. While running a single joint matching reduced the overall computational complexity, the heavy architecture prevented real-time applications or deployment to edge devices.   Inspired by recent progress in point matching, we propose LightGlueStick, a lightweight matcher for points and line segments. The key novel component in our architecture is the Attentional Line Message Passing (ALMP), which explicitly exposes the connectivity of the lines to the network, allowing for efficient communication between nodes. In thorough experiments we show that LightGlueStick establishes a new state-of-the-art across different benchmarks. The code is available at https://github.com/aubingazhib/LightGlueStick.",
        "arxiv_id": "2510.16438",
        "ARXIVID": "2510.16438",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents). Introduces a lightweight, state-of-the-art point-line matcher for spatial correspondence, with a novel attentional message passing module.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16196": {
        "authors": [
            "Zheng Huang",
            "Enpei Zhang",
            "Yinghao Cai",
            "Weikang Qiu",
            "Carl Yang",
            "Elynn Chen",
            "Xiang Zhang",
            "Rex Ying",
            "Dawei Zhou",
            "Yujun Yan"
        ],
        "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI",
        "abstract": "arXiv:2510.16196v1 Announce Type: new  Abstract: Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli, essentially images, from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pretrained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively. We present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision based space or a joint text image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute relationship search module that automatically identifies key attributes and relationships that best align with the neural activity. Extensive experiments on real world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction.",
        "arxiv_id": "2510.16196",
        "ARXIVID": "2510.16196",
        "COMMENT": "This paper proposes PRISM, a model for reconstructing visual stimuli from fMRI using structured text as an intermediate space, and finds that fMRI signals are more similar to text space than vision space. This is relevant to criterion 4 (vision foundation models and applications), as it uses generative models for image reconstruction and provides surprising empirical results about neural representations.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.16772": {
        "authors": [
            "Thuy Phuong Vu",
            "Dinh-Cuong Hoang",
            "Minhhuy Le",
            "Phan Xuan Tan"
        ],
        "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning",
        "abstract": "arXiv:2510.16772v1 Announce Type: new  Abstract: Recent research has made significant progress in localizing and editing image regions based on text. However, most approaches treat these regions in isolation, relying solely on local cues without accounting for how each part contributes to the overall visual and semantic composition. This often results in inconsistent edits, unnatural transitions, or loss of coherence across the image. In this work, we propose Region in Context, a novel framework for text-conditioned image editing that performs multilevel semantic alignment between vision and language, inspired by the human ability to reason about edits in relation to the whole scene. Our method encourages each region to understand its role within the global image context, enabling precise and harmonized changes. At its core, the framework introduces a dual-level guidance mechanism: regions are represented with full-image context and aligned with detailed region-level descriptions, while the entire image is simultaneously matched to a comprehensive scene-level description generated by a large vision-language model. These descriptions serve as explicit verbal references of the intended content, guiding both local modifications and global structure. Experiments show that it produces more coherent and instruction-aligned results. Code is available at: https://github.com/thuyvuphuong/Region-in-Context.git",
        "arxiv_id": "2510.16772",
        "ARXIVID": "2510.16772",
        "COMMENT": "Matches criterion 2: Proposes a new framework for text-conditioned image editing using multilevel semantic alignment with guidance from a large vision-language model.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16973": {
        "authors": [
            "Praveenbalaji Rajendran",
            "Mojtaba Safari",
            "Wenfeng He",
            "Mingzhe Hu",
            "Shansong Wang",
            "Jun Zhou",
            "Xiaofeng Yang"
        ],
        "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis",
        "abstract": "arXiv:2510.16973v1 Announce Type: new  Abstract: Recent advancements in artificial intelligence (AI), particularly foundation models (FMs), have revolutionized medical image analysis, demonstrating strong zero- and few-shot performance across diverse medical imaging tasks, from segmentation to report generation. Unlike traditional task-specific AI models, FMs leverage large corpora of labeled and unlabeled multimodal datasets to learn generalized representations that can be adapted to various downstream clinical applications with minimal fine-tuning. However, despite the rapid proliferation of FM research in medical imaging, the field remains fragmented, lacking a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities. To address this gap, this review article provides a comprehensive and structured analysis of FMs in medical image analysis. We systematically categorize studies into vision-only and vision-language FMs based on their architectural foundations, training strategies, and downstream clinical tasks. Additionally, a quantitative meta-analysis of the studies was conducted to characterize temporal trends in dataset utilization and application domains. We also critically discuss persistent challenges, including domain adaptation, efficient fine-tuning, computational constraints, and interpretability along with emerging solutions such as federated learning, knowledge distillation, and advanced prompting. Finally, we identify key future research directions aimed at enhancing the robustness, explainability, and clinical integration of FMs, thereby accelerating their translation into real-world medical practice.",
        "arxiv_id": "2510.16973",
        "ARXIVID": "2510.16973",
        "COMMENT": "Matches criterion 4: Systematic review and meta-analysis of vision foundation models in medical image analysis, including vision-language models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 5
    },
    "2510.17585": {
        "authors": [
            "Chuhong Wang",
            "Hua Li",
            "Chongyi Li",
            "Huazhong Liu",
            "Xiongxin Tang",
            "Sam Kwong"
        ],
        "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset",
        "abstract": "arXiv:2510.17585v1 Announce Type: new  Abstract: With the development of underwater exploration and marine protection, underwater vision tasks are widespread. Due to the degraded underwater environment, characterized by color distortion, low contrast, and blurring, camouflaged instance segmentation (CIS) faces greater challenges in accurately segmenting objects that blend closely with their surroundings. Traditional camouflaged instance segmentation methods, trained on terrestrial-dominated datasets with limited underwater samples, may exhibit inadequate performance in underwater scenes. To address these issues, we introduce the first underwater camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which comprises 3,953 images of camouflaged marine organisms with instance-level annotations. In addition, we propose an Underwater Camouflaged Instance Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM includes three key modules. First, the Channel Balance Optimization Module (CBOM) enhances channel characteristics to improve underwater feature learning, effectively addressing the model's limited understanding of underwater environments. Second, the Frequency Domain True Integration Module (FDTIM) is proposed to emphasize intrinsic object features and reduce interference from camouflage patterns, enhancing the segmentation performance of camouflaged objects blending with their surroundings. Finally, the Multi-scale Feature Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries of low-contrast camouflaged instances across multiple frequency bands, improving the model's ability to achieve more precise segmentation of camouflaged objects. Extensive experiments on the proposed UCIS4K and public benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.",
        "arxiv_id": "2510.17585",
        "ARXIVID": "2510.17585",
        "COMMENT": "This paper introduces the first underwater camouflaged instance segmentation dataset and a new segmentation network based on the Segment Anything Model, with modules for underwater feature learning and frequency domain integration. It is a new benchmark and method for a specific vision task, but not in the embodied AI or simulator context. Closest to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.16988": {
        "authors": [
            "Junhao Zhao",
            "Zishuai Liu",
            "Ruili Fang",
            "Jin Lu",
            "Linghan Zhang",
            "Fei Dou"
        ],
        "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams",
        "abstract": "arXiv:2510.16988v1 Announce Type: new  Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered ambient sensors is an essential task in Ambient Assisted Living, yet existing methods remain constrained by representation-level limitations. Sequence-based approaches preserve temporal order of sensor activations but are sensitive to noise and lack spatial awareness, while image-based approaches capture global patterns and implicit spatial correlations but compress fine-grained temporal dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation) fail to enforce alignment between sequence- and image-based representation views, underutilizing their complementary strengths. We propose Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an end-to-end framework that jointly optimizes representation learning via Sequence-Image Contrastive Alignment (SICA) and classification via cross-entropy, ensuring both cross-representation alignment and task-specific discriminability. CARE integrates (i) time-aware, noise-resilient sequence encoding with (ii) spatially-informed and frequency-sensitive image representations, and employs (iii) a joint contrastive-classification objective for end-to-end learning of aligned and discriminative embeddings. Evaluated on three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to sensor malfunctions and layout variability, highlighting its potential for reliable ADL recognition in smart homes.",
        "arxiv_id": "2510.16988",
        "ARXIVID": "2510.16988",
        "COMMENT": "This paper proposes CARE, a contrastive alignment framework for ADL recognition from event-triggered sensor streams, integrating temporal and spatial representations. It is relevant to criterion 1 (methodological improvements to spatial understanding on embodied agents), as it addresses spatial awareness and representation learning in sensor-based activity recognition.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.17684": {
        "authors": [
            "Xinwei Zhang",
            "Hu Chen",
            "Zhe Yuan",
            "Sukun Tian",
            "Peng Feng"
        ],
        "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model",
        "abstract": "arXiv:2510.17684v1 Announce Type: new  Abstract: Foundation models for medical image segmentation have achieved remarkable performance. Adaptive fine-tuning of natural image segmentation foundation models is crucial for medical image segmentation tasks. However, some limitations exist in existing fine-tuning methods: 1) insufficient representation of high-level features and 2) the fine-tuning process disrupts the structural integrity of pretrained weights. Inspired by these critical problems, we propose an intelligent communication mixture-of-experts boosted-medical image segmentation foundation model, named IC-MoE, with twofold ideas: 1) We construct basic experts, semantic experts, and adaptive experts. Moreover, we implement a pixel probability adaptive voting strategy, which enables expert selection and fusion through label consistency and load balancing. This approach preliminarily enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. 2) We propose a semantic-guided contrastive learning method to address the issue of weak supervision in contrastive learning. This method further enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. Extensive experiments across three public medical image segmentation datasets demonstrate that the IC-MoE outperforms other SOTA models. Consequently, the proposed IC-MoE effectively supplements foundational medical image segmentation models with high-level features and pretrained structural integrity. We also validate the superior generalizability of the IC-MoE across diverse medical image segmentation scenarios.",
        "arxiv_id": "2510.17684",
        "ARXIVID": "2510.17684",
        "COMMENT": "This paper introduces IC-MoE, a mixture-of-experts foundation model for medical image segmentation, with new expert selection and fusion strategies. It is relevant to criterion 4 (vision foundation models and applications), as it proposes a new method for adapting vision foundation models to medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.17611": {
        "authors": [
            "Jia Guo",
            "Shuai Lu",
            "Lei Fan",
            "Zelin Li",
            "Donglin Di",
            "Yang Song",
            "Weihang Zhang",
            "Wenbing Zhu",
            "Hong Yan",
            "Fang Chen",
            "Huiqi Li",
            "Hongen Liao"
        ],
        "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
        "abstract": "arXiv:2510.17611v1 Announce Type: new  Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized single-class models to unified multi-class models, yet existing multi-class models significantly underperform the most advanced one-for-one counterparts. Moreover, the field has fragmented into specialized methods tailored to specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment barriers and highlighting the need for a unified solution. In this paper, we present Dinomaly2, the first unified framework for full-spectrum image UAD, which bridges the performance gap in multi-class models while seamlessly extending across diverse data modalities and task settings. Guided by the \"less is more\" philosophy, we demonstrate that the orchestration of five simple element achieves superior performance in a standard reconstruction-based framework. This methodological minimalism enables natural extension across diverse tasks without modification, establishing that simplicity is the foundation of true universality. Extensive experiments on 12 UAD benchmarks demonstrate Dinomaly2's full-spectrum superiority across multiple modalities (2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class, inference-unified multi-class, few-shot) and application domains (industrial, biological, outdoor). For example, our multi-class model achieves unprecedented 99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art performance with minimum adaptations. Moreover, using only 8 normal examples per class, our method surpasses previous full-shot models, achieving 98.7% and 97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design, computational scalability, and universal applicability positions Dinomaly2 as a unified solution for the full spectrum of real-world anomaly detection applications.",
        "arxiv_id": "2510.17611",
        "ARXIVID": "2510.17611",
        "COMMENT": "This paper presents Dinomaly2, a unified framework for full-spectrum unsupervised anomaly detection across multiple modalities and task settings. While it is a strong vision paper with a new universal method and benchmark, it is not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models per se. Closest to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.16822": {
        "authors": [
            "Yahia Battach",
            "Abdulwahab Felemban",
            "Faizan Farooq Khan",
            "Yousef A. Radwan",
            "Xiang Li",
            "Fabio Marchese",
            "Sara Beery",
            "Burton H. Jones",
            "Francesca Benzoni",
            "Mohamed Elhoseiny"
        ],
        "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification",
        "abstract": "arXiv:2510.16822v1 Announce Type: new  Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as climate change, underscoring the urgent need for scalable, automated monitoring. We introduce ReefNet, a large public coral reef image dataset with point-label annotations mapped to the World Register of Marine Species (WoRMS). ReefNet aggregates imagery from 76 curated CoralNet sources and an additional site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level hard coral annotations with expert-verified labels. Unlike prior datasets, which are often limited by size, geography, or coarse labels and are not ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global scale to WoRMS. We propose two evaluation settings: (i) a within-source benchmark that partitions each source's images for localized evaluation, and (ii) a cross-source benchmark that withholds entire sources to test domain generalization. We analyze both supervised and zero-shot classification performance on ReefNet and find that while supervised within-source performance is promising, supervised performance drops sharply across domains, and performance is low across the board for zero-shot models, especially for rare and visually similar genera. This provides a challenging benchmark intended to catalyze advances in domain generalization and fine-grained coral classification. We will release our dataset, benchmarking code, and pretrained models to advance robust, domain-adaptive, global coral reef monitoring and conservation.",
        "arxiv_id": "2510.16822",
        "ARXIVID": "2510.16822",
        "COMMENT": "This paper introduces ReefNet, a large-scale, taxonomically enriched dataset and benchmark for hard coral classification, with a focus on domain generalization and fine-grained recognition. It is a new vision benchmark (criterion 4: vision foundation models and applications), but not directly about embodied AI or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.16326": {
        "authors": [
            "Yi Wei (College of Information Science",
            "Electronic Engineering",
            "Zhejiang University",
            "Hangzhou",
            "China)",
            "Shunpu Tang (College of Information Science",
            "Electronic Engineering",
            "Zhejiang University",
            "Hangzhou",
            "China)",
            "Liang Zhao (College of Information Science",
            "Electronic Engineering",
            "Zhejiang University",
            "Hangzhou",
            "China)",
            "Qiangian Yang (College of Information Science",
            "Electronic Engineering",
            "Zhejiang University",
            "Hangzhou",
            "China)"
        ],
        "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution",
        "abstract": "arXiv:2510.16326v1 Announce Type: new  Abstract: Recent advances in diffusion models have driven remarkable progress in image generation. However, the generation process remains computationally intensive, and users often need to iteratively refine prompts to achieve the desired results, further increasing latency and placing a heavy burden on cloud resources. To address this challenge, we propose DiffusionX, a cloud-edge collaborative framework for efficient multi-round, prompt-based generation. In this system, a lightweight on-device diffusion model interacts with users by rapidly producing preview images, while a high-capacity cloud model performs final refinements after the prompt is finalized. We further introduce a noise level predictor that dynamically balances the computation load, optimizing the trade-off between latency and cloud workload. Experiments show that DiffusionX reduces average generation time by 15.8% compared with Stable Diffusion v1.5, while maintaining comparable image quality. Moreover, it is only 0.9% slower than Tiny-SD with significantly improved image quality, thereby demonstrating efficiency and scalability with minimal overhead.",
        "arxiv_id": "2510.16326",
        "ARXIVID": "2510.16326",
        "COMMENT": "This paper presents a new edge-cloud collaborative framework for efficient image generation with diffusion models, which is relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.17299": {
        "authors": [
            "Siran Dai",
            "Qianqian Xu",
            "Peisong Wen",
            "Yang Liu",
            "Qingming Huang"
        ],
        "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
        "abstract": "arXiv:2510.17299v1 Announce Type: new  Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised learning (SSL): longer training may impair the performance of dense prediction tasks (e.g., semantic segmentation). We refer to this phenomenon as Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence across sixteen state-of-the-art SSL methods with various losses, architectures, and datasets. When the model performs suboptimally on dense tasks at the end of training, measuring the performance during training becomes essential. However, evaluating dense performance effectively without annotations remains an open challenge. To tackle this issue, we introduce a Dense representation Structure Estimator (DSE), composed of a class-relevance measure and an effective dimensionality measure. The proposed DSE is both theoretically grounded and empirically validated to be closely correlated with the downstream performance. Based on this metric, we introduce a straightforward yet effective model selection strategy and a DSE-based regularization method. Experiments on sixteen SSL methods across four benchmarks confirm that model selection improves mIoU by $3.0\\%$ on average with negligible computational cost. Additionally, DSE regularization consistently mitigates the effects of dense degradation. Code is available at https://github.com/EldercatSAM/SSL-Degradation.",
        "arxiv_id": "2510.17299",
        "ARXIVID": "2510.17299",
        "COMMENT": "This paper investigates a counterintuitive phenomenon in self-supervised learning for dense prediction tasks and introduces a new estimator and regularization method. It provides surprising empirical results and clever statistical tricks, but does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, or vision foundation models. General CV/ML interest.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.16791": {
        "authors": [
            "Chengxuan Zhu",
            "Shuchen Weng",
            "Jiacong Fang",
            "Peixuan Zhang",
            "Si Li",
            "Chao Xu",
            "Boxin Shi"
        ],
        "title": "Personalized Image Filter: Mastering Your Photographic Style",
        "abstract": "arXiv:2510.16791v1 Announce Type: new  Abstract: Photographic style, as a composition of certain photographic concepts, is the charm behind renowned photographers. But learning and transferring photographic style need a profound understanding of how the photo is edited from the unknown original appearance. Previous works either fail to learn meaningful photographic concepts from reference images, or cannot preserve the content of the content image. To tackle these issues, we proposed a Personalized Image Filter (PIF). Based on a pretrained text-to-image diffusion model, the generative prior enables PIF to learn the average appearance of photographic concepts, as well as how to adjust them according to text prompts. PIF then learns the photographic style of reference images with the textual inversion technique, by optimizing the prompts for the photographic concepts. PIF shows outstanding performance in extracting and transferring various kinds of photographic style. Project page: https://pif.pages.dev/",
        "arxiv_id": "2510.16791",
        "ARXIVID": "2510.16791",
        "COMMENT": "This paper proposes a personalized image filter using a pretrained text-to-image diffusion model and textual inversion for learning and transferring photographic style. It is a generative modeling paper with multi-modal learning aspects, but does not directly address spatial understanding, embodied agents, or VLLMs/MLLMs. Closest to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.16342": {
        "authors": [
            "Tong Zhang",
            "Ru Zhang",
            "Jianyi Liu",
            "Zhen Yang",
            "Gongshen Liu"
        ],
        "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts",
        "abstract": "arXiv:2510.16342v1 Announce Type: new  Abstract: Existing concept erasure methods for text-to-image diffusion models commonly rely on fixed anchor strategies, which often lead to critical issues such as concept re-emergence and erosion. To address this, we conduct causal tracing to reveal the inherent sensitivity of erasure to anchor selection and define Sibling Exclusive Concepts as a superior class of anchors. Based on this insight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for Contextual Targeting), a dynamic anchor selection framework designed to overcome the limitations of fixed anchors. Our framework introduces a novel two-stage evaluation mechanism that automatically discovers optimal anchors for precise erasure while identifying critical boundary anchors to preserve related concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor solution, not only efficiently adapts to multiple erasure frameworks but also consistently outperforms existing baselines across key performance metrics, averaging only 4 seconds for anchor mining of a single concept.",
        "arxiv_id": "2510.16342",
        "ARXIVID": "2510.16342",
        "COMMENT": "This paper proposes SELECT, a new dynamic anchor selection framework for concept erasure in text-to-image diffusion models. It is relevant to criterion 4 (vision foundation models and applications), as it deals with text-to-image diffusion models and concept erasure, which is a hot topic in generative vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.15980": {
        "authors": [
            "Dong Liu",
            "Yanxuan Yu"
        ],
        "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition",
        "abstract": "arXiv:2510.15980v1 Announce Type: new  Abstract: We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level interpretability framework for deep models, inspired by Cognitive Load Theory in human cognition. CLTs are defined as symbolic, temporally varying functions that quantify model-internal resource allocation. Formally, we represent CLTs as a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t, \\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and \\emph{Germane} load. Each component is instantiated through measurable proxies such as attention entropy, KV-cache miss ratio, representation dispersion, and decoding stability. We propose both symbolic formulations and visualization methods (load curves, simplex diagrams) that enable interpretable analysis of reasoning dynamics. Experiments on reasoning and planning benchmarks show that CLTs predict error-onset, reveal cognitive strategies, and enable load-guided interventions that improve reasoning efficiency by 15-30\\% while maintaining accuracy.",
        "arxiv_id": "2510.15980",
        "ARXIVID": "2510.15980",
        "COMMENT": "This paper introduces Cognitive Load Traces (CLTs) as a new interpretability framework for deep models, with symbolic and visual accounts of model cognition. It proposes new visualization and symbolic methods for analyzing reasoning and planning in deep models. This is most closely related to criterion 1 (methodological improvements to spatial understanding/spatial intelligence on embodied agents), though it is more about interpretability and reasoning than spatial intelligence per se.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.16660": {
        "authors": [
            "Yuntian Wang",
            "Xilin Yang",
            "Che-Yung Shen",
            "Nir Pillar",
            "Aydogan Ozcan"
        ],
        "title": "Universal and Transferable Attacks on Pathology Foundation Models",
        "abstract": "arXiv:2510.16660v1 Announce Type: new  Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for pathology foundation models that reveal critical vulnerabilities in their capabilities. Optimized using deep learning, UTAP comprises a fixed and weak noise pattern that, when added to a pathology image, systematically disrupts the feature representation capabilities of multiple pathology foundation models. Therefore, UTAP induces performance drops in downstream tasks that utilize foundation models, including misclassification across a wide range of unseen data distributions. In addition to compromising the model performance, we demonstrate two key features of UTAP: (1) universality: its perturbation can be applied across diverse field-of-views independent of the dataset that UTAP was developed on, and (2) transferability: its perturbation can successfully degrade the performance of various external, black-box pathology foundation models - never seen before. These two features indicate that UTAP is not a dedicated attack associated with a specific foundation model or image dataset, but rather constitutes a broad threat to various emerging pathology foundation models and their applications. We systematically evaluated UTAP across various state-of-the-art pathology foundation models on multiple datasets, causing a significant drop in their performance with visually imperceptible modifications to the input images using a fixed noise pattern. The development of these potent attacks establishes a critical, high-standard benchmark for model robustness evaluation, highlighting a need for advancing defense mechanisms and potentially providing the necessary assets for adversarial training to ensure the safe and reliable deployment of AI in pathology.",
        "arxiv_id": "2510.16660",
        "ARXIVID": "2510.16660",
        "COMMENT": "This paper introduces universal and transferable adversarial attacks on pathology foundation models, revealing vulnerabilities and providing a new robustness benchmark. It is relevant to vision foundation models and their robustness (criterion 4), but not directly about embodied AI or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.16865": {
        "authors": [
            "Yuyang Yu",
            "Zhengwei Chen",
            "Xuemiao Xu",
            "Lei Zhang",
            "Haoxin Yang",
            "Yongwei Nie",
            "Shengfeng He"
        ],
        "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection",
        "abstract": "arXiv:2510.16865v1 Announce Type: new  Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality control, aiming to identify structural defects with high reliability. However, current memory bank-based methods often suffer from inconsistent feature transformations and limited discriminative capacity, particularly in capturing local geometric details and achieving rotation invariance. These limitations become more pronounced when registration fails, leading to unreliable detection results. We argue that point-cloud registration plays an essential role not only in aligning geometric structures but also in guiding feature extraction toward rotation-invariant and locally discriminative representations. To this end, we propose a registration-induced, rotation-invariant feature extraction framework that integrates the objectives of point-cloud registration and memory-based anomaly detection. Our key insight is that both tasks rely on modeling local geometric structures and leveraging feature similarity across samples. By embedding feature extraction into the registration learning process, our framework jointly optimizes alignment and representation learning. This integration enables the network to acquire features that are both robust to rotations and highly effective for anomaly detection. Extensive experiments on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method consistently outperforms existing approaches in effectiveness and generalizability.",
        "arxiv_id": "2510.16865",
        "ARXIVID": "2510.16865",
        "COMMENT": "This paper proposes a registration-induced, rotation-invariant feature extraction framework for 3D anomaly detection in point clouds. While it introduces a new method for spatial understanding (rotation invariance, geometric alignment), it is not directly about embodied agents or VLLMs/MLLMs, but is relevant to spatial intelligence (criterion 1).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.17105": {
        "authors": [
            "Xiaogang Xu",
            "Jian Wang",
            "Yunfan Lu",
            "Ruihang Chu",
            "Ruixing Wang",
            "Jiafei Wu",
            "Bei Yu",
            "Liang Lin"
        ],
        "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
        "abstract": "arXiv:2510.17105v1 Announce Type: new  Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable Diffusion via ControlNet, have achieved remarkable performance in several low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods often sacrifice content fidelity to attain higher perceptual realism. This issue is exacerbated in low-light scenarios, where severely degraded information caused by the darkness limits effective control. We identify two primary causes of fidelity loss: the absence of suitable conditional latent modeling and the lack of bidirectional interaction between the conditional latent and noisy latent in the diffusion process. To address this, we propose a novel optimization strategy for conditioning in pre-trained diffusion models, enhancing fidelity while preserving realism and aesthetics. Our method introduces a mechanism to recover spatial details lost during VAE encoding, i.e., a latent refinement pipeline incorporating generative priors. Additionally, the refined latent condition interacts dynamically with the noisy latent, leading to improved restoration performance. Our approach is plug-and-play, seamlessly integrating into existing diffusion networks to provide more effective control. Extensive experiments demonstrate significant fidelity improvements in PTDB methods.",
        "arxiv_id": "2510.17105",
        "ARXIVID": "2510.17105",
        "COMMENT": "This paper proposes a new method for improving fidelity in pre-trained diffusion models for low-light image enhancement. It introduces a novel conditioning and latent refinement pipeline, which is relevant to generative modeling in vision, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or benchmarks. Closest to criterion 4 (vision foundation models and applications), as it leverages pre-trained diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.16777": {
        "authors": [
            "Junbo Li",
            "Weimin Yuan",
            "Yinuo Wang",
            "Yue Zeng",
            "Shihao Shu",
            "Cai Meng",
            "Xiangzhi Bai"
        ],
        "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation",
        "abstract": "arXiv:2510.16777v1 Announce Type: new  Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer vision, and current research typically predicts the 6D pose by establishing correspondences between 2D image features and 3D model features. However, these methods often face difficulties with textureless objects and varying illumination conditions. To overcome these limitations, we propose GS2POSE, a novel approach for 6D object pose estimation. GS2POSE formulates a pose regression algorithm inspired by the principles of Bundle Adjustment (BA). By leveraging Lie algebra, we extend the capabilities of 3DGS to develop a pose-differentiable rendering pipeline, which iteratively optimizes the pose by comparing the input image to the rendered image. Additionally, GS2POSE updates color parameters within the 3DGS model, enhancing its adaptability to changes in illumination. Compared to previous models, GS2POSE demonstrates accuracy improvements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and LineMod datasets, respectively.",
        "arxiv_id": "2510.16777",
        "ARXIVID": "2510.16777",
        "COMMENT": "Somewhat related to criterion 4: Proposes GS2POSE, a method for 6D object pose estimation using Gaussian Splatting and pose-differentiable rendering, which is a novel application of generative modeling in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.16136": {
        "authors": [
            "Sayan Deb Sarkar",
            "Sinisa Stekovic",
            "Vincent Lepetit",
            "Iro Armeni"
        ],
        "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
        "abstract": "arXiv:2510.16136v1 Announce Type: new  Abstract: Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions.",
        "arxiv_id": "2510.16136",
        "ARXIVID": "2510.16136",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications): Proposes a new method for 3D appearance transfer using pretrained rectified flow models, with a novel guidance mechanism and evaluation protocol. While not directly about foundation models, it leverages pretrained models and introduces a new evaluation approach.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.16382": {
        "authors": [
            "Ze Tao",
            "Jian Zhang",
            "Haowei Li",
            "Xianshuai Li",
            "Yifei Peng",
            "Xiyao Liu",
            "Senzhang Wang",
            "Chao Liu",
            "Sheng Ren",
            "Shichao Zhang"
        ],
        "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization",
        "abstract": "arXiv:2510.16382v1 Announce Type: new  Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a novel causal framework inspired by human intelligence, designed to overcome the limitations of conventional domain generalization models. Unlike approaches that rely on statistics to capture data-label dependencies and learn distortion-invariant representations, HSCM replicates the hierarchical processing and multi-level learning of human vision systems, focusing on modeling fine-grained causal mechanisms. By disentangling and reweighting key image attributes such as color, texture, and shape, HSCM enhances generalization across diverse domains, ensuring robust performance and interpretability. Leveraging the flexibility and adaptability of human intelligence, our approach enables more effective transfer and learning in dynamic, complex environments. Through both theoretical and empirical evaluations, we demonstrate that HSCM outperforms existing domain generalization models, providing a more principled method for capturing causal relationships and improving model robustness. The code is available at https://github.com/lambett/HSCM.",
        "arxiv_id": "2510.16382",
        "ARXIVID": "2510.16382",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications) via causal representation learning for domain generalization, inspired by human vision. Not directly about VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.17529": {
        "authors": [
            "Yovin Yahathugoda",
            "Davide Prezzi",
            "Piyalitt Ittichaiwong",
            "Vicky Goh",
            "Sebastien Ourselin",
            "Michela Antonelli"
        ],
        "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
        "abstract": "arXiv:2510.17529v1 Announce Type: new  Abstract: Active Surveillance (AS) is a treatment option for managing low and intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while monitoring disease progression through serial MRI and clinical follow-up. Accurate prostate segmentation is an important preliminary step for automating this process, enabling automated detection and diagnosis of PCa. However, existing deep-learning segmentation models are often trained on single-time-point and expertly annotated datasets, making them unsuitable for longitudinal AS analysis, where multiple time points and a scarcity of expert labels hinder their effective fine-tuning. To address these challenges, we propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation architecture that computes the segmentation for time point t by leveraging the MRI and the corresponding segmentation mask from the previous time point. We introduce two new components: (i) a Mamba-enhanced Cross-Attention Module, which integrates the Mamba block into cross attention to efficiently capture temporal evolution and long-range spatial dependencies, and (ii) a Shape Extractor Module that encodes the previous segmentation mask into a latent anatomical representation for refined zone delination. Moreover, we introduce a semi-supervised self-training strategy that leverages pseudo-labels generated from a pre-trained nnU-Net, enabling effective learning without expert annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results showed that it significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even when trained on limited and noisy data.",
        "arxiv_id": "2510.17529",
        "ARXIVID": "2510.17529",
        "COMMENT": "This paper introduces a new semi-supervised dual-scan 3D segmentation architecture for longitudinal MRI, with a Mamba-enhanced cross-attention module for temporal and spatial dependencies. It is a methodological improvement in spatial intelligence for medical imaging, but not for embodied agents or vision-language models. Closest to criterion 1 (spatial understanding), but not in the embodied agent context.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.17171": {
        "authors": [
            "Feihong Yan",
            "Peiru Wang",
            "Yao Zhu",
            "Kaiyu Pang",
            "Qingyan Wei",
            "Huiqi Li",
            "Linfeng Zhang"
        ],
        "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
        "abstract": "arXiv:2510.17171v1 Announce Type: new  Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual generation than autoregressive (AR) models for the ability of parallel generation, yet their acceleration potential remains constrained by the modeling complexity of spatially correlated visual tokens in a single step. To address this limitation, we introduce Generation then Reconstruction (GtR), a training-free hierarchical sampling strategy that decomposes generation into two stages: structure generation establishing global semantic scaffolding, followed by detail reconstruction efficiently completing remaining tokens. Assuming that it is more difficult to create an image from scratch than to complement images based on a basic image framework, GtR is designed to achieve acceleration by computing the reconstruction stage quickly while maintaining the generation quality by computing the generation stage slowly. Moreover, observing that tokens on the details of an image often carry more semantic information than tokens in the salient regions, we further propose Frequency-Weighted Token Selection (FTS) to offer more computation budget to tokens on image details, which are localized based on the energy of high frequency information. Extensive experiments on ImageNet class-conditional and text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), substantially outperforming existing acceleration methods across various model scales and generation tasks. Our codes will be released in https://github.com/feihongyan1/GtR.",
        "arxiv_id": "2510.17171",
        "ARXIVID": "2510.17171",
        "COMMENT": "This paper proposes a new hierarchical sampling strategy (Generation then Reconstruction) and a frequency-weighted token selection for accelerating masked autoregressive models in visual generation. It is relevant to generative modeling in computer vision, but does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.16730": {
        "authors": [
            "Tianyang Dou",
            "Ming Li",
            "Jiangying Qin",
            "Xuan Liao",
            "Jiageng Zhong",
            "Armin Gruen",
            "Mengyi Deng"
        ],
        "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
        "abstract": "arXiv:2510.16730v1 Announce Type: new  Abstract: Coral reefs are vital yet fragile ecosystems that require accurate large-scale mapping for effective conservation. Although global products such as the Allen Coral Atlas provide unprecedented coverage of global coral reef distri-bution, their predictions are frequently limited in spatial precision and semantic consistency, especially in regions requiring fine-grained boundary delineation. To address these challenges, we propose UKANFormer, a novel se-mantic segmentation model designed to achieve high-precision mapping under noisy supervision derived from Allen Coral Atlas. Building upon the UKAN architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans) block in the decoder, enabling the extraction of both global semantic structures and local boundary details. In experiments, UKANFormer achieved a coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming conventional baselines under the same noisy labels setting. Remarkably, the model produces predictions that are visually and structurally more accurate than the noisy labels used for training. These results challenge the notion that data quality directly limits model performance, showing that architectural design can mitigate label noise and sup-port scalable mapping under imperfect supervision. UKANFormer provides a foundation for ecological monitoring where reliable labels are scarce.",
        "arxiv_id": "2510.16730",
        "ARXIVID": "2510.16730",
        "COMMENT": "Somewhat related to criterion 4: Proposes UKANFormer, a new semantic segmentation model for coral reef mapping, showing that architectural design can overcome noisy labels, but not directly about foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.17188": {
        "authors": [
            "Vaibhav Rathore",
            "Divyam Gupta",
            "Biplab Banerjee"
        ],
        "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
        "abstract": "arXiv:2510.17188v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into either seen categories** -- available during training -- or novel ones, without relying on label supervision. Most existing GCD methods assume simultaneous access to labeled and unlabeled data during training and arising from the same domain, limiting applicability in open-world scenarios involving distribution shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by requiring models to generalize to unseen domains containing novel categories, without accessing targetdomain data during training. The only prior DG-GCD method, DG2CD-Net, relies on episodic training with multiple synthetic domains and task vector aggregation, incurring high computational cost and error accumulation. We propose HIDISC, a hyperbolic representation learning framework that achieves domain and category-level generalization without episodic simulation. To expose the model to minimal but diverse domain variations, we augment the source domain using GPT-guided diffusion, avoiding overfitting while maintaining efficiency. To structure the representation space, we introduce Tangent CutMix, a curvature-aware interpolation that synthesizes pseudo-novel samples in tangent space, preserving manifold consistency. A unified loss -- combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion -- **facilitates compact, semantically structured embeddings. A learnable curvature parameter further adapts the geometry to dataset complexity. HIDISC achieves state-of-the-art results on PACS , Office-Home , and DomainNet, consistently outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
        "arxiv_id": "2510.17188",
        "ARXIVID": "2510.17188",
        "COMMENT": "Does not match any specific criterion. Focuses on domain generalization and category discovery with hyperbolic representation learning, but not in a vision-language or embodied AI context.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.17181": {
        "authors": [
            "Haonan He",
            "Yufeng Zheng",
            "Jie Song"
        ],
        "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video",
        "abstract": "arXiv:2510.17181v1 Announce Type: new  Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR. However, most methods focus solely on facial regions, ignoring natural hand-face interactions, such as a hand resting on the chin or fingers gently touching the cheek, which convey cognitive states like pondering. In this work, we present a novel framework that jointly learns detailed head avatars and the non-rigid deformations induced by hand-face interactions.   There are two principal challenges in this task. First, naively tracking hand and face separately fails to capture their relative poses. To overcome this, we propose to combine depth order loss with contact regularization during pose tracking, ensuring correct spatial relationships between the face and hand. Second, no publicly available priors exist for hand-induced deformations, making them non-trivial to learn from monocular videos. To address this, we learn a PCA basis specific to hand-induced facial deformations from a face-hand interaction dataset. This reduces the problem to estimating a compact set of PCA parameters rather than a full spatial deformation field. Furthermore, inspired by physics-based simulation, we incorporate a contact loss that provides additional supervision, significantly reducing interpenetration artifacts and enhancing the physical plausibility of the results.   We evaluate our approach on RGB(D) videos captured by an iPhone. Additionally, to better evaluate the reconstructed geometry, we construct a synthetic dataset of avatars with various types of hand interactions. We show that our method can capture better appearance and more accurate deforming geometry of the face than SOTA surface reconstruction methods.",
        "arxiv_id": "2510.17181",
        "ARXIVID": "2510.17181",
        "COMMENT": "Relevant to 3D vision and avatar modeling, but does not directly match any specific criterion. Proposes a new framework for capturing head avatars with hand contacts from monocular video.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17699": {
        "authors": [
            "Aleksandr Oganov",
            "Ilya Bykov",
            "Eva Neudachina",
            "Mishan Aliev",
            "Alexander Tolmachev",
            "Alexander Sidorov",
            "Aleksandr Zuev",
            "Andrey Okhotin",
            "Denis Rakitin",
            "Aibek Alanov"
        ],
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "abstract": "arXiv:2510.17699v1 Announce Type: new  Abstract: While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
        "arxiv_id": "2510.17699",
        "ARXIVID": "2510.17699",
        "COMMENT": "Relevant to generative modeling, but does not directly match any specific criterion. Proposes a new adversarial solver for diffusion ODEs, improving sample quality and efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17484": {
        "authors": [
            "Muhammad Umer Ramzan",
            "Ali Zia",
            "Abdelwahed Khamis",
            "Noman Ali",
            "Usman Ali",
            "Wei Xiang"
        ],
        "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
        "abstract": "arXiv:2510.17484v1 Announce Type: new  Abstract: Salient object detection (SOD) aims to segment visually prominent regions in images and serves as a foundational task for various computer vision applications. We posit that SOD can now reach near-supervised accuracy without a single pixel-level label, but only when reliable pseudo-masks are available. We revisit the prototype-based line of work and make two key observations. First, boundary pixels and interior pixels obey markedly different geometry; second, the global consistency enforced by optimal transport (OT) is underutilized if prototype quality is weak. To address this, we introduce POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's single k-means step with an entropy-guided dual-clustering head: high-entropy pixels are organized by spectral clustering, low-entropy pixels by k-means, and the two prototype sets are subsequently aligned by OT. This split-fuse-transport design yields sharper, part-aware pseudo-masks in a single forward pass, without handcrafted priors. Those masks supervise a standard MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end unsupervised SOD pipeline that eliminates SelfMask's offline voting yet improves both accuracy and training efficiency. Extensive experiments on five benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure, further narrowing the gap to fully supervised models.",
        "arxiv_id": "2510.17484",
        "ARXIVID": "2510.17484",
        "COMMENT": "This paper introduces a new annotation-free saliency detection method using dual clustering and optimal transport alignment, resulting in an unsupervised SOD pipeline. It is a novel method in computer vision, but does not address embodied agents, VLLMs/MLLMs, or vision foundation models. General CV interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17318": {
        "authors": [
            "Sangyoon Bae",
            "Jiook Cha"
        ],
        "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
        "abstract": "arXiv:2510.17318v1 Announce Type: new  Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental limitations in fMRI-based causal inference: the ill-posed nature of inferring neural causality from hemodynamically distorted BOLD signals and the computational intractability of existing methods like Dynamic Causal Modeling (DCM). Our approach decomposes this complex inverse problem into two tractable stages: BOLD deconvolution to recover latent neural activity, followed by causal graph inference using a novel Conditional Mamba architecture. On simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically, when applied to real task fMRI data, our method recovers well-established neural pathways with 88% fidelity, whereas conventional approaches fail to identify these canonical circuits in over 99% of subjects. Furthermore, our network analysis of working memory data reveals that the brain strategically shifts its primary causal hub-recruiting executive or salience networks depending on the stimulus-a sophisticated reconfiguration that remains undetected by traditional methods. This work provides neuroscientists with a practical tool for large-scale causal inference that captures both fundamental circuit motifs and flexible network dynamics underlying cognitive function.",
        "arxiv_id": "2510.17318",
        "ARXIVID": "2510.17318",
        "COMMENT": "Not directly matching any criterion, but uses state space models for neural causal inference, which is outside the main focus on spatial intelligence, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16614": {
        "authors": [
            "Xuan Zhang",
            "Ruixiao Li",
            "Zhijian Zhou",
            "Long Li",
            "Yulei Qin",
            "Ke Li",
            "Xing Sun",
            "Xiaoyu Tan",
            "Chao Qu",
            "Yuan Qi"
        ],
        "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards",
        "abstract": "arXiv:2510.16614v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.",
        "arxiv_id": "2510.16614",
        "ARXIVID": "2510.16614",
        "COMMENT": "Does not match any specific criterion. Proposes a count-based intrinsic reward for LLM reasoning, but not in a multimodal or vision context.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16742": {
        "authors": [
            "Paul Saves",
            "Pramudita Satria Palar",
            "Muhammad Daffa Robani",
            "Nicolas Verstaevel",
            "Moncef Garouani",
            "Julien Aligon",
            "Benoit Gaudou",
            "Koji Shimoyama",
            "Joseph Morlier"
        ],
        "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration",
        "abstract": "arXiv:2510.16742v1 Announce Type: new  Abstract: Complex systems are increasingly explored through simulation-driven engineering workflows that combine physics-based and empirical models with optimization and analytics. Despite their power, these workflows face two central obstacles: (1) high computational cost, since accurate exploration requires many expensive simulator runs; and (2) limited transparency and reliability when decisions rely on opaque blackbox components. We propose a workflow that addresses both challenges by training lightweight emulators on compact designs of experiments that (i) provide fast, low-latency approximations of expensive simulators, (ii) enable rigorous uncertainty quantification, and (iii) are adapted for global and local Explainable Artificial Intelligence (XAI) analyses. This workflow unifies every simulation-based complex-system analysis tool, ranging from engineering design to agent-based models for socio-environmental understanding. In this paper, we proposea comparative methodology and practical recommendations for using surrogate-based explainability tools within the proposed workflow. The methodology supports continuous and categorical inputs, combines global-effect and uncertainty analyses with local attribution, and evaluates the consistency of explanations across surrogate models, thereby diagnosing surrogate adequacy and guiding further data collection or model refinement. We demonstrate the approach on two contrasting case studies: a multidisciplinary design analysis of a hybrid-electric aircraft and an agent-based model of urban segregation. Results show that the surrogate model and XAI coupling enables large-scale exploration in seconds, uncovers nonlinear interactions and emergent behaviors, identifies key design and policy levers, and signals regions where surrogates require more data or alternative architectures.",
        "arxiv_id": "2510.16742",
        "ARXIVID": "2510.16742",
        "COMMENT": "Presents a workflow for surrogate modeling and explainable AI for simulation exploration, including agent-based models, but does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17064": {
        "authors": [
            "Rongbin Li",
            "Wenbo Chen",
            "Zhao Li",
            "Rodrigo Munoz-Castaneda",
            "Jinbo Li",
            "Neha S. Maurya",
            "Arnav Solanki",
            "Huan He",
            "Hanwen Xing",
            "Meaghan Ramlakhan",
            "Zachary Wise",
            "Zhuhao Wu",
            "Hua Xu",
            "Michael Hawrylycz",
            "W. Jim Zheng"
        ],
        "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation",
        "abstract": "arXiv:2510.17064v1 Announce Type: new  Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.",
        "arxiv_id": "2510.17064",
        "ARXIVID": "2510.17064",
        "COMMENT": "Describes a multi-agent AI system and LLMs for annotating brain cell types, which is an application of LLMs but not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16541": {
        "authors": [
            "Binyuan Huang",
            "Yongdong Luo",
            "Xianda Guo",
            "Xiawu Zheng",
            "Zheng Zhu",
            "Jiahui Pan",
            "Chengju Zhou"
        ],
        "title": "Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition",
        "abstract": "arXiv:2510.16541v1 Announce Type: new  Abstract: Deep learning-based gait recognition has achieved great success in various applications. The key to accurate gait recognition lies in considering the unique and diverse behavior patterns in different motion regions, especially when covariates affect visual appearance. However, existing methods typically use predefined regions for temporal modeling, with fixed or equivalent temporal scales assigned to different types of regions, which makes it difficult to model motion regions that change dynamically over time and adapt to their specific patterns. To tackle this problem, we introduce a Region-aware Dynamic Aggregation and Excitation framework (GaitRDAE) that automatically searches for motion regions, assigns adaptive temporal scales and applies corresponding attention. Specifically, the framework includes two core modules: the Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the optimal temporal receptive field for each region, and the Region-aware Dynamic Excitation (RDE) module, which emphasizes the learning of motion regions containing more stable behavior patterns while suppressing attention to static regions that are more susceptible to covariates. Experimental results show that GaitRDAE achieves state-of-the-art performance on several benchmark datasets.",
        "arxiv_id": "2510.16541",
        "ARXIVID": "2510.16541",
        "COMMENT": "Relevant to computer vision and spatial modeling, but does not directly match any specific criterion. Proposes a new region-aware dynamic aggregation framework for gait recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17685": {
        "authors": [
            "Min Cao",
            "Xinyu Zhou",
            "Ding Jiang",
            "Bo Du",
            "Mang Ye",
            "Min Zhang"
        ],
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
        "abstract": "arXiv:2510.17685v1 Announce Type: new  Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.",
        "arxiv_id": "2510.17685",
        "ARXIVID": "2510.17685",
        "COMMENT": "Relevant to multi-modal learning and cross-modal retrieval, but does not directly match any specific criterion. Proposes a multilingual text-to-image person retrieval benchmark and a new alignment framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16115": {
        "authors": [
            "Jianhan Lin",
            "Yuchu Qin",
            "Shuai Gao",
            "Yikang Rui",
            "Jie Liu",
            "Yanjie Lv"
        ],
        "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection",
        "abstract": "arXiv:2510.16115v1 Announce Type: new  Abstract: Well-maintained road networks are crucial for achieving Sustainable Development Goal (SDG) 11. Road surface damage not only threatens traffic safety but also hinders sustainable urban development. Accurate detection, however, remains challenging due to the diverse shapes of damages, the difficulty of capturing slender cracks with high aspect ratios, and the high error rates in small-scale damage recognition. To address these issues, we propose StripRFNet, a novel deep neural network comprising three modules: (1) a Shape Perception Module (SPM) that enhances shape discrimination via large separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a Strip Receptive Field Module (SRFM) that employs large strip convolutions and pooling to capture features of slender cracks; and (3) a Small-Scale Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a dedicated detection head, and dynamic upsampling to improve small-object detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses existing methods. On the Chinese subset, it improves F1-score, mAP50, and mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline, respectively. On the full dataset, it achieves the highest F1-score of 80.33% compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while maintaining competitive inference speed. These results demonstrate that StripRFNet achieves state-of-the-art accuracy and real-time efficiency, offering a promising tool for intelligent road maintenance and sustainable infrastructure management.",
        "arxiv_id": "2510.16115",
        "ARXIVID": "2510.16115",
        "COMMENT": "This paper presents a new network (StripRFNet) for road damage detection, with modules for shape perception, strip receptive fields, and small-scale enhancement. It is a novel vision model for a specific application, but does not address embodied agents, VLLMs/MLLMs, or vision foundation models. General computer vision interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16146": {
        "authors": [
            "Thanh-Huy Nguyen",
            "Hoang-Thien Nguyen",
            "Vi Vu",
            "Ba-Thinh Lam",
            "Phat Huynh",
            "Tianyang Wang",
            "Xingjian Li",
            "Ulas Bagci",
            "Min Xu"
        ],
        "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization",
        "abstract": "arXiv:2510.16146v1 Announce Type: new  Abstract: The limited availability of annotated data in medical imaging makes semi-supervised learning increasingly appealing for its ability to learn from imperfect supervision. Recently, teacher-student frameworks have gained popularity for their training benefits and robust performance. However, jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios. To address this for medical image segmentation, we propose DuetMatch, a novel dual-branch semi-supervised framework with asynchronous optimization, where each branch optimizes either the encoder or decoder while keeping the other frozen. To improve consistency under noisy conditions, we introduce Decoupled Dropout Perturbation, enforcing regularization across branches. We also design Pair-wise CutMix Cross-Guidance to enhance model diversity by exchanging pseudo-labels through augmented input pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose Consistency Matching, refining labels using stable predictions from frozen teacher models. Extensive experiments on benchmark brain MRI segmentation datasets, including ISLES2022 and BraTS, show that DuetMatch consistently outperforms state-of-the-art methods, demonstrating its effectiveness and robustness across diverse semi-supervised segmentation scenarios.",
        "arxiv_id": "2510.16146",
        "ARXIVID": "2510.16146",
        "COMMENT": "This paper proposes DuetMatch, a dual-branch semi-supervised framework for brain MRI segmentation. While it is in the area of computer vision and machine learning, it does not directly match any of the four criteria, as it is not about spatial intelligence in embodied agents, VLLMs/MLLMs, new embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16658": {
        "authors": [
            "Shihao Yang",
            "Xiying Huang",
            "Danilo Bernardo",
            "Jun-En Ding",
            "Andrew Michael",
            "Jingmei Yang",
            "Patrick Kwan",
            "Ashish Raj",
            "Feng Liu"
        ],
        "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review",
        "abstract": "arXiv:2510.16658v1 Announce Type: new  Abstract: The advent of large-scale artificial intelligence (AI) models has a transformative effect on neuroscience research, which represents a paradigm shift from the traditional computational methods through the facilitation of end-to-end learning from raw brain signals and neural data. In this paper, we explore the transformative effects of large-scale AI models on five major neuroscience domains: neuroimaging and data processing, brain-computer interfaces and neural decoding, molecular neuroscience and genomic modeling, clinical assistance and translational frameworks, and disease-specific applications across neurological and psychiatric disorders. These models are demonstrated to address major computational neuroscience challenges, including multimodal neural data integration, spatiotemporal pattern interpretation, and the derivation of translational frameworks for clinical deployment. Moreover, the interaction between neuroscience and AI has become increasingly reciprocal, as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models. This review highlights both the notable promise of such technologies and key implementation considerations, with particular emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience datasets used to derive and validate large-scale AI models across diverse research applications is provided.",
        "arxiv_id": "2510.16658",
        "ARXIVID": "2510.16658",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Comprehensive review of foundation and large-scale AI models in neuroscience, with emphasis on multimodal data integration and applications.",
        "RELEVANCE": 5,
        "NOVELTY": 3
    },
    "2510.16234": {
        "authors": [
            "Hanane Nour Moussa",
            "Patrick Queiroz Da Silva",
            "Daniel Adu-Ampratwum",
            "Alyson East",
            "Zitong Lu",
            "Nikki Puccetti",
            "Mingyi Xue",
            "Huan Sun",
            "Bodhisattwa Prasad Majumder",
            "Sachin Kumar"
        ],
        "title": "ScholarEval: Research Idea Evaluation Grounded in Literature",
        "abstract": "arXiv:2510.16234v1 Announce Type: new  Abstract: As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness - the empirical validity of proposed methods based on existing literature, and contribution - the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprised of 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We openly release our code, dataset, and ScholarEval tool for the community to use and build on.",
        "arxiv_id": "2510.16234",
        "ARXIVID": "2510.16234",
        "COMMENT": "Does not match any specific criterion. Focuses on research idea evaluation using retrieval-augmented LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16179": {
        "authors": [
            "Xavier Giro-i-Nieto",
            "Nefeli Andreou",
            "Anqi Liang",
            "Manel Baradad",
            "Francesc Moreno-Noguer",
            "Aleix Martinez"
        ],
        "title": "Cost Savings from Automatic Quality Assessment of Generated Images",
        "abstract": "arXiv:2510.16179v1 Announce Type: new  Abstract: Deep generative models have shown impressive progress in recent years, making it possible to produce high quality images with a simple text prompt or a reference image. However, state of the art technology does not yet meet the quality standards offered by traditional photographic methods. For this reason, production pipelines that use generated images often include a manual stage of image quality assessment (IQA). This process is slow and expensive, especially because of the low yield of automatically generated images that pass the quality bar. The IQA workload can be reduced by introducing an automatic pre-filtering stage, that will increase the overall quality of the images sent to review and, therefore, reduce the average cost required to obtain a high quality image. We present a formula that estimates the cost savings depending on the precision and pass yield of a generic IQA engine. This formula is applied in a use case of background inpainting, showcasing a significant cost saving of 51.61% obtained with a simple AutoML solution.",
        "arxiv_id": "2510.16179",
        "ARXIVID": "2510.16179",
        "COMMENT": "Focuses on cost savings from automatic quality assessment of generated images, which is an application of generative models but not a methodological advance in spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.16724": {
        "authors": [
            "Minhua Lin",
            "Zongyu Wu",
            "Zhichao Xu",
            "Hui Liu",
            "Xianfeng Tang",
            "Qi He",
            "Charu Aggarwal",
            "Hui Liu",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
        "abstract": "arXiv:2510.16724v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
        "arxiv_id": "2510.16724",
        "ARXIVID": "2510.16724",
        "COMMENT": "This is a survey on RL-based agentic search, focusing on LLMs and retrieval-augmented generation. While it is relevant to multi-modal learning and agentic systems, it does not directly match any of the four criteria, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.16396": {
        "authors": [
            "Yeh Keng Hao",
            "Hsu Tzu Wei",
            "Sun Min"
        ],
        "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation",
        "abstract": "arXiv:2510.16396v1 Announce Type: new  Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and performance. We design a light framework that adopts an encoder-decoder architecture and introduces several key contributions aimed at improving both efficiency and accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency improvement. Moreover, we propose our SPLite decoder. This new architecture significantly boosts the decoding process's frame rate by 3.1x on the Raspberry Pi 5, while maintaining accuracy on par. To further optimize performance, we apply quantization-aware training, reducing memory usage while preserving accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5 CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on compound benchmark datasets, demonstrating comparable accuracy to state-of-the-art approaches while significantly enhancing computational efficiency.",
        "arxiv_id": "2510.16396",
        "ARXIVID": "2510.16396",
        "COMMENT": "Somewhat related to general computer vision and efficiency, but does not match any specific criterion (focuses on lightweight 3D hand pose estimation for edge devices).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}