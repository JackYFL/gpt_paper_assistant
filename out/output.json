{
    "2601.14339": {
        "authors": [
            "Haotian Xu",
            "Yue Hu",
            "Zhengqiu Zhu",
            "Chen Gao",
            "Ziyou Wang",
            "Junreng Rao",
            "Wenhao Lu",
            "Weishi Li",
            "Quanjun Yin",
            "Yong Li"
        ],
        "title": "CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments",
        "abstract": "arXiv:2601.14339v1 Announce Type: new  Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.",
        "arxiv_id": "2601.14339",
        "ARXIVID": "2601.14339",
        "COMMENT": "This paper introduces CityCube, a new benchmark for cross-view spatial reasoning in urban environments, specifically evaluating vision-language models' spatial intelligence. It directly matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 3 (new benchmarks for embodied AI with novel angles).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.15016": {
        "authors": [
            "Xiaodong Wang",
            "Langling Huang",
            "Zhirong Wu",
            "Xu Zhao",
            "Teng Xu",
            "Xuhong Xia",
            "Peixi Peng"
        ],
        "title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding",
        "abstract": "arXiv:2601.15016v1 Announce Type: new  Abstract: The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.",
        "arxiv_id": "2601.15016",
        "ARXIVID": "2601.15016",
        "COMMENT": "LiViBench is a new omnimodal benchmark for interactive livestream video understanding, and the paper introduces LiVi-LLM-7B, a new MLLM tailored for this domain. This is a strong match to criterion 2 (new MLLMs) and criterion 3 (new benchmark for multi-modal, interactive video understanding).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.15260": {
        "authors": [
            "Dominik R\\\"o{\\ss}le",
            "Xujun Xie",
            "Adithya Mohan",
            "Venkatesh Thirugnana Sambandham",
            "Daniel Cremers",
            "Torsten Sch\\\"on"
        ],
        "title": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration",
        "abstract": "arXiv:2601.15260v1 Announce Type: new  Abstract: Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.",
        "arxiv_id": "2601.15260",
        "ARXIVID": "2601.15260",
        "COMMENT": "This paper introduces DrivIng, a large-scale multimodal driving dataset with a full digital twin for sim-to-real evaluation and benchmarking. It matches criterion 3 (embodied AI papers on building new benchmarks/simulators with novel angles), as it enables new types of evaluation and simulation for autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.14702": {
        "authors": [
            "Zecong Tang",
            "Zixu Wang",
            "Yifei Wang",
            "Weitong Lian",
            "Tianjian Gao",
            "Haoran Li",
            "Tengju Ru",
            "Lingyi Meng",
            "Zhejun Cui",
            "Yichen Zhu",
            "Qi Kang",
            "Kaixuan Wang",
            "Yu Zhang"
        ],
        "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
        "abstract": "arXiv:2601.14702v1 Announce Type: new  Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
        "arxiv_id": "2601.14702",
        "ARXIVID": "2601.14702",
        "COMMENT": "AutoDriDM is a new benchmark for evaluating decision-making in vision-language models for autonomous driving, with a focus on explainability and the perception-to-decision gap. This is a strong match to criterion 3 (embodied AI, new benchmark for VLMs in a simulator-like, decision-centric context) and also touches on criterion 2 (VLLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.14652": {
        "authors": [
            "Zixuan Ke",
            "Yifei Ming",
            "Austin Xu",
            "Ryan Chin",
            "Xuan-Phi Nguyen",
            "Prathyusha Jwalapuram",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks",
        "abstract": "arXiv:2601.14652v1 Announce Type: new  Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.",
        "arxiv_id": "2601.14652",
        "ARXIVID": "2601.14652",
        "COMMENT": "This paper proposes MAS-Orchestra, a new framework for multi-agent reasoning and orchestration, and introduces MASBENCH, a controlled benchmark for multi-agent systems. It matches criterion 3 (embodied AI papers on new benchmarks/methods with novel angles), especially in multi-agent intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2601.14827": {
        "authors": [
            "Ben Schaper",
            "Maxime Di Folco",
            "Bernhard Kainz",
            "Julia A. Schnabel",
            "Cosmin I. Bercea"
        ],
        "title": "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies",
        "abstract": "arXiv:2601.14827v1 Announce Type: new  Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.",
        "arxiv_id": "2601.14827",
        "ARXIVID": "2601.14827",
        "COMMENT": "This paper benchmarks VLMs for chest X-ray classification using hierarchical metrics and proposes methods to align VLM abstraction with medical taxonomies. It directly matches criterion 4 (vision foundation models and applications), with a focus on evaluation and alignment in a high-stakes domain, and provides insightful empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15286": {
        "authors": [
            "Shantanu Jaiswal",
            "Mihir Prabhudesai",
            "Nikash Bhardwaj",
            "Zheyang Qin",
            "Amir Zadeh",
            "Chuan Li",
            "Katerina Fragkiadaki",
            "Deepak Pathak"
        ],
        "title": "Iterative Refinement Improves Compositional Image Generation",
        "abstract": "arXiv:2601.15286v1 Announce Type: new  Abstract: Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
        "arxiv_id": "2601.15286",
        "ARXIVID": "2601.15286",
        "COMMENT": "This paper proposes an iterative refinement strategy for compositional image generation, using a vision-language model as a critic in the loop. It directly addresses compositionality in generative vision-language models and shows surprising empirical results. Closely matches criterion 4 (vision foundation models and applications) and is also relevant to criterion 2 (VLLMs/MLLMs) due to the use of VLMs as critics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14691": {
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Jaekyeom Kim",
            "Sungryull Sohn",
            "Yunxiang Zhang",
            "Moontae Lee",
            "Hao Peng",
            "Lu Wang",
            "Honglak Lee"
        ],
        "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
        "abstract": "arXiv:2601.14691v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.",
        "arxiv_id": "2601.14691",
        "ARXIVID": "2601.14691",
        "COMMENT": "This paper analyzes vulnerabilities in LLM/VLM-based agent evaluation, showing that unfaithful chain-of-thought can manipulate judge outcomes. It provides surprising empirical results about VLMs/LLMs, matching criterion 2 (VLLMs/MLLMs) and is of high interest for its insight into evaluation reliability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15120": {
        "authors": [
            "Qian Xiong",
            "Yuekai Huang",
            "Yujia Zheng",
            "Tianhao Li",
            "Ziyou Jiang",
            "Zhiyuan Chang",
            "Zhaoyang Li",
            "Huanxiang Feng",
            "Mingyang Li"
        ],
        "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories",
        "abstract": "arXiv:2601.15120v1 Announce Type: new  Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.",
        "arxiv_id": "2601.15120",
        "ARXIVID": "2601.15120",
        "COMMENT": "This paper proposes RISE, a method for mitigating intent deviation in tool-using agents by synthesizing virtual trajectories and negative samples for preference learning. It is highly relevant to criterion 3 (embodied AI, new methods for agent evaluation and training, especially with a novel angle on intent deviation and synthetic data generation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14802": {
        "authors": [
            "Donnate Hooft",
            "Stefan M. Fischer",
            "Cosmin Bercea",
            "Jan C. Peeken",
            "Julia A. Schnabel"
        ],
        "title": "LocBAM: Advancing 3D Patch-Based Image Segmentation by Integrating Location Contex",
        "abstract": "arXiv:2601.14802v1 Announce Type: new  Abstract: Patch-based methods are widely used in 3D medical image segmentation to address memory constraints in processing high-resolution volumetric data. However, these approaches often neglect the patch's location within the global volume, which can limit segmentation performance when anatomical context is important. In this paper, we investigate the role of location context in patch-based 3D segmentation and propose a novel attention mechanism, LocBAM, that explicitly processes spatial information. Experiments on BTCV, AMOS22, and KiTS23 demonstrate that incorporating location context stabilizes training and improves segmentation performance, particularly under low patch-to-volume coverage where global context is missing. Furthermore, LocBAM consistently outperforms classical coordinate encoding via CoordConv. Code is publicly available at https://github.com/compai-lab/2026-ISBI-hooft",
        "arxiv_id": "2601.14802",
        "ARXIVID": "2601.14802",
        "COMMENT": "This paper proposes LocBAM, an attention mechanism that explicitly processes spatial information for 3D patch-based segmentation. It directly addresses criterion 1 (new methodological improvements to spatial understanding on embodied agents), as it focuses on spatial context in segmentation, which is a core spatial intelligence problem.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.14978": {
        "authors": [
            "Nilanjana Chatterjee",
            "Sidharatha Garg",
            "A V Subramanyam",
            "Brejesh Lall"
        ],
        "title": "Unified Multi-Dataset Training for TBPS",
        "abstract": "arXiv:2601.14978v1 Announce Type: new  Abstract: Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.",
        "arxiv_id": "2601.14978",
        "ARXIVID": "2601.14978",
        "COMMENT": "This paper proposes a unified multi-dataset training method for text-based person search (TBPS) using vision-language models, addressing dataset shift and scaling issues. It matches criterion 2 (new VLLMs/MLLMs) and is relevant to multi-modal learning and clever statistical tricks for dataset curation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.14757": {
        "authors": [
            "Kangcheng Zhou",
            "Jun Jiang",
            "Qing Zhang",
            "Shuang Zheng",
            "Qingli Li",
            "Shugong Xu"
        ],
        "title": "ReinPath: A Multimodal Reinforcement Learning Approach for Pathology",
        "abstract": "arXiv:2601.14757v1 Announce Type: new  Abstract: Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.",
        "arxiv_id": "2601.14757",
        "ARXIVID": "2601.14757",
        "COMMENT": "ReinPath introduces a multimodal pathology large language model with a new semantic reward strategy and a new VQA dataset for pathology. It is relevant to criterion 2 (new MLLMs) and also introduces a new dataset, which is a partial match to criterion 3.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.14790": {
        "authors": [
            "Zhi Qiu",
            "Jiazheng Sun",
            "Chenxiao Xia",
            "Jun Zheng",
            "Xin Peng"
        ],
        "title": "CI4A: Semantic Component Interfaces for Agents Empowering Web Automation",
        "abstract": "arXiv:2601.14790v1 Announce Type: new  Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.",
        "arxiv_id": "2601.14790",
        "ARXIVID": "2601.14790",
        "COMMENT": "This paper introduces CI4A, a semantic interface abstraction for agents to interact with web UI components, and upgrades a benchmark for web automation. It is relevant to criterion 3 (embodied AI, new benchmark/method for agent-environment interaction, focusing on agent-optimized interfaces rather than human-centric ones).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.14776": {
        "authors": [
            "Xiaofan Yang",
            "Yubin Liu",
            "Wei Pan",
            "Guoqing Chu",
            "Junming Zhang",
            "Jie Zhao",
            "Zhuoqi Man",
            "Xuanming Cao"
        ],
        "title": "M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention",
        "abstract": "arXiv:2601.14776v1 Announce Type: new  Abstract: Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.",
        "arxiv_id": "2601.14776",
        "ARXIVID": "2601.14776",
        "COMMENT": "This paper introduces M2I2HA, a multi-modal object detection method using intra- and inter-modal hypergraph attention. It is relevant to criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications), as it advances multi-modal perception and fusion, though it is not about VLLMs/MLLMs per se.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2601.14261": {
        "authors": [
            "Taoliang Tan",
            "Chengwei Ma",
            "Zhen Tian",
            "Zhao Lin",
            "Dongdong Li",
            "Si Shi"
        ],
        "title": "Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models",
        "abstract": "arXiv:2601.14261v1 Announce Type: new  Abstract: The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.",
        "arxiv_id": "2601.14261",
        "ARXIVID": "2601.14261",
        "COMMENT": "This paper introduces a three-stage framework for intelligent review of power grid drawings using pre-trained MLLMs and advanced prompt engineering. It matches criterion 2 (shows new MLLMs) and is also relevant to criterion 4 (vision foundation models and applications), as it applies MLLMs to a novel real-world task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.14330": {
        "authors": [
            "Mengyu Sun",
            "Ziyuan Yang",
            "Andrew Beng Jin Teoh",
            "Junxu Liu",
            "Haibo Hu",
            "Yi Zhang"
        ],
        "title": "LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models",
        "abstract": "arXiv:2601.14330v1 Announce Type: new  Abstract: Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.",
        "arxiv_id": "2601.14330",
        "ARXIVID": "2601.14330",
        "COMMENT": "This paper is about concept erasure and reawakening in diffusion models, with a novel method (LURE) for reconstructing latent space and guiding sampling. It is relevant to generative modeling in multi-modal learning, but does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Closest to criterion 4 (vision foundation models and applications) due to its focus on diffusion models and latent space manipulation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.14741": {
        "authors": [
            "Chongbin Yi",
            "Yuxin Liang",
            "Ziqi Zhou",
            "Peng Yang"
        ],
        "title": "Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution",
        "abstract": "arXiv:2601.14741v1 Announce Type: new  Abstract: Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users' Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.",
        "arxiv_id": "2601.14741",
        "ARXIVID": "2601.14741",
        "COMMENT": "This paper proposes a hybrid super-resolution framework for text-to-image generation, combining diffusion-based and lightweight SR models for efficient, high-quality image generation. It is relevant to criterion 4 (vision foundation models and applications), especially in generative modeling, but does not introduce a new VLLM/MLLM or embodied AI method.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.14706": {
        "authors": [
            "Chao Gao",
            "Siqiao Xue",
            "Yimin Peng",
            "Jiwen Fu",
            "Tingyi Gu",
            "Shanshan Li",
            "Fan Zhou"
        ],
        "title": "LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval",
        "abstract": "arXiv:2601.14706v1 Announce Type: new  Abstract: In this paper, we present LookBench (We use the term \"look\" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.",
        "arxiv_id": "2601.14706",
        "ARXIVID": "2601.14706",
        "COMMENT": "This paper presents LookBench, a new benchmark for fashion image retrieval, including AI-generated images and a live leaderboard. It is a new benchmark in vision, but not for embodied AI or simulators. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.14703": {
        "authors": [
            "Xinquan Yang",
            "Xuguang Li",
            "Mianjie Zheng",
            "Xuefen Liu",
            "Kun Tang",
            "Kian Ming Lim",
            "He Meng",
            "Jianfeng Ren",
            "Linlin Shen"
        ],
        "title": "RegFreeNet: A Registration-Free Network for CBCT-based 3D Dental Implant Planning",
        "abstract": "arXiv:2601.14703v1 Announce Type: new  Abstract: As the commercial surgical guide design software usually does not support the export of implant position for pre-implantation data, existing methods have to scan the post-implantation data and map the implant to pre-implantation space to get the label of implant position for training. Such a process is time-consuming and heavily relies on the accuracy of registration algorithm. Moreover, not all hospitals have paired CBCT data, limitting the construction of multi-center dataset. Inspired by the way dentists determine the implant position based on the neighboring tooth texture, we found that even if the implant area is masked, it will not affect the determination of the implant position. Therefore, we propose to mask the implants in the post-implantation data so that any CBCT containing the implants can be used as training data. This paradigm enables us to discard the registration process and makes it possible to construct a large-scale multi-center implant dataset. On this basis, we proposes ImplantFairy, a comprehensive, publicly accessible dental implant dataset with voxel-level 3D annotations of 1622 CBCT data. Furthermore, according to the area variation characteristics of the tooth's spatial structure and the slope information of the implant, we designed a slope-aware implant position prediction network. Specifically, a neighboring distance perception (NDP) module is designed to adaptively extract tooth area variation features, and an implant slope prediction branch assists the network in learning more robust features through additional implant supervision information. Extensive experiments conducted on ImplantFairy and two public dataset demonstrate that the proposed RegFreeNet achieves the state-of-the-art performance.",
        "arxiv_id": "2601.14703",
        "ARXIVID": "2601.14703",
        "COMMENT": "This paper introduces a new method for 3D dental implant planning using CBCT data, with a novel registration-free approach and a new dataset. It focuses on spatial understanding in a medical context, but not on embodied agents or general spatial intelligence. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.15133": {
        "authors": [
            "Andr\\'e Eberhard",
            "Gerhard Neumann",
            "Pascal Friederich"
        ],
        "title": "Graph Recognition via Subgraph Prediction",
        "abstract": "arXiv:2601.15133v1 Announce Type: new  Abstract: Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \\textbf{Gra}ph Recognition via \\textbf{S}ubgraph \\textbf{P}rediction (\\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.",
        "arxiv_id": "2601.15133",
        "ARXIVID": "2601.15133",
        "COMMENT": "This paper introduces GraSP, a method for visual graph recognition via subgraph prediction, aiming for a unified approach across tasks. It is relevant to computer vision and machine learning, but does not match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.15110": {
        "authors": [
            "Aoran Liu",
            "Kun Hu",
            "Clinton Ansun Mo",
            "Qiuxia Wu",
            "Wenxiong Kang",
            "Zhiyong Wang"
        ],
        "title": "Pb4U-GNet: Resolution-Adaptive Garment Simulation via Propagation-before-Update Graph Network",
        "abstract": "arXiv:2601.15110v1 Announce Type: new  Abstract: Garment simulation is fundamental to various applications in computer vision and graphics, from virtual try-on to digital human modelling. However, conventional physics-based methods remain computationally expensive, hindering their application in time-sensitive scenarios. While graph neural networks (GNNs) offer promising acceleration, existing approaches exhibit poor cross-resolution generalisation, demonstrating significant performance degradation on higher-resolution meshes beyond the training distribution. This stems from two key factors: (1) existing GNNs employ fixed message-passing depth that fails to adapt information aggregation to mesh density variation, and (2) vertex-wise displacement magnitudes are inherently resolution-dependent in garment simulation. To address these issues, we introduce Propagation-before-Update Graph Network (Pb4U-GNet), a resolution-adaptive framework that decouples message propagation from feature updates. Pb4U-GNet incorporates two key mechanisms: (1) dynamic propagation depth control, adjusting message-passing iterations based on mesh resolution, and (2) geometry-aware update scaling, which scales predictions according to local mesh characteristics. Extensive experiments show that even trained solely on low-resolution meshes, Pb4U-GNet exhibits strong generalisability across diverse mesh resolutions, addressing a fundamental challenge in neural garment simulation.",
        "arxiv_id": "2601.15110",
        "ARXIVID": "2601.15110",
        "COMMENT": "Pb4U-GNet is a resolution-adaptive graph network for garment simulation, addressing cross-resolution generalization. While it is a novel method in computer vision/graphics, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.14821": {
        "authors": [
            "Bert Ramlot",
            "Martijn Courteaux",
            "Peter Lambert",
            "Glenn Van Wallendael"
        ],
        "title": "POTR: Post-Training 3DGS Compression",
        "abstract": "arXiv:2601.14821v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis. 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements. To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques. First, POTR introduces a novel pruning approach that uses a modified 3DGS rasterizer to efficiently calculate every splat's individual removal effect simultaneously. This technique results in 2-4x fewer splats than other post-training pruning techniques and as a result also significantly accelerates inference with experiments demonstrating 1.5-2x faster inference than other compressed models. Second, we propose a novel method to recompute lighting coefficients, significantly reducing their entropy without using any form of training. Our fast and highly parallel approach especially increases AC lighting coefficient sparsity, with experiments demonstrating increases from 70% to 97%, with minimal loss in quality. Finally, we extend POTR with a simple fine-tuning scheme to further enhance pruning, inference, and rate-distortion performance. Experiments demonstrate that POTR, even without fine-tuning, consistently outperforms all other post-training compression techniques in both rate-distortion performance and inference speed.",
        "arxiv_id": "2601.14821",
        "ARXIVID": "2601.14821",
        "COMMENT": "POTR is a post-training compression method for 3D Gaussian Splatting in 3D scene reconstruction. While it is a technical advance in 3D vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. It is relevant to computer vision and generative modeling, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.14584": {
        "authors": [
            "Cheng Wan",
            "Bahram Jafrasteh",
            "Ehsan Adeli",
            "Miaomiao Zhang",
            "Qingyu Zhao"
        ],
        "title": "Anatomically Guided Latent Diffusion for Brain MRI Progression Modeling",
        "abstract": "arXiv:2601.14584v1 Announce Type: new  Abstract: Accurately modeling longitudinal brain MRI progression is crucial for understanding neurodegenerative diseases and predicting individualized structural changes. Existing state-of-the-art approaches, such as Brain Latent Progression (BrLP), often use multi-stage training pipelines with auxiliary conditioning modules but suffer from architectural complexity, suboptimal use of conditional clinical covariates, and limited guarantees of anatomical consistency. We propose Anatomically Guided Latent Diffusion Model (AG-LDM), a segmentation-guided framework that enforces anatomically consistent progression while substantially simplifying the training pipeline. AG-LDM conditions latent diffusion by directly fusing baseline anatomy, noisy follow-up states, and clinical covariates at the input level, a strategy that avoids auxiliary control networks by learning a unified, end-to-end model that represents both anatomy and progression. A lightweight 3D tissue segmentation model (WarpSeg) provides explicit anatomical supervision during both autoencoder fine-tuning and diffusion model training, ensuring consistent brain tissue boundaries and morphometric fidelity. Experiments on 31,713 ADNI longitudinal pairs and zero-shot evaluation on OASIS-3 demonstrate that AG-LDM matches or surpasses more complex diffusion models, achieving state-of-the-art image quality and 15-20\\% reduction in volumetric errors in generated images. AG-LDM also exhibits markedly stronger utilization of temporal and clinical covariates (up to 31.5x higher sensitivity than BrLP) and generates biologically plausible counterfactual trajectories, accurately capturing hallmarks of Alzheimer's progression such as limbic atrophy and ventricular expansion. These results highlight AG-LDM as an efficient, anatomically grounded framework for reliable brain MRI progression modeling.",
        "arxiv_id": "2601.14584",
        "ARXIVID": "2601.14584",
        "COMMENT": "This paper introduces a new anatomically guided latent diffusion model for brain MRI progression modeling, leveraging segmentation guidance and clinical covariates. While it is a novel generative model for medical imaging, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. It is most relevant to generative modeling in vision, but not a direct match to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.14514": {
        "authors": [
            "Tony Chen",
            "Sam Cheyette",
            "Kelsey Allen",
            "Joshua Tenenbaum",
            "Kevin Smith"
        ],
        "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
        "abstract": "arXiv:2601.14514v1 Announce Type: new  Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.",
        "arxiv_id": "2601.14514",
        "ARXIVID": "2601.14514",
        "COMMENT": "This paper presents a cognitive model of human planning and reasoning using 'just-in-time' world modeling. While it is interesting for spatial reasoning, it is a cognitive science paper and does not introduce new methods for spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}