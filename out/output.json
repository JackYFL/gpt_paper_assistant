{
    "2510.23497": {
        "authors": [
            "Walid Bousselham",
            "Hilde Kuehne",
            "Cordelia Schmid"
        ],
        "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
        "abstract": "arXiv:2510.23497v1 Announce Type: new  Abstract: Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher.",
        "arxiv_id": "2510.23497",
        "ARXIVID": "2510.23497",
        "COMMENT": "Matches criterion 2 (new method for transferring reasoning from LLMs to VLMs, improving VLM reasoning) and criterion 4 (vision-language foundation models and their training). VOLD is a new framework for reasoning transfer to VLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.23473": {
        "authors": [
            "Shijian Wang",
            "Jiarui Jin",
            "Xingjian Wang",
            "Linxin Song",
            "Runhao Fu",
            "Hecheng Wang",
            "Zongyuan Ge",
            "Yuan Lu",
            "Xuelian Cheng"
        ],
        "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
        "abstract": "arXiv:2510.23473v1 Announce Type: new  Abstract: Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
        "arxiv_id": "2510.23473",
        "ARXIVID": "2510.23473",
        "COMMENT": "Matches criterion 2: Shows new MLLMs. Also relevant to criterion 4. Introduces Video-Thinker, a method for enabling MLLMs to perform video reasoning via reinforcement learning and chain-of-thought, with a new dataset and training strategy.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.23607": {
        "authors": [
            "Yujia Zhang",
            "Xiaoyang Wu",
            "Yixing Lao",
            "Chengyao Wang",
            "Zhuotao Tian",
            "Naiyan Wang",
            "Hengshuang Zhao"
        ],
        "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
        "abstract": "arXiv:2510.23607v1 Announce Type: new  Abstract: Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
        "arxiv_id": "2510.23607",
        "ARXIVID": "2510.23607",
        "COMMENT": "Concerto proposes a joint 2D-3D self-supervised learning framework for spatial representation, with strong results in spatial understanding and open-world perception via CLIP alignment. This directly matches criterion 1 (spatial understanding/spatial intelligence for embodied agents) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.23594": {
        "authors": [
            "Yusu Qian",
            "Cheng Wan",
            "Chao Jia",
            "Yinfei Yang",
            "Qingyu Zhao",
            "Zhe Gan"
        ],
        "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
        "abstract": "arXiv:2510.23594v1 Announce Type: new  Abstract: We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
        "arxiv_id": "2510.23594",
        "ARXIVID": "2510.23594",
        "COMMENT": "PRISM-Bench is a new benchmark for puzzle-based visual tasks with chain-of-thought error detection, focusing on fine-grained assessment of multimodal reasoning in MLLMs. This directly matches criterion 3 (new embodied AI benchmarks with novel evaluation) and criterion 2 (MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.21867": {
        "authors": [
            "Haicheng Liao",
            "Bonan Wang",
            "Junxian Yang",
            "Chengyue Wang",
            "Zhengbin He",
            "Guohui Zhang",
            "Chengzhong Xu",
            "Zhenning Li"
        ],
        "title": "Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs",
        "abstract": "arXiv:2510.21867v1 Announce Type: new  Abstract: Accurate and reliable motion forecasting is essential for the safe deployment of autonomous vehicles (AVs), particularly in rare but safety-critical scenarios known as corner cases. Existing models often underperform in these situations due to an over-representation of common scenes in training data and limited generalization capabilities. To address this limitation, we present WM-MoE, the first world model-based motion forecasting framework that unifies perception, temporal memory, and decision making to address the challenges of high-risk corner-case scenarios. The model constructs a compact scene representation that explains current observations, anticipates future dynamics, and evaluates the outcomes of potential actions. To enhance long-horizon reasoning, we leverage large language models (LLMs) and introduce a lightweight temporal tokenizer that maps agent trajectories and contextual cues into the LLM's feature space without additional training, enriching temporal context and commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to decompose complex corner cases into subproblems and allocate capacity across scenario types, and a router assigns scenes to specialized experts that infer agent intent and perform counterfactual rollouts. In addition, we introduce nuScenes-corner, a new benchmark that comprises four real-world corner-case scenarios for rigorous evaluation. Extensive experiments on four benchmark datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently outperforms state-of-the-art (SOTA) baselines and remains robust under corner-case and data-missing conditions, indicating the promise of world model-based architectures for robust and generalizable motion forecasting in fully AVs.",
        "arxiv_id": "2510.21867",
        "ARXIVID": "2510.21867",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and methods, novel angle) and criterion 2 (LLMs in multi-modal context). Proposes a world model-based approach for autonomous driving, introduces a new benchmark for corner cases, and leverages LLMs for temporal reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.22443": {
        "authors": [
            "Vijay Veerabadran",
            "Fanyi Xiao",
            "Nitin Kamra",
            "Pedro Matias",
            "Joy Chen",
            "Caley Drooff",
            "Brett D Roads",
            "Riley Williams",
            "Ethan Henderson",
            "Xuanyi Zhao",
            "Kevin Carlberg",
            "Joseph Tighe",
            "Karl Ridgeway"
        ],
        "title": "Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents",
        "abstract": "arXiv:2510.22443v1 Announce Type: new  Abstract: There has been a surge of interest in assistive wearable agents: agents embodied in wearable form factors (e.g., smart glasses) who take assistive actions toward a user's goal/query (e.g. \"Where did I leave my keys?\"). In this work, we consider the important complementary problem of inferring that goal from multi-modal contextual observations. Solving this \"goal inference\" problem holds the promise of eliminating the effort needed to interact with such an agent. This work focuses on creating WAGIBench, a strong benchmark to measure progress in solving this problem using vision-language models (VLMs). Given the limited prior work in this area, we collected a novel dataset comprising 29 hours of multimodal data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside accompanying visual, audio, digital, and longitudinal contextual observations. We validate that human performance exceeds model performance, achieving 93% multiple-choice accuracy compared with 84% for the best-performing VLM. Generative benchmark results that evaluate several families of modern vision-language models show that larger models perform significantly better on the task, yet remain far from practical usefulness, as they produce relevant goals only 55% of the time. Through a modality ablation, we show that models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities.",
        "arxiv_id": "2510.22443",
        "ARXIVID": "2510.22443",
        "COMMENT": "This paper introduces WAGIBench, a new benchmark for egocentric multimodal goal inference for assistive wearable agents, using vision-language models. It is a new embodied AI benchmark with a novel angle (goal inference from multimodal data), matching criterion 3.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.22973": {
        "authors": [
            "Bohan Li",
            "Xin Jin",
            "Hu Zhu",
            "Hongsi Liu",
            "Ruikai Li",
            "Jiazhe Guo",
            "Kaiwen Cai",
            "Chao Ma",
            "Yueming Jin",
            "Hao Zhao",
            "Xiaokang Yang",
            "Wenjun Zeng"
        ],
        "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
        "abstract": "arXiv:2510.22973v1 Announce Type: new  Abstract: Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
        "arxiv_id": "2510.22973",
        "ARXIVID": "2510.22973",
        "COMMENT": "This paper presents a new large-scale semantic occupancy dataset (Nuplan-Occ) and a unified generative framework for driving scene generation, including novel techniques for multi-modal simulation. It is a new benchmark and method for embodied AI with a simulator focus, matching criterion 3.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.22946": {
        "authors": [
            "Zeyu Wang",
            "Zilong Chen",
            "Chenhui Gou",
            "Feng Li",
            "Chaorui Deng",
            "Deyao Zhu",
            "Kunchang Li",
            "Weihao Yu",
            "Haoqin Tu",
            "Haoqi Fan",
            "Cihang Xie"
        ],
        "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2510.22946v1 Announce Type: new  Abstract: Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.",
        "arxiv_id": "2510.22946",
        "ARXIVID": "2510.22946",
        "COMMENT": "Strongly matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes LightBagel, a unified multimodal model with a double fusion mechanism, achieving strong results on multiple benchmarks and releasing code and weights.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.22665": {
        "authors": [
            "Qiwei Ma",
            "Zhiyu Wang",
            "Wang Liu",
            "Xukun Lu",
            "Bin Deng",
            "Puhong Duan",
            "Xudong Kang",
            "Shutao Li"
        ],
        "title": "SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery",
        "abstract": "arXiv:2510.22665v1 Announce Type: new  Abstract: Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due to its all-weather capabilities. While recent advancements in self-supervised learning and Masked Image Modeling (MIM) have paved the way for SAR foundation models, these approaches primarily focus on low-level visual features, often overlooking multimodal alignment and zero-shot target recognition within SAR imagery. To address this limitation, we construct SARCLIP-1M, a large-scale vision language dataset comprising over one million text-image pairs aggregated from existing datasets. We further introduce SARCLIP, the first vision language foundation model tailored for the SAR domain. Our SARCLIP model is trained using a contrastive vision language learning approach by domain transferring strategy, enabling it to bridge the gap between SAR imagery and textual descriptions. Extensive experiments on image-text retrieval and zero-shot classification tasks demonstrate the superior performance of SARCLIP in feature extraction and interpretation, significantly outperforming state-of-the-art foundation models and advancing the semantic understanding of SAR imagery. The code and datasets will be released soon.",
        "arxiv_id": "2510.22665",
        "ARXIVID": "2510.22665",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications) and 2 (new vision-language foundation model for SAR imagery). Introduces SARCLIP, a vision-language foundation model for semantic understanding and target recognition in SAR imagery, with a new large-scale dataset.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.23190": {
        "authors": [
            "Pascal Benschop",
            "Cristian Meo",
            "Justin Dauwels",
            "Jelte P. Mense"
        ],
        "title": "Evaluation of Vision-LLMs in Surveillance Video",
        "abstract": "arXiv:2510.23190v1 Announce Type: new  Abstract: The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
        "arxiv_id": "2510.23190",
        "ARXIVID": "2510.23190",
        "COMMENT": "Matches criteria 1 (spatial reasoning in embodied agents) and 2 (evaluation of vision-LLMs/VLMs). The paper investigates spatial reasoning abilities of vision-language models in surveillance video, focusing on zero-shot anomaly detection and spatial grounding, and discusses concrete paths to improve spatial grounding in VLMs for embodied video understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.21794": {
        "authors": [
            "Kejia Chen",
            "Jiawen Zhang",
            "Jiacong Hu",
            "Kewei Gao",
            "Jian Lou",
            "Zunlei Feng",
            "Mingli Song"
        ],
        "title": "Token-Level Inference-Time Alignment for Vision-Language Models",
        "abstract": "arXiv:2510.21794v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have become essential backbones of modern multimodal intelligence, yet their outputs remain prone to hallucination-plausible text misaligned with visual inputs. Existing alignment approaches often rely on expensive fine-tuning with annotated preference data or sequence-level inference strategies that provide only coarse, delayed feedback. To overcome these limitations, we present TITA (Token-level Inference-Time Alignment), a lightweight framework that freezes the base VLM and instead trains a reward model to approximate its distribution. During inference, implicit preference signals are extracted as log-probability ratios between the reward model and the target VLM, yielding dense autoregressive feedback. This formulation can be viewed as an inference-time variant of Direct Preference Optimization (DPO), providing token-level corrective signals without retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and 6.7% on POPE, indicating stronger general understanding and reduced hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B show comparable gains, especially in hallucination reduction and VQA accuracy, while incurring negligible inference overhead.",
        "arxiv_id": "2510.21794",
        "ARXIVID": "2510.21794",
        "COMMENT": "Matches criterion 2 (new VLLM methods) and criterion 4 (vision foundation models and applications). Proposes a novel inference-time alignment method for VLMs to reduce hallucinations, with strong empirical results across multiple benchmarks and models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.23574": {
        "authors": [
            "Hongkai Lin",
            "Dingkang Liang",
            "Mingyang Du",
            "Xin Zhou",
            "Xiang Bai"
        ],
        "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
        "abstract": "arXiv:2510.23574v1 Announce Type: new  Abstract: Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at https://github.com/H-EmbodVis/MERGE",
        "arxiv_id": "2510.23574",
        "ARXIVID": "2510.23574",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding via depth estimation using text-to-image diffusion models) and criterion 4 (vision foundation models and applications). Proposes a unified model (MERGE) that enables both image generation and depth estimation from a pre-trained text-to-image diffusion model, with a novel plug-and-play framework and parameter reuse mechanism.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.21786": {
        "authors": [
            "Qile Su",
            "Shoutai Zhu",
            "Shuai Zhang",
            "Baoyu Liang",
            "Chao Tong"
        ],
        "title": "EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction",
        "abstract": "arXiv:2510.21786v1 Announce Type: new  Abstract: Script event induction, which aims to predict the subsequent event based on the context, is a challenging task in NLP, achieving remarkable success in practical applications. However, human events are mostly recorded and presented in the form of videos rather than scripts, yet there is a lack of related research in the realm of vision. To address this problem, we introduce AVEP (Action-centric Video Event Prediction), a task that distinguishes itself from existing video prediction tasks through its incorporation of more complex logic and richer semantic information. We present a large structured dataset, which consists of about $35K$ annotated videos and more than $178K$ video clips of event, built upon existing video event datasets to support this task. The dataset offers more fine-grained annotations, where the atomic unit is represented as a multimodal event argument node, providing better structured representations of video events. Due to the complexity of event structures, traditional visual models that take patches or frames as input are not well-suited for AVEP. We propose EventFormer, a node-graph hierarchical attention based video event prediction model, which can capture both the relationships between events and their arguments and the coreferencial relationships between arguments. We conducted experiments using several SOTA video prediction models as well as LVLMs on AVEP, demonstrating both the complexity of the task and the value of the dataset. Our approach outperforms all these video prediction models. We will release the dataset and code for replicating the experiments and annotations.",
        "arxiv_id": "2510.21786",
        "ARXIVID": "2510.21786",
        "COMMENT": "This paper introduces a new benchmark (AVEP) and dataset for action-centric video event prediction, and proposes a novel hierarchical attention transformer (EventFormer). It directly matches criterion 3 (embodied AI, new benchmarks/methods with novel angles), and is also relevant to multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.21850": {
        "authors": [
            "Gyubeum Lim",
            "Yemo Koo",
            "Vijay Krishna Madisetti"
        ],
        "title": "SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models",
        "abstract": "arXiv:2510.21850v1 Announce Type: new  Abstract: Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to reduce the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.",
        "arxiv_id": "2510.21850",
        "ARXIVID": "2510.21850",
        "COMMENT": "This paper introduces SCoPE VLM, a vision-language model for efficient document navigation, with a novel selective context processing mechanism and reinforcement learning for agentic reading. This matches criterion 2 (new VLLMs/MLLMs) and is also relevant to criterion 4 (vision foundation models and applications), with a focus on agentic tasks and long-context understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.22936": {
        "authors": [
            "Mouxiao Huang",
            "Borui Jiang",
            "Dehua Zheng",
            "Hailin Hu",
            "Kai Han",
            "Xinghao Chen"
        ],
        "title": "Positional Preservation Embedding for Multimodal Large Language Models",
        "abstract": "arXiv:2510.22936v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks, yet often suffer from inefficiencies due to redundant visual tokens. Existing token merging methods reduce sequence length but frequently disrupt spatial layouts and temporal continuity by disregarding positional relationships. In this work, we propose a novel encoding operator dubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding (\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal structure during visual token compression. PPE explicitly introduces the disentangled encoding of 3D positions in the token dimension, enabling each compressed token to encapsulate different positions from multiple original tokens. Furthermore, we show that PPE can effectively support cascade clustering -- a progressive token compression strategy that leads to better performance retention. PPE is a parameter-free and generic operator that can be seamlessly integrated into existing token merging methods without any adjustments. Applied to state-of-the-art token merging framework, PPE achieves consistent improvements of $2\\%\\sim5\\%$ across multiple vision-language benchmarks, including MMBench (general vision understanding), TextVQA (layout understanding) and VideoMME (temporal understanding). These results demonstrate that preserving positional cues is critical for efficient and effective MLLM reasoning.",
        "arxiv_id": "2510.22936",
        "ARXIVID": "2510.22936",
        "COMMENT": "This paper introduces Positional Preservation Embedding (PPE), a new operator for multimodal large language models (MLLMs) that preserves spatiotemporal structure during visual token compression. It is a methodological improvement for MLLMs and directly matches criterion 2.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.22102": {
        "authors": [
            "Xingjian Tao",
            "Yiwei Wang",
            "Yujun Cai",
            "Yihong Luo",
            "Jing Tang"
        ],
        "title": "Mitigating Coordinate Prediction Bias from Positional Encoding Failures",
        "abstract": "arXiv:2510.22102v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.",
        "arxiv_id": "2510.22102",
        "ARXIVID": "2510.22102",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding in MLLMs). Analyzes and mitigates coordinate prediction bias in MLLMs due to positional encoding failures, proposing a training-free correction method for spatial reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.22577": {
        "authors": [
            "Feng He",
            "Guodong Tan",
            "Qiankun Li",
            "Jun Yu",
            "Quan Wen"
        ],
        "title": "From Pixels to Views: Learning Angular-Aware and Physics-Consistent Representations for Light Field Microscopy",
        "abstract": "arXiv:2510.22577v1 Announce Type: new  Abstract: Light field microscopy (LFM) has become an emerging tool in neuroscience for large-scale neural imaging in vivo, notable for its single-exposure volumetric imaging, broad field of view, and high temporal resolution. However, learning-based 3D reconstruction in XLFM remains underdeveloped due to two core challenges: the absence of standardized datasets and the lack of methods that can efficiently model its angular-spatial structure while remaining physically grounded. We address these challenges by introducing three key contributions. First, we construct the XLFM-Zebrafish benchmark, a large-scale dataset and evaluation suite for XLFM reconstruction. Second, we propose Masked View Modeling for Light Fields (MVN-LF), a self-supervised task that learns angular priors by predicting occluded views, improving data efficiency. Third, we formulate the Optical Rendering Consistency Loss (ORC Loss), a differentiable rendering constraint that enforces alignment between predicted volumes and their PSF-based forward projections. On the XLFM-Zebrafish benchmark, our method improves PSNR by 7.7% over state-of-the-art baselines.",
        "arxiv_id": "2510.22577",
        "ARXIVID": "2510.22577",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark and methods with novel angles). Introduces a new benchmark for light field microscopy and proposes a self-supervised method and a physics-consistent loss for 3D reconstruction, addressing previously ignored challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.22171": {
        "authors": [
            "Erum Mushtaq",
            "Zalan Fabian",
            "Yavuz Faruk Bakman",
            "Anil Ramakrishna",
            "Mahdi Soltanolkotabi",
            "Salman Avestimehr"
        ],
        "title": "HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models",
        "abstract": "arXiv:2510.22171v1 Announce Type: new  Abstract: The growing deployment of Vision-Language Models (VLMs) in high-stakes applications such as autonomous driving and assistive technologies for visually impaired individuals necessitates reliable mechanisms to assess the trustworthiness of their generation. Uncertainty Estimation (UE) plays a central role in quantifying the reliability of model outputs and reducing unsafe generations via selective prediction. In this regard, most existing probability-based UE approaches rely on output probability distributions, aggregating token probabilities into a single uncertainty score using predefined functions such as length-normalization. Another line of research leverages model hidden representations and trains MLP-based models to predict uncertainty. However, these methods often fail to capture the complex multimodal relationships between semantic and textual tokens and struggle to identify biased probabilities often influenced by language priors. Motivated by these observations, we propose a novel UE framework, HARMONY, that jointly leverages fused multimodal information in model activations and the output distribution of the VLM to determine the reliability of responses. The key hypothesis of our work is that both the model's internal belief in its visual understanding, captured by its hidden representations, and the produced token probabilities carry valuable reliability signals that can be jointly leveraged to improve UE performance, surpassing approaches that rely on only one of these components. Experimental results on three open-ended VQA benchmarks, A-OKVQA, VizWiz, and PathVQA, and three state-of-the-art VLMs, LLaVa-7b, LLaVA-13b and InstructBLIP demonstrate that our method consistently performs on par with or better than existing approaches, achieving up to 4\\% improvement in AUROC, and 6\\% in PRR, establishing new state of the art in uncertainty estimation for VLMs.",
        "arxiv_id": "2510.22171",
        "ARXIVID": "2510.22171",
        "COMMENT": "Matches criterion 2 (new VLLM methods) and criterion 4 (vision foundation models and applications). Proposes a new uncertainty estimation framework for VLMs, leveraging both hidden activations and output distributions, with state-of-the-art results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.22067": {
        "authors": [
            "Zheng Qi",
            "Chao Shang",
            "Evangelia Spiliopoulou",
            "Nikolaos Pappas"
        ],
        "title": "Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation",
        "abstract": "arXiv:2510.22067v1 Announce Type: new  Abstract: Vision language models (VLMs) often generate hallucination, i.e., content that cannot be substantiated by either textual or visual inputs. Prior work primarily attributes this to over-reliance on linguistic prior knowledge rather than visual inputs. Some methods attempt to mitigate hallucination by amplifying visual token attention proportionally to their attention scores. However, these methods overlook the visual attention sink problem, where attention is frequently misallocated to task-irrelevant visual regions, and neglect cross-modal fusion balance by enhancing only visual attention without adjusting attention to the user query. This can result in amplifying incorrect areas while failing to properly interpret the user query. To address these challenges, we propose a simple yet effective method called Gaze Shift-Guided Cross-modal Fusion Enhancement (GIFT). GIFT pre-computes a holistic visual saliency map by tracking positive changes in visual attention, or \"gaze shifts\", during user query comprehension, and leverages this map to amplify attention to both salient visual information and the user query at each decoding step. This reduces the impact of visual attention sink, as irrelevant tokens exhibit minimal shifts, while ensuring balanced cross-modal fusion for well-integrated representation. Extensive experiments show that GIFT effectively mitigates hallucination in VLMs across both generative and classification tasks, achieving up to 20.7% improvement over greedy decoding, while maintaining general vision-language performance with low computational overhead.",
        "arxiv_id": "2510.22067",
        "ARXIVID": "2510.22067",
        "COMMENT": "Matches criterion 2: Shows new VLLM method. Proposes GIFT, a cross-modal fusion enhancement to mitigate hallucination in VLMs using gaze shift-guided attention, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.23205": {
        "authors": [
            "Hoonhee Cho",
            "Jae-Young Kang",
            "Giwon Lee",
            "Hyemin Yang",
            "Heejun Park",
            "Seokwoo Jung",
            "Kuk-Jin Yoon"
        ],
        "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
        "abstract": "arXiv:2510.23205v1 Announce Type: new  Abstract: End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems.",
        "arxiv_id": "2510.23205",
        "ARXIVID": "2510.23205",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for simulators/benchmarks, novel angle). Proposes a new E2E driving framework robust to viewpoint changes, introduces a new benchmark for this problem.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.23095": {
        "authors": [
            "Jie Huang",
            "Xuejing Liu",
            "Sibo Song",
            "Ruibing Hou",
            "Hong Chang",
            "Junyang Lin",
            "Shuai Bai"
        ],
        "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
        "abstract": "arXiv:2510.23095v1 Announce Type: new  Abstract: Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.",
        "arxiv_id": "2510.23095",
        "ARXIVID": "2510.23095",
        "COMMENT": "This paper systematically analyzes and improves multimodal positional encoding in vision-language models, proposing new plug-and-play variants. It is a methodological improvement for VLLMs/MLLMs, matching criterion 2.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.22694": {
        "authors": [
            "Shu Zhao",
            "Tianyi Shen",
            "Nilesh Ahuja",
            "Omesh Tickoo",
            "Vijaykrishnan Narayanan"
        ],
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "abstract": "arXiv:2510.22694v1 Announce Type: new  Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising method to generate factual and up-to-date responses of Multimodal Large Language Models (MLLMs) by incorporating non-parametric knowledge from external knowledge bases. However, existing MRAG approaches suffer from static retrieval strategies, inflexible modality selection, and suboptimal utilization of retrieved information, leading to three critical challenges: determining when to retrieve, what modality to incorporate, and how to utilize retrieved information effectively. To address these challenges, we introduce Windsock, a query-dependent module making decisions on retrieval necessity and modality selection, effectively reducing computational overhead and improving response quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize retrieved information while maintaining robustness against noise. Moreover, we adopt a self-assessment approach leveraging knowledge within MLLMs to convert question-answering datasets to MRAG training datasets. Extensive experiments demonstrate that our proposed method significantly improves the generation quality by 17.07% while reducing 8.95% retrieval times.",
        "arxiv_id": "2510.22694",
        "ARXIVID": "2510.22694",
        "COMMENT": "Matches criteria 2 (new MLLM method) and 4 (vision foundation models and applications). Proposes Windsock, an adaptive multimodal retrieval-augmented generation framework for MLLMs, with dynamic retrieval and modality selection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22391": {
        "authors": [
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Jian Wang",
            "Chengpei Tang",
            "Keze Wang"
        ],
        "title": "Top-Down Semantic Refinement for Image Captioning",
        "abstract": "arXiv:2510.22391v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) face an inherent contradiction in image captioning: their powerful single-step generation capabilities often lead to a myopic decision-making process. This makes it difficult to maintain global narrative coherence while capturing rich details, a limitation that is particularly pronounced in tasks that require multi-step and complex scene description. To overcome this fundamental challenge, we redefine image captioning as a goal-oriented hierarchical refinement planning problem, and further propose a novel framework, named Top-Down Semantic Refinement (TDSR), which models the generation process as a Markov Decision Process (MDP). However, planning within the vast state space of a VLM presents a significant computational hurdle. Our core contribution, therefore, is the design of a highly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By incorporating a visual-guided parallel expansion and a lightweight value network, our TDSR reduces the call frequency to the expensive VLM by an order of magnitude without sacrificing planning quality. Furthermore, an adaptive early stopping mechanism dynamically matches computational overhead to the image's complexity. Extensive experiments on multiple benchmarks, including DetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a plug-and-play module, can significantly enhance the performance of existing VLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly competitive results in fine-grained description, compositional generalization, and hallucination suppression.",
        "arxiv_id": "2510.22391",
        "ARXIVID": "2510.22391",
        "COMMENT": "Matches criteria 2 (improvement to VLMs for image captioning) and 4 (vision foundation models and applications). Proposes a novel top-down semantic refinement framework for image captioning using VLMs, with a new MCTS algorithm for efficient planning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.23007": {
        "authors": [
            "Youcan Xu",
            "Zhen Wang",
            "Jiaxin Shi",
            "Kexin Li",
            "Feifei Shao",
            "Jun Xiao",
            "Yi Yang",
            "Jun Yu",
            "Long Chen"
        ],
        "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
        "abstract": "arXiv:2510.23007v1 Announce Type: new  Abstract: While recent text-to-video models excel at generating diverse scenes, they struggle with precise motion control, particularly for complex, multi-subject motions. Although methods for single-motion customization have been developed to address this gap, they fail in compositional scenarios due to two primary challenges: motion-appearance entanglement and ineffective multi-motion blending. This paper introduces CoMo, a novel framework for $\\textbf{compositional motion customization}$ in text-to-video generation, enabling the synthesis of multiple, distinct motions within a single video. CoMo addresses these issues through a two-phase approach. First, in the single-motion learning phase, a static-dynamic decoupled tuning paradigm disentangles motion from appearance to learn a motion-specific module. Second, in the multi-motion composition phase, a plug-and-play divide-and-merge strategy composes these learned motions without additional training by spatially isolating their influence during the denoising process. To facilitate research in this new domain, we also introduce a new benchmark and a novel evaluation metric designed to assess multi-motion fidelity and blending. Extensive experiments demonstrate that CoMo achieves state-of-the-art performance, significantly advancing the capabilities of controllable video generation. Our project page is at https://como6.github.io/.",
        "arxiv_id": "2510.23007",
        "ARXIVID": "2510.23007",
        "COMMENT": "Related to generative modeling in multi-modal learning (criteria 4), with a novel compositional motion customization framework for text-to-video generation. Introduces a new benchmark and evaluation metric for multi-motion fidelity.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2510.23224": {
        "authors": [
            "Hongyi Wang",
            "Zhengjie Zhu",
            "Jiabo Ma",
            "Fang Wang",
            "Yue Shi",
            "Bo Luo",
            "Jili Wang",
            "Qiuyu Cai",
            "Xiuming Zhang",
            "Yen-Wei Chen",
            "Lanfen Lin",
            "Hao Chen"
        ],
        "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
        "abstract": "arXiv:2510.23224v1 Announce Type: new  Abstract: The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology.",
        "arxiv_id": "2510.23224",
        "ARXIVID": "2510.23224",
        "COMMENT": "Matches criteria 2 (new multi-modal large language model for vision-language alignment) and 4 (vision foundation models and applications). The paper presents a scalable multimodal retrieval framework for pathology, using vision-language contrastive learning and attentive alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.23167": {
        "authors": [
            "Zhao Yang",
            "Thomas M. Moerland",
            "Mike Preuss",
            "Aske Plaat",
            "Vincent Fran\\c{c}ois-Lavet",
            "Edward S. Hu"
        ],
        "title": "Guiding Skill Discovery with Foundation Models",
        "abstract": "arXiv:2510.23167v1 Announce Type: new  Abstract: Learning diverse skills without hand-crafted reward functions could accelerate reinforcement learning in downstream tasks. However, existing skill discovery methods focus solely on maximizing the diversity of skills without considering human preferences, which leads to undesirable behaviors and possibly dangerous skills. For instance, a cheetah robot trained using previous methods learns to roll in all directions to maximize skill diversity, whereas we would prefer it to run without flipping or entering hazardous areas. In this work, we propose a Foundation model Guided (FoG) skill discovery method, which incorporates human intentions into skill discovery through foundation models. Specifically, FoG extracts a score function from foundation models to evaluate states based on human intentions, assigning higher values to desirable states and lower to undesirable ones. These scores are then used to re-weight the rewards of skill discovery algorithms. By optimizing the re-weighted skill discovery rewards, FoG successfully learns to eliminate undesirable behaviors, such as flipping or rolling, and to avoid hazardous areas in both state-based and pixel-based tasks. Interestingly, we show that FoG can discover skills involving behaviors that are difficult to define. Interactive visualisations are available from https://sites.google.com/view/submission-fog.",
        "arxiv_id": "2510.23167",
        "ARXIVID": "2510.23167",
        "COMMENT": "Matches criterion 4 (application of foundation models to guide skill discovery in RL/embodied agents). Also relevant to criterion 1 (spatial intelligence in embodied agents via foundation model guidance).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.23151": {
        "authors": [
            "Sixian Liu",
            "Chen Xu",
            "Qiang Wang",
            "Donghai Shi",
            "Yiwen Li"
        ],
        "title": "AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes",
        "abstract": "arXiv:2510.23151v1 Announce Type: new  Abstract: Multimodal camera-LiDAR fusion technology has found extensive application in 3D object detection, demonstrating encouraging performance. However, existing methods exhibit significant performance degradation in challenging scenarios characterized by sensor degradation or environmental disturbances. We propose a novel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates cross-modal knowledge by identifying reliable patterns for robust detection in complex scenes. Specifically, we first project features from each modality into a unified BEV space and enhance them using a window-based attention mechanism. Subsequently, an adaptive gated fusion module based on cross-modal attention is designed to integrate these features into reliable BEV representations robust to challenging environments. Furthermore, we construct a new dataset named Excavator3D (E3D) focusing on challenging excavator operation scenarios to benchmark performance in complex conditions. Our method not only achieves competitive performance on the standard KITTI dataset with 93.92% accuracy, but also significantly outperforms the baseline by 24.88% on the challenging E3D dataset, demonstrating superior robustness to unreliable modal information in complex industrial scenes.",
        "arxiv_id": "2510.23151",
        "ARXIVID": "2510.23151",
        "COMMENT": "Matches criterion 1 (spatial intelligence in 3D object detection with adaptive fusion) and criterion 3 (new dataset for complex industrial scenes, Excavator3D). The adaptive gated fusion is a novel method for robust spatial understanding in challenging environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.23043": {
        "authors": [
            "Joungbin An",
            "Kristen Grauman"
        ],
        "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
        "abstract": "arXiv:2510.23043v1 Announce Type: new  Abstract: Video temporal grounding, the task of localizing the start and end times of a natural language query in untrimmed video, requires capturing both global context and fine-grained temporal detail. This challenge is particularly pronounced in long videos, where existing methods often compromise temporal fidelity by over-downsampling or relying on fixed windows. We present HieraMamba, a hierarchical architecture that preserves temporal structure and semantic richness across scales. At its core are Anchor-MambaPooling (AMP) blocks, which utilize Mamba's selective scanning to produce compact anchor tokens that summarize video content at multiple granularities. Two complementary objectives, anchor-conditioned and segment-pooled contrastive losses, encourage anchors to retain local detail while remaining globally discriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and TACoS, demonstrating precise, temporally faithful localization in long, untrimmed videos.",
        "arxiv_id": "2510.23043",
        "ARXIVID": "2510.23043",
        "COMMENT": "Matches criterion 1 (new method for temporal/spatial understanding in video grounding) and criterion 4 (hierarchical architecture for vision tasks). HieraMamba introduces a new hierarchical pooling for temporal grounding, which is a spatial-temporal understanding problem.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22529": {
        "authors": [
            "Xiang Fei",
            "Tina Tian",
            "Howie Choset",
            "Lu Li"
        ],
        "title": "Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing",
        "abstract": "arXiv:2510.22529v1 Announce Type: new  Abstract: Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency. However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency. The core innovation lies in the introduction of word groups, which captures the spatial co-occurrence and proximity of visual words to construct an online dictionary. Additionally, drawing inspiration from probabilistic transition models, we incorporate temporal consistency directly into similarity computation with an adaptive scheme, substantially improving precision-recall performance. The method is further strengthened by a feature distribution analysis module and dedicated post-verification mechanisms. To evaluate the effectiveness of our method, we conduct experiments on both public datasets and a confined-pipe dataset we constructed. Results demonstrate that BoWG surpasses state-of-the-art methods, including both traditional and learning-based approaches, in terms of precision-recall and computational efficiency. Our approach also exhibits excellent scalability, achieving an average processing time of 16 ms per image across 17,565 images in the Bicocca25b dataset.",
        "arxiv_id": "2510.22529",
        "ARXIVID": "2510.22529",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents) and criterion 3 (new dataset for loop closure in confined pipes, a novel angle). The method introduces spatial co-occurrence of visual words and temporal consistency for robust loop closure, which is a spatial intelligence problem in embodied SLAM.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22693": {
        "authors": [
            "Wenlong Li",
            "Yifei Xu",
            "Yuan Rao",
            "Zhenhua Wang",
            "Shuiguang Deng"
        ],
        "title": "VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree",
        "abstract": "arXiv:2510.22693v1 Announce Type: new  Abstract: Video anomaly detection (VAD) focuses on identifying anomalies in videos. Supervised methods demand substantial in-domain training data and fail to deliver clear explanations for anomalies. In contrast, training-free methods leverage the knowledge reserves and language interactivity of large pre-trained models to detect anomalies. However, the current fixed-length temporal window sampling approaches struggle to accurately capture anomalies with varying temporal spans. Therefore, we propose VADTree that utilizes a Hierarchical Granularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree leverages the knowledge embedded in a pre-trained Generic Event Boundary Detection (GEBD) model to characterize potential anomaly event boundaries. Specifically, VADTree decomposes the video into generic event nodes based on boundary confidence, and performs adaptive coarse-fine hierarchical structuring and redundancy removal to construct the HGTree. Then, the multi-dimensional priors are injected into the visual language models (VLMs) to enhance the node-wise anomaly perception, and anomaly reasoning for generic event nodes is achieved via large language models (LLMs). Finally, an inter-cluster node correlation method is used to integrate the multi-granularity anomaly scores. Extensive experiments on three challenging datasets demonstrate that VADTree achieves state-of-the-art performance in training-free settings while drastically reducing the number of sampled video segments. The code will be available at https://github.com/wenlongli10/VADTree.",
        "arxiv_id": "2510.22693",
        "ARXIVID": "2510.22693",
        "COMMENT": "Matches criterion 2: Shows new VLLMs/MLLMs. Also relevant to criterion 4. Proposes VADTree, a training-free video anomaly detection method leveraging pre-trained VLMs and LLMs for explainable reasoning, with a novel hierarchical tree structure.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.23415": {
        "authors": [
            "Moona Mazher",
            "Geoff J. M. Parker",
            "Daniel C. Alexander"
        ],
        "title": "Towards Generalisable Foundation Models for 3D Brain MRI",
        "abstract": "arXiv:2510.23415v1 Announce Type: new  Abstract: Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.",
        "arxiv_id": "2510.23415",
        "ARXIVID": "2510.23415",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. Introduces BrainFound, a 3D brain MRI foundation model adapted from DINO-v2, showing methodological improvements for 3D medical imaging.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22282": {
        "authors": [
            "Tianhui Liu",
            "Hetian Pang",
            "Xin Zhang",
            "Jie Feng",
            "Yong Li",
            "Pan Hui"
        ],
        "title": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning",
        "abstract": "arXiv:2510.22282v1 Announce Type: new  Abstract: Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce \\textbf{CityRiSE}, a novel framework for \\textbf{R}eason\\textbf{i}ng urban \\textbf{S}ocio-\\textbf{E}conomic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.",
        "arxiv_id": "2510.22282",
        "ARXIVID": "2510.22282",
        "COMMENT": "Matches criterion 2 (new VLLMs) and criterion 4 (vision foundation models and applications). Also relevant to criterion 3 (novel methods in embodied AI) as it uses RL to improve reasoning in LVLMs for urban socio-economic status prediction from visual data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22827": {
        "authors": [
            "Zahraa Al Sahili",
            "Maryam Fetanat",
            "Maimuna Nowaz",
            "Ioannis Patras",
            "Matthew Purver"
        ],
        "title": "FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment",
        "abstract": "arXiv:2510.22827v1 Announce Type: new  Abstract: Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes. Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability). We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges. It scores alignment with an explanation-oriented rubric mapped to [-1, 1]; constrains judgments to a closed label set; requires evidence grounded in the visible content; and mandates abstention when cues are insufficient. Unlike CLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions; unlike mitigation that alters generators, it targets evaluation fairness. We evaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to religion, culture, and disability; and assess profession correctness and alignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions. We also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes. Across datasets, judge models outperform contrastive and face-centric baselines on demographic prediction and improve mean alignment while maintaining high profession accuracy, enabling more reliable, reproducible fairness audits.",
        "arxiv_id": "2510.22827",
        "ARXIVID": "2510.22827",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications). Introduces FairJudge, a protocol using multimodal LLMs as judges for prompt-image alignment and social attribute evaluation, with a new dataset and evaluation rubric.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22119": {
        "authors": [
            "Lihuang Fang",
            "Xiao Hu",
            "Yuchen Zou",
            "Hong Zhang"
        ],
        "title": "CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding",
        "abstract": "arXiv:2510.22119v1 Announce Type: new  Abstract: Deep stereo matching has advanced significantly on benchmark datasets through fine-tuning but falls short of the zero-shot generalization seen in foundation models in other vision tasks. We introduce CogStereo, a novel framework that addresses challenging regions, such as occlusions or weak textures, without relying on dataset-specific priors. CogStereo embeds implicit spatial cognition into the refinement process by using monocular depth features as priors, capturing holistic scene understanding beyond local correspondences. This approach ensures structurally coherent disparity estimation, even in areas where geometry alone is inadequate. CogStereo employs a dual-conditional refinement mechanism that combines pixel-wise uncertainty with cognition-guided features for consistent global correction of mismatches. Extensive experiments on Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate that CogStereo not only achieves state-of-the-art results but also excels in cross-domain generalization, shifting stereo vision towards a cognition-driven approach.",
        "arxiv_id": "2510.22119",
        "ARXIVID": "2510.22119",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes CogStereo, a stereo matching framework with implicit spatial cognition embedding, improving generalization and scene understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22697": {
        "authors": [
            "Vittorio Bernuzzi",
            "Leonardo Rossi",
            "Tomaso Fontanini",
            "Massimo Bertozzi",
            "Andrea Prati"
        ],
        "title": "WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing",
        "abstract": "arXiv:2510.22697v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has recently emerged as a key strategy for building foundation models in remote sensing, where the scarcity of annotated data limits the applicability of fully supervised approaches. In this work, we introduce WaveMAE, a masked autoencoding framework tailored for multispectral satellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE leverages a multi-level Discrete Wavelet Transform (DWT) to disentangle frequency components and guide the encoder toward learning scale-aware high-frequency representations. We further propose a Geo-conditioned Positional Encoding (GPE), which incorporates geographical priors via Spherical Harmonics, encouraging embeddings that respect both semantic and geospatial structure. To ensure fairness in evaluation, all methods are pretrained on the same dataset (fMoW-S2) and systematically evaluated on the diverse downstream tasks of the PANGAEA benchmark, spanning semantic segmentation, regression, change detection, and multilabel classification. Extensive experiments demonstrate that WaveMAE achieves consistent improvements over prior state-of-the-art approaches, with substantial gains on segmentation and regression benchmarks. The effectiveness of WaveMAE pretraining is further demonstrated by showing that even a lightweight variant, containing only 26.4% of the parameters, achieves state-of-the-art performance. Our results establish WaveMAE as a strong and geographically informed foundation model for multispectral remote sensing imagery.",
        "arxiv_id": "2510.22697",
        "ARXIVID": "2510.22697",
        "COMMENT": "This paper introduces WaveMAE, a masked autoencoder foundation model for multispectral remote sensing imagery, with a novel wavelet-based approach and geo-conditioned positional encoding. This matches criterion 4 (vision foundation models and applications) and also shows a clever statistical trick (wavelet decomposition and spherical harmonics for geospatial priors).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22684": {
        "authors": [
            "Jiuniu Wang",
            "Gongjie Zhang",
            "Quanhao Qian",
            "Junlong Gao",
            "Deli Zhao",
            "Ran Xu"
        ],
        "title": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance",
        "abstract": "arXiv:2510.22684v1 Announce Type: new  Abstract: Scalable Vector Graphics (SVGs) are fundamental to digital design and robot control, encoding not only visual structure but also motion paths in interactive drawings. In this work, we introduce RoboSVG, a unified multimodal framework for generating interactive SVGs guided by textual, visual, and numerical signals. Given an input query, the RoboSVG model first produces multimodal guidance, then synthesizes candidate SVGs through dedicated generation modules, and finally refines them under numerical guidance to yield high-quality outputs. To support this framework, we construct RoboDraw, a large-scale dataset of one million examples, each pairing an SVG generation condition (e.g., text, image, and partial SVG) with its corresponding ground-truth SVG code. RoboDraw dataset enables systematic study of four tasks, including basic generation (Text-to-SVG, Image-to-SVG) and interactive generation (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments demonstrate that RoboSVG achieves superior query compliance and visual fidelity across tasks, establishing a new state of the art in versatile SVG generation. The dataset and source code of this project will be publicly available soon.",
        "arxiv_id": "2510.22684",
        "ARXIVID": "2510.22684",
        "COMMENT": "Matches criterion 2 (new multimodal models) and criterion 4 (vision foundation models and applications). Introduces RoboSVG, a unified multimodal framework for SVG generation with a large-scale dataset and strong results in interactive SVG generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.22335": {
        "authors": [
            "Xu Zhang",
            "Ruijie Quan",
            "Wenguan Wang",
            "Yi Yang"
        ],
        "title": "Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction",
        "abstract": "arXiv:2510.22335v1 Announce Type: new  Abstract: Reconstructing visual stimuli from fMRI signals is a central challenge bridging machine learning and neuroscience. Recent diffusion-based methods typically map fMRI activity to a single high-level embedding, using it as fixed guidance throughout the entire generation process. However, this fixed guidance collapses hierarchical neural information and is misaligned with the stage-dependent demands of image reconstruction. In response, we propose MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on scale-wise autoregressive modeling. MindHier introduces three components: a Hierarchical fMRI Encoder to extract multi-level neural embeddings, a Hierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy to inject these embeddings into autoregression at matching scales. These designs make MindHier an efficient and cognitively-aligned alternative to diffusion-based methods by enabling a hierarchical reconstruction process that synthesizes global semantics before refining local details, akin to human visual perception. Extensive experiments on the NSD dataset show that MindHier achieves superior semantic fidelity, 4.67x faster inference, and more deterministic results than the diffusion-based baselines.",
        "arxiv_id": "2510.22335",
        "ARXIVID": "2510.22335",
        "COMMENT": "Related to generative modeling and vision-language alignment (criteria 4), with a new hierarchy-to-hierarchy autoregressive model for fMRI-to-image reconstruction. Not directly about spatial intelligence or embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2510.23087": {
        "authors": [
            "Taoyu Wu",
            "Yiyi Miao",
            "Jiaxin Guo",
            "Ziyan Chen",
            "Sihang Zhao",
            "Zhuoxiao Li",
            "Zhe Tang",
            "Baoru Huang",
            "Limin Yu"
        ],
        "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
        "abstract": "arXiv:2510.23087v1 Announce Type: new  Abstract: In robot-assisted minimally invasive surgery, accurate 3D reconstruction from endoscopic video is vital for downstream tasks and improved outcomes. However, endoscopic scenarios present unique challenges, including photometric inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most 3DGS-based methods that rely solely on appearance constraints for optimizing 3DGS are often insufficient in this context, as these dynamic visual artifacts can mislead the optimization process and lead to inaccurate reconstructions. To address these limitations, we present EndoWave, a unified spatio-temporal Gaussian Splatting framework by incorporating an optical flow-based geometric constraint and a multi-resolution rational wavelet supervision. First, we adopt a unified spatio-temporal Gaussian representation that directly optimizes primitives in a 4D domain. Second, we propose a geometric constraint derived from optical flow to enhance temporal coherence and effectively constrain the 3D structure of the scene. Third, we propose a multi-resolution rational orthogonal wavelet as a constraint, which can effectively separate the details of the endoscope and enhance the rendering performance. Extensive evaluations on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our method EndoWave achieves state-of-the-art reconstruction quality and visual accuracy compared to the baseline method.",
        "arxiv_id": "2510.23087",
        "ARXIVID": "2510.23087",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel 4D Gaussian Splatting method for endoscopic 3D reconstruction, with new constraints for handling dynamic visual artifacts.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.22868": {
        "authors": [
            "Yang Zhang",
            "Qianyu Zhou",
            "Farhad Imani",
            "Jiong Tang"
        ],
        "title": "Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models",
        "abstract": "arXiv:2510.22868v1 Announce Type: new  Abstract: Wind turbine blades operate in harsh environments, making timely damage detection essential for preventing failures and optimizing maintenance. Drone-based inspection and deep learning are promising, but typically depend on large, labeled datasets, which limit their ability to detect rare or evolving damage types. To address this, we propose a zero-shot-oriented inspection framework that integrates Retrieval-Augmented Generation (RAG) with Vision-Language Models (VLM). A multimodal knowledge base is constructed, comprising technical documentation, representative reference images, and domain-specific guidelines. A hybrid text-image retriever with keyword-aware reranking assembles the most relevant context to condition the VLM at inference, injecting domain knowledge without task-specific training. We evaluate the framework on 30 labeled blade images covering diverse damage categories. Although the dataset is small due to the difficulty of acquiring verified blade imagery, it covers multiple representative defect types. On this test set, the RAG-grounded VLM correctly classified all samples, whereas the same VLM without retrieval performed worse in both accuracy and precision. We further compare against open-vocabulary baselines and incorporate uncertainty Clopper-Pearson confidence intervals to account for the small-sample setting. Ablation studies indicate that the key advantage of the framework lies in explainability and generalizability: retrieved references ground the reasoning process and enable the detection of previously unseen defects by leveraging domain knowledge rather than relying solely on visual cues. This research contributes a data-efficient solution for industrial inspection that reduces dependence on extensive labeled datasets.",
        "arxiv_id": "2510.22868",
        "ARXIVID": "2510.22868",
        "COMMENT": "Matches criterion 2: Shows new VLM application. Also relevant to criterion 4. Proposes a knowledge-augmented VLM for zero-shot wind turbine blade inspection, integrating retrieval-augmented generation and multimodal knowledge base for explainable, generalizable results.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.21879": {
        "authors": [
            "Shu-Hao Zhang",
            "Wei-Cheng Tang",
            "Chen Wu",
            "Peng Hu",
            "Nan Li",
            "Liang-Jie Zhang",
            "Qi Zhang",
            "Shao-Qun Zhang"
        ],
        "title": "TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge",
        "abstract": "arXiv:2510.21879v1 Announce Type: new  Abstract: Recent years have witnessed an increasing interest in image-text contrastive modeling, exemplified by models such as Contrastive Language-Image Pretraining (CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational framework that converts connection weights of both vision and text encoders of CLIP into the ternary format, instead of full-precision or floating ones. TernaryCLIP incorporates quantization-aware training and distillation modules, preventing precision degradation and enabling low-cost and high-efficiency computations. Comprehensive experiments demonstrate that TernaryCLIP can achieve up to 99\\% ternarized weights with 1.58-bit representation, 16.98 $\\times$ compression ratio, 2.3 $\\times$ inference acceleration, 16 $\\times$ storage reduction, 10 $\\times$ memory optimization, and 60\\% sparsity while maintaining promising performance on zero-shot image classification and image-text retrieval tasks across 41 commonly used datasets. Our work highlights the feasibility of extreme quantization for large multimodal models, supporting effective and efficient deployment on resource-constrained devices. The model and code can be accessed from Hugging Face and GitHub.",
        "arxiv_id": "2510.21879",
        "ARXIVID": "2510.21879",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes TernaryCLIP, an efficient quantized version of CLIP with ternary weights, enabling deployment on resource-constrained devices.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.22534": {
        "authors": [
            "Chen Chen",
            "Majid Abdolshah",
            "Violetta Shevchenko",
            "Hongdong Li",
            "Chang Xu",
            "Pulak Purkait"
        ],
        "title": "SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning",
        "abstract": "arXiv:2510.22534v1 Announce Type: new  Abstract: Existing diffusion-based super-resolution approaches often exhibit semantic ambiguities due to inaccuracies and incompleteness in their text conditioning, coupled with the inherent tendency for cross-attention to divert towards irrelevant pixels. These limitations can lead to semantic misalignment and hallucinated details in the generated high-resolution outputs. To address these, we propose a novel, plug-and-play spatially re-focused super-resolution (SRSR) framework that consists of two core components: first, we introduce Spatially Re-focused Cross-Attention (SRCA), which refines text conditioning at inference time by applying visually-grounded segmentation masks to guide cross-attention. Second, we introduce a Spatially Targeted Classifier-Free Guidance (STCFG) mechanism that selectively bypasses text influences on ungrounded pixels to prevent hallucinations. Extensive experiments on both synthetic and real-world datasets demonstrate that SRSR consistently outperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR and SSIM) across all datasets, and in perceptual quality measures (LPIPS and DISTS) on two real-world benchmarks, underscoring its effectiveness in achieving both high semantic fidelity and perceptual quality in super-resolution.",
        "arxiv_id": "2510.22534",
        "ARXIVID": "2510.22534",
        "COMMENT": "This paper proposes SRSR, a diffusion-based super-resolution framework with spatially re-focused text conditioning and classifier-free guidance. It is a vision foundation model application with clever cross-attention tricks, matching criterion 4.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.22229": {
        "authors": [
            "Jeongin Kim",
            "Wonho Bae",
            "YouLee Han",
            "Giyeong Oh",
            "Youngjae Yu",
            "Danica J. Sutherland",
            "Junhyug Noh"
        ],
        "title": "Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation",
        "abstract": "arXiv:2510.22229v1 Announce Type: new  Abstract: Semantic segmentation demands dense pixel-level annotations, which can be prohibitively expensive - especially under extremely constrained labeling budgets. In this paper, we address the problem of low-budget active learning for semantic segmentation by proposing a novel two-stage selection pipeline. Our approach leverages a pre-trained diffusion model to extract rich multi-scale features that capture both global structure and fine details. In the first stage, we perform a hierarchical, representation-based candidate selection by first choosing a small subset of representative pixels per image using MaxHerding, and then refining these into a diverse global pool. In the second stage, we compute an entropy-augmented disagreement score (eDALD) over noisy multi-scale diffusion features to capture both epistemic uncertainty and prediction confidence, selecting the most informative pixels for annotation. This decoupling of diversity and uncertainty lets us achieve high segmentation accuracy with only a tiny fraction of labeled pixels. Extensive experiments on four benchmarks (CamVid, ADE-Bed, Cityscapes, and Pascal-Context) demonstrate that our method significantly outperforms existing baselines under extreme pixel-budget regimes. Our code is available at https://github.com/jn-kim/two-stage-edald.",
        "arxiv_id": "2510.22229",
        "ARXIVID": "2510.22229",
        "COMMENT": "This paper proposes a new two-stage active learning method for semantic segmentation using diffusion models for feature extraction. It introduces a novel entropy-augmented disagreement score and decouples diversity and uncertainty in pixel selection. This is a methodological improvement in spatial understanding for vision tasks, matching criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.23118": {
        "authors": [
            "Gianfranco Basile",
            "Johannes Jakubik",
            "Benedikt Blumenstiel",
            "Thomas Brunschwiler",
            "Juan Bernabe Moreno"
        ],
        "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
        "abstract": "arXiv:2510.23118v1 Announce Type: new  Abstract: We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods by 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license.",
        "arxiv_id": "2510.23118",
        "ARXIVID": "2510.23118",
        "COMMENT": "Matches criterion 2 (multimodal fusion, cross-modal generation) and criterion 4 (vision foundation models and applications). The paper proposes a task-agnostic framework for fusing time series and imagery, with masked correlation learning and strong empirical results in Earth observation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.22675": {
        "authors": [
            "Jinpeng Dong",
            "Chen Li",
            "Yutong Lin",
            "Jingwen Fu",
            "Sanping Zhou",
            "Nanning Zheng"
        ],
        "title": "DAMap: Distance-aware MapNet for High Quality HD Map Construction",
        "abstract": "arXiv:2510.22675v1 Announce Type: new  Abstract: Predicting High-definition (HD) map elements with high quality (high classification and localization scores) is crucial to the safety of autonomous driving vehicles. However, current methods perform poorly in high quality predictions due to inherent task misalignment. Two main factors are responsible for misalignment: 1) inappropriate task labels due to one-to-many matching queries sharing the same labels, and 2) sub-optimal task features due to task-shared sampling mechanism. In this paper, we reveal two inherent defects in current methods and develop a novel HD map construction method named DAMap to address these problems. Specifically, DAMap consists of three components: Distance-aware Focal Loss (DAFL), Hybrid Loss Scheme (HLS), and Task Modulated Deformable Attention (TMDA). The DAFL is introduced to assign appropriate classification labels for one-to-many matching samples. The TMDA is proposed to obtain discriminative task-specific features. Furthermore, the HLS is proposed to better utilize the advantages of the DAFL. We perform extensive experiments and consistently achieve performance improvement on the NuScenes and Argoverse2 benchmarks under different metrics, baselines, splits, backbones, and schedules. Code will be available at https://github.com/jpdong-xjtu/DAMap.",
        "arxiv_id": "2510.22675",
        "ARXIVID": "2510.22675",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method for HD map construction with novel loss functions and attention mechanisms, relevant for spatial understanding in autonomous driving.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.22276": {
        "authors": [
            "Issa Sugiura",
            "Shuhei Kurita",
            "Yusuke Oda",
            "Daisuke Kawahara",
            "Yasuo Okabe",
            "Naoaki Okazaki"
        ],
        "title": "WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models",
        "abstract": "arXiv:2510.22276v1 Announce Type: new  Abstract: Large-scale and high-quality image-text pair datasets play an important role in developing high-performing Vision-Language Models (VLMs). In this work, we introduce WAON, a large-scale and high-quality Japanese image-text pair dataset containing approximately 155 million examples, collected from Common Crawl. Our dataset construction pipeline employs various techniques, including filtering and deduplication, which have been shown to be effective in previous studies. To evaluate its effectiveness, we also construct WAON-Bench, a manually curated benchmark for Japanese cultural image classification, consisting of 374 classes. To assess the effectiveness of our dataset, we conduct experiments using both WAON and the Japanese subset of ReLAION, one of the most widely used vision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on both datasets. The results demonstrate that WAON enhances model performance on WAON-Bench more efficiently than ReLAION and achieves higher accuracy across all evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves state-of-the-art performance on several Japanese cultural benchmarks. We release our dataset, model, and code at https://speed1313.github.io/WAON.",
        "arxiv_id": "2510.22276",
        "ARXIVID": "2510.22276",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a large-scale Japanese image-text dataset and a new benchmark, with empirical improvements for VLMs in Japanese cultural contexts.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.23506": {
        "authors": [
            "Hyeongseop Rha",
            "Jeong Hun Yeo",
            "Yeonju Kim",
            "Yong Man Ro"
        ],
        "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier",
        "abstract": "arXiv:2510.23506v1 Announce Type: new  Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.",
        "arxiv_id": "2510.23506",
        "ARXIVID": "2510.23506",
        "COMMENT": "Matches criterion 2 (new MLLMs) and is relevant to multi-modal learning. Proposes an Emotional Rationale Verifier to improve emotion-prediction consistency in MLLMs, enhancing interpretability and trustworthiness in emotion recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.21862": {
        "authors": [
            "Muhammad Tayyab Khan",
            "Zane Yong",
            "Lequn Chen",
            "Wenhe Feng",
            "Nicholas Yew Jin Tan",
            "Seung Ki Moon"
        ],
        "title": "A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model",
        "abstract": "arXiv:2510.21862v1 Announce Type: new  Abstract: Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.",
        "arxiv_id": "2510.21862",
        "ARXIVID": "2510.21862",
        "COMMENT": "This paper proposes a multi-stage hybrid framework using vision language models (VLMs) for automated interpretation of engineering drawings. It applies VLMs in a novel domain, matching criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.22964": {
        "authors": [
            "Liling Yang",
            "Ning Chen",
            "Jun Yue",
            "Yidan Liu",
            "Jiayi Ma",
            "Pedram Ghamisi",
            "Antonio Plaza",
            "Leyuan Fang"
        ],
        "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
        "abstract": "arXiv:2510.22964v1 Announce Type: new  Abstract: Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.",
        "arxiv_id": "2510.22964",
        "ARXIVID": "2510.22964",
        "COMMENT": "This is a comprehensive survey of multimodal geospatial foundation models, including vision-language models, their techniques, applications, and challenges. It directly matches criterion 4 (vision foundation models and applications), and also touches on spatial understanding in geospatial contexts (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 5
    },
    "2510.22035": {
        "authors": [
            "Patrick Koller (Northwestern University",
            "Evanston",
            "Illinois",
            "United States)",
            "Amil V. Dravid (University of California",
            "Berkeley",
            "California",
            "United States)",
            "Guido M. Schuster (Eastern Switzerland University of Applied Sciences",
            "Rapperswil",
            "St. Gallen",
            "Switzerland)",
            "Aggelos K. Katsaggelos (Northwestern University",
            "Evanston",
            "Illinois",
            "United States)"
        ],
        "title": "Caption-Driven Explainability: Probing CNNs for Bias via CLIP",
        "abstract": "arXiv:2510.22035v1 Announce Type: new  Abstract: Robustness has become one of the most critical problems in machine learning (ML). The science of interpreting ML models to understand their behavior and improve their robustness is referred to as explainable artificial intelligence (XAI). One of the state-of-the-art XAI methods for computer vision problems is to generate saliency maps. A saliency map highlights the pixel space of an image that excites the ML model the most. However, this property could be misleading if spurious and salient features are present in overlapping pixel spaces. In this paper, we propose a caption-based XAI method, which integrates a standalone model to be explained into the contrastive language-image pre-training (CLIP) model using a novel network surgery approach. The resulting caption-based XAI model identifies the dominant concept that contributes the most to the models prediction. This explanation minimizes the risk of the standalone model falling for a covariate shift and contributes significantly towards developing robust ML models.",
        "arxiv_id": "2510.22035",
        "ARXIVID": "2510.22035",
        "COMMENT": "This paper proposes a caption-based XAI method integrating a standalone model into CLIP for explainability and bias probing. It leverages a vision-language foundation model (CLIP) for a novel explainability approach. Closely matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.23145": {
        "authors": [
            "Yaoyan Zheng",
            "Huiqun Wang",
            "Nan Zhou",
            "Di Huang"
        ],
        "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
        "abstract": "arXiv:2510.23145v1 Announce Type: new  Abstract: Transferability estimation identifies the best pre-trained models for downstream tasks without incurring the high computational cost of full fine-tuning. This capability facilitates deployment and advances the pre-training and fine-tuning paradigm. However, existing methods often struggle to accurately assess transferability for emerging pre-trained models with diverse architectures, training strategies, and task alignments. In this work, we propose Implicit Transferability Modeling (ITM), a novel framework that implicitly models each model's intrinsic transferability, coupled with a Divide-and-Conquer Variational Approximation (DVA) strategy to efficiently approximate embedding space evolution. This design enables generalization across a broader range of models and downstream tasks. Extensive experiments on a comprehensive benchmark--spanning extensive training regimes and a wider variety of model types--demonstrate that ITM consistently outperforms existing methods in terms of stability, effectiveness, and efficiency.",
        "arxiv_id": "2510.23145",
        "ARXIVID": "2510.23145",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new framework for transferability estimation of vision foundation models, with a novel divide-and-conquer variational approximation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.23116": {
        "authors": [
            "Hebaixu Wang",
            "Jing Zhang",
            "Haoyang Chen",
            "Haonan Guo",
            "Di Wang",
            "Jiayi Ma",
            "Bo Du"
        ],
        "title": "Residual Diffusion Bridge Model for Image Restoration",
        "abstract": "arXiv:2510.23116v1 Announce Type: new  Abstract: Diffusion bridge models establish probabilistic paths between arbitrary paired distributions and exhibit great potential for universal image restoration. Most existing methods merely treat them as simple variants of stochastic interpolants, lacking a unified analytical perspective. Besides, they indiscriminately reconstruct images through global noise injection and removal, inevitably distorting undegraded regions due to imperfect reconstruction. To address these challenges, we propose the Residual Diffusion Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic differential equations of generalized diffusion bridge and derive the analytical formulas of its forward and reverse processes. Crucially, we leverage the residuals from given distributions to modulate the noise injection and removal, enabling adaptive restoration of degraded regions while preserving intact others. Moreover, we unravel the fundamental mathematical essence of existing bridge models, all of which are special cases of RDBM and empirically demonstrate the optimality of our proposed models. Extensive experiments are conducted to demonstrate the state-of-the-art performance of our method both qualitatively and quantitatively across diverse image restoration tasks. Code is publicly available at https://github.com/MiliLab/RDBM.",
        "arxiv_id": "2510.23116",
        "ARXIVID": "2510.23116",
        "COMMENT": "Relevant to generative modeling (criteria 4) via diffusion models for image restoration, with a new residual diffusion bridge model and theoretical analysis. Not directly about spatial intelligence or VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.21857": {
        "authors": [
            "Anthony Zhang",
            "Mahmut Gokmen",
            "Dennis Hein",
            "Rongjun Ge",
            "Wenjun Xia",
            "Ge Wang",
            "Jin Chen"
        ],
        "title": "Poisson Flow Consistency Training",
        "abstract": "arXiv:2510.21857v1 Announce Type: new  Abstract: The Poisson Flow Consistency Model (PFCM) is a consistency-style model based on the robust Poisson Flow Generative Model++ (PFGM++) which has achieved success in unconditional image generation and CT image denoising. Yet the PFCM can only be trained in distillation which limits the potential of the PFCM in many data modalities. The objective of this research was to create a method to train the PFCM in isolation called Poisson Flow Consistency Training (PFCT). The perturbation kernel was leveraged to remove the pretrained PFGM++, and the sinusoidal discretization schedule and Beta noise distribution were introduced in order to facilitate adaptability and improve sample quality. The model was applied to the task of low dose computed tomography image denoising and improved the low dose image in terms of LPIPS and SSIM. It also displayed similar denoising effectiveness as models like the Consistency Model. PFCT is established as a valid method of training the PFCM from its effectiveness in denoising CT images, showing potential with competitive results to other generative models. Further study is needed in the precise optimization of PFCT and in its applicability to other generative modeling tasks. The framework of PFCT creates more flexibility for the ways in which a PFCM can be created and can be applied to the field of generative modeling.",
        "arxiv_id": "2510.21857",
        "ARXIVID": "2510.21857",
        "COMMENT": "Matches criterion 4 (vision foundation models and generative modeling). Proposes a new training method for Poisson Flow Consistency Models, with applications in image denoising and generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.21791": {
        "authors": [
            "Niraj Chaudhari",
            "Manmeet Singh",
            "Naveen Sudharsan",
            "Amit Kumar Srivastava",
            "Harsh Kamath",
            "Dushyant Mahajan",
            "Ayan Paul"
        ],
        "title": "Exploring the design space of diffusion and flow models for data fusion",
        "abstract": "arXiv:2510.21791v1 Announce Type: new  Abstract: Data fusion is an essential task in various domains, enabling the integration of multi-source information to enhance data quality and insights. One key application is in satellite remote sensing, where fusing multi-sensor observations can improve spatial and temporal resolution. In this study, we explore the design space of diffusion and flow models for data fusion, focusing on the integration of Defense Meteorological Satellite Program's Operational Linescan System (DMSP-OLS) and Visible Infrared Imaging Radiometer Suite (VIIRS) nighttime lights data. Our approach leverages a diverse set of 2D image-to-image generative models, including UNET, diffusion, and flow modeling architectures. We evaluate the effectiveness of these architectures in satellite remote sensing data fusion, identifying diffusion models based on UNet as particularly adept at preserving fine-grained spatial details and generating high-fidelity fused images. We also provide guidance on the selection of noise schedulers in diffusion-based models, highlighting the trade-offs between iterative solvers for faster inference and discrete schedulers for higher-quality reconstructions. Additionally, we explore quantization techniques to optimize memory efficiency and computational cost without compromising performance. Our findings offer practical insights into selecting the most effective diffusion and flow model architectures for data fusion tasks, particularly in remote sensing applications, and provide recommendations for leveraging noise scheduling strategies to enhance fusion quality.",
        "arxiv_id": "2510.21791",
        "ARXIVID": "2510.21791",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to criterion 1 (spatial understanding in remote sensing). Explores diffusion and flow models for data fusion in satellite imagery, with practical insights on model selection and noise scheduling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.22995": {
        "authors": [
            "Md Mostafijur Rahman",
            "Radu Marculescu"
        ],
        "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
        "abstract": "arXiv:2510.22995v1 Announce Type: new  Abstract: U-shaped networks output logits at multiple spatial scales, each capturing a different blend of coarse context and fine detail. Yet, training still treats these logits in isolation - either supervising only the final, highest-resolution logits or applying deep supervision with identical loss weights at every scale - without exploring mixed-scale combinations. Consequently, the decoder output misses the complementary cues that arise only when coarse and fine predictions are fused. To address this issue, we introduce LoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that generates new mixed-scale outputs and learns how exactly each of them should guide the training process. More precisely, LoMix mixes the multi-scale decoder logits with four lightweight fusion operators: addition, multiplication, concatenation, and attention-based weighted fusion, yielding a rich set of synthetic mutant maps. Every original or mutant map is given a softplus loss weight that is co-optimized with network parameters, mimicking a one-step architecture search that automatically discovers the most useful scales, mixtures, and operators. Plugging LoMix into recent U-shaped architectures (i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset improves DICE by +4.2% over single-output supervision, +2.2% over deep supervision, and +1.5% over equally weighted additive fusion, all with zero inference overhead. When training data are scarce (e.g., one or two labeled scans), the advantage grows to +9.23%, underscoring LoMix's data efficiency. Across four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up to +13.5% over single-output supervision, confirming that learnable weighted mixed-scale fusion generalizes broadly while remaining data efficient, fully interpretable, and overhead-free at inference. Our code is available at https://github.com/SLDGroup/LoMix.",
        "arxiv_id": "2510.22995",
        "ARXIVID": "2510.22995",
        "COMMENT": "Matches criterion 1 (methodological improvement to spatial understanding in segmentation) and is relevant to criterion 4 (vision foundation models, as it proposes a plug-and-play module for U-shaped networks). The LoMix module introduces learnable weighted multi-scale logits mixing for medical image segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.22803": {
        "authors": [
            "Hai-Dang Nguyen",
            "Minh-Anh Dang",
            "Minh-Tan Le",
            "Minh-Tuan Le"
        ],
        "title": "MedXplain-VQA: Multi-Component Explainable Medical Visual Question Answering",
        "abstract": "arXiv:2510.22803v1 Announce Type: new  Abstract: Explainability is critical for the clinical adoption of medical visual question answering (VQA) systems, as physicians require transparent reasoning to trust AI-generated diagnoses. We present MedXplain-VQA, a comprehensive framework integrating five explainable AI components to deliver interpretable medical image analysis. The framework leverages a fine-tuned BLIP-2 backbone, medical query reformulation, enhanced Grad-CAM attention, precise region extraction, and structured chain-of-thought reasoning via multi-modal language models. To evaluate the system, we introduce a medical-domain-specific framework replacing traditional NLP metrics with clinically relevant assessments, including terminology coverage, clinical structure quality, and attention region relevance. Experiments on 500 PathVQA histopathology samples demonstrate substantial improvements, with the enhanced system achieving a composite score of 0.683 compared to 0.378 for baseline methods, while maintaining high reasoning confidence (0.890). Our system identifies 3-5 diagnostically relevant regions per sample and generates structured explanations averaging 57 words with appropriate clinical terminology. Ablation studies reveal that query reformulation provides the most significant initial improvement, while chain-of-thought reasoning enables systematic diagnostic processes. These findings underscore the potential of MedXplain-VQA as a robust, explainable medical VQA system. Future work will focus on validation with medical experts and large-scale clinical datasets to ensure clinical readiness.",
        "arxiv_id": "2510.22803",
        "ARXIVID": "2510.22803",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) and partially criterion 4 (vision foundation models and applications), as it presents a new explainable medical VQA system leveraging a fine-tuned BLIP-2 backbone and multi-modal language models, with a focus on explainability in medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2510.23564": {
        "authors": [
            "Zhaoyang Yu",
            "Jiayi Zhang",
            "Huixue Su",
            "Yufan Zhao",
            "Yifan Wu",
            "Mingyi Deng",
            "Jinyu Xiang",
            "Yizhang Lin",
            "Lingxiao Tang",
            "Yingchao Li",
            "Yuyu Luo",
            "Bang Liu",
            "Chenglin Wu"
        ],
        "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
        "abstract": "arXiv:2510.23564v1 Announce Type: new  Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.",
        "arxiv_id": "2510.23564",
        "ARXIVID": "2510.23564",
        "COMMENT": "Somewhat related to embodied AI and planning (criteria 3), but focuses on LLM-based agents and recursive code generation for unifying planning and action. Not directly about spatial intelligence or vision.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.23285": {
        "authors": [
            "Ruoyu Wang",
            "Beier Zhu",
            "Junzhi Li",
            "Liangyu Yuan",
            "Chi Zhang"
        ],
        "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
        "abstract": "arXiv:2510.23285v1 Announce Type: new  Abstract: Diffusion-based generative processes, formulated as differential equation solving, frequently balance computational speed with sample quality. Our theoretical investigation of ODE- and SDE-based solvers reveals complementary weaknesses: ODE solvers accumulate irreducible gradient error along deterministic trajectories, while SDE methods suffer from amplified discretization errors when the step budget is limited. Building upon this insight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify the efficiency of ODEs with the error resilience of SDEs. Specifically, we introduce a single per-step learnable coefficient, estimated via lightweight distillation, which dynamically regulates the error correction strength to accelerate diffusion sampling. Notably, our framework can be integrated with existing solvers to enhance their capabilities. Extensive experiments demonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores of 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available in https://github.com/WLU-wry02/AdaSDE.",
        "arxiv_id": "2510.23285",
        "ARXIVID": "2510.23285",
        "COMMENT": "Relevant to generative modeling and diffusion models, but does not directly match any specific criterion. Proposes AdaSDE, a new SDE solver for diffusion sampling with improved efficiency and sample quality.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.22161": {
        "authors": [
            "Shuhong Liu",
            "Lin Gu",
            "Ziteng Cui",
            "Xuangeng Chu",
            "Tatsuya Harada"
        ],
        "title": "I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions",
        "abstract": "arXiv:2510.22161v1 Announce Type: new  Abstract: Participating in efforts to endow generative AI with the 3D physical world perception, we propose I2-NeRF, a novel neural radiance field framework that enhances isometric and isotropic metric perception under media degradation. While existing NeRF models predominantly rely on object-centric sampling, I2-NeRF introduces a reverse-stratified upsampling strategy to achieve near-uniform sampling across 3D space, thereby preserving isometry. We further present a general radiative formulation for media degradation that unifies emission, absorption, and scattering into a particle model governed by the Beer-Lambert attenuation law. By composing the direct and media-induced in-scatter radiance, this formulation extends naturally to complex media environments such as underwater, haze, and even low-light scenes. By treating light propagation uniformly in both vertical and horizontal directions, I2-NeRF enables isotropic metric perception and can even estimate medium properties such as water depth. Experiments on real-world datasets demonstrate that our method significantly improves both reconstruction fidelity and physical plausibility compared to existing approaches.",
        "arxiv_id": "2510.22161",
        "ARXIVID": "2510.22161",
        "COMMENT": "I2-NeRF introduces a physically-grounded neural radiance field framework for 3D perception under media degradation, with a novel sampling and radiative formulation. It is a methodological improvement in 3D vision, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to general interest in computer vision.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.21986": {
        "authors": [
            "Dogyun Park",
            "Moayed Haji-Ali",
            "Yanyu Li",
            "Willi Menapace",
            "Sergey Tulyakov",
            "Hyunwoo J. Kim",
            "Aliaksandr Siarohin",
            "Anil Kag"
        ],
        "title": "Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers",
        "abstract": "arXiv:2510.21986v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\\\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.",
        "arxiv_id": "2510.21986",
        "ARXIVID": "2510.21986",
        "COMMENT": "SPRINT proposes a sparse-dense residual fusion method for efficient diffusion transformers, enabling aggressive token dropping and efficient training. This is a novel method for generative modeling in vision, but does not directly match the specific criteria (no VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model).",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.22107": {
        "authors": [
            "Bailey Trang",
            "Parham Saremi",
            "Alan Q. Wang",
            "Fangrui Huang",
            "Zahra TehraniNasab",
            "Amar Kumar",
            "Tal Arbel",
            "Li Fei-Fei",
            "Ehsan Adeli"
        ],
        "title": "Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation",
        "abstract": "arXiv:2510.22107v1 Announce Type: new  Abstract: Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.",
        "arxiv_id": "2510.22107",
        "ARXIVID": "2510.22107",
        "COMMENT": "This paper introduces Rainbow, a framework for diverse conditional image generation using GFlowNets to discover latent graphs. It is a novel generative modeling approach for multi-modal learning and diversity, which is of interest, but does not directly match the listed criteria (no VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model).",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2510.23144": {
        "authors": [
            "Ziyu Wang",
            "Wenhao Li",
            "Ji Wu"
        ],
        "title": "DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios",
        "abstract": "arXiv:2510.23144v1 Announce Type: new  Abstract: 3D object detection from multi-view images in traffic scenarios has garnered significant attention in recent years. Many existing approaches rely on object queries that are generated from 3D reference points to localize objects. However, a limitation of these methods is that some reference points are often far from the target object, which can lead to false positive detections. In this paper, we propose a depth-guided query generator for 3D object detection (DQ3D) that leverages depth information and 2D detections to ensure that reference points are sampled from the surface or interior of the object. Furthermore, to address partially occluded objects in current frame, we introduce a hybrid attention mechanism that fuses historical detection results with depth-guided queries, thereby forming hybrid queries. Evaluation on the nuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\% in terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection Score (NDS).",
        "arxiv_id": "2510.23144",
        "ARXIVID": "2510.23144",
        "COMMENT": "Somewhat related to spatial understanding (criteria 1) in 3D object detection, but not focused on embodied agents or multi-modal learning. The method is a new depth-guided query for transformer-based 3D object detection.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.22736": {
        "authors": [
            "Ningli Xu",
            "Rongjun Qin"
        ],
        "title": "Cross-view Localization and Synthesis - Datasets, Challenges and Opportunities",
        "abstract": "arXiv:2510.22736v1 Announce Type: new  Abstract: Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery. These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality. Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery. Both tasks remain challenging due to significant differences in viewing perspective, resolution, and occlusion, which are widely embedded in cross-view datasets. Recent years have witnessed rapid progress driven by the availability of large-scale datasets and novel approaches. Typically, cross-view localization is formulated as an image retrieval problem where ground-level features are matched with tiled overhead images feature, extracted by convolutional neural networks (CNNs) or vision transformers (ViTs) for cross-view feature embedding. Cross-view synthesis, on the other hand, seeks to generate ground-level views based on information from overhead imagery, generally using generative adversarial networks (GANs) or diffusion models. This paper presents a comprehensive survey of advances in cross-view localization and synthesis, reviewing widely used datasets, highlighting key challenges, and providing an organized overview of state-of-the-art techniques. Furthermore, it discusses current limitations, offers comparative analyses, and outlines promising directions for future research. We also include the project page via https://github.com/GDAOSU/Awesome-Cross-View-Methods.",
        "arxiv_id": "2510.22736",
        "ARXIVID": "2510.22736",
        "COMMENT": "Survey paper on cross-view localization and synthesis, which is relevant to spatial understanding (criterion 1) and vision foundation models (criterion 4), but as a survey, less novel.",
        "RELEVANCE": 6,
        "NOVELTY": 4
    },
    "2510.23301": {
        "authors": [
            "Yingying Feng",
            "Jie Li",
            "Jie Hu",
            "Yukang Zhang",
            "Lei Tan",
            "Jiayi Ji"
        ],
        "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
        "abstract": "arXiv:2510.23301v1 Announce Type: new  Abstract: Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively. The code is available at: \\textcolor{magenta}{https://github.com/stone96123/MDReID}.",
        "arxiv_id": "2510.23301",
        "ARXIVID": "2510.23301",
        "COMMENT": "Relevant to computer vision and multi-modal learning, but does not directly match any specific criterion. Proposes a modality-decoupled learning framework for object re-identification across different sensor modalities.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.22981": {
        "authors": [
            "Jin Hu",
            "Jiakai Wang",
            "Linna Jing",
            "Haolin Li",
            "Haodong Liu",
            "Haotong Qin",
            "Aishan Liu",
            "Ke Xu",
            "Xianglong Liu"
        ],
        "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
        "abstract": "arXiv:2510.22981v1 Announce Type: new  Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which are directly generated from natural language instructions, have become a promising avenue for future research due to their flexible attacking forms. To generate SemanticAEs, current methods fall short of satisfactory attacking ability as the key underlying factors of semantic uncertainty in human instructions, such as referring diversity, descriptive incompleteness, and boundary ambiguity, have not been fully investigated. To tackle the issues, this paper develops a multi-dimensional instruction uncertainty reduction (InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable, adaptive, and effective. Specifically, in the dimension of the sampling method, we propose the residual-driven attacking direction stabilization to alleviate the unstable adversarial optimization caused by the diversity of language references. By coarsely predicting the language-guided sampling process, the optimization process will be stabilized by the designed ResAdv-DDIM sampler, therefore releasing the transferable and robust adversarial capability of multi-step diffusion models. In task modeling, we propose the context-encoded attacking scenario constraint to supplement the missing knowledge from incomplete human instructions. Guidance masking and renderer integration are proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger scenario-adapted attacks. Moreover, in the dimension of generator evaluation, we propose the semantic-abstracted attacking evaluation enhancement by clarifying the evaluation boundary, facilitating the development of more effective SemanticAE generators. Extensive experiments demonstrate the superiority of the transfer attack performance of InSUR. Moreover, we realize the reference-free generation of semantically constrained 3D adversarial examples for the first time.",
        "arxiv_id": "2510.22981",
        "ARXIVID": "2510.22981",
        "COMMENT": "This paper develops a framework for generating semantically constrained adversarial examples from natural language instructions, with novel uncertainty reduction techniques. It is relevant to multi-modal learning and adversarial robustness, but does not directly match the listed criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.22605": {
        "authors": [
            "Yuang Wang",
            "Pengfei Jin",
            "Siyeop Yoon",
            "Matthew Tivnan",
            "Shaoyang Zhang",
            "Li Zhang",
            "Quanzheng Li",
            "Zhiqiang Chen",
            "Dufan Wu"
        ],
        "title": "Projection Embedded Diffusion Bridge for CT Reconstruction from Incomplete Data",
        "abstract": "arXiv:2510.22605v1 Announce Type: new  Abstract: Reconstructing CT images from incomplete projection data remains challenging due to the ill-posed nature of the problem. Diffusion bridge models have recently shown promise in restoring clean images from their corresponding Filtered Back Projection (FBP) reconstructions, but incorporating data consistency into these models remains largely underexplored. Incorporating data consistency can improve reconstruction fidelity by aligning the reconstructed image with the observed projection data, and can enhance detail recovery by integrating structural information contained in the projections. In this work, we propose the Projection Embedded Diffusion Bridge (PEDB). PEDB introduces a novel reverse stochastic differential equation (SDE) to sample from the distribution of clean images conditioned on both the FBP reconstruction and the incomplete projection data. By explicitly conditioning on the projection data in sampling the clean images, PEDB naturally incorporates data consistency. We embed the projection data into the score function of the reverse SDE. Under certain assumptions, we derive a tractable expression for the posterior score. In addition, we introduce a free parameter to control the level of stochasticity in the reverse process. We also design a discretization scheme for the reverse SDE to mitigate discretization error. Extensive experiments demonstrate that PEDB achieves strong performance in CT reconstruction from three types of incomplete data, including sparse-view, limited-angle, and truncated projections. For each of these types, PEDB outperforms evaluated state-of-the-art diffusion bridge models across standard, noisy, and domain-shift evaluations.",
        "arxiv_id": "2510.22605",
        "ARXIVID": "2510.22605",
        "COMMENT": "This paper proposes a new diffusion bridge model for CT reconstruction from incomplete data, introducing a novel reverse SDE and explicit data consistency. While it is a methodological improvement in generative modeling for vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.22473": {
        "authors": [
            "Jing Yang",
            "Yufeng Yang"
        ],
        "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
        "abstract": "arXiv:2510.22473v1 Announce Type: new  Abstract: Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.",
        "arxiv_id": "2510.22473",
        "ARXIVID": "2510.22473",
        "COMMENT": "Related to generative modeling in vision (4D dynamic content), but not a new vision foundation model or embodied AI benchmark/method. Uses 4D Gaussian Splatting and pose estimation.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.23588": {
        "authors": [
            "Guangting Zheng",
            "Qinyu Zhao",
            "Tao Yang",
            "Fei Xiao",
            "Zhijie Lin",
            "Jie Wu",
            "Jiajun Deng",
            "Yanyong Zhang",
            "Rui Zhu"
        ],
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "abstract": "arXiv:2510.23588v1 Announce Type: new  Abstract: Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.",
        "arxiv_id": "2510.23588",
        "ARXIVID": "2510.23588",
        "COMMENT": "FARMER is a new generative framework unifying normalizing flows and autoregressive models for tractable likelihood estimation and high-quality image synthesis. While it is a novel generative model, it does not directly match the specific criteria (no VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.23515": {
        "authors": [
            "Yaoli Liu",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "abstract": "arXiv:2510.23515v1 Announce Type: new  Abstract: This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at https://future-item.github.io/FreeFuse/",
        "arxiv_id": "2510.23515",
        "ARXIVID": "2510.23515",
        "COMMENT": "This paper introduces FreeFuse, a training-free approach for multi-subject text-to-image generation by fusing multiple subject LoRAs using cross-attention-derived masks. It is a novel generative modeling method for multi-subject image synthesis, but does not directly match the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2510.23340": {
        "authors": [
            "Anwesha Das",
            "John Duff",
            "J\\\"org Hoffmann",
            "Vera Demberg"
        ],
        "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps",
        "abstract": "arXiv:2510.23340v1 Announce Type: new  Abstract: Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.",
        "arxiv_id": "2510.23340",
        "ARXIVID": "2510.23340",
        "COMMENT": "Does not match any specific criterion. Focuses on adaptive agent communication and human-AI collaboration, not spatial intelligence, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.22140": {
        "authors": [
            "Guangan Jiang",
            "Tianzi Zhang",
            "Dong Li",
            "Zhenjun Zhao",
            "Haoang Li",
            "Mingrui Li",
            "Hongyu Wang"
        ],
        "title": "STG-Avatar: Animatable Human Avatars via Spacetime Gaussian",
        "abstract": "arXiv:2510.22140v1 Announce Type: new  Abstract: Realistic animatable human avatars from monocular videos are crucial for advancing human-robot interaction and enhancing immersive virtual experiences. While recent research on 3DGS-based human avatars has made progress, it still struggles with accurately representing detailed features of non-rigid objects (e.g., clothing deformations) and dynamic regions (e.g., rapidly moving limbs). To address these challenges, we present STG-Avatar, a 3DGS-based framework for high-fidelity animatable human avatar reconstruction. Specifically, our framework introduces a rigid-nonrigid coupled deformation framework that synergistically integrates Spacetime Gaussians (STG) with linear blend skinning (LBS). In this hybrid design, LBS enables real-time skeletal control by driving global pose transformations, while STG complements it through spacetime adaptive optimization of 3D Gaussians. Furthermore, we employ optical flow to identify high-dynamic regions and guide the adaptive densification of 3D Gaussians in these regions. Experimental results demonstrate that our method consistently outperforms state-of-the-art baselines in both reconstruction quality and operational efficiency, achieving superior quantitative metrics while retaining real-time rendering capabilities. Our code is available at https://github.com/jiangguangan/STG-Avatar",
        "arxiv_id": "2510.22140",
        "ARXIVID": "2510.22140",
        "COMMENT": "Relevant to computer vision and 3D human avatars, but does not directly match any specific criterion. Proposes a new framework for animatable human avatars using Spacetime Gaussians and linear blend skinning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.22056": {
        "authors": [
            "Mohammad Ali Etemadi Naeen",
            "Hoda Mohammadzade",
            "Saeed Bagheri Shouraki"
        ],
        "title": "Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning",
        "abstract": "arXiv:2510.22056v1 Announce Type: new  Abstract: Anomaly detection in surveillance videos remains a challenging task due to the diversity of abnormal events, class imbalance, and scene-dependent visual clutter. To address these issues, we propose a robust deep learning framework that integrates human-centric preprocessing with spatio-temporal modeling for multi-class anomaly classification. Our pipeline begins by applying YOLO-World - an open-vocabulary vision-language detector - to identify human instances in raw video clips, followed by ByteTrack for consistent identity-aware tracking. Background regions outside detected bounding boxes are suppressed via Gaussian blurring, effectively reducing scene-specific distractions and focusing the model on behaviorally relevant foreground content. The refined frames are then processed by an ImageNet-pretrained InceptionV3 network for spatial feature extraction, and temporal dynamics are captured using a bidirectional LSTM (BiLSTM) for sequence-level classification. Evaluated on a five-class subset of the UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our method achieves a mean test accuracy of 92.41% across three independent trials, with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation metrics - including confusion matrices, ROC curves, and macro/weighted averages - demonstrate strong generalization and resilience to class imbalance. The results confirm that foreground-focused preprocessing significantly enhances anomaly discrimination in real-world surveillance scenarios.",
        "arxiv_id": "2510.22056",
        "ARXIVID": "2510.22056",
        "COMMENT": "Relevant to computer vision and spatio-temporal modeling, but does not directly match any specific criterion. Uses YOLO-World (a vision-language detector) and spatio-temporal deep learning for anomaly detection in surveillance videos.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.22260": {
        "authors": [
            "Tianhao Zhao",
            "Yiyang Zou",
            "Zihao Mao",
            "Peilun Xiao",
            "Yulin Huang",
            "Hongda Yang",
            "Yuxuan Li",
            "Qun Li",
            "Guobin Wu",
            "Yutian Lin"
        ],
        "title": "Accident Anticipation via Temporal Occurrence Prediction",
        "abstract": "arXiv:2510.22260v1 Announce Type: new  Abstract: Accident anticipation aims to predict potential collisions in an online manner, enabling timely alerts to enhance road safety. Existing methods typically predict frame-level risk scores as indicators of hazard. However, these approaches rely on ambiguous binary supervision (labeling all frames in accident videos as positive) despite the fact that risk varies continuously over time, leading to unreliable learning and false alarms. To address this, we propose a novel paradigm that shifts the prediction target from current-frame risk scoring to directly estimating accident scores at multiple future time steps (e.g., 0.1s-2.0s ahead), leveraging precisely annotated accident timestamps as supervision. Our method employs a snippet-level encoder to jointly model spatial and temporal dynamics, and a Transformer-based temporal decoder that predicts accident scores for all future horizons simultaneously using dedicated temporal queries. Furthermore, we introduce a refined evaluation protocol that reports Time-to-Accident (TTA) and recall (evaluated at multiple pre-accident intervals (0.5s, 1.0s, and 1.5s)) only when the false alarm rate (FAR) remains within an acceptable range, ensuring practical relevance. Experiments show that our method achieves superior performance in both recall and TTA under realistic FAR constraints.",
        "arxiv_id": "2510.22260",
        "ARXIVID": "2510.22260",
        "COMMENT": "This paper proposes a new paradigm for accident anticipation in videos, with a novel temporal prediction target and evaluation protocol. While it is a methodological improvement in video understanding, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to general interest in computer vision.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.22039": {
        "authors": [
            "Po-Chen Kuo",
            "Han Hou",
            "Will Dabney",
            "Edgar Y. Walker"
        ],
        "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability",
        "abstract": "arXiv:2510.22039v1 Announce Type: new  Abstract: Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.",
        "arxiv_id": "2510.22039",
        "ARXIVID": "2510.22039",
        "COMMENT": "Related to meta-RL and representation learning, but not directly to spatial intelligence on embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.22743": {
        "authors": [
            "Raihan Ahamed Rifat",
            "Fuyad Hasan Bhoyan",
            "Md Humaion Kabir Mehedi",
            "Md Kaviul Hossain",
            "Md. Jakir Hossen",
            "M. F. Mridha"
        ],
        "title": "ConMatFormer: A Multi-attention and Transformer Integrated ConvNext based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification",
        "abstract": "arXiv:2510.22743v1 Announce Type: new  Abstract: Diabetic foot ulcer (DFU) detection is a clinically significant yet challenging task due to the scarcity and variability of publicly available datasets. To solve these problems, we propose ConMatFormer, a new hybrid deep learning architecture that combines ConvNeXt blocks, multiple attention mechanisms convolutional block attention module (CBAM) and dual attention network (DANet), and transformer modules in a way that works together. This design facilitates the extraction of better local features and understanding of the global context, which allows us to model small skin patterns across different types of DFU very accurately. To address the class imbalance, we used data augmentation methods. A ConvNeXt block was used to obtain detailed local features in the initial stages. Subsequently, we compiled the model by adding a transformer module to enhance long-range dependency. This enabled us to pinpoint the DFU classes that were underrepresented or constituted minorities. Tests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed that ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural network (CNN) and Vision Transformer (ViT) models in terms of accuracy, reliability, and flexibility. The proposed method achieved an accuracy of 0.8961 and a precision of 0.9160 in a single experiment, which is a significant improvement over the current standards for classifying DFUs. In addition, by 4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with a standard deviation of only 0.0031. We further applied explainable artificial intelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to consistently monitor the transparency and trustworthiness of the decision-making process.. Our findings set a new benchmark for DFU classification and provide a hybrid attention transformer framework for medical image analysis.",
        "arxiv_id": "2510.22743",
        "ARXIVID": "2510.22743",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it proposes a hybrid ConvNeXt+Transformer model for medical image analysis, but not a new foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2510.22969": {
        "authors": [
            "Kechen Meng",
            "Sinuo Zhang",
            "Rongpeng Li",
            "Xiangming Meng",
            "Chan Wang",
            "Ming Lei",
            "Zhifeng Zhao"
        ],
        "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner",
        "abstract": "arXiv:2510.22969v1 Announce Type: new  Abstract: In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.",
        "arxiv_id": "2510.22969",
        "ARXIVID": "2510.22969",
        "COMMENT": "This paper introduces a multi-agent conditional diffusion model with mean field communication for wireless resource allocation. While it is a novel application of diffusion models in multi-agent systems, it does not focus on embodied AI benchmarks, spatial intelligence, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.23494": {
        "authors": [
            "Elisabeth J\\\"uttner",
            "Leona Krath",
            "Stefan Korfhage",
            "Hannah Dr\\\"oge",
            "Matthias B. Hullin",
            "Markus Plack"
        ],
        "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
        "abstract": "arXiv:2510.23494v1 Announce Type: new  Abstract: Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.",
        "arxiv_id": "2510.23494",
        "ARXIVID": "2510.23494",
        "COMMENT": "This paper proposes a hybrid relighting framework for volumetric video using diffusion-derived priors and physically motivated rendering. While it is a generative vision method, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.23581": {
        "authors": [
            "Junyoung Seo",
            "Rodrigo Mira",
            "Alexandros Haliassos",
            "Stella Bounareli",
            "Honglie Chen",
            "Linh Tran",
            "Seungryong Kim",
            "Zoe Landgraf",
            "Jie Shen"
        ],
        "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
        "abstract": "arXiv:2510.23581v1 Announce Type: new  Abstract: Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.",
        "arxiv_id": "2510.23581",
        "ARXIVID": "2510.23581",
        "COMMENT": "This paper proposes Lookahead Anchoring for audio-driven human animation, addressing identity drift in autoregressive generation. While it is a novel method for temporal conditioning in generative models, it does not directly match any of the four criteria but is relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.23009": {
        "authors": [
            "Pan Zhao",
            "Hui Yuan",
            "Chongzhen Tian",
            "Tian Guo",
            "Raouf Hamzaoui",
            "Zhigeng Pan"
        ],
        "title": "UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds",
        "abstract": "arXiv:2510.23009v1 Announce Type: new  Abstract: Lossy compression of point clouds reduces storage and transmission costs; however, it inevitably leads to irreversible distortion in geometry structure and attribute information. To address these issues, we propose a unified geometry and attribute enhancement (UGAE) framework, which consists of three core components: post-geometry enhancement (PoGE), pre-attribute enhancement (PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based sparse convolutional U-Net is used to reconstruct the geometry structure with high precision by predicting voxel occupancy probabilities. Building on the refined geometry structure, PAE introduces an innovative enhanced geometry-guided recoloring strategy, which uses a detail-aware K-Nearest Neighbors (DA-KNN) method to achieve accurate recoloring and effectively preserve high-frequency details before attribute compression. Finally, at the decoder side, PoAE uses an attribute residual prediction network with a weighted mean squared error (W-MSE) loss to enhance the quality of high-frequency regions while maintaining the fidelity of low-frequency regions. UGAE significantly outperformed existing methods on three benchmark datasets: 8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29), UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings for geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with 56.88% BD-bitrate savings for attributes on the Y component. Additionally, it improved perceptual quality significantly.",
        "arxiv_id": "2510.23009",
        "ARXIVID": "2510.23009",
        "COMMENT": "Does not match any specific criterion. Focuses on point cloud compression and enhancement, not spatial intelligence for embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.22943": {
        "authors": [
            "Yongbo Wang",
            "Haonan Wang",
            "Guodong Mu",
            "Ruixin Zhang",
            "Jiaqi Chen",
            "Jingyun Zhang",
            "Jun Wang",
            "Yuan Xie",
            "Zhizhong Zhang",
            "Shouhong Ding"
        ],
        "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
        "abstract": "arXiv:2510.22943v1 Announce Type: new  Abstract: With the ever-increasing volume of visual data, the efficient and lossless transmission, along with its subsequent interpretation and understanding, has become a critical bottleneck in modern information systems. The emerged codebook-based solution utilize a globally shared codebook to quantize and dequantize each token, controlling the bpp by adjusting the number of tokens or the codebook size. However, for facial images, which are rich in attributes, such global codebook strategies overlook both the category-specific correlations within images and the semantic differences among tokens, resulting in suboptimal performance, especially at low bpp. Motivated by these observations, we propose a Switchable Token-Specific Codebook Quantization for face image compression, which learns distinct codebook groups for different image categories and assigns an independent codebook to each token. By recording the codebook group to which each token belongs with a small number of bits, our method can reduce the loss incurred when decreasing the size of each codebook group. This enables a larger total number of codebooks under a lower overall bpp, thereby enhancing the expressive capability and improving reconstruction performance. Owing to its generalizable design, our method can be integrated into any existing codebook-based representation learning approach and has demonstrated its effectiveness on face recognition datasets, achieving an average accuracy of 93.51% for reconstructed images at 0.05 bpp.",
        "arxiv_id": "2510.22943",
        "ARXIVID": "2510.22943",
        "COMMENT": "Proposes a new codebook quantization method for face image compression. This is a vision method but not directly related to foundation models, VLLMs/MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.22454": {
        "authors": [
            "Linhan Wang",
            "Jianwen Dou",
            "Wang Li",
            "Shengkun Wang",
            "Zhiwu Xie",
            "Chang-Tien Lu",
            "Yinlin Chen"
        ],
        "title": "SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning",
        "abstract": "arXiv:2510.22454v1 Announce Type: new  Abstract: Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution. Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck. Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized. In this work, we present a fast, label-efficient semi-supervised framework that exploits this untapped data. Our framework consists of two components: (i) an end-to-end heatmap-supervised detection model inspired by keypoint detection, and (ii) a teacher-student co-training mechanism that enhances performance under sparse labeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a CryoET-specific DropBlock augmentation strategy to further boost performance. Extensive evaluations on the large-scale CZII dataset show that our approach improves F1 by 10% over supervised baselines, underscoring the promise of semi-supervised learning for leveraging unlabeled CryoET data.",
        "arxiv_id": "2510.22454",
        "ARXIVID": "2510.22454",
        "COMMENT": "Applies semi-supervised learning to CryoET particle picking, not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.21829": {
        "authors": [
            "Yi Yin",
            "Yuntao Shou",
            "Zao Dai",
            "Yun Peng",
            "Tao Meng",
            "Wei Ai",
            "Keqin Li"
        ],
        "title": "A Flow Model with Low-Rank Transformers for Incomplete Multimodal Survival Analysis",
        "abstract": "arXiv:2510.21829v1 Announce Type: new  Abstract: In recent years, multimodal medical data-based survival analysis has attracted much attention. However, real-world datasets often suffer from the problem of incomplete modality, where some patient modality information is missing due to acquisition limitations or system failures. Existing methods typically infer missing modalities directly from observed ones using deep neural networks, but they often ignore the distributional discrepancy across modalities, resulting in inconsistent and unreliable modality reconstruction. To address these challenges, we propose a novel framework that combines a low-rank Transformer with a flow-based generative model for robust and flexible multimodal survival prediction. Specifically, we first formulate the concerned problem as incomplete multimodal survival analysis using the multi-instance representation of whole slide images (WSIs) and genomic profiles. To realize incomplete multimodal survival analysis, we propose a class-specific flow for cross-modal distribution alignment. Under the condition of class labels, we model and transform the cross-modal distribution. By virtue of the reversible structure and accurate density modeling capabilities of the normalizing flow model, the model can effectively construct a distribution-consistent latent space of the missing modality, thereby improving the consistency between the reconstructed data and the true distribution. Finally, we design a lightweight Transformer architecture to model intra-modal dependencies while alleviating the overfitting problem in high-dimensional modality fusion by virtue of the low-rank Transformer. Extensive experiments have demonstrated that our method not only achieves state-of-the-art performance under complete modality settings, but also maintains robust and superior accuracy under the incomplete modalities scenario.",
        "arxiv_id": "2510.21829",
        "ARXIVID": "2510.21829",
        "COMMENT": "Related to multi-modal learning and generative modeling, but not a new VLLM/MLLM or vision foundation model. Uses flow models and transformers for survival analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.22582": {
        "authors": [
            "Jian Sun",
            "Kangdao Liu",
            "Chi Zhang",
            "Chuangquan Chen",
            "Junge Shen",
            "Chi-Man Vong"
        ],
        "title": "Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement",
        "abstract": "arXiv:2510.22582v1 Announce Type: new  Abstract: Cross-view geo-localization (CVGL) enables UAV localization by matching aerial images to geo-tagged satellite databases, which is critical for autonomous navigation in GNSS-denied environments. However, existing methods rely on resource-intensive fine-grained feature extraction and alignment, where multiple branches and modules significantly increase inference costs, limiting their deployment on edge devices. We propose Precision-Focused Efficient Design (PFED), a resource-efficient framework combining hierarchical knowledge transfer and multi-view representation refinement. This innovative method comprises two key components: 1) During training, Hierarchical Distillation paradigm for fast and accurate CVGL (HD-CVGL), coupled with Uncertainty-Aware Prediction Alignment (UAPA) to distill essential information and mitigate the data imbalance without incurring additional inference overhead. 2) During inference, an efficient Multi-view Refinement Module (MRM) leverages mutual information to filter redundant samples and effectively utilize the multi-view data. Extensive experiments show that PFED achieves state-of-the-art performance in both accuracy and efficiency, reaching 97.15\\% Recall@1 on University-1652 while being over $5 \\times$ more efficient in FLOPs and $3 \\times$ faster than previous top methods. Furthermore, PFED runs at 251.5 FPS on the AGX Orin edge device, demonstrating its practical viability for real-time UAV applications. The project is available at https://github.com/SkyEyeLoc/PFED",
        "arxiv_id": "2510.22582",
        "ARXIVID": "2510.22582",
        "COMMENT": "This paper presents a new efficient framework for cross-view UAV geo-localization, with hierarchical distillation and multi-view refinement. While it is an interesting vision application, it does not match the specific criteria (no VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.22366": {
        "authors": [
            "Jindong Yang",
            "Han Fang",
            "Weiming Zhang",
            "Nenghai Yu",
            "Kejiang Chen"
        ],
        "title": "T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models",
        "abstract": "arXiv:2510.22366v1 Announce Type: new  Abstract: Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \\href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.",
        "arxiv_id": "2510.22366",
        "ARXIVID": "2510.22366",
        "COMMENT": "This paper proposes a new watermarking method for diffusion models, focusing on balancing robustness and diversity. While it is relevant to generative modeling, it does not match the specific criteria (no VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.23241": {
        "authors": [
            "Stefan M. Fischer",
            "Johannes Kiechle",
            "Laura Daza",
            "Lina Felsner",
            "Richard Osuala",
            "Daniel M. Lang",
            "Karim Lekadir",
            "Jan C. Peeken",
            "Julia A. Schnabel"
        ],
        "title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation",
        "abstract": "arXiv:2510.23241v1 Announce Type: new  Abstract: In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.",
        "arxiv_id": "2510.23241",
        "ARXIVID": "2510.23241",
        "COMMENT": "This paper introduces a curriculum learning approach for 3D medical image segmentation by progressively increasing patch size. While it is a methodological improvement in spatial understanding, it is focused on medical imaging and not embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.22780": {
        "authors": [
            "Zora Zhiruo Wang",
            "Yijia Shao",
            "Omar Shaikh",
            "Daniel Fried",
            "Graham Neubig",
            "Diyi Yang"
        ],
        "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations",
        "abstract": "arXiv:2510.22780v1 Announce Type: new  Abstract: AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.",
        "arxiv_id": "2510.22780",
        "ARXIVID": "2510.22780",
        "COMMENT": "Analysis of AI agent workflows vs. humans, not focused on spatial intelligence, VLLMs, or vision foundation models. More of a human-computer interaction study.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.22243": {
        "authors": [
            "Amir Mohammad Khadem Hosseini",
            "Sattar Mirzakuchaki"
        ],
        "title": "Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework",
        "abstract": "arXiv:2510.22243v1 Announce Type: new  Abstract: Semantic segmentation has emerged as a fundamental problem in computer vision, gaining particular importance in real-time applications such as autonomous driving. The main challenge is achieving high accuracy while operating under computational and hardware constraints. In this research, we present an FPGA-based implementation of real-time semantic segmentation leveraging the lightweight LMIINet architecture and the Coarse-Grained Reconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The model was trained using Quantization-Aware Training (QAT) with 8-bit precision on the Cityscapes dataset, reducing memory footprint by a factor of four while enabling efficient fixed-point computations. Necessary modifications were applied to adapt the model to CGRA4ML constraints, including simplifying skip connections, employing hardware-friendly operations such as depthwise-separable and 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our implementation achieves approximately 90% pixel accuracy and 45% mean Intersection-over-Union (mIoU), operating in real-time at 20 frames per second (FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate the potential of CGRA4ML, with its flexibility in mapping modern layers and off-chip memory utilization for skip connections, provides a path for implementing advanced semantic segmentation networks on FPGA for real-time applications to outperform traditional GPU solutions in terms of power efficiency while maintaining competitive accuracy. The code for this project is publicly available at https://github.com/STAmirr/ cgra4ml_semantic_segmentation",
        "arxiv_id": "2510.22243",
        "ARXIVID": "2510.22243",
        "COMMENT": "Application of semantic segmentation on FPGA for autonomous vehicles. While relevant to computer vision, it does not introduce new spatial intelligence methods, VLLMs, or foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.22630": {
        "authors": [
            "Adinath Dukre",
            "Ankan Deria",
            "Yutong Xie",
            "Imran Razzak"
        ],
        "title": "Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization",
        "abstract": "arXiv:2510.22630v1 Announce Type: new  Abstract: Atypical mitotic figures are important biomarkers of tumor aggressiveness in histopathology, yet reliable recognition remains challenging due to severe class imbalance and variability across imaging domains. We present a DenseNet-121-based framework tailored for atypical mitosis classification in the MIDOG 2025 (Track 2) setting. Our method integrates stain-aware augmentation (Macenko), geometric and intensity transformations, and imbalance-aware learning via weighted sampling with a hybrid objective combining class-weighted binary cross-entropy and focal loss. Trained end-to-end with AdamW and evaluated across multiple independent domains, the model demonstrates strong generalization under scanner and staining shifts, achieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and specificity 80.9% on the official test set. These results indicate that combining DenseNet-121 with stain-aware augmentation and imbalance-adaptive objectives yields a robust, domain-generalizable framework for atypical mitosis classification suitable for real-world computational pathology workflows.",
        "arxiv_id": "2510.22630",
        "ARXIVID": "2510.22630",
        "COMMENT": "Does not match any specific criterion. Focuses on medical image classification with DenseNet and domain generalization, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.22390": {
        "authors": [
            "Aitor Iglesias",
            "Nerea Aranjuelo",
            "Patricia Javierre",
            "Ainhoa Menendez",
            "Ignacio Arganda-Carreras",
            "Marcos Nieto"
        ],
        "title": "A Fully Interpretable Statistical Approach for Roadside LiDAR Background Subtraction",
        "abstract": "arXiv:2510.22390v1 Announce Type: new  Abstract: We present a fully interpretable and flexible statistical method for background subtraction in roadside LiDAR data, aimed at enhancing infrastructure-based perception in automated driving. Our approach introduces both a Gaussian distribution grid (GDG), which models the spatial statistics of the background using background-only scans, and a filtering algorithm that uses this representation to classify LiDAR points as foreground or background. The method supports diverse LiDAR types, including multiline 360 degree and micro-electro-mechanical systems (MEMS) sensors, and adapts to various configurations. Evaluated on the publicly available RCooper dataset, it outperforms state-of-the-art techniques in accuracy and flexibility, even with minimal background data. Its efficient implementation ensures reliable performance on low-resource hardware, enabling scalable real-world deployment.",
        "arxiv_id": "2510.22390",
        "ARXIVID": "2510.22390",
        "COMMENT": "Presents a new interpretable statistical method for LiDAR background subtraction. While it involves spatial understanding, it is not about embodied agents or spatial intelligence in agents, so does not match the criteria directly.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.23408": {
        "authors": [
            "Abolfazl Younesi",
            "Zahra Najafabadi Samani",
            "Thomas Fahringer"
        ],
        "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines",
        "abstract": "arXiv:2510.23408v1 Announce Type: new  Abstract: Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.",
        "arxiv_id": "2510.23408",
        "ARXIVID": "2510.23408",
        "COMMENT": "Does not match any specific criteria. Focuses on LLMs for data stream processing pipelines, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.23079": {
        "authors": [
            "Joel Honkamaa",
            "Pekka Marttinen"
        ],
        "title": "Strategies for Robust Deep Learning Based Deformable Registration",
        "abstract": "arXiv:2510.23079v1 Announce Type: new  Abstract: Deep learning based deformable registration methods have become popular in recent years. However, their ability to generalize beyond training data distribution can be poor, significantly hindering their usability. LUMIR brain registration challenge for Learn2Reg 2025 aims to advance the field by evaluating the performance of the registration on contrasts and modalities different from those included in the training set. Here we describe our submission to the challenge, which proposes a very simple idea for significantly improving robustness by transforming the images into MIND feature space before feeding them into the model. In addition, a special ensembling strategy is proposed that shows a small but consistent improvement.",
        "arxiv_id": "2510.23079",
        "ARXIVID": "2510.23079",
        "COMMENT": "General computer vision (medical image registration), but does not match any specific criterion. No spatial intelligence for embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    },
    "2510.21810": {
        "authors": [
            "Arpan Maity",
            "Aviroop Pal",
            "MD. Samiul Islam",
            "Tamal Ghosh"
        ],
        "title": "Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights",
        "abstract": "arXiv:2510.21810v1 Announce Type: new  Abstract: Diabetic Retinopathy (DR), a vision-threatening complication of Dia-betes Mellitus (DM), is a major global concern, particularly in India, which has one of the highest diabetic populations. Prolonged hyperglycemia damages reti-nal microvasculature, leading to DR symptoms like microaneurysms, hemor-rhages, and fluid leakage, which, if undetected, cause irreversible vision loss. Therefore, early screening is crucial as DR is asymptomatic in its initial stages. Fundus imaging aids precise diagnosis by detecting subtle retinal lesions. This paper introduces a hybrid diagnostic framework combining traditional feature extraction and deep learning (DL) to enhance DR detection. While handcrafted features capture key clinical markers, DL automates hierarchical pattern recog-nition, improving early diagnosis. The model synergizes interpretable clinical data with learned features, surpassing standalone DL approaches that demon-strate superior classification and reduce false negatives. This multimodal AI-driven approach enables scalable, accurate DR screening, crucial for diabetes-burdened regions.",
        "arxiv_id": "2510.21810",
        "ARXIVID": "2510.21810",
        "COMMENT": "This paper focuses on diabetic retinopathy detection using a hybrid of traditional features and deep learning. It is a multimodal approach but does not introduce new VLLMs/MLLMs, spatial intelligence for embodied agents, or vision foundation models. It is more of a medical imaging application.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}