{
    "2602.18424": {
        "authors": [
            "Xia Su",
            "Ruiqi Chen",
            "Benlin Liu",
            "Jingwei Ma",
            "Zonglin Di",
            "Ranjay Krishna",
            "Jon Froehlich"
        ],
        "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation",
        "abstract": "arXiv:2602.18424v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav",
        "arxiv_id": "2602.18424",
        "ARXIVID": "2602.18424",
        "COMMENT": "This paper introduces CapNav, a benchmark for evaluating VLMs on capability-conditioned indoor navigation, focusing on agent-specific mobility constraints. It matches criterion 3 (new benchmark for embodied AI with a novel angle: capability-aware navigation) and criterion 1 (spatial intelligence on embodied agents).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.18094": {
        "authors": [
            "Ling Lin",
            "Yang Bai",
            "Heng Su",
            "Congcong Zhu",
            "Yaoxing Wang",
            "Yang Zhou",
            "Huazhu Fu",
            "Jingrun Chen"
        ],
        "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models",
        "abstract": "arXiv:2602.18094v1 Announce Type: new  Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.",
        "arxiv_id": "2602.18094",
        "ARXIVID": "2602.18094",
        "COMMENT": "This paper introduces OODBench, a new benchmark for evaluating VLMs on out-of-distribution data, and proposes an automated assessment metric. It matches criterion 3 (new benchmark for embodied AI/VLMs, focusing on OOD, a previously underexplored angle) and criterion 4 (vision foundation models and their evaluation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.17951": {
        "authors": [
            "Guoheng Sun",
            "Tingting Du",
            "Kaixi Feng",
            "Chenxiang Luo",
            "Xingguo Ding",
            "Zheyu Shen",
            "Ziyao Wang",
            "Yexiao He",
            "Ang Li"
        ],
        "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
        "abstract": "arXiv:2602.17951v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, na\\\"ive multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.",
        "arxiv_id": "2602.17951",
        "ARXIVID": "2602.17951",
        "COMMENT": "Strongly matches criterion 1 (spatial understanding on embodied agents), criterion 3 (new methods for embodied AI with a novel multi-layer alignment for spatially-aware VLA models), and criterion 4 (vision foundation models and their application to robotics). The paper introduces a residual-oriented multi-layer alignment for spatially-aware vision-language-action models, with theoretical and empirical analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.18432": {
        "authors": [
            "Evonne Ng",
            "Siwei Zhang",
            "Zhang Chen",
            "Michael Zollhoefer",
            "Alexander Richard"
        ],
        "title": "SARAH: Spatially Aware Real-time Agentic Humans",
        "abstract": "arXiv:2602.18432v1 Announce Type: new  Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.",
        "arxiv_id": "2602.18432",
        "ARXIVID": "2602.18432",
        "COMMENT": "Matches criteria 1 (spatial intelligence for embodied agents) and 3 (novel real-time, spatially-aware conversational motion method for VR agents, with new gaze control mechanism).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.18064": {
        "authors": [
            "Ziyue Wang",
            "Linghan Cai",
            "Chang Han Low",
            "Haofeng Liu",
            "Junde Wu",
            "Jingyu Wang",
            "Rui Wang",
            "Lei Song",
            "Jiang Bian",
            "Jingjing Fu",
            "Yueming Jin"
        ],
        "title": "3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis",
        "abstract": "arXiv:2602.18064v1 Announce Type: new  Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \\href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.",
        "arxiv_id": "2602.18064",
        "ARXIVID": "2602.18064",
        "COMMENT": "Matches criteria 2 (new MLLMs for 3D medical analysis) and 3 (introduces a new benchmark, DeepChestVQA, for 3D perception-to-understanding). Also relevant to vision foundation models (criterion 4) as it adapts 2D MLLMs for 3D data.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.17770": {
        "authors": [
            "Balamurugan Thambiraja",
            "Omid Taheri",
            "Radek Danecek",
            "Giorgio Becherini",
            "Gerard Pons-Moll",
            "Justus Thies"
        ],
        "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild",
        "abstract": "arXiv:2602.17770v1 Announce Type: new  Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.",
        "arxiv_id": "2602.17770",
        "ARXIVID": "2602.17770",
        "COMMENT": "This paper introduces a new LLM-based hand animation system (CLUTCH) and a large-scale in-the-wild 3D hand motion dataset, using VLMs for annotation and a novel VQ-VAE architecture. It matches criterion 2 (new VLLMs/MLLMs) and criterion 3 (new benchmark and method for embodied AI, focusing on hand motion in the wild, which is a novel angle).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.18422": {
        "authors": [
            "Linxi Xie",
            "Lisong C. Sun",
            "Ashley Neall",
            "Tong Wu",
            "Shengqu Cai",
            "Gordon Wetzstein"
        ],
        "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control",
        "abstract": "arXiv:2602.18422v1 Announce Type: new  Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.",
        "arxiv_id": "2602.18422",
        "ARXIVID": "2602.18422",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for world simulation and interactive video generation) and criterion 4 (vision foundation models and applications). Proposes a human-centric video world model for XR with 3D head and hand control, and demonstrates improved user control in egocentric virtual environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.18006": {
        "authors": [
            "Ahsan Baidar Bakht",
            "Mohamad Alansari",
            "Muhayy Ud Din",
            "Muzammal Naseer",
            "Sajid Javed",
            "Irfan Hussain",
            "Jiri Matas",
            "Arif Mahmood"
        ],
        "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method",
        "abstract": "arXiv:2602.18006v1 Announce Type: new  Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.",
        "arxiv_id": "2602.18006",
        "ARXIVID": "2602.18006",
        "COMMENT": "Matches criterion 3 (introduces a large new multimodal underwater tracking benchmark and a new tracking method using vision-language fusion and knowledge distillation). Also relevant to criterion 4 (vision foundation models applied to tracking).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.18020": {
        "authors": [
            "Jiabing Yang",
            "Yixiang Chen",
            "Yuan Xu",
            "Peiyan Li",
            "Xiangnan Wu",
            "Zichen Wen",
            "Bowen Fang",
            "Tao Yu",
            "Zhengbo Zhang",
            "Yingda Li",
            "Kai Wang",
            "Jing Liu",
            "Nianfeng Liu",
            "Yan Huang",
            "Liang Wang"
        ],
        "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
        "abstract": "arXiv:2602.18020v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.",
        "arxiv_id": "2602.18020",
        "ARXIVID": "2602.18020",
        "COMMENT": "Matches criteria 1 (methodological improvement to spatial intelligence in embodied agents) and 3 (new method for embodied VLA models, plug-and-play uncertainty-aware observation reinjection).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18057": {
        "authors": [
            "Hongsong Wang",
            "Wenjing Yan",
            "Qiuxia Lai",
            "Xin Geng"
        ],
        "title": "Temporal Consistency-Aware Text-to-Motion Generation",
        "abstract": "arXiv:2602.18057v1 Announce Type: new  Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.",
        "arxiv_id": "2602.18057",
        "ARXIVID": "2602.18057",
        "COMMENT": "This paper proposes a temporal consistency-aware framework for text-to-motion generation, introducing a new VQ-VAE and transformer-based model. It is relevant to criterion 2 (new MLLMs for text-to-motion) and criterion 3 (novel method for embodied AI, focusing on temporal consistency in motion generation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18434": {
        "authors": [
            "Vatsal Agarwal",
            "Saksham Suri",
            "Matthew Gwilliam",
            "Pulkit Kumar",
            "Abhinav Shrivastava"
        ],
        "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory",
        "abstract": "arXiv:2602.18434v1 Announce Type: new  Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.",
        "arxiv_id": "2602.18434",
        "ARXIVID": "2602.18434",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new method for scaling token budgets in video stream understanding for VQA, with adaptive selection and retrieval mixture-of-experts, showing significant empirical gains.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18025": {
        "authors": [
            "Haruki Abe",
            "Takayuki Osa",
            "Yusuke Mukuta",
            "Tatsuya Harada"
        ],
        "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets",
        "abstract": "arXiv:2602.18025v1 Announce Type: new  Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.",
        "arxiv_id": "2602.18025",
        "ARXIVID": "2602.18025",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods and benchmarks). Proposes a cross-embodiment offline RL paradigm for heterogeneous robot datasets, introduces a new grouping strategy, and provides a new benchmark suite.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.17869": {
        "authors": [
            "Yuxiao Chen",
            "Jue Wang",
            "Zhikang Zhang",
            "Jingru Yi",
            "Xu Zhang",
            "Yang Zou",
            "Zhaowei Cai",
            "Jianbo Yuan",
            "Xinyu Li",
            "Hao Yang",
            "Davide Modolo"
        ],
        "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models",
        "abstract": "arXiv:2602.17869v1 Announce Type: new  Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.",
        "arxiv_id": "2602.17869",
        "ARXIVID": "2602.17869",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new framework for long-form video understanding using adaptive video sampling and spatiotemporal compression integrated with a multimodal LLM.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18309": {
        "authors": [
            "Ziyue Liu",
            "Davide Talon",
            "Federico Girella",
            "Zanxi Ruan",
            "Mattia Mondo",
            "Loris Bazzani",
            "Yiming Wang",
            "Marco Cristani"
        ],
        "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation",
        "abstract": "arXiv:2602.18309v1 Announce Type: new  Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.",
        "arxiv_id": "2602.18309",
        "ARXIVID": "2602.18309",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new multi-modal diffusion-based image generation framework combining text and sketch, and introduces a new dataset for fashion image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.18016": {
        "authors": [
            "Jiamin Luo",
            "Xuqian Gu",
            "Jingjing Wang",
            "Jiahong Lu"
        ],
        "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating",
        "abstract": "arXiv:2602.18016v1 Announce Type: new  Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.",
        "arxiv_id": "2602.18016",
        "ARXIVID": "2602.18016",
        "COMMENT": "Matches criterion 2 (new multimodal LLM for affective visual customization) and criterion 4 (application of vision foundation models to emotion manipulation in images).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.18252": {
        "authors": [
            "Rishika Bhagwatkar",
            "Irina Rish",
            "Nicolas Flammarion",
            "Francesco Croce"
        ],
        "title": "On the Adversarial Robustness of Discrete Image Tokenizers",
        "abstract": "arXiv:2602.18252v1 Announce Type: new  Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.",
        "arxiv_id": "2602.18252",
        "ARXIVID": "2602.18252",
        "COMMENT": "This paper studies adversarial robustness of discrete image tokenizers in multimodal systems and proposes an unsupervised adversarial training method to improve robustness. It is relevant to criterion 4 (vision foundation models and their applications), as discrete tokenizers are core to many vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.17807": {
        "authors": [
            "Narges Norouzi",
            "Idil Esen Zulfikar",
            "Niccol`o Cavagnero",
            "Tommie Kerssies",
            "Bastian Leibe",
            "Gijs Dubbelman",
            "Daan de Geus"
        ],
        "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model",
        "abstract": "arXiv:2602.17807v1 Announce Type: new  Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/",
        "arxiv_id": "2602.17807",
        "ARXIVID": "2602.17807",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Shows that ViT models can be used for video segmentation with a simple encoder-only architecture, providing surprising empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.17785": {
        "authors": [
            "Xinwei Ju",
            "Rema Daher",
            "Danail Stoyanov",
            "Sophia Bano",
            "Francisco Vasconcelos"
        ],
        "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision",
        "abstract": "arXiv:2602.17785v1 Announce Type: new  Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.",
        "arxiv_id": "2602.17785",
        "ARXIVID": "2602.17785",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) due to the novel edge-guided self-supervision and intrinsic decomposition for monocular depth and pose estimation in endoscopy, which is a spatial intelligence task in embodied medical robotics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.18394": {
        "authors": [
            "Stefan Becker",
            "Simon Weiss",
            "Wolfgang H\\\"ubner",
            "Michael Arens"
        ],
        "title": "Self-Aware Object Detection via Degradation Manifolds",
        "abstract": "arXiv:2602.18394v1 Announce Type: new  Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.   We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.   To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.   Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.",
        "arxiv_id": "2602.18394",
        "ARXIVID": "2602.18394",
        "COMMENT": "This paper introduces a degradation-aware self-awareness framework for object detection, structuring feature space by degradation type. It is relevant to criterion 4 (vision foundation models and applications), as it proposes a detector-agnostic method for self-aware detection.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.17910": {
        "authors": [
            "Hanjing Shi",
            "Dominic DiFranzo"
        ],
        "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems",
        "abstract": "arXiv:2602.17910v1 Announce Type: new  Abstract: Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.",
        "arxiv_id": "2602.17910",
        "ARXIVID": "2602.17910",
        "COMMENT": "This paper introduces a new runtime scheduling layer (APEMO) for long-horizon agentic systems, reframing alignment as a temporal control problem. It is relevant to criterion 1 (methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 3 (novel methods for embodied AI, especially with a temporal/trajectory focus).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.18043": {
        "authors": [
            "Hongyu Qu",
            "Xiangbo Shu",
            "Rui Yan",
            "Hailiang Gao",
            "Wenguan Wang",
            "Jinhui Tang"
        ],
        "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition",
        "abstract": "arXiv:2602.18043v1 Announce Type: new  Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.",
        "arxiv_id": "2602.18043",
        "ARXIVID": "2602.18043",
        "COMMENT": "Matches criterion 1 (spatial understanding) and criterion 2 (leverages LLMs for spatio-temporal knowledge in few-shot action recognition). Proposes a decomposition-incorporation framework using LLMs to extract spatial and temporal knowledge for action recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.18322": {
        "authors": [
            "Ziteng Cui",
            "Shuhong Liu",
            "Xiaoyu Dong",
            "Xuangeng Chu",
            "Lin Gu",
            "Ming-Hsuan Yang",
            "Tatsuya Harada"
        ],
        "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis",
        "abstract": "arXiv:2602.18322v1 Announce Type: new  Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.",
        "arxiv_id": "2602.18322",
        "ARXIVID": "2602.18322",
        "COMMENT": "This paper proposes a new framework (Luminance-GS++) for robust 3D novel view synthesis under diverse illumination, with unsupervised objectives for photometric consistency. It is relevant to criterion 4 (vision foundation models and applications), as it builds on 3DGS and NeRF, but does not directly address spatial intelligence or embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.18193": {
        "authors": [
            "Yiran Yang",
            "Zhaowei Liu",
            "Yuan Yuan",
            "Yukun Song",
            "Xiong Ma",
            "Yinghao Song",
            "Xiangji Zeng",
            "Lu Sun",
            "Yulu Wang",
            "Hai Zhou",
            "Shuai Cui",
            "Zhaohan Gong",
            "Jiefei Zhang"
        ],
        "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards",
        "abstract": "arXiv:2602.18193v1 Announce Type: new  Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.",
        "arxiv_id": "2602.18193",
        "ARXIVID": "2602.18193",
        "COMMENT": "Somewhat matches criterion 2 (multimodal LLMs for ad moderation with chain-of-thought and policy-aligned rewards), but the focus is on moderation rather than general vision-language reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.18178": {
        "authors": [
            "Poonam Poonam",
            "Pere-Pau V\\'azquez",
            "Timo Ropinski"
        ],
        "title": "Evaluating Graphical Perception Capabilities of Vision Transformers",
        "abstract": "arXiv:2602.18178v1 Announce Type: new  Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.",
        "arxiv_id": "2602.18178",
        "ARXIVID": "2602.18178",
        "COMMENT": "Somewhat relevant to criterion 4 (evaluates vision foundation models, specifically ViTs, for graphical perception), but is more about perceptual alignment than new methods or applications.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.18093": {
        "authors": [
            "Hanshuai Cui",
            "Zhiqing Tang",
            "Qianli Ma",
            "Zhi Yao",
            "Weijia Jia"
        ],
        "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers",
        "abstract": "arXiv:2602.18093v1 Announce Type: new  Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.",
        "arxiv_id": "2602.18093",
        "ARXIVID": "2602.18093",
        "COMMENT": "This paper proposes PrediT, a training-free acceleration framework for Diffusion Transformers using linear multistep forecasting. While relevant to generative modeling, it does not directly match any of the four criteria, but is of general interest for efficient generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}