{
    "2509.24527": {
        "authors": [
            "Danijar Hafner",
            "Wilson Yan",
            "Timothy Lillicrap"
        ],
        "title": "Training Agents Inside of Scalable World Models",
        "abstract": "arXiv:2509.24527v1 Announce Type: new  Abstract: World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.",
        "arxiv_id": "2509.24527",
        "ARXIVID": "2509.24527",
        "COMMENT": "Presents Dreamer 4, a scalable world model for training agents in complex environments (Minecraft), with improved prediction of object interactions and generalization from offline data. This is a strong match for criterion 3 (embodied AI, new methods/benchmarks, novel angles in simulators/world models).",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2509.22720": {
        "authors": [
            "Zezhong Fan",
            "Xiaohan Li",
            "Luyi Ma",
            "Kai Zhao",
            "Liang Peng",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kaushiki Nag",
            "Kannan Achan"
        ],
        "title": "LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning",
        "abstract": "arXiv:2509.22720v1 Announce Type: new  Abstract: Designing realistic multi-object scenes requires not only generating images, but also planning spatial layouts that respect semantic relations and physical plausibility. On one hand, while recent advances in diffusion models have enabled high-quality image generation, they lack explicit spatial reasoning, leading to unrealistic object layouts. On the other hand, traditional spatial planning methods in robotics emphasize geometric and relational consistency, but they struggle to capture semantic richness in visual scenes. To bridge this gap, in this paper, we propose LayoutAgent, an agentic framework that unifies vision-language reasoning with compositional diffusion for layout generation. Given multiple input images with target objects in them, our method first employs visual-language model to preprocess the inputs through segmentation, object size estimation, scene graph construction, and prompt rewriting. Then we leverage compositional diffusion-a method traditionally used in robotics-to synthesize bounding boxes that respect object relations encoded in the scene graph for spatial layouts. In the end, a foreground-conditioned image generator composes the complete scene by rendering the objects into the planned layout guided by designed prompts. Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.",
        "arxiv_id": "2509.22720",
        "ARXIVID": "2509.22720",
        "COMMENT": "Matches criteria 1 (new methodological improvements to spatial understanding on embodied agents) and 2 (shows new VLLMs/MLLMs). The paper proposes LayoutAgent, which unifies vision-language reasoning with compositional diffusion for spatial layout planning, explicitly addressing spatial reasoning and layout generation. It leverages VLMs for preprocessing and compositional diffusion for spatial layout, which is a novel integration.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.23661": {
        "authors": [
            "Xiang An",
            "Yin Xie",
            "Kaicheng Yang",
            "Wenkang Zhang",
            "Xiuwei Zhao",
            "Zheng Cheng",
            "Yirui Wang",
            "Songcen Xu",
            "Changrui Chen",
            "Chunsheng Wu",
            "Huajie Tan",
            "Chunyuan Li",
            "Jing Yang",
            "Jie Yu",
            "Xiyao Wang",
            "Bin Qin",
            "Yumeng Wang",
            "Zizhen Yan",
            "Ziyong Feng",
            "Ziwei Liu",
            "Bo Li",
            "Jiankang Deng"
        ],
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "abstract": "arXiv:2509.23661v1 Announce Type: new  Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction dataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed multimodal tokens. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community to await further updates.",
        "arxiv_id": "2509.23661",
        "ARXIVID": "2509.23661",
        "COMMENT": "Directly matches criterion 2 (new VLLMs/MLLMs). Introduces LLaVA-OneVision-1.5, a new family of large multimodal models with open, efficient, and reproducible training, and strong empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24980": {
        "authors": [
            "Shuang Liang",
            "Jing He",
            "Chuanmeizhi Wang",
            "Lejun Liao",
            "Guo Zhang",
            "Yingcong Chen",
            "Yuan Yuan"
        ],
        "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation",
        "abstract": "arXiv:2509.24980v1 Announce Type: new  Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \\textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance.",
        "arxiv_id": "2509.24980",
        "ARXIVID": "2509.24980",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications). Proposes SDPose, a method leveraging pre-trained diffusion models (vision foundation models) for robust human pose estimation, with strong cross-domain generalization and new benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25162": {
        "authors": [
            "Bowei Chen",
            "Sai Bi",
            "Hao Tan",
            "He Zhang",
            "Tianyuan Zhang",
            "Zhengqi Li",
            "Yuanjun Xiong",
            "Jianming Zhang",
            "Kai Zhang"
        ],
        "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
        "abstract": "arXiv:2509.25162v1 Announce Type: new  Abstract: In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.",
        "arxiv_id": "2509.25162",
        "ARXIVID": "2509.25162",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications). Proposes a new method to align pretrained visual foundation encoders as tokenizers for diffusion models, improving semantic richness and generation quality.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.23951": {
        "authors": [
            "Siyu Cao",
            "Hangting Chen",
            "Peng Chen",
            "Yiji Cheng",
            "Yutao Cui",
            "Xinchi Deng",
            "Ying Dong",
            "Kipper Gong",
            "Tianpeng Gu",
            "Xiusen Gu",
            "Tiankai Hang",
            "Duojun Huang",
            "Jie Jiang",
            "Zhengkai Jiang",
            "Weijie Kong",
            "Changlin Li",
            "Donghao Li",
            "Junzhe Li",
            "Xin Li",
            "Yang Li",
            "Zhenxi Li",
            "Zhimin Li",
            "Jiaxin Lin",
            "Linus",
            "Lucaz Liu",
            "Shu Liu",
            "Songtao Liu",
            "Yu Liu",
            "Yuhong Liu",
            "Yanxin Long",
            "Fanbin Lu",
            "Qinglin Lu",
            "Yuyang Peng",
            "Yuanbo Peng",
            "Xiangwei Shen",
            "Yixuan Shi",
            "Jiale Tao",
            "Yangyu Tao",
            "Qi Tian",
            "Pengfei Wan",
            "Chunyu Wang",
            "Kai Wang",
            "Lei Wang",
            "Linqing Wang",
            "Lucas Wang",
            "Qixun Wang",
            "Weiyan Wang",
            "Hao Wen",
            "Bing Wu",
            "Jianbing Wu",
            "Yue Wu",
            "Senhao Xie",
            "Fang Yang",
            "Miles Yang",
            "Xiaofeng Yang",
            "Xuan Yang",
            "Zhantao Yang",
            "Jingmiao Yu",
            "Zheng Yuan",
            "Chao Zhang",
            "Jian-Wei Zhang",
            "Peizhen Zhang",
            "Shi-Xue Zhang",
            "Tao Zhang",
            "Weigang Zhang",
            "Yepeng Zhang",
            "Yingfang Zhang",
            "Zihao Zhang",
            "Zijian Zhang",
            "Penghao Zhao",
            "Zhiyuan Zhao",
            "Xuefei Zhe",
            "Jianchen Zhu",
            "Zhao Zhong"
        ],
        "title": "HunyuanImage 3.0 Technical Report",
        "abstract": "arXiv:2509.23951v1 Announce Type: new  Abstract: We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
        "arxiv_id": "2509.23951",
        "ARXIVID": "2509.23951",
        "COMMENT": "HunyuanImage 3.0 is a large, open-source multimodal foundation model for image generation and understanding, with a Mixture-of-Experts architecture and state-of-the-art performance. This is a new VLLM/MLLM (criterion 2) and a vision foundation model (criterion 4).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24200": {
        "authors": [
            "Jiabin Luo",
            "Junhui Lin",
            "Zeyu Zhang",
            "Biao Wu",
            "Meng Fang",
            "Ling Chen",
            "Hao Tang"
        ],
        "title": "UniVid: The Open-Source Unified Video Model",
        "abstract": "arXiv:2509.24200v1 Announce Type: new  Abstract: Unified video modeling that combines generation and understanding capabilities is increasingly important but faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the limitations of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, a unified architecture that couples an MLLM with a diffusion decoder through a lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
        "arxiv_id": "2509.24200",
        "ARXIVID": "2509.24200",
        "COMMENT": "Matches criterion 2 (new MLLM for video), criterion 4 (vision foundation models and applications). Proposes UniVid, a unified video model for both generation and understanding, with a novel architecture coupling MLLM and diffusion decoder, and new techniques for temporal reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25047": {
        "authors": [
            "Ram Ramrakhya",
            "Andrew Szot",
            "Omar Attia",
            "Yuhao Yang",
            "Anh Nguyen",
            "Bogdan Mazoure",
            "Zhe Gan",
            "Harsh Agrawal",
            "Alexander Toshev"
        ],
        "title": "Scaling Synthetic Task Generation for Agents via Exploration",
        "abstract": "arXiv:2509.25047v1 Announce Type: new  Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation.",
        "arxiv_id": "2509.25047",
        "ARXIVID": "2509.25047",
        "COMMENT": "This paper introduces AutoPlay, a scalable pipeline for generating synthetic tasks for agents via exploration in interactive environments, using MLLMs for both exploration and task generation. This is a direct match for criterion 3 (embodied AI, new benchmarks/methods, novel angles) and criterion 2 (MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24231": {
        "authors": [
            "Yang Bai",
            "Haoran Cheng",
            "Yang Zhou",
            "Jun Zhou",
            "Arun Thirunavukarasu",
            "Yuhe Ke",
            "Jie Yao",
            "Kanae Fukutsu",
            "Chrystie Wan Ning Quek",
            "Ashley Hong",
            "Laura Gutierrez",
            "Zhen Ling Teo",
            "Darren Shu Jeng Ting",
            "Brian T. Soetikno",
            "Christopher S. Nielsen",
            "Tobias Elze",
            "Zengxiang Li",
            "Linh Le Dinh",
            "Hiok Hong Chan",
            "Victor Koh",
            "Marcus Tan",
            "Kelvin Z. Li",
            "Leonard Yip",
            "Ching Yu Cheng",
            "Yih Chung Tham",
            "Gavin Siew Wei Tan",
            "Leopold Schmetterer",
            "Marcus Ang",
            "Rahat Hussain",
            "Jod Mehta",
            "Tin Aung",
            "Lionel Tim-Ee Cheng",
            "Tran Nguyen Tuan Anh",
            "Chee Leong Cheng",
            "Tien Yin Wong",
            "Nan Liu",
            "Iain Beehuat Tan",
            "Soon Thye Lim",
            "Eyal Klang",
            "Tony Kiat Hon Lim",
            "Rick Siow Mong Goh",
            "Yong Liu",
            "Daniel Shu Wei Ting"
        ],
        "title": "EVLF-FM: Explainable Vision Language Foundation Model for Medicine",
        "abstract": "arXiv:2509.24231v1 Announce Type: new  Abstract: Despite the promise of foundation models in medical AI, current systems remain limited - they are modality-specific and lack transparent reasoning processes, hindering clinical adoption. To address this gap, we present EVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify broad diagnostic capability with fine-grain explainability. The development and testing of EVLF-FM encompassed over 1.3 million total samples from 23 global datasets across eleven imaging modalities related to six clinical specialties: dermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology. External validation employed 8,884 independent test samples from 10 additional datasets across five imaging modalities. Technically, EVLF-FM is developed to assist with multiple disease diagnosis and visual question answering with pixel-level visual grounding and reasoning capabilities. In internal validation for disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858) and F1-score (0.797), outperforming leading generalist and specialist models. In medical visual grounding, EVLF-FM also achieved stellar performance across nine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External validations further confirmed strong zero-shot and few-shot performance, with competitive F1-scores despite a smaller model size. Through a hybrid training strategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not only achieves state-of-the-art accuracy but also exhibits step-by-step reasoning, aligning outputs with visual evidence. EVLF-FM is an early multi-disease VLM model with explainability and reasoning capabilities that could advance adoption of and trust in foundation models for real-world clinical deployment.",
        "arxiv_id": "2509.24231",
        "ARXIVID": "2509.24231",
        "COMMENT": "This paper presents EVLF-FM, a multimodal vision-language foundation model for medicine, with explainability and reasoning capabilities. This is a direct match for criterion 4 (vision foundation models and applications) and criterion 2 (VLLMs/MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22737": {
        "authors": [
            "Jie Cai",
            "Kangning Yang",
            "Lan Fu",
            "Jiaming Ding",
            "Jinlong Li",
            "Huiming Sun",
            "Daitao Xing",
            "Jinglin Shen",
            "Zibo Meng"
        ],
        "title": "CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models",
        "abstract": "arXiv:2509.22737v1 Announce Type: new  Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.",
        "arxiv_id": "2509.22737",
        "ARXIVID": "2509.22737",
        "COMMENT": "This paper introduces CompareBench, a new benchmark for visual comparison reasoning in VLMs, with a focus on spatial, temporal, geometric, and quantity reasoning. This directly matches criterion 3 (new benchmark for embodied/vision-language models with a novel angle) and criterion 2 (VLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24837": {
        "authors": [
            "Youngeun Kim",
            "Youjia Zhang",
            "Huiling Liu",
            "Aecheon Jung",
            "Sunwoo Lee",
            "Sungeun Hong"
        ],
        "title": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models",
        "abstract": "arXiv:2509.24837v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) enable strong multimodal reasoning but incur heavy inference costs from redundant visual tokens. Token pruning alleviates this issue, yet existing approaches face limitations. Attention-based methods rely on raw attention scores, which are often unstable across layers and heads and can lead to redundant selections. Diversity-based methods improve robustness by selecting tokens far apart in feature space but risk dropping regions needed for accurate prediction. We propose \\ours, a training-free framework built on a simple intuition: tokens with higher sensitivity are more likely to influence the model's output, and they should also capture complementary visual cues rather than overlapping information. To achieve this, we estimate token sensitivity using zeroth-order perturbations at the projection layer, a shallow and computationally light component of the model. This approach measures how small random perturbations affect the projection outputs, allowing us to approximate each token's influence through lightweight forward passes without backpropagation. Extensive experiments across multiple VLMs and benchmarks show that \\ours consistently outperforms prior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and significantly improving efficiency, achieving up to 2.30x faster end-to-end inference over the baseline.",
        "arxiv_id": "2509.24837",
        "ARXIVID": "2509.24837",
        "COMMENT": "This paper introduces a training-free token pruning method for Vision-Language Models (VLMs) using zeroth-order gradient estimation. This is a direct methodological improvement for VLLMs (criterion 2), with a clever statistical trick for efficiency.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24709": {
        "authors": [
            "Yang Chen",
            "Minghao Liu",
            "Yufan Shen",
            "Yunwen Li",
            "Tianyuan Huang",
            "Xinyu Fang",
            "Tianyu Zheng",
            "Wenxuan Huang",
            "Cheng Yang",
            "Daocheng Fu",
            "Jianbiao Mei",
            "Rong Wu",
            "Licheng Wen",
            "Xuemeng Yang",
            "Song Mao",
            "Qunshu Lin",
            "Zhi Yu",
            "Yongliang Shen",
            "Yu Qiao",
            "Botian Shi"
        ],
        "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?",
        "abstract": "arXiv:2509.24709v1 Announce Type: new  Abstract: The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench.",
        "arxiv_id": "2509.24709",
        "ARXIVID": "2509.24709",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (IWR-Bench) for evaluating LVLMs on interactive webpage reconstruction from user interaction videos, focusing on dynamic, event-driven logic that previous static benchmarks ignored.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24473": {
        "authors": [
            "Shijie Lian",
            "Changti Wu",
            "Laurence Tianruo Yang",
            "Hang Yuan",
            "Bin Yu",
            "Lei Zhang",
            "Kai Chen"
        ],
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
        "abstract": "arXiv:2509.24473v1 Announce Type: new  Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
        "arxiv_id": "2509.24473",
        "ARXIVID": "2509.24473",
        "COMMENT": "This paper directly addresses criterion 1 (new methodological improvements to spatial understanding/spatial intelligence in VLLMs/MLLMs) by introducing geometry-centric fine-tuning for spatial reasoning in vision-language models, and also matches criterion 2 (shows new VLLMs/MLLMs) by finetuning Qwen2.5VL and RoboBrain2.0. It is a systematic study on improving spatial skills in MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.23922": {
        "authors": [
            "Haibao Yu",
            "Wenxian Yang",
            "Ruiyang Hao",
            "Chuanye Wang",
            "Jiaru Zhong",
            "Ping Luo",
            "Zaiqing Nie"
        ],
        "title": "DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation",
        "abstract": "arXiv:2509.23922v1 Announce Type: new  Abstract: Closed-loop evaluation is increasingly critical for end-to-end autonomous driving. Current closed-loop benchmarks using the CARLA simulator rely on manually configured traffic scenarios, which can diverge from real-world conditions, limiting their ability to reflect actual driving performance. To address these limitations, we introduce a simple yet challenging closed-loop evaluation framework that closely integrates real-world driving scenarios into the CARLA simulator with infrastructure cooperation. Our approach involves extracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour video dataset captured by high-mounted infrastructure sensors, and creating static digital twin assets for 15 real-world intersections with consistent visual appearance. These digital twins accurately replicate the traffic and environmental characteristics of their real-world counterparts, enabling more realistic simulations in CARLA. This evaluation is challenging due to the diversity of driving behaviors, locations, weather conditions, and times of day at complex urban intersections. In addition, we provide a comprehensive closed-loop benchmark for evaluating end-to-end autonomous driving models. Project URL: \\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.",
        "arxiv_id": "2509.23922",
        "ARXIVID": "2509.23922",
        "COMMENT": "This paper introduces a new closed-loop benchmark for end-to-end autonomous driving by integrating real-world scenarios into the CARLA simulator. This directly matches criterion 3 (embodied AI, new benchmark/simulator, novel angle: real-to-simulation with digital twins of real intersections).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22820": {
        "authors": [
            "Jaeik Kim",
            "Woojin Kim",
            "Woohyeon Park",
            "Jaeyoung Do"
        ],
        "title": "MMPB: It's Time for Multi-Modal Personalization",
        "abstract": "arXiv:2509.22820v1 Announce Type: new  Abstract: Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
        "arxiv_id": "2509.22820",
        "ARXIVID": "2509.22820",
        "COMMENT": "Matches criterion 3 (new benchmark for VLM personalization, multi-modal learning) and criterion 2 (evaluation of 23 VLMs on personalization). Introduces MMPB, a large-scale benchmark for multi-modal personalization, with surprising empirical findings on VLM limitations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25146": {
        "authors": [
            "Richeek Das",
            "Kostas Daniilidis",
            "Pratik Chaudhari"
        ],
        "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
        "abstract": "arXiv:2509.25146v1 Announce Type: new  Abstract: This paper develops a mathematical argument and algorithms for building representations of data from event-based cameras, that we call Fast Feature Field ($\\text{F}^3$). We learn this representation by predicting future events from past events and show that it preserves scene structure and motion information. $\\text{F}^3$ exploits the sparsity of event data and is robust to noise and variations in event rates. It can be computed efficiently using ideas from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and 440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous spatiotemporal volume as a multi-channel image, enabling a range of downstream tasks. We obtain state-of-the-art performance on optical flow estimation, semantic segmentation, and monocular metric depth estimation, on data from three robotic platforms (a car, a quadruped robot and a flying platform), across different lighting conditions (daytime, nighttime), environments (indoors, outdoors, urban, as well as off-road) and dynamic vision sensors (resolutions and event rates). Our implementations can predict these tasks at 25-75 Hz at HD resolution.",
        "arxiv_id": "2509.25146",
        "ARXIVID": "2509.25146",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel representation for event-based cameras, with strong results on embodied platforms). Proposes Fast Feature Field for predictive event representation, enabling high-speed, robust spatial understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.25142": {
        "authors": [
            "Nicholas Budny",
            "Kia Ghods",
            "Declan Campbell",
            "Raja Marjieh",
            "Amogh Joshi",
            "Sreejan Kumar",
            "Jonathan D. Cohen",
            "Taylor W. Webb",
            "Thomas L. Griffiths"
        ],
        "title": "Visual serial processing deficits explain divergences in human and VLM reasoning",
        "abstract": "arXiv:2509.25142v1 Announce Type: new  Abstract: Why do Vision Language Models (VLMs), despite success on standard benchmarks, often fail to match human performance on surprisingly simple visual reasoning tasks? While the underlying computational principles are still debated, we hypothesize that a crucial factor is a deficit in visually-grounded serial processing. To test this hypothesis, we compared human and VLM performance across tasks designed to vary serial processing demands in three distinct domains: geometric reasoning, perceptual enumeration, and mental rotation. Tasks within each domain varied serial processing load by manipulating factors such as geometric concept complexity, perceptual individuation load, and transformation difficulty. Across all domains, our results revealed a consistent pattern: decreased VLM accuracy was strongly correlated with increased human reaction time (used as a proxy for serial processing load). As tasks require more demanding serial processing -- whether composing concepts, enumerating items, or performing mental transformations -- the VLM-human performance gap widens reliably. These findings support our hypothesis, indicating that limitations in serial, visually grounded reasoning represent a fundamental bottleneck that distinguishes current VLMs from humans.",
        "arxiv_id": "2509.25142",
        "ARXIVID": "2509.25142",
        "COMMENT": "This paper empirically investigates why VLMs diverge from humans on visual reasoning tasks, identifying serial processing deficits as a key bottleneck. It provides surprising empirical results and insights into VLMs (criterion 2), and is highly relevant for understanding vision-language model limitations.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.23589": {
        "authors": [
            "Shu Liu",
            "Wenlin Chen",
            "Weihao Li",
            "Zheng Wang",
            "Lijin Yang",
            "Jianing Huang",
            "Yipin Zhang",
            "Zhongzhan Huang",
            "Ze Cheng",
            "Hao Yang"
        ],
        "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving",
        "abstract": "arXiv:2509.23589v1 Announce Type: new  Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 5% over prior arts.",
        "arxiv_id": "2509.23589",
        "ARXIVID": "2509.23589",
        "COMMENT": "BridgeDrive introduces a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning in autonomous driving, achieving SOTA on Bench2Drive. This is highly relevant to criterion 3 (embodied AI, new methods/benchmarks, simulator-related), and uses generative modeling (diffusion) in a novel way for planning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24365": {
        "authors": [
            "Jitai Hao",
            "Hao Liu",
            "Xinyan Xiao",
            "Qiang Huang",
            "Jun Yu"
        ],
        "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
        "abstract": "arXiv:2509.24365v1 Announce Type: new  Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X",
        "arxiv_id": "2509.24365",
        "ARXIVID": "2509.24365",
        "COMMENT": "Matches criterion 2: Proposes a new unified multimodal model (UMM) architecture (Uni-X) to address modality conflict in vision-language models, with strong empirical results and parameter efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24563": {
        "authors": [
            "Zi-Yuan Hu",
            "Shuo Liang",
            "Duo Zheng",
            "Yanyang Li",
            "Yeyao Tao",
            "Shijia Huang",
            "Wei Feng",
            "Jia Qin",
            "Jianguang Yu",
            "Jing Huang",
            "Meng Fang",
            "Yin Li",
            "Liwei Wang"
        ],
        "title": "NeMo: Needle in a Montage for Video-Language Understanding",
        "abstract": "arXiv:2509.24563v1 Announce Type: new  Abstract: Recent advances in video large language models (VideoLLMs) call for new evaluation protocols and benchmarks for complex temporal reasoning in video-language understanding. Inspired by the needle in a haystack test widely used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed to assess VideoLLMs' critical reasoning capabilities, including long-context recall and temporal grounding. To generate video question answering data for our task, we develop a scalable automated data generation pipeline that facilitates high-quality data synthesis. Built upon the proposed pipeline, we present NeMoBench, a video-language benchmark centered on our task. Specifically, our full set of NeMoBench features 31,378 automatically generated question-answer (QA) pairs from 13,486 videos with various durations ranging from seconds to hours. Experiments demonstrate that our pipeline can reliably and automatically generate high-quality evaluation data, enabling NeMoBench to be continuously updated with the latest videos. We evaluate 20 state-of-the-art models on our benchmark, providing extensive results and key insights into their capabilities and limitations. Our project page is available at: https://lavi-lab.github.io/NeMoBench.",
        "arxiv_id": "2509.24563",
        "ARXIVID": "2509.24563",
        "COMMENT": "NeMo introduces a new benchmark (NeMoBench) for evaluating VideoLLMs on complex temporal reasoning, with a scalable data generation pipeline and extensive evaluation. This is a new benchmark for embodied AI and VLLMs (criteria 2 and 3), with a novel angle on long-context and temporal grounding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24776": {
        "authors": [
            "Yizhuo Ding",
            "Mingkang Chen",
            "Zhibang Feng",
            "Tong Xiao",
            "Wanying Qu",
            "Wenqi Shao",
            "Yanwei Fu"
        ],
        "title": "VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding",
        "abstract": "arXiv:2509.24776v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: https://github.com/yizhuoDi/VTPerceprion-R1.",
        "arxiv_id": "2509.24776",
        "ARXIVID": "2509.24776",
        "COMMENT": "Matches criterion 2: introduces VTPerception-R1, a new MLLM framework with explicit visual and textual perceptual grounding, and provides empirical insights into perception strategies for multimodal reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.23993": {
        "authors": [
            "Muleilan Pei",
            "Shaoshuai Shi",
            "Shaojie Shen"
        ],
        "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning",
        "abstract": "arXiv:2509.23993v1 Announce Type: new  Abstract: Scalable and realistic simulation of multi-agent traffic behavior is critical for advancing autonomous driving technologies. Although existing data-driven simulators have made significant strides in this domain, they predominantly rely on supervised learning to align simulated distributions with real-world driving scenarios. A persistent challenge, however, lies in the distributional shift that arises between training and testing, which often undermines model generalization in unseen environments. To address this limitation, we propose SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for next-token prediction models to better align agent behavior with human preferences and evaluation metrics. Our approach introduces a metric-oriented policy optimization algorithm to improve distribution alignment and an iterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance gains. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) validate the effectiveness of this simple yet powerful R1-style training framework in enhancing foundation models. The results on the Waymo Open Sim Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art performance with an overall realism meta score of 0.7858, ranking first on the leaderboard at the time of submission.",
        "arxiv_id": "2509.23993",
        "ARXIVID": "2509.23993",
        "COMMENT": "This paper proposes SMART-R1, a new reinforcement fine-tuning paradigm for multi-agent traffic simulation, directly matching criterion 3 (embodied AI, new method for simulator realism, novel angle: R1-style RL for next-token prediction in simulators).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.24251": {
        "authors": [
            "Bangzheng Li",
            "Ximeng Sun",
            "Jiang Liu",
            "Ze Wang",
            "Jialian Wu",
            "Xiaodong Yu",
            "Hao Chen",
            "Emad Barsoum",
            "Muhao Chen",
            "Zicheng Liu"
        ],
        "title": "Latent Visual Reasoning",
        "abstract": "arXiv:2509.24251v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in various tasks by incorporating Chain-of-Thought (CoT) reasoning in language spaces. Recent work extends this direction by leveraging external tools for visual editing, thereby enhancing the visual signal along the reasoning trajectories. Nevertheless, these approaches remain fundamentally constrained: reasoning is still confined to the language space, with visual information treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a new paradigm that enables autoregressive reasoning directly in the visual embedding space. A visual encoder first projects images into visual tokens within a joint semantic space shared with the language model. The language model is then trained to generate latent states that reconstruct key visual tokens critical for answering the query, constituting the process of latent visual reasoning. By interleaving LVR with standard text generation, our model achieves substantial gains on perception-intensive visual question answering tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement learning on latent reasoning, further balancing LVR and textual generation. We show that LVR substantially improves fine-grained visual understanding and perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code base and model weights will be released later.",
        "arxiv_id": "2509.24251",
        "ARXIVID": "2509.24251",
        "COMMENT": "This paper introduces Latent Visual Reasoning (LVR), a new paradigm for autoregressive reasoning in the visual embedding space of MLLMs, and adapts reinforcement learning for latent reasoning. This matches criterion 2 (shows new MLLMs/VLLMs with a novel reasoning paradigm) and is highly relevant for vision-language reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24897": {
        "authors": [
            "Yang Shi",
            "Yuhao Dong",
            "Yue Ding",
            "Yuran Wang",
            "Xuanyu Zhu",
            "Sheng Zhou",
            "Wenting Liu",
            "Haochen Tian",
            "Rundong Wang",
            "Huanqian Wang",
            "Zuyan Liu",
            "Bohan Zeng",
            "Ruizhe Chen",
            "Qixun Wang",
            "Zhuoran Zhang",
            "Xinlong Chen",
            "Chengzhuo Tong",
            "Bozhou Li",
            "Chaoyou Fu",
            "Qiang Liu",
            "Haotian Wang",
            "Wenjing Yang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Yi-Fan Zhang",
            "Ziwei Liu"
        ],
        "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark",
        "abstract": "arXiv:2509.24897v1 Announce Type: new  Abstract: The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.",
        "arxiv_id": "2509.24897",
        "ARXIVID": "2509.24897",
        "COMMENT": "This paper introduces RealUnify, a new benchmark specifically designed to evaluate the synergy between understanding and generation in unified multimodal models. This matches criterion 2 (new VLLMs/MLLMs, as it evaluates unified models) and criterion 3 (new benchmark for multi-modal models, focusing on a novel angle: bidirectional synergy).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24099": {
        "authors": [
            "Prerit Gupta",
            "Shourya Verma",
            "Ananth Grama",
            "Aniket Bera"
        ],
        "title": "Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow",
        "abstract": "arXiv:2509.24099v1 Announce Type: new  Abstract: Generating realistic, context-aware two-person motion conditioned on diverse modalities remains a central challenge in computer graphics, animation, and human-computer interaction. We introduce DualFlow, a unified and efficient framework for multi-modal two-person motion generation. DualFlow conditions 3D motion synthesis on diverse inputs, including text, music, and prior motion sequences. Leveraging rectified flow, it achieves deterministic straight-line sampling paths between noise and data, reducing inference time and mitigating error accumulation common in diffusion-based models. To enhance semantic grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that retrieves motion exemplars using music features and LLM-based text decompositions of spatial relations, body movements, and rhythmic patterns. We use contrastive objective that further strengthens alignment with conditioning signals and introduce synchronization loss that improves inter-person coordination. Extensive evaluations across text-to-motion, music-to-motion, and multi-modal interactive benchmarks show consistent gains in motion quality, responsiveness, and efficiency. DualFlow produces temporally coherent and rhythmically synchronized motions, setting state-of-the-art in multi-modal human motion generation.",
        "arxiv_id": "2509.24099",
        "ARXIVID": "2509.24099",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications): Introduces a multi-modal 3D motion generation framework using rectified flow and retrieval-augmented generation, with explicit spatial relation modeling and vision-language alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.23322": {
        "authors": [
            "Hongrui Jia",
            "Chaoya Jiang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "title": "Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning",
        "abstract": "arXiv:2509.23322v1 Announce Type: new  Abstract: Significant advancements in the reasoning capabilities of Large Language Models (LLMs) are now driven by test-time scaling laws, particularly those leveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these breakthroughs, researchers have extended these paradigms to Large Multimodal Models (LMMs). However, a critical limitation emerges: as their reasoning chains extend, LMMs increasingly rely on textual logic, progressively losing grounding in the underlying visual information. This leads to reasoning paths that diverge from the image content, culminating in erroneous conclusions. To address this, we introduce a strikingly simple yet effective training-free visual-reasoning pipeline. The core concept is to decouple the reasoning and perception processes. A powerful LLM orchestrates the high-level reasoning, strategically interrogating a LMM to extract specific visual information required for its logical chain. The LMM, in turn, functions exclusively as a visual question-answering engine, supplying the necessary perceptual details on demand. This lightweight, plug-and-play approach requires no additional training or architectural changes. Comprehensive evaluations validate that our framework effectively governs the visual reasoning process, leading to a significant reduction in visually-unfounded reasoning steps and a substantial improvement in reasoning fidelity.",
        "arxiv_id": "2509.23322",
        "ARXIVID": "2509.23322",
        "COMMENT": "Matches criterion 2 (new VLLM/MLLM methods) and criterion 4 (vision foundation models and applications). Proposes a new LLM-LMM framework for faithful visual reasoning, decoupling reasoning and perception, with a training-free, plug-and-play approach and strong empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.25172": {
        "authors": [
            "Yuxin Jiang",
            "Yuchao Gu",
            "Yiren Song",
            "Ivor Tsang",
            "Mike Zheng Shou"
        ],
        "title": "Personalized Vision via Visual In-Context Learning",
        "abstract": "arXiv:2509.25172v1 Announce Type: new  Abstract: Modern vision models, trained on large-scale annotated datasets, excel at predefined tasks but struggle with personalized vision -- tasks defined at test time by users with customized objects or novel objectives. Existing personalization approaches rely on costly fine-tuning or synthetic data pipelines, which are inflexible and restricted to fixed task formats. Visual in-context learning (ICL) offers a promising alternative, yet prior methods confine to narrow, in-domain tasks and fail to generalize to open-ended personalization. We introduce Personalized In-Context Operator (PICO), a simple four-panel framework that repurposes diffusion transformers as visual in-context learners. Given a single annotated exemplar, PICO infers the underlying transformation and applies it to new inputs without retraining. To enable this, we construct VisRel, a compact yet diverse tuning dataset, showing that task diversity, rather than scale, drives robust generalization. We further propose an attention-guided seed scorer that improves reliability via efficient inference scaling. Extensive experiments demonstrate that PICO (i) surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to novel user-defined tasks, and (iii) generalizes across both recognition and generation.",
        "arxiv_id": "2509.25172",
        "ARXIVID": "2509.25172",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications) and partially criterion 2 (new methods for visual in-context learning, which is a form of VLM/MLLM). Proposes a new framework (PICO) for personalized visual in-context learning using diffusion transformers, with strong empirical results and a new dataset.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24910": {
        "authors": [
            "Songze Li",
            "Zun Wang",
            "Gengze Zhou",
            "Jialu Li",
            "Xiangyu Zeng",
            "Limin Wang",
            "Yu Qiao",
            "Qi Wu",
            "Mohit Bansal",
            "Yi Wang"
        ],
        "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale",
        "abstract": "arXiv:2509.24910v1 Announce Type: new  Abstract: Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
        "arxiv_id": "2509.24910",
        "ARXIVID": "2509.24910",
        "COMMENT": "Matches criterion 3 (Embodied AI, new methods for navigation): Proposes a self-improving demonstration pipeline for goal-oriented language-guided navigation, improving exploration and generalization in embodied agents. Demonstrates significant empirical gains and a novel iterative training approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.23105": {
        "authors": [
            "Junxiao Xue",
            "Quan Deng",
            "Xuecheng Wu",
            "Kelu Yao",
            "Xinyi Yin",
            "Fei Yu",
            "Wei Zhou",
            "Yanfei Zhong",
            "Yang Liu",
            "Dingkang Yang"
        ],
        "title": "Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM",
        "abstract": "arXiv:2509.23105v1 Announce Type: new  Abstract: Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced vision-guided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S*m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method.",
        "arxiv_id": "2509.23105",
        "ARXIVID": "2509.23105",
        "COMMENT": "Matches criteria 2 (new VLMs/MLLMs) and 4 (vision foundation models and applications). The paper introduces a new large-scale dataset (ChangeIMTI) and a novel dual-granularity vision-language model (ChangeVG) for remote sensing change understanding, with a dual-branch architecture for spatial and semantic features, and demonstrates improved performance on multiple tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.24081": {
        "authors": [
            "Sucheng Ren",
            "Chen Chen",
            "Zhenbang Wang",
            "Liangchen Song",
            "Xiangxin Zhu",
            "Alan Yuille",
            "Yinfei Yang",
            "Jiasen Lu"
        ],
        "title": "Autoregressive Video Generation beyond Next Frames Prediction",
        "abstract": "arXiv:2509.24081v1 Announce Type: new  Abstract: Autoregressive models for video generation typically operate frame-by-frame, extending next-token prediction from language to video's temporal dimension. We question that unlike word as token is universally agreed in language if frame is a appropriate prediction unit? To address this, we present VideoAR, a unified framework that supports a spectrum of prediction units including full frames, key-detail frames, multiscale refinements, and spatiotemporal cubes. Among these designs, we find model video generation using \\textit{spatiotemporal} cubes as prediction units, which allows autoregressive models to operate across both spatial and temporal dimensions simultaneously. This approach eliminates the assumption that frames are the natural atomic units for video autoregression. We evaluate VideoAR across diverse prediction strategies, finding that cube-based prediction consistently delivers superior quality, speed, and temporal coherence. By removing the frame-by-frame constraint, our video generator surpasses state-of-the-art baselines on VBench while achieving faster inference and enabling seamless scaling to minute-long sequences. We hope this work will motivate rethinking sequence decomposition in video and other spatiotemporal domains.",
        "arxiv_id": "2509.24081",
        "ARXIVID": "2509.24081",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is highly relevant to generative modeling in video. Proposes a new autoregressive framework for video generation using spatiotemporal cubes, challenging the standard frame-by-frame approach.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25122": {
        "authors": [
            "Jan Held",
            "Renaud Vandeghen",
            "Sanghyun Son",
            "Daniel Rebain",
            "Matheus Gadelha",
            "Yi Zhou",
            "Ming C. Lin",
            "Marc Van Droogenbroeck",
            "Andrea Tagliasacchi"
        ],
        "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
        "abstract": "arXiv:2509.25122v1 Announce Type: new  Abstract: Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
        "arxiv_id": "2509.25122",
        "ARXIVID": "2509.25122",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is highly relevant to computer vision, proposing a new differentiable rendering method with mesh-based primitives for novel view synthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25139": {
        "authors": [
            "Yue Zhang",
            "Tianyi Ma",
            "Zun Wang",
            "Yanyuan Qiao",
            "Parisa Kordjamshidi"
        ],
        "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
        "abstract": "arXiv:2509.25139v1 Announce Type: new  Abstract: Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.",
        "arxiv_id": "2509.25139",
        "ARXIVID": "2509.25139",
        "COMMENT": "This paper proposes a new method for Vision-and-Language Navigation (VLN) by leveraging analogical textual descriptions in LLMs to improve spatial and contextual understanding for embodied agents. This matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (novel embodied AI method for navigation).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.25183": {
        "authors": [
            "Ting-Hsuan Liao",
            "Haowen Liu",
            "Yiran Xu",
            "Songwei Ge",
            "Gengshan Yang",
            "Jia-Bin Huang"
        ],
        "title": "PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos",
        "abstract": "arXiv:2509.25183v1 Announce Type: new  Abstract: We present PAD3R, a method for reconstructing deformable 3D objects from casually captured, unposed monocular videos. Unlike existing approaches, PAD3R handles long video sequences featuring substantial object deformation, large-scale camera movement, and limited view coverage that typically challenge conventional systems. At its core, our approach trains a personalized, object-centric pose estimator, supervised by a pre-trained image-to-3D model. This guides the optimization of deformable 3D Gaussian representation. The optimization is further regularized by long-term 2D point tracking over the entire input video. By combining generative priors and differentiable rendering, PAD3R reconstructs high-fidelity, articulated 3D representations of objects in a category-agnostic way. Extensive qualitative and quantitative results show that PAD3R is robust and generalizes well across challenging scenarios, highlighting its potential for dynamic scene understanding and 3D content creation.",
        "arxiv_id": "2509.25183",
        "ARXIVID": "2509.25183",
        "COMMENT": "Matches criterion 4 and partially criterion 1: PAD3R reconstructs dynamic, deformable 3D objects from casual videos using generative priors and differentiable rendering, advancing dynamic scene understanding and 3D content creation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.24072": {
        "authors": [
            "Hosein Hasani",
            "Amirmohammad Izadi",
            "Fatemeh Askari",
            "Mobin Bagherian",
            "Sadegh Mohammadian",
            "Mohammad Izadi",
            "Mahdieh Soleymani Baghshah"
        ],
        "title": "Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding",
        "abstract": "arXiv:2509.24072v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as robust within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism explaining how external cues enhance multimodal binding, offering both interpretability and practical improvements in robustness.",
        "arxiv_id": "2509.24072",
        "ARXIVID": "2509.24072",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): Investigates internal mechanisms of grounding in large vision-language models, introducing the concept of 'Grounding IDs' and providing interpretability and practical improvements. This is a novel empirical and analytical study on VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23263": {
        "authors": [
            "Tao Xiong",
            "Xavier Hu",
            "Yurun Chen",
            "Yuhang Liu",
            "Changqiao Wu",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "Shengyu Zhang"
        ],
        "title": "GUI-PRA: Process Reward Agent for GUI Tasks",
        "abstract": "arXiv:2509.23263v1 Announce Type: new  Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a \"lost in the middle\" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context.",
        "arxiv_id": "2509.23263",
        "ARXIVID": "2509.23263",
        "COMMENT": "Matches criteria 2 (new MLLMs) and partially 3 (embodied AI methods for GUI agents, with a novel process reward agent and dynamic memory for GUI tasks). The paper introduces GUI-PRA, a judge agent for GUI tasks, with a dynamic memory mechanism and adaptive UI perception, which are new methods for improving MLLM-based agents in interactive environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23919": {
        "authors": [
            "Longtao Jiang",
            "Mingfei Han",
            "Lei Chen",
            "Yongqiang Yu",
            "Feng Zhao",
            "Xiaojun Chang",
            "Zhihui Li"
        ],
        "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models",
        "abstract": "arXiv:2509.23919v1 Announce Type: new  Abstract: Text-guided image inpainting aims to inpaint masked image regions based on a textual prompt while preserving the background. Although diffusion-based methods have become dominant, their property of modeling the entire image in latent space makes it challenging for the results to align well with prompt details and maintain a consistent background. To address these issues, we explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports image inpainting by generating latent tokens corresponding to mask regions, enabling better local controllability without altering the background. However, directly applying MAR to this task makes the inpainting content either ignore the prompts or be disharmonious with the background context. Through analysis of the attention maps from the inpainting images, we identify the impact of background tokens on text tokens during the MAR generation, and leverage this to design \\textbf{Token Painter}, a training-free text-guided image inpainting method based on MAR. Our approach introduces two key components: (1) Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and context information from text and background in frequency domain to produce novel guidance tokens, allowing MAR to generate text-faithful inpainting content while keeping harmonious with background context. (2) Adaptive Decoder Attention Score Enhancing (ADAE), which adaptively enhances attention scores on guidance tokens and inpainting tokens to further enhance the alignment of prompt details and the content visual quality. Extensive experiments demonstrate that our training-free method outperforms prior state-of-the-art methods across almost all metrics and delivers superior visual results. Codes will be released.",
        "arxiv_id": "2509.23919",
        "ARXIVID": "2509.23919",
        "COMMENT": "Token Painter proposes a training-free, text-guided image inpainting method using Mask AutoRegressive models, with novel attention-based fusion and decoder tricks. This is directly relevant to criterion 4 (vision foundation models and applications), and also involves clever statistical/architectural tricks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22799": {
        "authors": [
            "Xuan He",
            "Dongfu Jiang",
            "Ping Nie",
            "Minghao Liu",
            "Zhengxuan Jiang",
            "Mingyi Su",
            "Wentao Ma",
            "Junru Lin",
            "Chun Ye",
            "Yi Lu",
            "Keming Wu",
            "Benjamin Schneider",
            "Quy Duc Do",
            "Zhuofeng Li",
            "Yiming Jia",
            "Yuxuan Zhang",
            "Guo Cheng",
            "Haozhe Wang",
            "Wangchunshu Zhou",
            "Qunshu Lin",
            "Yuanxing Zhang",
            "Ge Zhang",
            "Wenhao Huang",
            "Wenhu Chen"
        ],
        "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
        "abstract": "arXiv:2509.22799v1 Announce Type: new  Abstract: Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
        "arxiv_id": "2509.22799",
        "ARXIVID": "2509.22799",
        "COMMENT": "This paper introduces VideoScore2, a new multi-dimensional, interpretable evaluation framework for generative video models, with chain-of-thought rationales and reward modeling. It is highly relevant to criterion 4 (vision foundation models and applications) and also touches on generative modeling evaluation, which is of interest.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25180": {
        "authors": [
            "Wenkun He",
            "Yuchao Gu",
            "Junyu Chen",
            "Dongyun Zou",
            "Yujun Lin",
            "Zhekai Zhang",
            "Haocheng Xi",
            "Muyang Li",
            "Ligeng Zhu",
            "Jincheng Yu",
            "Junsong Chen",
            "Enze Xie",
            "Song Han",
            "Han Cai"
        ],
        "title": "DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space",
        "abstract": "arXiv:2509.25180v1 Announce Type: new  Abstract: Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.",
        "arxiv_id": "2509.25180",
        "ARXIVID": "2509.25180",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new framework for accelerating text-to-image diffusion models (foundation models) via deeply compressed latent space, with significant speedup for high-res image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23733": {
        "authors": [
            "Hangtian Zhao",
            "Xiang Chen",
            "Yizhe Li",
            "Qianhao Wang",
            "Haibo Lu",
            "Fei Gao"
        ],
        "title": "FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention",
        "abstract": "arXiv:2509.23733v1 Announce Type: new  Abstract: In this paper we propose FastViDAR, a novel framework that takes four fisheye camera inputs and produces a full $360^\\circ$ depth map along with per-camera depth, fusion depth, and confidence estimates. Our main contributions are: (1) We introduce Alternative Hierarchical Attention (AHA) mechanism that efficiently fuses features across views through separate intra-frame and inter-frame windowed self-attention, achieving cross-view feature mixing with reduced overhead. (2) We propose a novel ERP fusion approach that projects multi-view depth estimates to a shared equirectangular coordinate system to obtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D and 2D3D-S datasets for comprehensive evaluation, demonstrating competitive zero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA Orin NX embedded hardware. Project page: \\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}",
        "arxiv_id": "2509.23733",
        "ARXIVID": "2509.23733",
        "COMMENT": "Matches criterion 1 (spatial intelligence for embodied agents) and criterion 3 (novel method for omnidirectional depth estimation, with real-time performance and new ERP fusion approach). The AHA mechanism and ERP fusion are new angles for multi-camera depth estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22889": {
        "authors": [
            "Federico Chinello",
            "Giacomo Boracchi"
        ],
        "title": "Convolutional Set Transformer",
        "abstract": "arXiv:2509.22889v1 Announce Type: new  Abstract: We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer).",
        "arxiv_id": "2509.22889",
        "ARXIVID": "2509.22889",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on image sets) and criterion 4 (vision foundation models and applications). CST is a new architecture for set-based image understanding, directly operating on 3D image tensors, and supports transfer learning and explainability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24798": {
        "authors": [
            "Lei Tong",
            "Zhihua Liu",
            "Chaochao Lu",
            "Dino Oglic",
            "Tom Diethe",
            "Philip Teare",
            "Sotirios A. Tsaftaris",
            "Chen Jin"
        ],
        "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
        "abstract": "arXiv:2509.24798v1 Announce Type: new  Abstract: We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91\\% MAE reduction on Pendulum for accurate attribute control and 87\\% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation.",
        "arxiv_id": "2509.24798",
        "ARXIVID": "2509.24798",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. Proposes Causal-Adapter for counterfactual image generation using text-to-image diffusion models, with causal interventions and strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22688": {
        "authors": [
            "Xu Jia"
        ],
        "title": "Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization",
        "abstract": "arXiv:2509.22688v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language reasoning but often struggle with structured perception tasks requiring precise localization and robustness. We propose a reinforcement learning framework that augments Group Relative Policy Optimization (GRPO) with curriculum-based data scheduling and difficulty-aware filtering. This approach stabilizes optimization under sparse, noisy rewards and enables progressive adaptation to complex samples. Evaluations on autonomous driving benchmarks demonstrate substantial improvements in detection accuracy and robustness. Ablation studies confirm the importance of reward design, KL regularization, and curriculum pacing for convergence stability and generalization. Our findings highlight reinforcement-driven optimization with structured data curricula as a scalable path toward robust and interpretable multimodal detection.",
        "arxiv_id": "2509.22688",
        "ARXIVID": "2509.22688",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multimodal large language models) and applies reinforcement learning for robust object detection in autonomous driving, with curriculum learning and structured data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24786": {
        "authors": [
            "Shenghao Fu",
            "Qize Yang",
            "Yuan-Ming Li",
            "Xihan Wei",
            "Xiaohua Xie",
            "Wei-Shi Zheng"
        ],
        "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning",
        "abstract": "arXiv:2509.24786v1 Announce Type: new  Abstract: Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks.",
        "arxiv_id": "2509.24786",
        "ARXIVID": "2509.24786",
        "COMMENT": "LOVE-R1 proposes an adaptive zoom-in mechanism for long video understanding in LVLMs, with a multi-step reasoning process and a new training approach. This is a methodological improvement for spatial intelligence in embodied agents (criterion 1) and a new method for VLLMs (criterion 2).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25143": {
        "authors": [
            "Junyi Zhang",
            "Jia-Chen Gu",
            "Wenbo Hu",
            "Yu Zhou",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models",
        "abstract": "arXiv:2509.25143v1 Announce Type: new  Abstract: Existing medical reasoning benchmarks for vision-language models primarily focus on analyzing a patient's condition based on an image from a single visit. However, this setting deviates significantly from real-world clinical practice, where doctors typically refer to a patient's historical conditions to provide a comprehensive assessment by tracking their changes over time. In this paper, we introduce TemMed-Bench, the first benchmark designed for analyzing changes in patients' conditions between different clinical visits, which challenges large vision-language models (LVLMs) to reason over temporal medical images. TemMed-Bench consists of a test set comprising three tasks - visual question-answering (VQA), report generation, and image-pair selection - and a supplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we conduct an evaluation of six proprietary and six open-source LVLMs. Our results show that most LVLMs lack the ability to analyze patients' condition changes over temporal medical images, and a large proportion perform only at a random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they have yet to reach the desired level. Furthermore, we explore augmenting the input with both retrieved visual and textual modalities in the medical domain. We also show that multi-modal retrieval augmentation yields notably higher performance gains than no retrieval and textual retrieval alone across most models on our benchmark, with the VQA task showing an average improvement of 2.59%. Overall, we compose a benchmark grounded on real-world clinical practice, and it reveals LVLMs' limitations in temporal medical image reasoning, as well as highlighting the use of multi-modal retrieval augmentation as a potentially promising direction worth exploring to address this challenge.",
        "arxiv_id": "2509.25143",
        "ARXIVID": "2509.25143",
        "COMMENT": "TemMed-Bench introduces a new benchmark for evaluating temporal medical image reasoning in vision-language models, revealing limitations and proposing multi-modal retrieval augmentation. This is a new benchmark for VLLMs (criteria 2 and 3), with a focus on temporal reasoning in medical images.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23635": {
        "authors": [
            "Ruibing Hou",
            "Mingshuang Luo",
            "Hongyu Pan",
            "Hong Chang",
            "Shiguang Shan"
        ],
        "title": "MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing",
        "abstract": "arXiv:2509.23635v1 Announce Type: new  Abstract: This paper proposes MotionVerse, a unified framework that harnesses the capabilities of Large Language Models (LLMs) to comprehend, generate, and edit human motion in both single-person and multi-person scenarios. To efficiently represent motion data, we employ a motion tokenizer with residual quantization, which converts continuous motion sequences into multi-stream discrete tokens. Furthermore, we introduce a \\textit{Delay Parallel} Modeling strategy, which temporally staggers the encoding of residual token streams. This design enables LLMs to effectively capture inter-stream dependencies while maintaining computational efficiency comparable to single-stream modeling. Moreover, to alleviate modality interference between motion and language, we design a \\textit{dual-tower architecture} with modality-specific parameters, ensuring stable integration of motion information for both comprehension and generation tasks. Comprehensive ablation studies demonstrate the effectiveness of each component in MotionVerse, and extensive experiments showcase its superior performance across a wide range of motion-relevant tasks.",
        "arxiv_id": "2509.23635",
        "ARXIVID": "2509.23635",
        "COMMENT": "MotionVerse is a unified multimodal framework for motion comprehension, generation, and editing, leveraging LLMs and introducing a motion tokenizer and dual-tower architecture. This is a new MLLM (criterion 2) and also a methodological improvement for spatial intelligence in embodied agents (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24020": {
        "authors": [
            "Jian Chen",
            "Zhuoran Zheng",
            "Han Hu",
            "Guijuan Zhang",
            "Dianjie Lu",
            "Liang Li",
            "Chen Lyu"
        ],
        "title": "Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba",
        "abstract": "arXiv:2509.24020v1 Announce Type: new  Abstract: To address the issues of physical information degradation and ineffective pedestrian interaction modeling in pedestrian trajectory prediction under hazy weather conditions, we propose a deep learning model that combines physical priors of atmospheric scattering with topological modeling of pedestrian relationships. Specifically, we first construct a differentiable atmospheric scattering model that decouples haze concentration from light degradation through a network with physical parameter estimation, enabling the learning of haze-mitigated feature representations. Second, we design an adaptive scanning state space model for feature extraction. Our adaptive Mamba variant achieves a 78% inference speed increase over native Mamba while preserving long-range dependency modeling.   Finally, to efficiently model pedestrian relationships, we develop a heterogeneous graph attention network, using graph matrices to model multi-granularity interactions between pedestrians and groups, combined with a spatio-temporal fusion module to capture the collaborative evolution patterns of pedestrian movements. Furthermore, we constructed a new pedestrian trajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of the proposed method. Experiments show that our method reduces the minADE / minFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in dense haze scenarios (visibility < 30m), providing a new modeling paradigm for reliable perception in intelligent transportation systems in adverse environments.",
        "arxiv_id": "2509.24020",
        "ARXIVID": "2509.24020",
        "COMMENT": "Matches criterion 3 (embodied AI, new method for pedestrian trajectory prediction in adverse weather, introduces a new dataset and combines physical priors with graph-Mamba modeling). Novel angle: haze-aware modeling and adaptive Mamba variant.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23376": {
        "authors": [
            "Jinghong Zheng",
            "Changlong Jiang",
            "Jiaqi Li",
            "Haohong Kuang",
            "Hang Xu",
            "Tingbing Yan"
        ],
        "title": "UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation",
        "abstract": "arXiv:2509.23376v1 Announce Type: new  Abstract: In this paper, we present UniPose, a unified cross-modality pose prior propagation method for weakly supervised 3D human pose estimation (HPE) using unannotated single-view RGB-D sequences (RGB, depth, and point cloud data). UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D sequences, eliminating the need for labor-intensive 3D keypoint annotations. This approach bridges the gap between 2D and 3D domains without suffering from issues related to multi-view camera calibration or synthetic-to-real data shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as weak supervision for point cloud networks, incorporating spatial-temporal constraints like body symmetry and joint motion. The 2D-to-3D back-projection loss and cross-modality interaction further enhance this process. By treating the point cloud network's 3D HPE results as pseudo ground truth, our anchor-to-joint prediction method performs 3D lifting on RGB and depth networks, making it more robust against inaccuracies in 2D HPE results compared to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show that UniPose achieves comparable performance to fully supervised methods. Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its performance under challenging conditions, demonstrating its potential for practical applications. Our proposed 3D lifting method also achieves state-of-the-art results.",
        "arxiv_id": "2509.23376",
        "ARXIVID": "2509.23376",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a cross-modality method for weakly supervised 3D human pose estimation from RGB-D data, with spatial-temporal constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24250": {
        "authors": [
            "Edward Kim",
            "Daniel He",
            "Jorge Chao",
            "Wiktor Rajca",
            "Mohammed Amin",
            "Nishant Malpani",
            "Ruta Desai",
            "Antti Oulasvirta",
            "Bjoern Hartmann",
            "Sanjit Seshia"
        ],
        "title": "Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations",
        "abstract": "arXiv:2509.24250v1 Announce Type: new  Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.",
        "arxiv_id": "2509.24250",
        "ARXIVID": "2509.24250",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for collaborative physical activity modeling from narrated demonstrations). Focuses on program synthesis for collaborative embodied tasks, a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24311": {
        "authors": [
            "Runmin Jiang",
            "Wanyue Feng",
            "Yuntian Yang",
            "Shriya Pingulkar",
            "Hong Wang",
            "Xi Xiao",
            "Xiaoyu Cao",
            "Genpei Zhang",
            "Xiao Wang",
            "Xiaolong Wu",
            "Tianyang Wang",
            "Yang Liu",
            "Xingjian Li",
            "Min Xu"
        ],
        "title": "Towards Foundation Models for Cryo-ET Subtomogram Analysis",
        "abstract": "arXiv:2509.24311v1 Announce Type: new  Abstract: Cryo-electron tomography (cryo-ET) enables in situ visualization of macromolecular structures, where subtomogram analysis tasks such as classification, alignment, and averaging are critical for structural determination. However, effective analysis is hindered by scarce annotations, severe noise, and poor generalization. To address these challenges, we take the first step towards foundation models for cryo-ET subtomograms. First, we introduce CryoEngine, a large-scale synthetic data generator that produces over 904k subtomograms from 452 particle classes for pretraining. Second, we design an Adaptive Phase Tokenization-enhanced Vision Transformer (APT-ViT), which incorporates adaptive phase tokenization as an equivariance-enhancing module that improves robustness to both geometric and semantic variations. Third, we introduce a Noise-Resilient Contrastive Learning (NRCL) strategy to stabilize representation learning under severe noise conditions. Evaluations across 24 synthetic and real datasets demonstrate state-of-the-art (SOTA) performance on all three major subtomogram tasks and strong generalization to unseen datasets, advancing scalable and robust subtomogram analysis in cryo-ET.",
        "arxiv_id": "2509.24311",
        "ARXIVID": "2509.24311",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it introduces a foundation model for cryo-ET subtomogram analysis, with a new synthetic data generator and robust transformer architecture.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2509.25140": {
        "authors": [
            "Siru Ouyang",
            "Jun Yan",
            "I-Hung Hsu",
            "Yanfei Chen",
            "Ke Jiang",
            "Zifeng Wang",
            "Rujun Han",
            "Long T. Le",
            "Samira Daruki",
            "Xiangru Tang",
            "Vishy Tirumalashetty",
            "George Lee",
            "Mahsan Rofouei",
            "Hangfei Lin",
            "Jiawei Han",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
        "abstract": "arXiv:2509.25140v1 Announce Type: new  Abstract: With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.",
        "arxiv_id": "2509.25140",
        "ARXIVID": "2509.25140",
        "COMMENT": "Matches criterion 1 (methodological improvements to agent memory and self-evolving reasoning) and is relevant to embodied agents' learning from experience. Also touches on memory-driven scaling, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24943": {
        "authors": [
            "Jiahua Li",
            "Kun Wei",
            "Zhe Xu",
            "Zibo Su",
            "Xu Yang",
            "Cheng Deng"
        ],
        "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents",
        "abstract": "arXiv:2509.24943v1 Announce Type: new  Abstract: Long videos, characterized by temporal complexity and sparse task-relevant information, pose significant reasoning challenges for AI systems. Although various Large Language Model (LLM)-based approaches have advanced long video understanding, they still struggle to achieve both completeness and efficiency in capturing task-critical information. Inspired by human progressive visual cognition, we propose CogniGPT, a framework that leverages an interactive loop between Multi-Granular Perception Agent (MGPA) and Verification-Enhanced Reflection Agent (VERA) for efficient and reliable long video understanding. Specifically, MGPA mimics human visual divergent and focused attention to capture task-related information, while VERA verifies perceived key clues to mitigate hallucination and optimize subsequent perception strategies. Through this interactive process, CogniGPT explores a minimal set of informative and reliable task-related clues. Extensive experiments on EgoSchema, Video-MME, NExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both accuracy and efficiency. Notably, on EgoSchema, it surpasses existing training-free methods using only 11.2 frames and achieves performance comparable to Gemini 1.5-Pro.",
        "arxiv_id": "2509.24943",
        "ARXIVID": "2509.24943",
        "COMMENT": "This paper introduces CogniGPT, a framework for long video understanding using interactive agents and multi-granular clue exploration. It is relevant to vision foundation models and multi-modal reasoning (criterion 4), and also involves agent-based approaches (criterion 3).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23316": {
        "authors": [
            "Siheng Wang",
            "Zhengdao Li",
            "Yanshu Li",
            "Canran Xiao",
            "Haibo Zhan",
            "Zhengtao Yao",
            "Xuzhi Zhang",
            "Jiale Kang",
            "Linshan Li",
            "Weiming Liu",
            "Zhikang Dong",
            "Jifeng Shen",
            "Junhao Dong",
            "Qiang Sun",
            "Piotr Koniusz"
        ],
        "title": "C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection",
        "abstract": "arXiv:2509.23316v1 Announce Type: new  Abstract: Object detection has advanced significantly in the closed-set setting, but real-world deployment remains limited by two challenges: poor generalization to unseen categories and insufficient robustness under adverse conditions. Prior research has explored these issues separately: visible-infrared detection improves robustness but lacks generalization, while open-world detection leverages vision-language alignment strategy for category diversity but struggles under extreme environments. This trade-off leaves robustness and diversity difficult to achieve simultaneously. To mitigate these issues, we propose \\textbf{C3-OWD}, a curriculum cross-modal contrastive learning framework that unifies both strengths. Stage~1 enhances robustness by pretraining with RGBT data, while Stage~2 improves generalization via vision-language alignment. To prevent catastrophic forgetting between two stages, we introduce an Exponential Moving Average (EMA) mechanism that theoretically guarantees preservation of pre-stage performance with bounded parameter lag and function consistency. Experiments on FLIR, OV-COCO, and OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$ AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\\text{Novel}}$ on OV-COCO, and $35.7$ mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness and diversity evaluations. Code available at: https://github.com/justin-herry/C3-OWD.git.",
        "arxiv_id": "2509.23316",
        "ARXIVID": "2509.23316",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a cross-modal contrastive learning framework for open-world detection, combining vision-language alignment and multi-modal robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24803": {
        "authors": [
            "Tong Guan",
            "Zijie Meng",
            "Dianqi Li",
            "Shiyu Wang",
            "Chao-Han Huck Yang",
            "Qingsong Wen",
            "Zuozhu Liu",
            "Sabato Marco Siniscalchi",
            "Ming Jin",
            "Shirui Pan"
        ],
        "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models",
        "abstract": "arXiv:2509.24803v1 Announce Type: new  Abstract: Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.",
        "arxiv_id": "2509.24803",
        "ARXIVID": "2509.24803",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmarks): Introduces a new benchmark and model for time series reasoning, including perception, causality, and decision-making, with a multi-modal focus. Novel angle on time series reasoning in multi-modal settings.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2509.24445": {
        "authors": [
            "Jianxin Liang",
            "Tan Yue",
            "Yuxuan Wang",
            "Yueqian Wang",
            "Zhihan Yin",
            "Huishuai Zhang",
            "Dongyan Zhao"
        ],
        "title": "Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA",
        "abstract": "arXiv:2509.24445v1 Announce Type: new  Abstract: The performance of Video Question Answering (VideoQA) models is fundamentally constrained by the nature of their supervision, which typically consists of isolated, factual question-answer pairs. This \"bag-of-facts\" approach fails to capture the underlying narrative and causal structure of events, limiting models to a shallow understanding of video content. To move beyond this paradigm, we introduce a framework to synthesize richer supervisory signals. We propose two complementary strategies: Question-Based Paraphrasing (QBP), which synthesizes the diverse inquiries (what, how, why) from a video's existing set of question-answer pairs into a holistic narrative paragraph that reconstructs the video's event structure; and Question-Based Captioning (QBC), which generates fine-grained visual rationales, grounding the answer to each question in specific, relevant evidence. Leveraging powerful generative models, we use this synthetic data to train VideoQA models under a unified next-token prediction objective. Extensive experiments on STAR and NExT-QA validate our approach, demonstrating significant accuracy gains and establishing new state-of-the-art results, such as improving a 3B model to 72.5\\% on STAR (+4.9\\%) and a 7B model to 80.8\\% on NExT-QA. Beyond accuracy, our analysis reveals that both QBP and QBC substantially enhance cross-dataset generalization, with QBP additionally accelerating model convergence by over 2.5x. These results demonstrate that shifting data synthesis from isolated facts to narrative coherence and grounded rationales yields a more accurate, efficient, and generalizable training paradigm.",
        "arxiv_id": "2509.24445",
        "ARXIVID": "2509.24445",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes a new framework for VideoQA using generative models to synthesize narrative and grounded supervision, improving vision-language model training and generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23626": {
        "authors": [
            "Beomseok Kang",
            "Niluthpol Chowdhury Mithun",
            "Mikhail Sizintsev",
            "Han-Pang Chiu",
            "Supun Samarasekera"
        ],
        "title": "Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models",
        "abstract": "arXiv:2509.23626v1 Announce Type: new  Abstract: Multi-task dense prediction, which aims to jointly solve tasks like semantic segmentation and depth estimation, is crucial for robotics applications but suffers from domain shift when deploying models in new environments. While unsupervised domain adaptation (UDA) addresses this challenge for single tasks, existing multi-task UDA methods primarily rely on adversarial learning approaches that are less effective than recent self-training techniques. In this paper, we introduce FAMDA, a simple yet effective UDA framework that bridges this gap by leveraging Vision Foundation Models (VFMs) as powerful teachers. Our approach integrates Segmentation and Depth foundation models into a self-training paradigm to generate high-quality pseudo-labels for the target domain, effectively distilling their robust generalization capabilities into a single, efficient student network. Extensive experiments show that FAMDA achieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA multi-task learning (MTL) benchmarks and a challenging new day-to-night adaptation task. Our framework enables the training of highly efficient models; a lightweight variant achieves SOTA accuracy while being more than 10$\\times$ smaller than foundation models, highlighting FAMDA's suitability for creating domain-adaptive and efficient models for resource-constrained robotics applications.",
        "arxiv_id": "2509.23626",
        "ARXIVID": "2509.23626",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 3 (new methods for domain-adaptive multi-task dense prediction in robotics, leveraging vision foundation models as teachers).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23605": {
        "authors": [
            "Zeren Xiong",
            "Yue Yu",
            "Zedong Zhang",
            "Shuo Chen",
            "Jian Yang",
            "Jun Li"
        ],
        "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis",
        "abstract": "arXiv:2509.23605v1 Announce Type: new  Abstract: Creating novel images by fusing visual cues from multiple sources is a fundamental yet underexplored problem in image-to-image generation, with broad applications in artistic creation, virtual reality and visual media. Existing methods often face two key challenges: coexistent generation, where multiple objects are simply juxtaposed without true integration, and bias generation, where one object dominates the output due to semantic imbalance. To address these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet effective diffusion-based framework that synthesizes a single, coherent object by integrating two input images at both noise and latent levels. Our approach comprises: (1) a hybrid sampling process that combines guided denoising, inversion, and spherical interpolation with adjustable parameters to achieve structure-aware fusion, mitigating coexistent generation; and (2) an efficient adaptive adjustment module, which introduces a novel similarity-based score to automatically and adaptively search for optimal parameters, countering semantic bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that our method outperforms strong baselines in visual quality, semantic consistency, and human-rated creativity.",
        "arxiv_id": "2509.23605",
        "ARXIVID": "2509.23605",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new diffusion-based framework for cross-object synthesis, addressing key challenges in image-to-image generation with a novel hybrid sampling process and adaptive adjustment module.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23700": {
        "authors": [
            "Yunjiang Xu",
            "Lingzhi Li",
            "Jin Wang",
            "Yupeng Ouyang",
            "Benyuan Yang"
        ],
        "title": "INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception",
        "abstract": "arXiv:2509.23700v1 Announce Type: new  Abstract: Collaborative perception systems overcome single-vehicle limitations in long-range detection and occlusion scenarios by integrating multi-agent sensory data, improving accuracy and safety. However, frequent cooperative interactions and real-time requirements impose stringent bandwidth constraints. Previous works proves that query-based instance-level interaction reduces bandwidth demands and manual priors, however, LiDAR-focused implementations in collaborative perception remain underdeveloped, with performance still trailing state-of-the-art approaches. To bridge this gap, we propose INSTINCT (INSTance-level INteraCtion ArchiTecture), a novel collaborative perception framework featuring three core components: 1) a quality-aware filtering mechanism for high-quality instance feature selection; 2) a dual-branch detection routing scheme to decouple collaboration-irrelevant and collaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion module to aggregate local hybrid instance features. Additionally, we enhance the ground truth (GT) sampling technique to facilitate training with diverse hybrid instance features. Extensive experiments across multiple datasets demonstrate that INSTINCT achieves superior performance. Specifically, our method achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and V2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared to state-of-the-art methods. The code is available at https://github.com/CrazyShout/INSTINCT.",
        "arxiv_id": "2509.23700",
        "ARXIVID": "2509.23700",
        "COMMENT": "Matches criterion 3 (Embodied AI, new methods for collaborative perception with a novel architecture and bandwidth-efficient design). Proposes a new instance-level interaction architecture for multi-agent perception, with strong empirical improvements and a new GT sampling technique.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24526": {
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models",
        "abstract": "arXiv:2509.24526v1 Announce Type: new  Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models.",
        "arxiv_id": "2509.24526",
        "ARXIVID": "2509.24526",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications): Introduces a new mid-training method for flow map models in vision generation, improving efficiency and stability in diffusion models. The method is general and could impact generative modeling in computer vision.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2509.23457": {
        "authors": [
            "Mohammad Hossein Sameti",
            "Amir M. Mansourian",
            "Arash Marioriyad",
            "Soheil Fadaee Oshyani",
            "Mohammad Hossein Rohban",
            "Mahdieh Soleymani Baghshah"
        ],
        "title": "No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation",
        "abstract": "arXiv:2509.23457v1 Announce Type: new  Abstract: Despite recent advances in text-to-image (T2I) models, they often fail to faithfully render all elements of complex prompts, frequently omitting or misrepresenting specific objects and attributes. Test-time optimization has emerged as a promising approach to address this limitation by refining generation without the need for retraining. In this paper, we propose a fine-grained test-time optimization framework that enhances compositional faithfulness in T2I generation. Unlike most of prior approaches that rely solely on a global image/text similarity score, our method decomposes the input prompt into semantic concepts and evaluates alignment at both the global and concept levels. A fine-grained variant of CLIP is used to compute concept-level correspondence, producing detailed feedback on missing or inaccurate concepts. This feedback is fed into an iterative prompt refinement loop, enabling the large language model to propose improved prompts. Experiments on DrawBench and CompBench prompts demonstrate that our method significantly improves concept coverage and human-judged faithfulness over both standard test-time optimization and the base T2I model. Code is available at: https://github.com/AmirMansurian/NoConceptLeftBehind",
        "arxiv_id": "2509.23457",
        "ARXIVID": "2509.23457",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications) and partially criterion 2 (VLLMs/MLLMs): Proposes a fine-grained test-time optimization for compositional text-to-image generation using a variant of CLIP and LLMs for prompt refinement. Focuses on compositional faithfulness in T2I models, which is a hot topic in vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23926": {
        "authors": [
            "Alexandros Doumanoglou",
            "Kurt Driessens",
            "Dimitrios Zarpalas"
        ],
        "title": "Learning Encoding-Decoding Direction Pairs to Unveil Concepts of Influence in Deep Vision Networks",
        "abstract": "arXiv:2509.23926v1 Announce Type: new  Abstract: Empirical evidence shows that deep vision networks represent concepts as directions in latent space, vectors we call concept embeddings. Each concept has a latent factor-a scalar-indicating its presence in an input patch. For a given patch, multiple latent factors are encoded into a compact representation by linearly combining concept embeddings, with the factors as coefficients. Since these embeddings enable such encoding, we call them encoding directions. A latent factor can be recovered via the inner product with a filter, a vector we call a decoding direction. These encoding-decoding direction pairs are not directly accessible, but recovering them helps open the black box of deep networks, enabling understanding, debugging, and improving models. Decoder directions attribute meaning to latent codes, while encoding directions assess concept influence on predictions, with both enabling model correction by unlearning irrelevant concepts. Unlike prior matrix decomposition, autoencoder, or dictionary learning methods that rely on feature reconstruction, we propose a new perspective: decoding directions are identified via directional clustering of activations, and encoding directions are estimated with signal vectors under a probabilistic view. We further leverage network weights through a novel technique, Uncertainty Region Alignment, which reveals interpretable directions affecting predictions. Our analysis shows that (a) on synthetic data, our method recovers ground-truth direction pairs; (b) on real data, decoding directions map to monosemantic, interpretable concepts and outperform unsupervised baselines; and (c) signal vectors faithfully estimate encoding directions, validated via activation maximization. Finally, we demonstrate applications in understanding global model behavior, explaining individual predictions, and intervening to produce counterfactuals or correct errors.",
        "arxiv_id": "2509.23926",
        "ARXIVID": "2509.23926",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method for interpreting and intervening in deep vision networks by learning encoding-decoding direction pairs, with applications to model understanding and correction.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.24817": {
        "authors": [
            "Zeyu Cai",
            "Ziyang Li",
            "Xiaoben Li",
            "Boqian Li",
            "Zeyu Wang",
            "Zhenyu Zhang",
            "Yuliang Xiu"
        ],
        "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections",
        "abstract": "arXiv:2509.24817v1 Announce Type: new  Abstract: We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body images with minimal occlusions, or well-calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce a data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in a single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is a pose-correlated feature aggregation module (PCFA), that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. We also introduce a perceiver-based multi-reference shape predictor, removing the need for pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task. Project Page: https://zcai0612.github.io/UP2You",
        "arxiv_id": "2509.24817",
        "ARXIVID": "2509.24817",
        "COMMENT": "This paper presents a new method for fast, tuning-free 3D human reconstruction from unconstrained photo collections, introducing a pose-correlated feature aggregation module and a perceiver-based multi-reference shape predictor. This is a methodological improvement in spatial understanding for embodied agents (criterion 1), and also relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23194": {
        "authors": [
            "Yifan Zhang",
            "Wei Zhang",
            "Chuangxin He",
            "Zhonghua Miao",
            "Junhui Hou"
        ],
        "title": "Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss",
        "abstract": "arXiv:2509.23194v1 Announce Type: new  Abstract: Unsupervised online 3D instance segmentation is a fundamental yet challenging task, as it requires maintaining consistent object identities across LiDAR scans without relying on annotated training data. Existing methods, such as UNIT, have made progress in this direction but remain constrained by limited training diversity, rigid temporal sampling, and heavy dependence on noisy pseudo-labels. We propose a new framework that enriches the training distribution through synthetic point cloud sequence generation, enabling greater diversity without relying on manual labels or simulation engines. To better capture temporal dynamics, our method incorporates a flexible sampling strategy that leverages both adjacent and non-adjacent frames, allowing the model to learn from long-range dependencies as well as short-term variations. In addition, a dynamic-weighting loss emphasizes confident and informative samples, guiding the network toward more robust representations. Through extensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method consistently outperforms UNIT and other unsupervised baselines, achieving higher segmentation accuracy and more robust temporal associations. The code will be publicly available at github.com/Eaphan/SFT3D.",
        "arxiv_id": "2509.23194",
        "ARXIVID": "2509.23194",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for 3D instance segmentation with synthetic sequences and dynamic loss, unsupervised, no simulation engine). Proposes a new framework for unsupervised online 3D instance segmentation with synthetic data generation and dynamic loss.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.24335": {
        "authors": [
            "Guolin Ke",
            "Hui Xue"
        ],
        "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
        "abstract": "arXiv:2509.24335v1 Announce Type: new  Abstract: Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.",
        "arxiv_id": "2509.24335",
        "ARXIVID": "2509.24335",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in vision. Proposes a new hyperspherical latent space for autoregressive image generation, with strong empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.25182": {
        "authors": [
            "Junyu Chen",
            "Wenkun He",
            "Yuchao Gu",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Junsong Chen",
            "Dongyun Zou",
            "Yujun Lin",
            "Zhekai Zhang",
            "Muyang Li",
            "Haocheng Xi",
            "Ligeng Zhu",
            "Enze Xie",
            "Song Han",
            "Han Cai"
        ],
        "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder",
        "abstract": "arXiv:2509.25182v1 Announce Type: new  Abstract: We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
        "arxiv_id": "2509.25182",
        "ARXIVID": "2509.25182",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a general post-training acceleration framework for video diffusion models, with strong empirical results and a new autoencoder design.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23555": {
        "authors": [
            "Javed Ahmad",
            "Penggang Gao",
            "Donatien Delehelle",
            "Mennuti Canio",
            "Nikhil Deshpande",
            "Jes\\'us Ortiz",
            "Darwin G. Caldwell",
            "Yonas Teodros Tefera"
        ],
        "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations",
        "abstract": "arXiv:2509.23555v1 Announce Type: new  Abstract: Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.",
        "arxiv_id": "2509.23555",
        "ARXIVID": "2509.23555",
        "COMMENT": "Matches criterion 4: comprehensive survey of neural scene representations (NeRF, 3DGS) and their applications in vision foundation models for 3D content creation, SLAM, and robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.25079": {
        "authors": [
            "Guanjun Wu",
            "Jiemin Fang",
            "Chen Yang",
            "Sikuang Li",
            "Taoran Yi",
            "Jia Lu",
            "Zanwei Zhou",
            "Jiazhong Cen",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Xinggang Wang",
            "Qi Tian"
        ],
        "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
        "abstract": "arXiv:2509.25079v1 Announce Type: new  Abstract: High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \\& code are available at https://unilat3d.github.io/",
        "arxiv_id": "2509.25079",
        "ARXIVID": "2509.25079",
        "COMMENT": "This paper introduces UniLat3D, a unified VAE-based latent space for single-stage 3D asset generation, which is a vision foundation model for 3D generation and could be considered under criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23723": {
        "authors": [
            "Zijun Li",
            "Hongyu Yan",
            "Shijie Li",
            "Kunming Luo",
            "Li Lu",
            "Xulei Yang",
            "Weisi Lin"
        ],
        "title": "DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion",
        "abstract": "arXiv:2509.23723v1 Announce Type: new  Abstract: Latent diffusion models (LDMs) have demonstrated remarkable generative capabilities across various low-level vision tasks. However, their potential for point cloud completion remains underexplored due to the unstructured and irregular nature of point clouds. In this work, we propose DiffPCN, a novel diffusion-based coarse-to-fine framework for point cloud completion. Our approach comprises two stages: an initial stage for generating coarse point clouds, and a refinement stage that improves their quality through point denoising and upsampling. Specifically, we first project the unordered and irregular partial point cloud into structured depth images, which serve as conditions for a well-designed DepthLDM to synthesize completed multi-view depth images that are used to form coarse point clouds. In this way, our DiffPCN can yield high-quality and high-completeness coarse point clouds by leveraging LDM' s powerful generation and comprehension capabilities. Then, since LDMs inevitably introduce outliers into the generated depth maps, we design a Point Denoising Network to remove artifacts from the coarse point cloud by predicting a per-point distance score. Finally, we devise an Association-Aware Point Upsampler, which guides the upsampling process by leveraging local association features between the input point cloud and the corresponding coarse points, further yielding a dense and high-fidelity output. Experimental results demonstrate that our DiffPCN achieves state-of-the-art performance in geometric accuracy and shape completeness, significantly improving the robustness and consistency of point cloud completion.",
        "arxiv_id": "2509.23723",
        "ARXIVID": "2509.23723",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a latent diffusion model for point cloud completion using multi-view depth images, leveraging vision foundation model techniques for 3D data.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.24935": {
        "authors": [
            "Sangeek Hyun",
            "MinKyu Lee",
            "Jae-Pil Heo"
        ],
        "title": "Scalable GANs with Transformers",
        "abstract": "arXiv:2509.24935v1 Announce Type: new  Abstract: Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
        "arxiv_id": "2509.24935",
        "ARXIVID": "2509.24935",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications, generative modeling in multi-modal learning). Proposes scalable GANs with transformers, with new insights into scaling and optimization for adversarial learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23304": {
        "authors": [
            "Liwen Hu",
            "Yang Li",
            "Mianzhi Liu",
            "Yijia Guo",
            "Shenghao Xie",
            "Ziluo Ding",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "title": "Seeing the Unseen in Low-light Spike Streams",
        "abstract": "arXiv:2509.23304v1 Announce Type: new  Abstract: Spike camera, a type of neuromorphic sensor with high-temporal resolution, shows great promise for high-speed visual tasks. Unlike traditional cameras, spike camera continuously accumulates photons and fires asynchronous spike streams. Due to unique data modality, spike streams require reconstruction methods to become perceptible to the human eye.   However, lots of methods struggle to handle spike streams in low-light high-speed scenarios due to severe noise and sparse information. In this work, we propose Diff-SPK, the first diffusion-based reconstruction method for spike camera. Diff-SPK effectively leverages generative priors to supplement texture information in low-light conditions. Specifically, it first employs an \\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval (ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI serves as a conditioning input for ControlNet to generate the high-speed scenes. To improve the quality of results, we introduce an ETFI-based feature fusion module during the generation process.   Moreover, we establish the first bona fide benchmark for the low-light spike stream reconstruction task. It significantly surpasses existing reconstruction datasets in scale and provides quantitative illumination information. The performance on real low-light spike streams demonstrates the superiority of Diff-SPK.",
        "arxiv_id": "2509.23304",
        "ARXIVID": "2509.23304",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications) as it introduces a diffusion-based method for spike camera data, and establishes a new benchmark for low-light spike stream reconstruction (criterion 3, new benchmarks in vision/embodied AI).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23677": {
        "authors": [
            "Dayu Tan",
            "Ziwei Zhang",
            "Yansan Su",
            "Xin Peng",
            "Yike Dai",
            "Chunhou Zheng",
            "Weimin Zhong"
        ],
        "title": "MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain Segmentation via Multi-scale Self-Distilled Fusion Strategy",
        "abstract": "arXiv:2509.23677v1 Announce Type: new  Abstract: Numerous CNN-Transformer hybrid models rely on high-complexity global attention mechanisms to capture long-range dependencies, which introduces non-linear computational complexity and leads to significant resource consumption. Although knowledge distillation and sparse attention mechanisms can improve efficiency, they often fall short of delivering the high segmentation accuracy necessary for complex tasks. Balancing model performance with computational efficiency remains a critical challenge. In this work, we propose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba, which integrates bidirectional spatial perception with multi-scale self-distillation. The bidirectional spatial aware branch effectively captures long-range spatial context dependencies across brain regions, while also incorporating a powerful nonlinear feature extraction mechanism that further enhances the model's ability to learn complex and heterogeneous patterns. In addition, the proposed multi-scale self-distilled fusion strategy strengthens hierarchical feature representations and improves the transfer of semantic information at different resolution levels. By jointly leveraging the bidirectional spatial perception branch and the multi-scale self-distilled fusion strategy, our framework effectively mitigates the bottleneck of quadratic computational complexity in volumetric segmentation, while simultaneously addressing the limitation of insufficient global perception. Extensive experiments on multiple standard benchmark datasets demonstrate that MSD-KMamba consistently outperforms state-of-the-art methods in segmentation accuracy, robustness, and generalization, while maintaining high computational efficiency and favorable scalability. The source code of MSD-KMamba is publicly available at https://github.com/daimao-zhang/MSD-KMamba.",
        "arxiv_id": "2509.23677",
        "ARXIVID": "2509.23677",
        "COMMENT": "Matches criterion 1 (New methodological improvements to spatial understanding on embodied agents): Proposes a new 3D multi-modal brain segmentation framework with bidirectional spatial perception and multi-scale self-distillation. Focuses on spatial context dependencies and efficient segmentation, which is a methodological improvement in spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23639": {
        "authors": [
            "Boyu Han",
            "Qianqian Xu",
            "Shilong Bao",
            "Zhiyong Yang",
            "Kangli Zi",
            "Qingming Huang"
        ],
        "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
        "abstract": "arXiv:2509.23639v1 Announce Type: new  Abstract: This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.",
        "arxiv_id": "2509.23639",
        "ARXIVID": "2509.23639",
        "COMMENT": "Matches criterion 4 (Vision foundation models and applications): Proposes a lightweight debiasing method for text-to-image diffusion models by fine-tuning text encoders, with a focus on fairness and efficiency. Uses CLIP and Stable Diffusion, both vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.24350": {
        "authors": [
            "Yan Ke",
            "Xin Yu",
            "Heming Du",
            "Scott Chapman",
            "Helen Huang"
        ],
        "title": "Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA",
        "abstract": "arXiv:2509.24350v1 Announce Type: new  Abstract: Agricultural visual question answering is essential for providing farmers and researchers with accurate and timely knowledge. However, many existing approaches are predominantly developed for evidence-constrained settings such as text-only queries or single-image cases. This design prevents them from coping with real-world agricultural scenarios that often require multi-image inputs with complementary views across spatial scales, and growth stages. Moreover, limited access to up-to-date external agricultural context makes these systems struggle to adapt when evidence is incomplete. In addition, rigid pipelines often lack systematic quality control. To address this gap, we propose a self-reflective and self-improving multi-agent framework that integrates four roles, the Retriever, the Reflector, the Answerer, and the Improver. They collaborate to enable context enrichment, reflective reasoning, answer drafting, and iterative improvement.   A Retriever formulates queries and gathers external information, while a Reflector assesses adequacy and triggers sequential reformulation and renewed retrieval. Two Answerers draft candidate responses in parallel to reduce bias. The Improver refines them through iterative checks while ensuring that information from multiple images is effectively aligned and utilized. Experiments on the AgMMU benchmark show that our framework achieves competitive performance on multi-image agricultural QA.",
        "arxiv_id": "2509.24350",
        "ARXIVID": "2509.24350",
        "COMMENT": "Matches criterion 2 (multi-modal VQA with multi-image input) and criterion 3 (novel multi-agent system for real-world multi-image agricultural VQA, addressing evidence incompleteness and quality control).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23849": {
        "authors": [
            "Michihiro Kuroki",
            "Toshihiko Yamasaki"
        ],
        "title": "CE-FAM: Concept-Based Explanation via Fusion of Activation Maps",
        "abstract": "arXiv:2509.23849v1 Announce Type: new  Abstract: Although saliency maps can highlight important regions to explain the reasoning behind image classification in artificial intelligence (AI), the meaning of these regions is left to the user's interpretation. In contrast, conceptbased explanations decompose AI predictions into humanunderstandable concepts, clarifying their contributions. However, few methods can simultaneously reveal what concepts an image classifier learns, which regions are associated with them, and how they contribute to predictions. We propose a novel concept-based explanation method, Concept-based Explanation via Fusion of Activation Maps (CE-FAM). It employs a branched network that shares activation maps with an image classifier and learns to mimic the embeddings of a Vision and Language Model (VLM). The branch network predicts concepts in an image, and their corresponding regions are represented by a weighted sum of activation maps, with weights given by the gradients of the concept prediction scores. Their contributions are quantified based on their impact on the image classification score. Our method provides a general framework for identifying the concept regions and their contributions while leveraging VLM knowledge to handle arbitrary concepts without requiring an annotated dataset. Furthermore, we introduce a novel evaluation metric to assess the accuracy of the concept regions. Our qualitative and quantitative evaluations demonstrate our method outperforms existing approaches and excels in zero-shot inference for unseen concepts.",
        "arxiv_id": "2509.23849",
        "ARXIVID": "2509.23849",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. Proposes a concept-based explanation method leveraging VLMs for zero-shot concept identification and region attribution, with a new evaluation metric.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.24353": {
        "authors": [
            "Yixuan Ren",
            "Hanyu Wang",
            "Hao Chen",
            "Bo He",
            "Abhinav Shrivastava"
        ],
        "title": "NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis",
        "abstract": "arXiv:2509.24353v1 Announce Type: new  Abstract: We present NeRV-Diffusion, an implicit latent video diffusion model that synthesizes videos via generating neural network weights. The generated weights can be rearranged as the parameters of a convolutional neural network, which forms an implicit neural representation (INR), and decodes into videos with frame indices as the input. Our framework consists of two stages: 1) A hypernetworkbased tokenizer that encodes raw videos from pixel space to neural parameter space, where the bottleneck latent serves as INR weights to decode. 2) An implicit diffusion transformer that denoises on the latent INR weights. In contrast to traditional video tokenizers that encode videos into frame-wise feature maps, NeRV-Diffusion compresses and generates a video holistically as a unified neural network. This enables efficient and high-quality video synthesis via obviating temporal cross-frame attentions in the denoiser and decoding video latent with dedicated decoders. To achieve Gaussian-distributed INR weights with high expressiveness, we reuse the bottleneck latent across all NeRV layers, as well as reform its weight assignment, upsampling connection and input coordinates. We also introduce SNR-adaptive loss weighting and scheduled sampling for effective training of the implicit diffusion model. NeRV-Diffusion reaches superior video generation quality over previous INR-based models and comparable performance to most recent state-of-the-art non-implicit models on real-world video benchmarks including UCF-101 and Kinetics-600. It also brings a smooth INR weight space that facilitates seamless interpolations between frames or videos.",
        "arxiv_id": "2509.24353",
        "ARXIVID": "2509.24353",
        "COMMENT": "NeRV-Diffusion introduces an implicit latent video diffusion model that synthesizes videos by generating neural network weights, with a novel tokenizer and diffusion transformer. This is relevant to generative modeling in multi-modal learning and vision foundation models (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24878": {
        "authors": [
            "Jiuhong Xiao",
            "Roshan Nayak",
            "Ning Zhang",
            "Daniel Tortei",
            "Giuseppe Loianno"
        ],
        "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation",
        "abstract": "arXiv:2509.24878v1 Announce Type: new  Abstract: Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day, Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: http://xjh19971.github.io/ThermalGen",
        "arxiv_id": "2509.24878",
        "ARXIVID": "2509.24878",
        "COMMENT": "Proposes ThermalGen, a flow-based generative model for RGB-to-thermal image translation, with a style-disentangled mechanism and large-scale datasets. This is relevant to criterion 4 (vision foundation models and applications), and also of interest for multi-modal generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.25127": {
        "authors": [
            "Mingyuan Zhou",
            "Yi Gu",
            "Huangjie Zheng",
            "Liangchen Song",
            "Guande He",
            "Yizhe Zhang",
            "Wenze Hu",
            "Yinfei Yang"
        ],
        "title": "Score Distillation of Flow Matching Models",
        "abstract": "arXiv:2509.25127v1 Announce Type: new  Abstract: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. We will make the PyTorch implementation publicly available.",
        "arxiv_id": "2509.25127",
        "ARXIVID": "2509.25127",
        "COMMENT": "Unifies score distillation for diffusion and flow matching models, showing it applies broadly to text-to-image flow matching models. This is relevant to criterion 4 (vision foundation models and applications), especially for generative modeling in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24899": {
        "authors": [
            "Mohsen Ghafoorian",
            "Denis Korzhenkov",
            "Amirhossein Habibian"
        ],
        "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer",
        "abstract": "arXiv:2509.24899v1 Announce Type: new  Abstract: Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \\textit{Attention Surgery}, an efficient framework for \\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40\\% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks.",
        "arxiv_id": "2509.24899",
        "ARXIVID": "2509.24899",
        "COMMENT": "This paper proposes 'Attention Surgery', a new method to linearize or hybridize attention in pretrained video diffusion transformers, improving efficiency without retraining from scratch. This is a methodological improvement in generative modeling for video, relevant to criterion 4 (vision foundation models and applications) and also of interest for generative modeling in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24695": {
        "authors": [
            "Junsong Chen",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Ruihang Chu",
            "Junyu Chen",
            "Shuai Yang",
            "Xianbang Wang",
            "Yicheng Pan",
            "Daquan Zhou",
            "Huan Ling",
            "Haozhe Liu",
            "Hongwei Yi",
            "Hao Zhang",
            "Muyang Li",
            "Yukang Chen",
            "Han Cai",
            "Sanja Fidler",
            "Ping Luo",
            "Song Han",
            "Enze Xie"
        ],
        "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
        "abstract": "arXiv:2509.24695v1 Announce Type: new  Abstract: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.",
        "arxiv_id": "2509.24695",
        "ARXIVID": "2509.24695",
        "COMMENT": "Related to criterion 4: introduces SANA-Video, an efficient video diffusion model with novel transformer and memory designs for high-quality, long video generation, relevant to generative modeling in vision.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24230": {
        "authors": [
            "Shaobin Ling",
            "Yun Wang",
            "Chenyou Fan",
            "Tin Lun Lam",
            "Junjie Hu"
        ],
        "title": "ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration",
        "abstract": "arXiv:2509.24230v1 Announce Type: new  Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems.",
        "arxiv_id": "2509.24230",
        "ARXIVID": "2509.24230",
        "COMMENT": "Somewhat related to criterion 1 and 3: proposes a new planning framework (ELHPlan) for efficient long-horizon multi-agent collaboration, relevant to embodied AI and spatial intelligence, with new efficiency metrics and planning primitives.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.23186": {
        "authors": [
            "Qimin Zhong",
            "Hao Liao",
            "Siwei Wang",
            "Mingyang Zhou",
            "Xiaoqun Wu",
            "Rui Mao",
            "Wei Chen"
        ],
        "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction",
        "abstract": "arXiv:2509.23186v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.",
        "arxiv_id": "2509.23186",
        "ARXIVID": "2509.23186",
        "COMMENT": "This paper analyzes and improves the planning capability of language models via multi-token prediction, with experiments on Blocksworld. It is relevant to embodied AI and planning (criterion 3), especially as it provides new insights and methods for structural planning in LLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.24927": {
        "authors": [
            "An Guo",
            "Shuoxiao Zhang",
            "Enyi Tang",
            "Xinyu Gao",
            "Haomin Pang",
            "Haoxiang Tian",
            "Yanzhou Mu",
            "Wu Wen",
            "Chunrong Fang",
            "Zhenyu Chen"
        ],
        "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?",
        "abstract": "arXiv:2509.24927v1 Announce Type: new  Abstract: With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.",
        "arxiv_id": "2509.24927",
        "ARXIVID": "2509.24927",
        "COMMENT": "This paper provides a large-scale empirical study of V2X cooperative perception in autonomous vehicles, analyzing error patterns and robustness. It is relevant to embodied AI and spatial intelligence (criteria 1 and 3), especially as it discusses perception systems in multi-agent, real-world settings.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23603": {
        "authors": [
            "Tangtangfang Fang",
            "Jingxi Hu",
            "Xiangjian He",
            "Jiaqi Yang"
        ],
        "title": "MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising",
        "abstract": "arXiv:2509.23603v1 Announce Type: new  Abstract: While diffusion models have set a new benchmark for quality in Low-Dose Computed Tomography (LDCT) denoising, their clinical adoption is critically hindered by extreme computational costs, with inference times often exceeding thousands of seconds per scan. To overcome this barrier, we introduce MAN, a Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising task. Our method operates in a compressed latent space via a perceptually-optimized autoencoder, enabling an attention-based conditional U-Net to perform the fast, deterministic conditional denoising diffusion process with drastically reduced overhead. On the LDCT and Projection dataset, our model achieves superior perceptual quality, surpassing CNN/GAN-based methods while rivaling the reconstruction fidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most critically, in the inference stage, our model is over 60x faster than representative pixel space diffusion denoisers, while remaining competitive on PSNR/SSIM scores. By bridging the gap between high fidelity and clinical viability, our work demonstrates a practical path forward for advanced generative models in medical imaging.",
        "arxiv_id": "2509.23603",
        "ARXIVID": "2509.23603",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a latent diffusion-based network for efficient and high-quality CT image denoising, bridging the gap between fidelity and clinical viability.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.24893": {
        "authors": [
            "Yu Ma",
            "Guoliang Wei",
            "Yue Cheng"
        ],
        "title": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping",
        "abstract": "arXiv:2509.24893v1 Announce Type: new  Abstract: Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D reconstruction, typically suffering from overfitting, geometric distortion, and incomplete scene recovery due to limited multi-view constraints. Although 3D Gaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it suffers from floating artifacts and structural inconsistencies under sparse-input settings. To address these issues, we propose DWGS, a novel unified framework that enhances 3DGS for sparse-view synthesis by integrating robust structural cues, virtual view constraints, and occluded region completion. Our approach introduces three principal contributions: a Hybrid-Loss Depth Estimation module that leverages dense matching priors with reprojection, point propagation, and smoothness constraints to enforce multi-view consistency; a Bidirectional Warping Virtual View Synthesis method generates virtual training views to impose stronger geometric and photometric constraints; and an Occlusion-Aware Reconstruction component that utilizes depth-difference mask and a learning-based inpainting model to recover obscured regions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU) show that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while retaining real-time inference capabilities.",
        "arxiv_id": "2509.24893",
        "ARXIVID": "2509.24893",
        "COMMENT": "DWGS proposes a unified framework for enhancing 3D Gaussian Splatting in sparse-view novel view synthesis, introducing hybrid-loss depth estimation and bidirectional warping. This is a methodological improvement in spatial understanding (criterion 1), and relevant to vision foundation models (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.22813": {
        "authors": [
            "Sahar Dastani",
            "Ali Bahri",
            "Gustavo Adolfo Vargas Hakim",
            "Moslem Yazdanpanah",
            "Mehrdad Noori",
            "David Osowiechi",
            "Samuel Barbeau",
            "Ismail Ben Ayed",
            "Herve Lombaert",
            "Christian Desrosiers"
        ],
        "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses",
        "abstract": "arXiv:2509.22813v1 Announce Type: new  Abstract: State Space Models (SSMs) have emerged as efficient alternatives to Vision Transformers (ViTs), with VMamba standing out as a pioneering architecture designed for vision tasks. However, their generalization performance degrades significantly under distribution shifts. To address this limitation, we propose TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel test-time adaptation (TTA) method that leverages diverse traversal permutations to generate multiple causal perspectives of the input image. Model predictions serve as pseudo-labels to guide updates of the Mamba-specific parameters, and the adapted weights are averaged to integrate the learned information across traversal scans. Altogether, TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation. Experiments on seven benchmarks show that TRUST consistently improves robustness and outperforms existing TTA methods.",
        "arxiv_id": "2509.22813",
        "ARXIVID": "2509.22813",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new test-time adaptation method for SSM-based vision models (VMamba), leveraging unique SSM properties for adaptation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23310": {
        "authors": [
            "Hao Liu",
            "Yongjie Zheng",
            "Yuhan Kang",
            "Mingyang Zhang",
            "Maoguo Gong",
            "Lorenzo Bruzzone"
        ],
        "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
        "abstract": "arXiv:2509.23310v1 Announce Type: new  Abstract: Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.",
        "arxiv_id": "2509.23310",
        "ARXIVID": "2509.23310",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 2 (multimodal diffusion models for remote sensing). Uses DDPMs for multimodal fusion and proposes a new balanced fusion framework with adaptive masking and mutual learning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23774": {
        "authors": [
            "Qifan Li",
            "Jiale Zou",
            "Jinhua Zhang",
            "Wei Long",
            "Xinyu Zhou",
            "Shuhang Gu"
        ],
        "title": "Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution",
        "abstract": "arXiv:2509.23774v1 Announce Type: new  Abstract: Vector-quantized based models have recently demonstrated strong potential for visual prior modeling. However, existing VQ-based methods simply encode visual features with nearest codebook items and train index predictor with code-level supervision. Due to the richness of visual signal, VQ encoding often leads to large quantization error. Furthermore, training predictor with code-level supervision can not take the final reconstruction errors into consideration, result in sub-optimal prior modeling accuracy. In this paper we address the above two issues and propose a Texture Vector-Quantization and a Reconstruction Aware Prediction strategy. The texture vector-quantization strategy leverages the task character of super-resolution and only introduce codebook to model the prior of missing textures. While the reconstruction aware prediction strategy makes use of the straight-through estimator to directly train index predictor with image-level supervision. Our proposed generative SR model (TVQ&RAP) is able to deliver photo-realistic SR results with small computational cost.",
        "arxiv_id": "2509.23774",
        "ARXIVID": "2509.23774",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes new vector-quantization and prediction strategies for generative super-resolution, with a focus on texture modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23393": {
        "authors": [
            "Takehiko Ohkawa",
            "Jihyun Lee",
            "Shunsuke Saito",
            "Jason Saragih",
            "Fabian Prado",
            "Yichen Xu",
            "Shoou-I Yu",
            "Ryosuke Furuta",
            "Yoichi Sato",
            "Takaaki Shiratori"
        ],
        "title": "Generative Modeling of Shape-Dependent Self-Contact Human Poses",
        "abstract": "arXiv:2509.23393v1 Announce Type: new  Abstract: One can hardly model self-contact of human poses without considering underlying body shapes. For example, the pose of rubbing a belly for a person with a low BMI leads to penetration of the hand into the belly for a person with a high BMI. Despite its relevance, existing self-contact datasets lack the variety of self-contact poses and precise body shapes, limiting conclusive analysis between self-contact poses and shapes. To address this, we begin by introducing the first extensive self-contact dataset with precise body shape registration, Goliath-SC, consisting of 383K self-contact poses across 130 subjects. Using this dataset, we propose generative modeling of self-contact prior conditioned by body shape parameters, based on a body-part-wise latent diffusion with self-attention. We further incorporate this prior into single-view human pose estimation while refining estimated poses to be in contact. Our experiments suggest that shape conditioning is vital to the successful modeling of self-contact pose distribution, hence improving single-view pose estimation in self-contact.",
        "arxiv_id": "2509.23393",
        "ARXIVID": "2509.23393",
        "COMMENT": "Somewhat related to criterion 1: proposes a generative model for shape-dependent self-contact human poses, which is relevant to spatial understanding in embodied agents, especially for pose estimation and modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.24276": {
        "authors": [
            "Linhao Luo",
            "Zicheng Zhao",
            "Junnan Liu",
            "Zhangchi Qiu",
            "Junnan Dong",
            "Serge Panev",
            "Chen Gong",
            "Thuy-Trang Vu",
            "Gholamreza Haffari",
            "Dinh Phung",
            "Alan Wee-Chung Liew",
            "Shirui Pan"
        ],
        "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge",
        "abstract": "arXiv:2509.24276v1 Announce Type: new  Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.",
        "arxiv_id": "2509.24276",
        "ARXIVID": "2509.24276",
        "COMMENT": "G-reasoner integrates graph and language foundation models for unified reasoning over graph-structured knowledge, introducing a standardized abstraction and a graph foundation model. This is relevant to vision foundation models and their applications (criterion 4), but not directly about spatial intelligence or embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24509": {
        "authors": [
            "Yihong Liu",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Hongyu Lu",
            "Ji-Rong Wen"
        ],
        "title": "Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design",
        "abstract": "arXiv:2509.24509v1 Announce Type: new  Abstract: Combinatorial optimization problems are traditionally tackled with handcrafted heuristic algorithms, which demand extensive domain expertise and significant implementation effort. Recent progress has highlighted the potential of automatic heuristics design powered by large language models (LLMs), enabling the automatic generation and refinement of heuristics. These approaches typically maintain a population of heuristics and employ LLMs as mutation operators to evolve them across generations. While effective, such methods often risk stagnating in local optima. To address this issue, we propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) for automatic algorithm design, a novel framework that integrates the island migration model with the elites selection algorithm to simulate diverse heuristics populations. In EvoPH, prompts are co-evolved with heuristic algorithms, guided by performance feedback. We evaluate our framework on two problems, i.e., Traveling Salesman Problem and Bin Packing Problem. Experimental results demonstrate that EvoPH achieves the lowest relative error against optimal solutions across both datasets, advancing the field of automatic algorithm design with LLMs.",
        "arxiv_id": "2509.24509",
        "ARXIVID": "2509.24509",
        "COMMENT": "Matches criterion 2 (LLMs for automatic algorithm design, co-evolution of prompts and heuristics). Focuses on LLMs for combinatorial optimization, not vision, but is a new method for LLM-based algorithm design.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24374": {
        "authors": [
            "Hao Chen",
            "Fang Xu",
            "Tamer Saleh",
            "Weifeng Hao",
            "Gui-Song Xia"
        ],
        "title": "Mask Clustering-based Annotation Engine for Large-Scale Submeter Land Cover Mapping",
        "abstract": "arXiv:2509.24374v1 Announce Type: new  Abstract: Recent advances in remote sensing technology have made submeter resolution imagery increasingly accessible, offering remarkable detail for fine-grained land cover analysis. However, its full potential remains underutilized - particularly for large-scale land cover mapping - due to the lack of sufficient, high-quality annotated datasets. Existing labels are typically derived from pre-existing products or manual annotation, which are often unreliable or prohibitively expensive, particularly given the rich visual detail and massive data volumes of submeter imagery. Inspired by the spatial autocorrelation principle, which suggests that objects of the same class tend to co-occur with similar visual features in local neighborhoods, we propose the Mask Clustering-based Annotation Engine (MCAE), which treats semantically consistent mask groups as the minimal annotating units to enable efficient, simultaneous annotation of multiple instances. It significantly improves annotation efficiency by one to two orders of magnitude, while preserving label quality, semantic diversity, and spatial representativeness. With MCAE, we build a high-quality annotated dataset of about 14 billion labeled pixels, referred to as HiCity-LC, which supports the generation of city-scale land cover maps across five major Chinese cities with classification accuracies above 85%. It is the first publicly available submeter resolution city-level land cover benchmark, highlighting the scalability and practical utility of MCAE for large-scale, submeter resolution mapping. The dataset is available at https://github.com/chenhaocs/MCAE",
        "arxiv_id": "2509.24374",
        "ARXIVID": "2509.24374",
        "COMMENT": "Related to criterion 4: presents a new annotation engine and a large-scale, high-resolution land cover dataset, which could be useful for vision foundation models in remote sensing applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23867": {
        "authors": [
            "Jiajin Tang",
            "Zhengxuan Wei",
            "Yuchen Zhu",
            "Cheng Shi",
            "Guanbin Li",
            "Liang Lin",
            "Sibei Yang"
        ],
        "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
        "abstract": "arXiv:2509.23867v1 Announce Type: new  Abstract: Temporal sentence grounding aims to identify exact moments in a video that correspond to a given textual query, typically addressed with detection transformer (DETR) solutions. However, we find that typical strategies designed to enhance DETR do not improve, and may even degrade, its performance in this task. We systematically analyze and identify the root causes of this abnormal behavior: (1) conflicts between queries from similar target moments and (2) internal query conflicts due to the tension between global semantics and local localization. Building on these insights, we propose a simple yet powerful baseline, Sim-DETR, which extends the standard DETR with two minor modifications in the decoder layers: (1) constraining self-attention between queries based on their semantic and positional overlap and (2) adding query-to-frame alignment to bridge the global and local contexts. Experiments demonstrate that Sim-DETR unlocks the full potential of DETR for temporal sentence grounding, offering a strong baseline for future research.",
        "arxiv_id": "2509.23867",
        "ARXIVID": "2509.23867",
        "COMMENT": "Related to criterion 4: proposes a new method (Sim-DETR) for temporal sentence grounding in videos, improving DETR-based models for vision-language tasks. Focuses on clever modifications to transformer attention for better multi-modal alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23836": {
        "authors": [
            "Chenyu Zhou",
            "Xiaoming Shi",
            "Hui Qiu",
            "Xiawu Zheng",
            "Haitao Leng",
            "Yankai Jiang",
            "Shaoguo Liu",
            "Tingting Gao",
            "Rongrong Ji"
        ],
        "title": "Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules",
        "abstract": "arXiv:2509.23836v1 Announce Type: new  Abstract: E-commerce agents contribute greatly to helping users complete their e-commerce needs. To promote further research and application of e-commerce agents, benchmarking frameworks are introduced for evaluating LLM agents in the e-commerce domain. Despite the progress, current benchmarks lack evaluating agents' capability to handle mixed-type e-commerce dialogue and complex domain rules. To address the issue, this work first introduces a novel corpus, termed Mix-ECom, which is constructed based on real-world customer-service dialogues with post-processing to remove user privacy and add CoT process. Specifically, Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce dialogue, covering four dialogue types (QA, recommendation, task-oriented dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics, after-sales), and 82 e-commerce rules. Furthermore, this work build baselines on Mix-Ecom and propose a dynamic framework to further improve the performance. Results show that current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, due to the hallucination cased by complex domain rules. The dataset will be publicly available.",
        "arxiv_id": "2509.23836",
        "ARXIVID": "2509.23836",
        "COMMENT": "Mix-Ecom introduces a new benchmark for mixed-type e-commerce dialogues with complex domain rules, and analyzes LLM agent performance. This is a new benchmark for multi-modal dialogue agents, but not specifically spatial or embodied AI, nor vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.23988": {
        "authors": [
            "Zirui Tang",
            "Weizheng Wang",
            "Zihang Zhou",
            "Yang Jiao",
            "Bangrui Xu",
            "Boyu Niu",
            "Xuanhe Zhou",
            "Guoliang Li",
            "Yeye He",
            "Wei Zhou",
            "Yitong Song",
            "Cheng Tan",
            "Bin Wang",
            "Conghui He",
            "Xiaoyang Wang",
            "Fan Wu"
        ],
        "title": "LLM/Agent-as-Data-Analyst: A Survey",
        "abstract": "arXiv:2509.23988v1 Announce Type: new  Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.",
        "arxiv_id": "2509.23988",
        "ARXIVID": "2509.23988",
        "COMMENT": "Matches criterion 2 (survey of LLMs/agents for multi-modal data analysis, including vision and heterogeneous data). Covers VLLMs/MLLMs and their applications in data analysis.",
        "RELEVANCE": 6,
        "NOVELTY": 4
    },
    "2509.23608": {
        "authors": [
            "Liubing Hu",
            "Chen Wu",
            "Anrui Wang",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching",
        "abstract": "arXiv:2509.23608v1 Announce Type: new  Abstract: Deep learning-based image enhancement methods face a fundamental trade-off between computational efficiency and representational capacity. For example, although a conventional three-dimensional Look-Up Table (3D LUT) can process a degraded image in real time, it lacks representational flexibility and depends solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel end-to-end model that integrates the efficiency of LUTs, multiple priors, and the parameter-independent characteristic of flow-matched reconstructed images. Specifically, firstly, the input image is transformed in color space by a collection of differentiable 3D LUTs (containing a large number of 3D LUTs with different priors). Subsequently, a lightweight content-aware dynamically predicts fusion weights, enabling scene-adaptive color correction with $\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs on multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color correction.Furthermore, to address the inherent representation limitations of LUTs, we design an innovative iterative flow matching method to restore local structural details and eliminate artifacts. Finally, the entire model is jointly optimized under a composite loss function enforcing perceptual and structural fidelity. Extensive experimental results demonstrate the effectiveness of our method on three benchmarks.",
        "arxiv_id": "2509.23608",
        "ARXIVID": "2509.23608",
        "COMMENT": "Somewhat related to vision foundation models and their applications (criterion 4), as it proposes a new efficient image enhancement method using differentiable LUTs and flow matching, but does not directly address foundation models or multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.25123": {
        "authors": [
            "Lifan Yuan",
            "Weize Chen",
            "Yuchen Zhang",
            "Ganqu Cui",
            "Hanbin Wang",
            "Ziming You",
            "Ning Ding",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Hao Peng"
        ],
        "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
        "abstract": "arXiv:2509.25123v1 Announce Type: new  Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
        "arxiv_id": "2509.25123",
        "ARXIVID": "2509.25123",
        "COMMENT": "Investigates how RL enables LLMs to compose new skills from old ones, providing insights into skill composition and transfer. While this is a novel empirical study in LLMs and RL, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.24266": {
        "authors": [
            "Wenjie Wei",
            "Malu Zhang",
            "Jieyuan Zhang",
            "Ammar Belatreche",
            "Shuai Wang",
            "Yimeng Shan",
            "Hanwen Liu",
            "Honglin Cao",
            "Guoqing Wang",
            "Yang Yang",
            "Haizhou Li"
        ],
        "title": "S$^2$NN: Sub-bit Spiking Neural Networks",
        "abstract": "arXiv:2509.24266v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) offer an energy-efficient paradigm for machine intelligence, but their continued scaling poses challenges for resource-limited deployment. Despite recent advances in binary SNNs, the storage and computational demands remain substantial for large-scale networks. To further explore the compression and acceleration potential of SNNs, we propose Sub-bit Spiking Neural Networks (S$^2$NNs) that represent weights with less than one bit. Specifically, we first establish an S$^2$NN baseline by leveraging the clustering patterns of kernels in well-trained binary SNNs. This baseline is highly efficient but suffers from \\textit{outlier-induced codeword selection bias} during training. To mitigate this issue, we propose an \\textit{outlier-aware sub-bit weight quantization} (OS-Quant) method, which optimizes codeword selection by identifying and adaptively scaling outliers. Furthermore, we propose a \\textit{membrane potential-based feature distillation} (MPFD) method, improving the performance of highly compressed S$^2$NN via more precise guidance from a teacher model. Extensive results on vision and non-vision tasks reveal that S$^2$NN outperforms existing quantized SNNs in both performance and efficiency, making it promising for edge computing applications.",
        "arxiv_id": "2509.24266",
        "ARXIVID": "2509.24266",
        "COMMENT": "This paper proposes sub-bit spiking neural networks for efficient machine intelligence. While it is a vision paper, it does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.25044": {
        "authors": [
            "Rohit Jena",
            "Vedant Zope",
            "Pratik Chaudhari",
            "James C. Gee"
        ],
        "title": "A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration",
        "abstract": "arXiv:2509.25044v1 Announce Type: new  Abstract: In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels supplemented with a distributed framework for image registration at unprecedented scales. Image registration is an inverse problem fundamental to biomedical and life sciences, but algorithms have not scaled in tandem with image acquisition capabilities. Our framework complements existing model parallelism techniques proposed for large-scale transformer training by optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding. We demonstrate unprecedented capabilities by performing multimodal registration of a 100 micron ex-vivo human brain MRI volume at native resolution - an inverse problem more than 570x larger than a standard clinical datum in about a minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art optimization and deep learning registration pipelines by upto 6 - 7x while reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250 micron dataset shows that FFDP can fit upto 64x larger problems than existing SOTA on a single GPU, and highlights both the performance and efficiency gains of FFDP compared to SOTA image registration methods.",
        "arxiv_id": "2509.25044",
        "ARXIVID": "2509.25044",
        "COMMENT": "Somewhat related to criterion 4: presents a scalable distributed framework for giga-voxel multimodal image registration, which is a technical advance in large-scale vision model infrastructure.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.23681": {
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Mingqiang Wu",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Yulun Zhang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification",
        "abstract": "arXiv:2509.23681v1 Announce Type: new  Abstract: Diffusion transformers exhibit remarkable video generation capability, yet their prohibitive computational and memory costs hinder practical deployment. Model quantization and attention sparsification are two promising directions for compression, but each alone suffers severe performance degradation under aggressive compression. Combining them promises compounded efficiency gains, but naive integration is ineffective. The sparsity-induced information loss exacerbates quantization noise, leading to amplified attention shifts. To address this, we propose \\textbf{QuantSparse}, a unified framework that integrates model quantization with attention sparsification. Specifically, we introduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages both global structural guidance and local salient supervision to mitigate quantization-induced bias. In addition, we develop \\textit{Second-Order Sparse Attention Reparameterization}, which exploits the temporal stability of second-order residuals to efficiently recover information lost under sparsity. Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88 PSNR, substantially outperforming the state-of-the-art quantization baseline Q-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$} reduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end inference. Our code will be released in https://github.com/wlfeng0509/QuantSparse.",
        "arxiv_id": "2509.23681",
        "ARXIVID": "2509.23681",
        "COMMENT": "This paper proposes QuantSparse, a unified framework for compressing video diffusion transformers using quantization and attention sparsification. While it is relevant to generative modeling and efficiency, it does not match the four criteria closely (no spatial intelligence/embodied agent, no VLLM/MLLM, no new benchmark/simulator, not a vision foundation model in the general sense).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.23056": {
        "authors": [
            "Ben Liang",
            "Yuan Liu",
            "Bingwen Qiu",
            "Yihong Wang",
            "Xiubao Sui",
            "Qian Chen"
        ],
        "title": "FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection",
        "abstract": "arXiv:2509.23056v1 Announce Type: new  Abstract: Aerial-view object detection is a critical technology for real-world applications such as natural resource monitoring, traffic management, and UAV-based search and rescue. Detecting tiny objects in high-resolution aerial imagery presents a long-standing challenge due to their limited visual cues and the difficulty of modeling global context in complex scenes. Existing methods are often hampered by delayed contextual fusion and inadequate non-linear modeling, failing to effectively use global information to refine shallow features and thus encountering a performance bottleneck. To address these challenges, we propose FMC-DETR, a novel framework with frequency-decoupled fusion for aerial-view object detection. First, we introduce the Wavelet Kolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet transforms to enhance global low-frequency context perception in shallow features while preserving fine-grained details, and employs Kolmogorov-Arnold networks to achieve adaptive non-linear modeling of multi-scale dependencies. Next, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy and improves multi-scale feature interaction. Finally, we introduce the Multi-Domain Feature Coordination (MDFC) module, which unifies spatial, frequency, and structural priors to to balance detail preservation and global enhancement. Extensive experiments on benchmark aerial-view datasets demonstrate that FMC-DETR achieves state-of-the-art performance with fewer parameters. On the challenging VisDrone dataset, our model achieves improvements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its effectiveness in tiny object detection. The code can be accessed at https://github.com/bloomingvision/FMC-DETR.",
        "arxiv_id": "2509.23056",
        "ARXIVID": "2509.23056",
        "COMMENT": "This paper proposes a new method for aerial-view object detection using frequency-decoupled fusion and a novel backbone (WeKat) with wavelet transforms and Kolmogorov-Arnold networks. While it introduces architectural innovations for spatial understanding in vision, it does not focus on embodied agents or spatial intelligence in embodied settings (criterion 1), nor does it present a new VLLM/MLLM (criterion 2), nor is it about benchmarks or simulators (criterion 3), nor is it about vision foundation models in a general sense (criterion 4).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.23725": {
        "authors": [
            "Siqi Ma",
            "Jiajie Huang",
            "Bolin Yang",
            "Fan Zhang",
            "Jinlin Wu",
            "Yue Shen",
            "Guohui Fan",
            "Zhu Zhang",
            "Zelin Zang"
        ],
        "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
        "abstract": "arXiv:2509.23725v1 Announce Type: new  Abstract: Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
        "arxiv_id": "2509.23725",
        "ARXIVID": "2509.23725",
        "COMMENT": "MedLA proposes a logic-driven multi-agent framework for complex medical reasoning with LLMs, using explicit logical trees and multi-agent discussion. While it is a new method for reasoning, it is not focused on spatial intelligence, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23566": {
        "authors": [
            "Pinyuan Feng",
            "Hossein Adeli",
            "Wenxuan Guo",
            "Fan Cheng",
            "Ethan Hwang",
            "Nikolaus Kriegeskorte"
        ],
        "title": "Towards Interpretable Visual Decoding with Attention to Brain Representations",
        "abstract": "arXiv:2509.23566v1 Announce Type: new  Abstract: Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models, helping brain science researchers interpret how the brain represents real-world scenes. However, most current approaches leverage mapping brain signals into intermediate image or text feature spaces before guiding the generative process, masking the effect of contributions from different brain areas on the final reconstruction output. In this work, we propose NeuroAdapter, a visual decoding framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on public fMRI datasets compared to prior work, while providing greater transparency into how brain signals shape the generation process. To this end, we contribute an Image-Brain BI-directional interpretability framework (IBBI) which investigates cross-attention mechanisms across diffusion denoising steps to reveal how different cortical areas influence the unfolding generative trajectory. Our results highlight the potential of end-to-end brain-to-image decoding and establish a path toward interpreting diffusion models through the lens of visual neuroscience.",
        "arxiv_id": "2509.23566",
        "ARXIVID": "2509.23566",
        "COMMENT": "Relevant to vision foundation models and generative modeling, but does not directly match any specific criterion. Focuses on brain-to-image decoding with diffusion models and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23601": {
        "authors": [
            "Han Hu",
            "Zhuoran Zheng",
            "Liang Li",
            "Chen Lyu"
        ],
        "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
        "abstract": "arXiv:2509.23601v1 Announce Type: new  Abstract: Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.",
        "arxiv_id": "2509.23601",
        "ARXIVID": "2509.23601",
        "COMMENT": "Relevant to computer vision and image restoration, but does not directly match any specific criterion. Proposes an adaptive Mamba-based architecture for image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23150": {
        "authors": [
            "Wenxuan Fang",
            "Jiangwei Weng",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "title": "WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning",
        "abstract": "arXiv:2509.23150v1 Announce Type: new  Abstract: Unsupervised image restoration under multi-weather conditions remains a fundamental yet underexplored challenge. While existing methods often rely on task-specific physical priors, their narrow focus limits scalability and generalization to diverse real-world weather scenarios. In this work, we propose \\textbf{WeatherCycle}, a unified unpaired framework that reformulates weather restoration as a bidirectional degradation-content translation cycle, guided by degradation-aware curriculum regularization. At its core, WeatherCycle employs a \\textit{lumina-chroma decomposition} strategy to decouple degradation from content without modeling complex weather, enabling domain conversion between degraded and clean images. To model diverse and complex degradations, we propose a \\textit{Lumina Degradation Guidance Module} (LDGM), which learns luminance degradation priors from a degraded image pool and injects them into clean images via frequency-domain amplitude modulation, enabling controllable and realistic degradation modeling. Additionally, we incorporate a \\textit{Difficulty-Aware Contrastive Regularization (DACR)} module that identifies hard samples via a CLIP-based classifier and enforces contrastive alignment between hard samples and restored features to enhance semantic consistency and robustness. Extensive experiments across serve multi-weather datasets, demonstrate that our method achieves state-of-the-art performance among unsupervised approaches, with strong generalization to complex weather degradations.",
        "arxiv_id": "2509.23150",
        "ARXIVID": "2509.23150",
        "COMMENT": "Relevant to computer vision and generative modeling, but does not directly match any specific criterion. Proposes a new unsupervised image restoration framework for multi-weather conditions.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22841": {
        "authors": [
            "Yi Luo",
            "Yike Guo",
            "Hamed Hooshangnejad",
            "Rui Zhang",
            "Xue Feng",
            "Quan Chen",
            "Wil Ngwa",
            "Kai Ding"
        ],
        "title": "Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging",
        "abstract": "arXiv:2509.22841v1 Announce Type: new  Abstract: Lung cancer remains the leading cause of cancerrelated deaths globally. Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is pivotal for optimal radiation therapy in mobile tumors such as lung cancer to account for tumor motion, yet is hindered by the limited availability of annotated IGTV datasets and attenuated PET signal intensity at tumor boundaries. In this study, we present a transfer learningbased methodology utilizing a multimodal interactive perception network with MAMBA, pre-trained on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a private IGTV cohort. This cohort constitutes the PET/CT subset of the Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the challenge of weak PET intensities in IGTV peripheral slices, we introduce a slice interaction module (SIM) within a 2.5D segmentation framework to effectively model inter-slice relationships. Our proposed module integrates channel and spatial attention branches with depthwise convolutions, enabling more robust learning of slice-to-slice dependencies and thereby improving overall segmentation performance. A comprehensive experimental evaluation demonstrates that our approach achieves a Dice of 0.609 on the private IGTV dataset, substantially surpassing the conventional baseline score of 0.385. This work highlights the potential of transfer learning, coupled with advanced multimodal techniques and a SIM to enhance the reliability and clinical relevance of IGTV segmentation for lung cancer radiation therapy planning.",
        "arxiv_id": "2509.22841",
        "ARXIVID": "2509.22841",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications), as it uses multimodal learning for medical image segmentation, but is more focused on a specific application (lung cancer PET/CT) and does not introduce a new foundation model.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.24978": {
        "authors": [
            "Maximilian N\\\"agele",
            "Florian Marquardt"
        ],
        "title": "Agentic Exploration of Physics Models",
        "abstract": "arXiv:2509.24978v1 Announce Type: new  Abstract: The process of scientific discovery relies on an interplay of observations, analysis, and hypothesis generation. Machine learning is increasingly being adopted to address individual aspects of this process. However, it remains an open challenge to fully automate the open-ended, heuristic, iterative loop required to discover the laws of an unknown system by exploring it through experiments and analysis, without tailoring the approach to the specifics of a given task. Here, we introduce SciExplorer, an agent that leverages large language model tool-use capabilities to enable free-form exploration of systems without any domain-specific blueprints, and apply it to the exploration of physical systems that are initially unknown to the agent. We test SciExplorer on a broad set of models spanning mechanical dynamical systems, wave evolution, and quantum many-body physics. Despite using a minimal set of tools, primarily based on code execution, we observe impressive performance on tasks such as recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values. The demonstrated effectiveness of this setup opens the door towards similar scientific exploration in other domains, without the need for finetuning or task-specific instructions.",
        "arxiv_id": "2509.24978",
        "ARXIVID": "2509.24978",
        "COMMENT": "Does not match any specific criteria. Focuses on agentic exploration for scientific discovery using LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23370": {
        "authors": [
            "Zhaohua Zhang",
            "Jianhuan Zhuo",
            "Muxi Chen",
            "Chenchen Zhao",
            "Wenyu Jiang",
            "Tianwen Jiang",
            "Mingyang Chen",
            "Yu Tang",
            "Qiuyong Xiao",
            "Jihong Zhang",
            "Zhixun Su"
        ],
        "title": "GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval",
        "abstract": "arXiv:2509.23370v1 Announce Type: new  Abstract: The CLIP model has become a cornerstone of large-scale retrieval systems by aligning text and image data in a unified embedding space. Despite its simplicity and efficiency, CLIP struggles when applied to tasks whose input distributions diverge from its training corpus, such as queries with multilingual, long-form, or multimodal differences. To avoid costly retraining, existing methods mainly adopt query-rewriting strategies with large language models (LLMs), aiming to mitigate distribution gaps at the query level. However, due to the lack of supervision signals, LLMs fail to generate the optimal one that fits the training distribution. We address this challenge with GRAPE (Grouped Ranking-Aware Policy Optimization Enhancement), a plug-and-play enhancement approach that incorporates ranking signals into retrieval-guided query rewriting with LLMs. Intuitively, GRAPE proposes to leverage GRPO to bridge distributional differences -- including length, multilingual, and modality shifts -- by transforming queries into forms better aligned with the retriever's training distribution. However, our preliminary experiment finds that naively finetuning LLM with similarity scores can lead to score inflation, where nearly all candidates are assigned unexpectedly high scores regardless of their true relevance. To address score inflation, we propose a corpus-relative ranking-based reward, which explicitly aligns optimization with ranking metrics while suppressing spurious score inflation. Extensive experiments demonstrate that GRAPE consistently improves retrieval performance under distributional shifts -- including multilingual differences (Flickr30k-CN, CVLUE, XM3600), length differences (Wikipedia), and multimodal differences (CIRR) -- achieving an average improvement of 4.9\\% in Recall\\@10. The code is available at https://github.com/Chinese0123456/GRAPE.git",
        "arxiv_id": "2509.23370",
        "ARXIVID": "2509.23370",
        "COMMENT": "Somewhat related to vision-language models (criterion 2), as it improves retrieval with CLIP and LLMs via query rewriting, but does not introduce a new VLLM/MLLM.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.24922": {
        "authors": [
            "Huihao Jing",
            "Wenbin Hu",
            "Hongyu Luo",
            "Jianhui Yang",
            "Wei Fan",
            "Haoran Li",
            "Yangqiu Song"
        ],
        "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning",
        "abstract": "arXiv:2509.24922v1 Announce Type: new  Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.",
        "arxiv_id": "2509.24922",
        "ARXIVID": "2509.24922",
        "COMMENT": "Introduces a benchmark for multi-agent systems in legal reasoning. While it is a new benchmark for multi-agent LLMs, it is not focused on embodied AI, spatial intelligence, or vision/multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25137": {
        "authors": [
            "Chuanyang Jin",
            "Jing Xu",
            "Bo Liu",
            "Leitian Tao",
            "Olga Golovneva",
            "Tianmin Shu",
            "Wenting Zhao",
            "Xian Li",
            "Jason Weston"
        ],
        "title": "The Era of Real-World Human Interaction: RL from User Conversations",
        "abstract": "arXiv:2509.25137v1 Announce Type: new  Abstract: We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
        "arxiv_id": "2509.25137",
        "ARXIVID": "2509.25137",
        "COMMENT": "This paper introduces RLHI, a paradigm for learning from real-world user conversations for continual model improvement and alignment. While interesting for RL and LLMs, it does not directly address any of the four criteria, especially not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23741": {
        "authors": [
            "Xincheng Yao",
            "Chao Shi",
            "Muming Zhao",
            "Guangtao Zhai",
            "Chongyang Zhang"
        ],
        "title": "ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature Learning",
        "abstract": "arXiv:2509.23741v1 Announce Type: new  Abstract: This paper explores the problem of class-agnostic anomaly detection (AD), where the objective is to train one class-agnostic AD model that can generalize to detect anomalies in diverse new classes from different domains without any retraining or fine-tuning on the target data. When applied for new classes, the performance of current single- and multi-class AD methods is still unsatisfactory. One fundamental reason is that representation learning in existing methods is still class-related, namely, feature correlation. To address this issue, we propose residual features and construct a simple but effective framework, termed ResAD. Our core insight is to learn the residual feature distribution rather than the initial feature distribution. Residual features are formed by matching and then subtracting normal reference features. In this way, we can effectively realize feature decorrelation. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. In addition, we think that residual features still have one issue: scale correlation. To this end, we propose a feature hypersphere constraining approach, which learns to constrain initial normal residual features into a spatial hypersphere for enabling the feature scales of different classes as consistent as possible. Furthermore, we propose a novel logbarrier bidirectional contraction OCC loss and vector quantization based feature distribution matching module to enhance ResAD, leading to the improved version of ResAD (ResAD++). Comprehensive experiments on eight real-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD results when directly used in new classes, outperforming state-of-the-art competing methods and also surpassing ResAD. The code is available at https://github.com/xcyao00/ResAD.",
        "arxiv_id": "2509.23741",
        "ARXIVID": "2509.23741",
        "COMMENT": "This paper proposes a new method for class-agnostic anomaly detection using residual feature learning. While it is a vision paper with clever statistical tricks, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24711": {
        "authors": [
            "Qingjie Zhang",
            "Yujia Fu",
            "Yang Wang",
            "Liu Yan",
            "Tao Wei",
            "Ke Xu",
            "Minlie Huang",
            "Han Qiu"
        ],
        "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries",
        "abstract": "arXiv:2509.24711v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.",
        "arxiv_id": "2509.24711",
        "ARXIVID": "2509.24711",
        "COMMENT": "This paper investigates self-awareness of capability boundaries in large reasoning models, focusing on optimization strategies for more efficient reasoning. While interesting for LLMs, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Closest to general LLM reliability.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23339": {
        "authors": [
            "Bin Wu",
            "Yahui Liu",
            "Chi Zhang",
            "Yao Zhao",
            "Wei Wang"
        ],
        "title": "LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning",
        "abstract": "arXiv:2509.23339v1 Announce Type: new  Abstract: Blind Face Restoration (BFR) encounters inherent challenges in exploring its large solution space, leading to common artifacts like missing details and identity ambiguity in the restored images. To tackle these challenges, we propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the first to apply online reinforcement learning (RL) to the BFR task. LRPO leverages rewards from sampled candidates to refine the policy network, increasing the likelihood of high-quality outputs while improving restoration performance on low-quality inputs. However, directly applying RL to BFR creates incompatibility issues, producing restoration results that deviate significantly from the ground truth. To balance perceptual quality and fidelity, we propose three key strategies: 1) a composite reward function tailored for face restoration assessment, 2) ground-truth guided likelihood regularization, and 3) noise-level advantage assignment. Extensive experiments demonstrate that our proposed LRPO significantly improves the face restoration quality over baseline methods and achieves state-of-the-art performance.",
        "arxiv_id": "2509.23339",
        "ARXIVID": "2509.23339",
        "COMMENT": "Does not match any specific criteria. Applies RL to blind face restoration, which is a vision task but not related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.24261": {
        "authors": [
            "Yuhua Jiang",
            "Jiawei Huang",
            "Yufeng Yuan",
            "Xin Mao",
            "Yu Yue",
            "Qianchuan Zhao",
            "Lin Yan"
        ],
        "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models",
        "abstract": "arXiv:2509.24261v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.",
        "arxiv_id": "2509.24261",
        "ARXIVID": "2509.24261",
        "COMMENT": "Does not match any specific criteria. Focuses on risk-sensitive RL for LLMs, not vision, spatial, or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23102": {
        "authors": [
            "Fang Wu",
            "Xu Huang",
            "Weihao Xuan",
            "Zhiwei Zhang",
            "Yijia Xiao",
            "Guancheng Wan",
            "Xiaomin Li",
            "Bing Hu",
            "Peng Xia",
            "Jure Leskovec",
            "Yejin Choi"
        ],
        "title": "Multiplayer Nash Preference Optimization",
        "abstract": "arXiv:2509.23102v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.",
        "arxiv_id": "2509.23102",
        "ARXIVID": "2509.23102",
        "COMMENT": "Does not match any specific criteria. Focuses on preference optimization for LLM alignment, not vision or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23619": {
        "authors": [
            "Xiangyu Wen",
            "Junhua Huang",
            "Zeju Li",
            "Min Li",
            "Jianyuan Zhong",
            "Zhijian Xu",
            "Mingxuan Yuan",
            "Yongxiang Huang",
            "Qiang Xu"
        ],
        "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
        "abstract": "arXiv:2509.23619v1 Announce Type: new  Abstract: The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.",
        "arxiv_id": "2509.23619",
        "ARXIVID": "2509.23619",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning distillation for LLMs, not spatial, vision, or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25075": {
        "authors": [
            "Huaizhi Qu",
            "Xiao Wang",
            "Gengwei Zhang",
            "Jie Peng",
            "Tianlong Chen"
        ],
        "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction",
        "abstract": "arXiv:2509.25075v1 Announce Type: new  Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.",
        "arxiv_id": "2509.25075",
        "ARXIVID": "2509.25075",
        "COMMENT": "Relevant to computer vision and generative modeling (3D Gaussian Splatting for cryo-EM), but not directly to any of the four criteria (no embodied agent, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23870": {
        "authors": [
            "Jingyu Liu",
            "Xiaopeng Wu",
            "Jingquan Peng",
            "Kehan Chen",
            "Chuan Yu",
            "Lizhong Ding",
            "Yong Liu"
        ],
        "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL",
        "abstract": "arXiv:2509.23870v1 Announce Type: new  Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.",
        "arxiv_id": "2509.23870",
        "ARXIVID": "2509.23870",
        "COMMENT": "Somewhat relevant to embodied AI (agentic RL), but does not introduce a new benchmark or simulator, nor does it focus on spatial intelligence or vision. Focuses on reward miscalibration and gradient coupling in RL.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.24836": {
        "authors": [
            "Zhen Bi",
            "Zhenlin Hu",
            "Jinnan Yang",
            "Mingyang Chen",
            "Cheng Deng",
            "Yida Xue",
            "Zeyu Yang",
            "Qing Shen",
            "Zhenfang Liu",
            "Kang Zhao",
            "Ningyu Zhang",
            "Jungang Lou"
        ],
        "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity",
        "abstract": "arXiv:2509.24836v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data.Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.",
        "arxiv_id": "2509.24836",
        "ARXIVID": "2509.24836",
        "COMMENT": "Relevant to LLMs and reasoning, but not directly to vision, spatial intelligence, VLLMs/MLLMs, or embodied AI. Focuses on logical reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23694": {
        "authors": [
            "Jianshuo Dong",
            "Sheng Guo",
            "Hao Wang",
            "Zhuotao Liu",
            "Tianwei Zhang",
            "Ke Xu",
            "Minlie Huang",
            "Han Qiu"
        ],
        "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents",
        "abstract": "arXiv:2509.23694v1 Announce Type: new  Abstract: Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.",
        "arxiv_id": "2509.23694",
        "ARXIVID": "2509.23694",
        "COMMENT": "SafeSearch presents an automated red-teaming framework and benchmark for LLM-based search agents, focusing on safety and robustness. While it is a new benchmark, it is not focused on embodied AI, spatial intelligence, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23676": {
        "authors": [
            "Jue Zhang",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
        "abstract": "arXiv:2509.23676v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.",
        "arxiv_id": "2509.23676",
        "ARXIVID": "2509.23676",
        "COMMENT": "This paper investigates how reasoning traces influence answer generation in distilled DeepSeek R1 models, using empirical, attention, and mechanistic analysis. It provides mechanistic insights into reasoning in LLMs, but does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24973": {
        "authors": [
            "Ishika Jain",
            "Siri Willems",
            "Steven Latre",
            "Tom De Schepper"
        ],
        "title": "On-the-Fly Data Augmentation for Brain Tumor Segmentation",
        "abstract": "arXiv:2509.24973v1 Announce Type: new  Abstract: Robust segmentation across both pre-treatment and post-treatment glioma scans can be helpful for consistent tumor monitoring and treatment planning. BraTS 2025 Task 1 addresses this by challenging models to generalize across varying tumor appearances throughout the treatment timeline. However, training such generalized models requires access to diverse, high-quality annotated data, which is often limited. While data augmentation can alleviate this, storing large volumes of augmented 3D data is computationally expensive. To address these challenges, we propose an on-the-fly augmentation strategy that dynamically inserts synthetic tumors using pretrained generative adversarial networks (GliGANs) during training. We evaluate three nnU-Net-based models and their ensembles: (1) a baseline without external augmentation, (2) a regular on-the-fly augmented model, and (3) a model with customized on-the-fly augmentation. Built upon the nnU-Net framework, our pipeline leverages pretrained GliGAN weights and tumor insertion methods from prior challenge-winning solutions. An ensemble of the three models achieves lesion-wise Dice scores of 0.79 (ET), 0.749 (NETC), 0.872 (RC), 0.825 (SNFH), 0.79 (TC), and 0.88 (WT) on the online BraTS 2025 validation platform. This work ranked first in the BraTS Lighthouse Challenge 2025 Task 1- Adult Glioma Segmentation.",
        "arxiv_id": "2509.24973",
        "ARXIVID": "2509.24973",
        "COMMENT": "Relevant to generative modeling and medical vision, but does not directly match any specific criterion. Proposes on-the-fly data augmentation for brain tumor segmentation using GANs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24194": {
        "authors": [
            "Zach Eidex",
            "Mojtaba Safari",
            "Jie Ding",
            "Richard Qiu",
            "Justin Roper",
            "David Yu",
            "Hui-Kuo Shu",
            "Zhen Tian",
            "Hui Mao",
            "Xiaofeng Yang"
        ],
        "title": "An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation",
        "abstract": "arXiv:2509.24194v1 Announce Type: new  Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N=2860), validation (N=612), and test (N=614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/- 0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/- 0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s/volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr/volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors.",
        "arxiv_id": "2509.24194",
        "ARXIVID": "2509.24194",
        "COMMENT": "Relevant to generative modeling in vision (medical imaging), but does not directly match any specific criterion. Focuses on efficient 3D latent diffusion for MRI generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23335": {
        "authors": [
            "Kaile Du",
            "Zihan Ye",
            "Junzhou Xie",
            "Fan Lyu",
            "Yixi Shen",
            "Yuyang Li",
            "Miaoxuan Zhu",
            "Fuyuan Hu",
            "Ling Shao",
            "Guangcan Liu"
        ],
        "title": "DDP: Dual-Decoupled Prompting for Multi-Label Class-Incremental Learning",
        "abstract": "arXiv:2509.23335v1 Announce Type: new  Abstract: Prompt-based methods have shown strong effectiveness in single-label class-incremental learning, but their direct extension to multi-label class-incremental learning (MLCIL) performs poorly due to two intrinsic challenges: semantic confusion from co-occurring categories and true-negative-false-positive confusion caused by partial labeling. We propose Dual-Decoupled Prompting (DDP), a replay-free and parameter-efficient framework that explicitly addresses both issues. DDP assigns class-specific positive-negative prompts to disentangle semantics and introduces Progressive Confidence Decoupling (PCD), a curriculum-inspired decoupling strategy that suppresses false positives. Past prompts are frozen as knowledge anchors, and interlayer prompting enhances efficiency. On MS-COCO and PASCAL VOC, DDP consistently outperforms prior methods and is the first replay-free MLCIL approach to exceed 80% mAP and 70% F1 under the standard MS-COCO B40-C10 benchmark.",
        "arxiv_id": "2509.23335",
        "ARXIVID": "2509.23335",
        "COMMENT": "Does not match any specific criteria. Focuses on prompt-based multi-label class-incremental learning, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24159": {
        "authors": [
            "Xiaoyang Cao",
            "Zelai Xu",
            "Mo Guang",
            "Kaiwen Long",
            "Michiel A. Bakker",
            "Yu Wang",
            "Chao Yu"
        ],
        "title": "Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback",
        "abstract": "arXiv:2509.24159v1 Announce Type: new  Abstract: Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively.",
        "arxiv_id": "2509.24159",
        "ARXIVID": "2509.24159",
        "COMMENT": "Does not match any specific criteria. Focuses on robust preference optimization for language model alignment, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23392": {
        "authors": [
            "Jinyi Han",
            "Ying Huang",
            "Ying Liao",
            "Zishang Jiang",
            "Xikun Lu",
            "Haiquan Zhao",
            "Xinyi Wang",
            "Guanghao Zhou",
            "Sihang Jiang",
            "Jiaqing Liang",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu",
            "Yanghua Xiao"
        ],
        "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking",
        "abstract": "arXiv:2509.23392v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.",
        "arxiv_id": "2509.23392",
        "ARXIVID": "2509.23392",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning efficiency in large reasoning models, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24913": {
        "authors": [
            "Tian Xia",
            "Matthew Sinclair",
            "Andreas Schuh",
            "Fabio De Sousa Ribeiro",
            "Raghav Mehta",
            "Rajat Rasal",
            "Esther Puyol-Ant\\'on",
            "Samuel Gerber",
            "Kersten Petersen",
            "Michiel Schaap",
            "Ben Glocker"
        ],
        "title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis",
        "abstract": "arXiv:2509.24913v1 Announce Type: new  Abstract: Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.",
        "arxiv_id": "2509.24913",
        "ARXIVID": "2509.24913",
        "COMMENT": "Relevant to generative modeling and vision, but does not match any specific criterion. Proposes a segmentor-guided counterfactual fine-tuning for image synthesis, with applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23255": {
        "authors": [
            "Sasan Sharifipour",
            "Constantino \\'Alvarez Casado",
            "Le Nguyen",
            "Tharindu Ekanayake",
            "Manuel Lage Ca\\~nellas",
            "Nhi Nguyen",
            "Miguel Bordallo L\\'opez"
        ],
        "title": "LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis",
        "abstract": "arXiv:2509.23255v1 Announce Type: new  Abstract: Human Activity Recognition supports applications in healthcare, manufacturing, and human-machine interaction. LiDAR point clouds offer a privacy-preserving alternative to cameras and are robust to illumination. We propose a HAR method based on graph spectral analysis. Each LiDAR frame is mapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is computed. Eigenvalues and statistics of eigenvectors form pose descriptors, and temporal statistics over sliding windows yield fixed vectors for classification with support vector machines and random forests. On the MM-Fi dataset with 40 subjects and 27 activities, under a strict subject-independent protocol, the method reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all 27 activities. It also surpasses the skeleton-based baselines reported for MM-Fi. The contribution is a compact and interpretable feature set derived directly from point cloud geometry that provides an accurate and efficient alternative to end-to-end deep learning.",
        "arxiv_id": "2509.23255",
        "ARXIVID": "2509.23255",
        "COMMENT": "Relevant to computer vision and machine learning, but does not match any specific criterion. Focuses on LiDAR-based human activity recognition using graph spectral analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24460": {
        "authors": [
            "Haotian Zhang",
            "Liu Liu",
            "Baosheng Yu",
            "Jiayan Qiu",
            "Likang Xiao",
            "Yanwei Ren",
            "Quan Chen",
            "Xianglong Liu"
        ],
        "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling",
        "abstract": "arXiv:2509.24460v1 Announce Type: new  Abstract: Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.",
        "arxiv_id": "2509.24460",
        "ARXIVID": "2509.24460",
        "COMMENT": "Does not directly match any specific criterion. Focuses on process reward models for LLMs and test-time scaling in reasoning, but not in spatial or vision/multimodal domains.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24267": {
        "authors": [
            "Fangrui Huang",
            "Alan Wang",
            "Binxu Li",
            "Bailey Trang",
            "Ridvan Yesiloglu",
            "Tianyu Hua",
            "Wei Peng",
            "Ehsan Adeli"
        ],
        "title": "Cycle Diffusion Model for Counterfactual Image Generation",
        "abstract": "arXiv:2509.24267v1 Announce Type: new  Abstract: Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.",
        "arxiv_id": "2509.24267",
        "ARXIVID": "2509.24267",
        "COMMENT": "Presents a cycle training framework for diffusion models to improve conditioning and realism in medical image generation. While it is a generative modeling paper, it is not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24358": {
        "authors": [
            "Dayu Tan",
            "Cheng Kong",
            "Yansen Su",
            "Hai Chen",
            "Dongliang Yang",
            "Junfeng Xia",
            "Chunhou Zheng"
        ],
        "title": "An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation",
        "abstract": "arXiv:2509.24358v1 Announce Type: new  Abstract: In the field of multi-organ medical image segmentation, recent methods frequently employ Transformers to capture long-range dependencies from image features. However, these methods overlook the high computational cost of Transformers and their deficiencies in extracting local detailed information. To address high computational costs and inadequate local detail information, we reassess the design of feature extraction modules and propose a new deep-learning network called LamFormer for fine-grained segmentation tasks across multiple organs. LamFormer is a novel U-shaped network that employs Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture multi-scale long-range dependencies. We construct the Parallel Hierarchical Feature Aggregation (PHFA) module to aggregate features from different layers of the encoder, narrowing the semantic gap among features while filtering information. Finally, we design the Reduced Transformer (RT), which utilizes a distinct computational approach to globally model up-sampled features. RRT enhances the extraction of detailed local information and improves the network's capability to capture long-range dependencies. LamFormer outperforms existing segmentation methods on seven complex and diverse datasets, demonstrating exceptional performance. Moreover, the proposed network achieves a balance between model performance and model complexity.",
        "arxiv_id": "2509.24358",
        "ARXIVID": "2509.24358",
        "COMMENT": "This paper proposes LamFormer, a new network for multi-organ medical image segmentation using linear attention and hierarchical feature aggregation. While it introduces new methods for spatial feature extraction, it is not about spatial intelligence in embodied agents (criterion 1), VLLMs/MLLMs (criterion 2), embodied AI benchmarks (criterion 3), or vision foundation models (criterion 4).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25164": {
        "authors": [
            "Ranjan Sapkota",
            "Rahul Harsha Cheppally",
            "Ajay Sharda",
            "Manoj Karkee"
        ],
        "title": "YOLO26: Key Architectural Enhancements and Performance Benchmarking for Real-Time Object Detection",
        "abstract": "arXiv:2509.25164v1 Announce Type: new  Abstract: This study presents Key Architectural Enhancements and Performance Benchmarking of Ultralytics YOLO26 for real-time edge object detection, providing a comprehensive overview of the design principles of YOLO26, technological advances, and deployment readiness. YOLO26, released in September 2025 by Ultralytics, represents the newest and most cutting-edge member of the You Only Look Once (YOLO) family, engineered to push the boundaries of efficiency and accuracy on edge and low-power devices. This paper highlights architectural innovations in YOLO26, including end-to-end NMS-free inference, removal of Distribution Focal Loss (DFL) for streamlined exports, introduction of ProgLoss and Small-Target-Aware Label Assignment (STAL) for improved stability and small-object detection, and the adoption of the MuSGD optimizer inspired by large language model training. In addition, we report performance benchmarks for YOLO26 across edge devices, specifically NVIDIA Orin Jetson platforms, and compare results against YOLOv8 and YOLO11 (previous Ultralytics releases) as well as YOLOv12 and YOLOv13, which bridged the lineage between YOLO11 and YOLO26. Our comparative analysis highlights superior efficiency of YOLO26, accuracy, and deployment versatility, establishing it as a pivotal milestone in the YOLO evolution.",
        "arxiv_id": "2509.25164",
        "ARXIVID": "2509.25164",
        "COMMENT": "This paper presents YOLO26, a new real-time object detection model with architectural enhancements. While it is relevant to computer vision and model efficiency, it does not match any of the four criteria closely (no spatial intelligence/embodied agent focus, no VLLM/MLLM, no new benchmark/simulator, not a vision foundation model in the sense of general-purpose multi-modal models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23375": {
        "authors": [
            "Yifan Yang",
            "Yuxiang Yan",
            "Boda Liu",
            "Jian Pu"
        ],
        "title": "CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation",
        "abstract": "arXiv:2509.23375v1 Announce Type: new  Abstract: Point clouds collected from real-world environments are often incomplete due to factors such as limited sensor resolution, single viewpoints, occlusions, and noise. These challenges make point cloud completion essential for various applications. A key difficulty in this task is predicting the overall shape and reconstructing missing regions from highly incomplete point clouds. To address this, we introduce CasPoinTr, a novel point cloud completion framework using cascaded networks and knowledge distillation. CasPoinTr decomposes the completion task into two synergistic stages: Shape Reconstruction, which generates auxiliary information, and Fused Completion, which leverages this information alongside knowledge distillation to generate the final output. Through knowledge distillation, a teacher model trained on denser point clouds transfers incomplete-complete associative knowledge to the student model, enhancing its ability to estimate the overall shape and predict missing regions. Together, the cascaded networks and knowledge distillation enhance the model's ability to capture global shape context while refining local details, effectively bridging the gap between incomplete inputs and complete targets. Experiments on ShapeNet-55 under different difficulty settings demonstrate that CasPoinTr outperforms existing methods in shape recovery and detail preservation, highlighting the effectiveness of our cascaded structure and distillation strategy.",
        "arxiv_id": "2509.23375",
        "ARXIVID": "2509.23375",
        "COMMENT": "Somewhat relevant to general computer vision interests (point cloud completion), but does not directly match any of the four criteria (no embodied agent, VLLM/MLLM, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24477": {
        "authors": [
            "Julio Hurtado",
            "Haoran Ni",
            "Duygu Sap",
            "Connor Mattinson",
            "Martin Lotz"
        ],
        "title": "Performance-Efficiency Trade-off for Fashion Image Retrieval",
        "abstract": "arXiv:2509.24477v1 Announce Type: new  Abstract: The fashion industry has been identified as a major contributor to waste and emissions, leading to an increased interest in promoting the second-hand market. Machine learning methods play an important role in facilitating the creation and expansion of second-hand marketplaces by enabling the large-scale valuation of used garments. We contribute to this line of work by addressing the scalability of second-hand image retrieval from databases. By introducing a selective representation framework, we can shrink databases to 10% of their original size without sacrificing retrieval accuracy. We first explore clustering and coreset selection methods to identify representative samples that capture the key features of each garment and its internal variability. Then, we introduce an efficient outlier removal method, based on a neighbour-homogeneity consistency score measure, that filters out uncharacteristic samples prior to selection. We evaluate our approach on three public datasets: DeepFashion Attribute, DeepFashion Con2Shop, and DeepFashion2. The results demonstrate a clear performance-efficiency trade-off by strategically pruning and selecting representative vectors of images. The retrieval system maintains near-optimal accuracy, while greatly reducing computational costs by reducing the images added to the vector database. Furthermore, applying our outlier removal method to clustering techniques yields even higher retrieval performance by removing non-discriminative samples before the selection.",
        "arxiv_id": "2509.24477",
        "ARXIVID": "2509.24477",
        "COMMENT": "Does not match any specific criterion. Focuses on efficient image retrieval for fashion, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.24731": {
        "authors": [
            "Luis F. W. Batista",
            "Tom Bourbon",
            "Cedric Pradalier"
        ],
        "title": "Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic Environments",
        "abstract": "arXiv:2509.24731v1 Announce Type: new  Abstract: Accurate segmentation of floating debris on water is often compromised by surface glare and changing outdoor illumination. Polarimetric imaging offers a single-sensor route to mitigate water-surface glare that disrupts semantic segmentation of floating objects. We benchmark state-of-the-art fusion networks on PoTATO, a public dataset of polarimetric images of plastic bottles in inland waterways, and compare their performance with single-image baselines using traditional models. Our results indicate that polarimetric cues help recover low-contrast objects and suppress reflection-induced false positives, raising mean IoU and lowering contour error relative to RGB inputs. These sharper masks come at a cost: the additional channels enlarge the models increasing the computational load and introducing the risk of new false positives. By providing a reproducible, diagnostic benchmark and publicly available code, we hope to help researchers choose if polarized cameras are suitable for their applications and to accelerate related research.",
        "arxiv_id": "2509.24731",
        "ARXIVID": "2509.24731",
        "COMMENT": "Relevant to vision and segmentation, but does not match any specific criterion. Benchmarks polarimetric fusion for semantic segmentation in aquatic environments.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.23009": {
        "authors": [
            "Masato Kobayashi",
            "Ning Ding",
            "Toru Tamaki"
        ],
        "title": "Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition",
        "abstract": "arXiv:2509.23009v1 Announce Type: new  Abstract: Action recognition models rely excessively on static cues rather than dynamic human motion, which is known as static bias. This bias leads to poor performance in real-world applications and zero-shot action recognition. In this paper, we propose a method to reduce static bias by separating temporal dynamic information from static scene information. Our approach uses a statistical independence loss between biased and unbiased streams, combined with a scene prediction loss. Our experiments demonstrate that this method effectively reduces static bias and confirm the importance of scene prediction loss.",
        "arxiv_id": "2509.23009",
        "ARXIVID": "2509.23009",
        "COMMENT": "Relevant to action recognition and vision, but does not match any specific criterion. Focuses on reducing static bias in action recognition by disentangling static and dynamic information.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.25154": {
        "authors": [
            "Dawei Li",
            "Zhen Tan",
            "Chengshuai Zhao",
            "Bohan Jiang",
            "Baixiang Huang",
            "Pingchuan Ma",
            "Abdullah Alnaibari",
            "Kai Shu",
            "Huan Liu"
        ],
        "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
        "abstract": "arXiv:2509.25154v1 Announce Type: new  Abstract: Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce \\textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \\textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.",
        "arxiv_id": "2509.25154",
        "ARXIVID": "2509.25154",
        "COMMENT": "Does not match any specific criteria. Focuses on LLM-generated judgments and their detectability, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}