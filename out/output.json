{
    "2511.20648": {
        "authors": [
            "Yunze Man",
            "Shihao Wang",
            "Guowen Zhang",
            "Johan Bjorck",
            "Zhiqi Li",
            "Liang-Yan Gui",
            "Jim Fan",
            "Jan Kautz",
            "Yu-Xiong Wang",
            "Zhiding Yu"
        ],
        "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
        "abstract": "arXiv:2511.20648v1 Announce Type: new  Abstract: To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
        "arxiv_id": "2511.20648",
        "ARXIVID": "2511.20648",
        "COMMENT": "Matches criterion 2 (VLM-native 3D detection), criterion 4 (vision foundation models and their application to 3D), and partially criterion 1 (spatial understanding in embodied agents). Strong focus on novel 3D detection interface for VLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2511.18450": {
        "authors": [
            "Rui Xu",
            "Dakuan Lu",
            "Zicheng Zhao",
            "Xiaoyu Tan",
            "Xintao Wang",
            "Siyu Yuan",
            "Jiangjie Chen",
            "Yinghui Xu"
        ],
        "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints",
        "abstract": "arXiv:2511.18450v1 Announce Type: new  Abstract: Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.",
        "arxiv_id": "2511.18450",
        "ARXIVID": "2511.18450",
        "COMMENT": "Matches criterion 1 (new benchmark for spatial reasoning in MLLMs, focusing on multi-step spatial reasoning and mathematical constraints) and criterion 3 (new embodied AI benchmark with novel origami-based tasks and interactive environment).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.20351": {
        "authors": [
            "Heyang Yu",
            "Yinan Han",
            "Xiangyu Zhang",
            "Baiqiao Yin",
            "Bowen Chang",
            "Xiangyu Han",
            "Xinhao Liu",
            "Jing Zhang",
            "Marco Pavone",
            "Chen Feng",
            "Saining Xie",
            "Yiming Li"
        ],
        "title": "Thinking in 360{\\deg}: Humanoid Visual Search in the Wild",
        "abstract": "arXiv:2511.20351v1 Announce Type: new  Abstract: Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360{\\deg}. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360{\\deg} panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.",
        "arxiv_id": "2511.20351",
        "ARXIVID": "2511.20351",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (new embodied AI benchmark and methods). Proposes a new benchmark (H* Bench) for humanoid visual search in 360\u00b0 real-world scenes, and analyzes the spatial reasoning challenges for MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.20573": {
        "authors": [
            "Chenhui Gou",
            "Zilong Chen",
            "Zeyu Wang",
            "Feng Li",
            "Deyao Zhu",
            "Zicheng Duan",
            "Kunchang Li",
            "Chaorui Deng",
            "Hongyi Yuan",
            "Haoqi Fan",
            "Cihang Xie",
            "Jianfei Cai",
            "Hamid Rezatofighi"
        ],
        "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
        "abstract": "arXiv:2511.20573v1 Announce Type: new  Abstract: This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
        "arxiv_id": "2511.20573",
        "ARXIVID": "2511.20573",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new data-centric framework and benchmark for visual question-visual answering (VQ-VA), and releases open-source models and datasets. Also provides strong empirical results and a new benchmark (IntelligentBench).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.19972": {
        "authors": [
            "Yun Xing",
            "Xiaobin Hu",
            "Qingdong He",
            "Jiangning Zhang",
            "Shuicheng Yan",
            "Shijian Lu",
            "Yu-Gang Jiang"
        ],
        "title": "Boosting Reasoning in Large Multimodal Models via Activation Replay",
        "abstract": "arXiv:2511.19972v1 Announce Type: new  Abstract: Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while high-entropy ones are less affected. We further demonstrate that such phenomena are associated with LMM reasoning by controlled experiments, suggesting a potentially beneficial role of modulating low-entropy activations. To this end, we propose Activation Replay, a novel simple yet effective training-free approach that boosts multimodal reasoning of post-trained LMMs without requiring expensive policy optimization. Our design involves manipulation of visual tokens at test time, replaying low-entropy activations from the input context of base LMMs to regulating the RLVR counterparts. Activation Replay triggers better reasoning across diverse scenarios, including mathematics, o3-like visual agents, and video reasoning. We further show that Activation Replay boosts Pass@K and mitigates narrower reasoning coverage of RLVR. Our design is compared against alternative choices, such as replaying high-entropy activations instead of low-entropy ones, or direct cross-model intervention instead of manipulating input tokens, demonstrating the superiority of our implementation. Codes will be made publicly available.",
        "arxiv_id": "2511.19972",
        "ARXIVID": "2511.19972",
        "COMMENT": "Matches criterion 2 (new method for boosting reasoning in LMMs/MLLMs), with a novel statistical trick (activation replay) for improving multimodal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.19900": {
        "authors": [
            "Jiaqi Liu",
            "Kaiwen Xiong",
            "Peng Xia",
            "Yiyang Zhou",
            "Haonian Ji",
            "Lu Feng",
            "Siwei Han",
            "Mingyu Ding",
            "Huaxiu Yao"
        ],
        "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
        "abstract": "arXiv:2511.19900v1 Announce Type: new  Abstract: Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \\href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.",
        "arxiv_id": "2511.19900",
        "ARXIVID": "2511.19900",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and is highly relevant to spatial intelligence and embodied agents (criterion 1), as it proposes a self-evolving vision-language agent with tool-integrated reasoning and self-improvement, showing strong empirical gains.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.19529": {
        "authors": [
            "Vidi Team",
            "Celong Liu",
            "Chia-Wen Kuo",
            "Chuang Huang",
            "Dawei Du",
            "Fan Chen",
            "Guang Chen",
            "Haoji Zhang",
            "Haojun Zhao",
            "Lingxi Zhang",
            "Lu Guo",
            "Lusha Li",
            "Longyin Wen",
            "Qihang Fan",
            "Qingyu Chen",
            "Rachel Deng",
            "Sijie Zhu",
            "Stuart Siew",
            "Tong Jin",
            "Weiyan Tao",
            "Wen Zhong",
            "Xiaohui Shen",
            "Xin Gu",
            "Zhenfang Chen",
            "Zuhua Lin"
        ],
        "title": "Vidi2: Large Multimodal Models for Video Understanding and Creation",
        "abstract": "arXiv:2511.19529v1 Announce Type: new  Abstract: Video has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production. Vidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR). In its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning. Given a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges. This end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark, VUE-STG, which offers four key improvements over existing STG datasets: 1) Video duration: spans from roughly 10s to 30 mins, enabling long-context reasoning; 2) Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness; 3) Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy; 4) Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme. In addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries. Remarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks.",
        "arxiv_id": "2511.19529",
        "ARXIVID": "2511.19529",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces Vidi2, a large multimodal model for video understanding and creation, with new benchmarks and strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.19661": {
        "authors": [
            "Xinhai Hou",
            "Shaoyuan Xu",
            "Manan Biyani",
            "Mayan Li",
            "Jia Liu",
            "Todd C. Hollon",
            "Bryan Wang"
        ],
        "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
        "abstract": "arXiv:2511.19661v1 Announce Type: new  Abstract: Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
        "arxiv_id": "2511.19661",
        "ARXIVID": "2511.19661",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new RL-based training method for agentic vision-language models to improve faithful visual reasoning, with a novel reward structure.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.19917": {
        "authors": [
            "Qin Ren",
            "Yufei Wang",
            "Lanqing Guo",
            "Wen Zhang",
            "Zhiwen Fan",
            "Chenyu You"
        ],
        "title": "Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models",
        "abstract": "arXiv:2511.19917v1 Announce Type: new  Abstract: Diffusion models have become the dominant paradigm in text-to-image generation, and test-time scaling (TTS) further improves quality by allocating more computation during inference. However, existing TTS methods operate at the full-image level, overlooking the fact that image quality is often spatially heterogeneous. This leads to unnecessary computation on already satisfactory regions and insufficient correction of localized defects. In this paper, we explore a new direction - Localized TTS - that adaptively resamples defective regions while preserving high-quality regions, thereby substantially reducing the search space. This paradigm poses two central challenges: accurately localizing defects and maintaining global consistency. We propose LoTTS, the first fully training-free framework for localized TTS. For defect localization, LoTTS contrasts cross- and self-attention signals under quality-aware prompts (e.g., high-quality vs. low-quality) to identify defective regions, and then refines them into coherent masks. For consistency, LoTTS perturbs only defective regions and denoises them locally, ensuring that corrections remain confined while the rest of the image remains undisturbed. Extensive experiments on SD2.1, SDXL, and FLUX demonstrate that LoTTS achieves state-of-the-art performance: it consistently improves both local quality and global fidelity, while reducing GPU cost by 2-4x compared to Best-of-N sampling. These findings establish localized TTS as a promising new direction for scaling diffusion models at inference time.",
        "arxiv_id": "2511.19917",
        "ARXIVID": "2511.19917",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new, training-free, localized test-time scaling method for diffusion models, which is a clever statistical trick for generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.20223": {
        "authors": [
            "Sen Nie",
            "Jie Zhang",
            "Jianxin Yan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs",
        "abstract": "arXiv:2511.20223v1 Announce Type: new  Abstract: Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.",
        "arxiv_id": "2511.20223",
        "ARXIVID": "2511.20223",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel adversarial attack method targeting value features in LVLMs, revealing new vulnerabilities and manipulation techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.20544": {
        "authors": [
            "Ege Ozguroglu",
            "Junbang Liang",
            "Ruoshi Liu",
            "Mia Chiquier",
            "Michael DeTienne",
            "Wesley Wei Qian",
            "Alexandra Horowitz",
            "Andrew Owens",
            "Carl Vondrick"
        ],
        "title": "New York Smells: A Large Multimodal Dataset for Olfaction",
        "abstract": "arXiv:2511.20544v1 Announce Type: new  Abstract: While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.",
        "arxiv_id": "2511.20544",
        "ARXIVID": "2511.20544",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new large multimodal dataset (images + olfactory signals) and explores cross-modal representation learning, which is a novel angle in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.19965": {
        "authors": [
            "Hongji Yang",
            "Yucheng Zhou",
            "Wencheng Han",
            "Runzhou Tao",
            "Zhongying Qiu",
            "Jianfei Yang",
            "Jianbing Shen"
        ],
        "title": "HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning",
        "abstract": "arXiv:2511.19965v1 Announce Type: new  Abstract: Recent advances in diffusion models have demonstrated impressive capability in generating high-quality images for simple prompts. However, when confronted with complex prompts involving multiple objects and hierarchical structures, existing models struggle to accurately follow instructions, leading to issues such as concept omission, confusion, and poor compositionality. To address these limitations, we propose a Hierarchical Compositional Generative framework (HiCoGen) built upon a novel Chain of Synthesis (CoS) paradigm. Instead of monolithic generation, HiCoGen first leverages a Large Language Model (LLM) to decompose complex prompts into minimal semantic units. It then synthesizes these units iteratively, where the image generated in each step provides crucial visual context for the next, ensuring all textual concepts are faithfully constructed into the final scene. To further optimize this process, we introduce a reinforcement learning (RL) framework. Crucially, we identify that the limited exploration of standard diffusion samplers hinders effective RL. We theoretically prove that sample diversity is maximized by concentrating stochasticity in the early generation stages and, based on this insight, propose a novel Decaying Stochasticity Schedule to enhance exploration. Our RL algorithm is then guided by a hierarchical reward mechanism that jointly evaluates the image at the global, subject, and relationship levels. We also construct HiCoPrompt, a new text-to-image benchmark with hierarchical prompts for rigorous evaluation. Experiments show our approach significantly outperforms existing methods in both concept coverage and compositional accuracy.",
        "arxiv_id": "2511.19965",
        "ARXIVID": "2511.19965",
        "COMMENT": "Matches criterion 4 (vision foundation models and generative modeling), with a novel RL-based compositional text-to-image diffusion framework. Also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.20462": {
        "authors": [
            "Jiatao Gu",
            "Ying Shen",
            "Tianrong Chen",
            "Laurent Dinh",
            "Yuyang Wang",
            "Miguel Angel Bautista",
            "David Berthelot",
            "Josh Susskind",
            "Shuangfei Zhai"
        ],
        "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
        "abstract": "arXiv:2511.20462v1 Announce Type: new  Abstract: Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
        "arxiv_id": "2511.20462",
        "ARXIVID": "2511.20462",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is highly relevant to generative modeling. Proposes STARFlow-V, a normalizing flow-based video generator with novel architecture and sampling strategies, showing strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.20415": {
        "authors": [
            "Zilong Huang",
            "Jun He",
            "Xiaobin Huang",
            "Ziyi Xiong",
            "Yang Luo",
            "Junyan Ye",
            "Weijia Li",
            "Yiping Chen",
            "Ting Han"
        ],
        "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
        "abstract": "arXiv:2511.20415v1 Announce Type: new  Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.",
        "arxiv_id": "2511.20415",
        "ARXIVID": "2511.20415",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 1 (spatial intelligence in 3D scene generation). Introduces a language-driven, controllable 3D city generation framework with a new dataset and evaluation metrics, and an interactive editing agent.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.19641": {
        "authors": [
            "Ruimin Feng",
            "Xingxin He",
            "Ronald Mercer",
            "Zachary Stewart",
            "Fang Liu"
        ],
        "title": "On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction",
        "abstract": "arXiv:2511.19641v1 Announce Type: new  Abstract: Purpose: To investigate whether a vision-language foundation model can enhance undersampled MRI reconstruction by providing high-level contextual information beyond conventional priors. Methods: We proposed a semantic distribution-guided reconstruction framework that uses a pre-trained vision-language foundation model to encode both the reconstructed image and auxiliary information into high-level semantic features. A contrastive objective aligns the reconstructed representation with the target semantic distribution, ensuring consistency with high-level perceptual cues. The proposed objective works with various deep learning-based reconstruction methods and can flexibly incorporate semantic priors from multimodal sources. To test the effectiveness of these semantic priors, we evaluated reconstruction results guided by priors derived from either image-only or image-language auxiliary information. Results: Experiments on knee and brain datasets demonstrate that semantic priors from images preserve fine anatomical structures and achieve superior perceptual quality, as reflected in lower LPIPS values, higher Tenengrad scores, and improved scores in the reader study, compared with conventional regularization. The image-language information further expands the semantic distribution and enables high-level control over reconstruction attributes. Across all evaluations, the contrastive objective consistently guided the reconstructed features toward the desired semantic distributions while maintaining data fidelity, demonstrating the effectiveness of the proposed optimization framework. Conclusion: The study highlights that vision-language foundation models can improve undersampled MRI reconstruction through semantic-space optimization.",
        "arxiv_id": "2511.19641",
        "ARXIVID": "2511.19641",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and also involves vision-language models for medical imaging. Uses a vision-language foundation model to guide MRI reconstruction, showing a novel application of VLMs in medical imaging.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.19527": {
        "authors": [
            "Hongyu Lyu",
            "Thomas Monninger",
            "Julie Stephany Berrio Perez",
            "Mao Shan",
            "Zhenxing Ming",
            "Stewart Worrall"
        ],
        "title": "MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training",
        "abstract": "arXiv:2511.19527v1 Announce Type: new  Abstract: Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.",
        "arxiv_id": "2511.19527",
        "ARXIVID": "2511.19527",
        "COMMENT": "Matches criterion 3 (new benchmark/method for embodied AI, simulator-related). Proposes a weakly supervised, NeRF-guided self-training framework for online HD map construction for autonomous driving, reducing reliance on 3D annotations and improving scalability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20278": {
        "authors": [
            "Yinghui Li",
            "Qianyu Zhou",
            "Di Shao",
            "Hao Yang",
            "Ye Zhu",
            "Richard Dazeley",
            "Xuequan Lu"
        ],
        "title": "DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion",
        "abstract": "arXiv:2511.20278v1 Announce Type: new  Abstract: Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.",
        "arxiv_id": "2511.20278",
        "ARXIVID": "2511.20278",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models). Proposes a new domain adaptive point cloud completion method using State Space Models, with novel modules for spatial and semantic alignment, which is relevant for spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.19899": {
        "authors": [
            "Yuyi Li",
            "Daoyuan Chen",
            "Zhen Wang",
            "Yutong Lu",
            "Yaliang Li"
        ],
        "title": "VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering",
        "abstract": "arXiv:2511.19899v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.",
        "arxiv_id": "2511.19899",
        "ARXIVID": "2511.19899",
        "COMMENT": "Matches criterion 3 (new benchmark for vision-language models in scientific visual QA) and criterion 2 (focus on LVLMs for SVQA). Introduces a new, auto-verified dataset and a scalable data generation/verification framework for scientific visual question answering, which is a novel and challenging domain for VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20158": {
        "authors": [
            "Ziqi Wang",
            "Chang Che",
            "Qi Wang",
            "Hui Ma",
            "Zenglin Shi",
            "Cees G. M. Snoek",
            "Meng Wang"
        ],
        "title": "Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs",
        "abstract": "arXiv:2511.20158v1 Announce Type: new  Abstract: While continual visual instruction tuning (CVIT) has shown promise in adapting multimodal large language models (MLLMs), existing studies predominantly focus on models without safety alignment. This critical oversight ignores the fact that real-world MLLMs inherently require such mechanisms to mitigate potential risks. In this work, we shift our focus to CVIT for safety-aligned MLLMs and observe that during continual adaptation, the model not only suffers from task forgetting but also exhibits degradation in its safety. Achieving a harmonious balance between safety and task performance remains a crucial challenge. To address this, we propose Harmonious Parameter Adaptation (HPA), a post-training framework composed of focusing-based parameter partition, harmoniously balanced parameter selection, and orthogonal parameter adjustment. Specifically, HPA partitions parameters into two types based on their focus on safety or task performance, and selects the focused ones to preserve from a balanced perspective. In addition, HPA imposes orthogonality constraints on parameter updates to further alleviate catastrophic forgetting. Extensive experiments on the CVIT benchmark and safety evaluation datasets demonstrate that HPA better maintains high safety and mitigates forgetting than existing baselines.",
        "arxiv_id": "2511.20158",
        "ARXIVID": "2511.20158",
        "COMMENT": "Matches criterion 2 (focuses on continual visual instruction tuning for safety-aligned MLLMs) and criterion 4 (vision foundation models and their adaptation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20549": {
        "authors": [
            "Guanjie Chen",
            "Shirui Huang",
            "Kai Liu",
            "Jianchen Zhu",
            "Xiaoye Qu",
            "Peng Chen",
            "Yu Cheng",
            "Yifu Sun"
        ],
        "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
        "abstract": "arXiv:2511.20549v1 Announce Type: new  Abstract: Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
        "arxiv_id": "2511.20549",
        "ARXIVID": "2511.20549",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes Flash-DMD, a new framework for efficient, high-fidelity few-step image generation with distillation and joint RL, with clever statistical tricks for stable training.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20515": {
        "authors": [
            "Kuniaki Saito",
            "Risa Shinoda",
            "Shohei Tanaka",
            "Tosho Hirasawa",
            "Fumio Okura",
            "Yoshitaka Ushiku"
        ],
        "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
        "abstract": "arXiv:2511.20515v1 Announce Type: new  Abstract: Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.",
        "arxiv_id": "2511.20515",
        "ARXIVID": "2511.20515",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a new benchmark (AlignBench) for fine-grained image-text alignment, with surprising empirical findings about CLIP-based models and VLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20280": {
        "authors": [
            "Yang Liu",
            "Xilin Zhao",
            "Peisong Wen",
            "Siran Dai",
            "Qingming Huang"
        ],
        "title": "Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement",
        "abstract": "arXiv:2511.20280v1 Announce Type: new  Abstract: Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.",
        "arxiv_id": "2511.20280",
        "ARXIVID": "2511.20280",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes a novel, training-free, plug-and-play framework using VLMs for physics-aware video generation, with empirical improvements on a physics benchmark.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20201": {
        "authors": [
            "Dionysia Danai Brilli",
            "Dimitrios Mallis",
            "Vassilis Pitsikalis",
            "Petros Maragos"
        ],
        "title": "GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering",
        "abstract": "arXiv:2511.20201v1 Announce Type: new  Abstract: We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.",
        "arxiv_id": "2511.20201",
        "ARXIVID": "2511.20201",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI). Proposes a graph-guided hierarchical relational reasoning framework for video QA, focusing on human-object interactions and spatiotemporal dynamics, with significant improvements on a challenging dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.19846": {
        "authors": [
            "Thomas M Metz",
            "Matthew Q Hill",
            "Alice J O'Toole"
        ],
        "title": "Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum",
        "abstract": "arXiv:2511.19846v1 Announce Type: new  Abstract: Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.",
        "arxiv_id": "2511.19846",
        "ARXIVID": "2511.19846",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a unified embedding space for face, person, and object recognition using foundation models, with a novel interleaved multi-domain curriculum.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20651": {
        "authors": [
            "Xuelu Feng",
            "Yunsheng Li",
            "Ziyu Wan",
            "Zixuan Gao",
            "Junsong Yuan",
            "Dongdong Chen",
            "Chunming Qiao"
        ],
        "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
        "abstract": "arXiv:2511.20651v1 Announce Type: new  Abstract: Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
        "arxiv_id": "2511.20651",
        "ARXIVID": "2511.20651",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new RL-based reward design for text-to-image generation, with interpretable and composable rubrics, which is a clever statistical trick for generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.19882": {
        "authors": [
            "Lei Ding",
            "Tong Liu",
            "Xuanguang Liu",
            "Xiangyun Liu",
            "Haitao Guo",
            "Jun Lu"
        ],
        "title": "ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images",
        "abstract": "arXiv:2511.19882v1 Announce Type: new  Abstract: Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba.",
        "arxiv_id": "2511.19882",
        "ARXIVID": "2511.19882",
        "COMMENT": "Matches criterion 1 (spatial understanding improvements) and criterion 4 (vision foundation models and applications). Proposes a structure-aware state-space model for change detection in remote sensing, with novel interleaving and fusion strategies.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20306": {
        "authors": [
            "Han Guo",
            "Chenyang Liu",
            "Haotian Zhang",
            "Bowen Chen",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "title": "TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection",
        "abstract": "arXiv:2511.20306v1 Announce Type: new  Abstract: Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.",
        "arxiv_id": "2511.20306",
        "ARXIVID": "2511.20306",
        "COMMENT": "Matches criterion 1 (spatial understanding improvements) and criterion 4 (vision foundation models and applications). Proposes a new method for spatio-temporal semantic consistency in remote sensing change detection, using text-guided transition features.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20640": {
        "authors": [
            "Ryan Burgert",
            "Charles Herrmann",
            "Forrester Cole",
            "Michael S Ryoo",
            "Neal Wadhwa",
            "Andrey Voynov",
            "Nataniel Ruiz"
        ],
        "title": "MotionV2V: Editing Motion in a Video",
        "abstract": "arXiv:2511.20640v1 Announce Type: new  Abstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
        "arxiv_id": "2511.20640",
        "ARXIVID": "2511.20640",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a novel method for motion editing in videos using generative diffusion models and motion counterfactuals, with strong empirical results and a new editing paradigm.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.19953": {
        "authors": [
            "Wen Zhang",
            "Qin Ren",
            "Wenjing Liu",
            "Haibin Ling",
            "Chenyu You"
        ],
        "title": "Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting",
        "abstract": "arXiv:2511.19953v1 Announce Type: new  Abstract: Accurate nuclear instance segmentation is a pivotal task in computational pathology, supporting data-driven clinical insights and facilitating downstream translational applications. While large vision foundation models have shown promise for zero-shot biomedical segmentation, most existing approaches still depend on dense supervision and computationally expensive fine-tuning. Consequently, training-free methods present a compelling research direction, yet remain largely unexplored. In this work, we introduce SPROUT, a fully training- and annotation-free prompting framework for nuclear instance segmentation. SPROUT leverages histology-informed priors to construct slide-specific reference prototypes that mitigate domain gaps. These prototypes progressively guide feature alignment through a partial optimal transport scheme. The resulting foreground and background features are transformed into positive and negative point prompts, enabling the Segment Anything Model (SAM) to produce precise nuclear delineations without any parameter updates. Extensive experiments across multiple histopathology benchmarks demonstrate that SPROUT achieves competitive performance without supervision or retraining, establishing a novel paradigm for scalable, training-free nuclear instance segmentation in pathology.",
        "arxiv_id": "2511.19953",
        "ARXIVID": "2511.19953",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a training- and annotation-free prompting framework for nuclear instance segmentation using the Segment Anything Model (SAM), leveraging foundation models for zero-shot biomedical segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20123": {
        "authors": [
            "Min Zhao",
            "Hongzhou Zhu",
            "Yingze Wang",
            "Bokai Yan",
            "Jintao Zhang",
            "Guande He",
            "Ling Yang",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
        "abstract": "arXiv:2511.20123v1 Announce Type: new  Abstract: Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.",
        "arxiv_id": "2511.20123",
        "ARXIVID": "2511.20123",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes UltraViCo, a plug-and-play method for video diffusion transformers to improve video length extrapolation, with surprising empirical results.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20650": {
        "authors": [
            "Tooba Tehreem Sheikh",
            "Jean Lahoud",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Hisham Cholakkal"
        ],
        "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
        "abstract": "arXiv:2511.20650v1 Announce Type: new  Abstract: Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
        "arxiv_id": "2511.20650",
        "ARXIVID": "2511.20650",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces MedROV, an open-vocabulary detection model for medical imaging, leveraging large pre-trained foundation models and a new large-scale dataset.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.20307": {
        "authors": [
            "Shengqian Li",
            "Ming Gao",
            "Yi Liu",
            "Zuzeng Lin",
            "Feng Wang",
            "Feng Dai"
        ],
        "title": "TReFT: Taming Rectified Flow Models For One-Step Image Translation",
        "abstract": "arXiv:2511.20307v1 Announce Type: new  Abstract: Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.",
        "arxiv_id": "2511.20307",
        "ARXIVID": "2511.20307",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. Proposes a new method (TReFT) for one-step image translation using rectified flow models, with clever theoretical and empirical insights.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.19834": {
        "authors": [
            "Haoqing Li",
            "Jun Shi",
            "Xianmeng Chen",
            "Qiwei Jia",
            "Rui Wang",
            "Wei Wei",
            "Hong An",
            "Xiaowen Hu"
        ],
        "title": "Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation",
        "abstract": "arXiv:2511.19834v1 Announce Type: new  Abstract: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnosis via Computed Tomography (CT) imaging. While Multimodal Large Language Models (MLLMs) demonstrate diagnostic potential fo such rare diseases, the absence of domain-specific knowledge and referable radiological features intensify hallucination risks. To address this problem, we propose BHD-RAG, a multimodal retrieval-augmented generation framework that integrates DCLD-specific expertise and clinical precedents with MLLMs to improve BHD diagnostic accuracy. BHDRAG employs: (1) a specialized agent generating imaging manifestation descriptions of CT images to construct a multimodal corpus of DCLDs cases. (2) a cosine similarity-based retriever pinpointing relevant imagedescription pairs for query images, and (3) an MLLM synthesizing retrieved evidence with imaging data for diagnosis. BHD-RAG is validated on the dataset involving four types of DCLDs, achieving superior accuracy and generating evidence-based descriptions closely aligned with expert insights.",
        "arxiv_id": "2511.19834",
        "ARXIVID": "2511.19834",
        "COMMENT": "Matches criterion 2 (MLLMs for medical diagnosis) and criterion 4 (application of vision-language models in healthcare). Proposes a retrieval-augmented generation framework for rare disease diagnosis using MLLMs, with a novel multimodal corpus and retrieval mechanism.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.20629": {
        "authors": [
            "Chieh-Yun Chen",
            "Zhonghao Wang",
            "Qi Chen",
            "Zhifan Ye",
            "Min Shi",
            "Yue Zhao",
            "Yinan Zhao",
            "Hui Qu",
            "Wei-An Lin",
            "Yiru Shen",
            "Ajinkya Kale",
            "Irfan Essa",
            "Humphrey Shi"
        ],
        "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
        "abstract": "arXiv:2511.20629v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
        "arxiv_id": "2511.20629",
        "ARXIVID": "2511.20629",
        "COMMENT": "Matches criterion 2 (new methods for multi-modal generative models, including text-to-image, text-to-video, and language tasks). Introduces MapReduce LoRA and RaTE for multi-preference optimization in generative models, with strong empirical results across modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.20258": {
        "authors": [
            "Xiaohan Wang",
            "Zhangtao Cheng",
            "Ting Zhong",
            "Leiting Chen",
            "Fan Zhou"
        ],
        "title": "Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization",
        "abstract": "arXiv:2511.20258v1 Announce Type: new  Abstract: Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.",
        "arxiv_id": "2511.20258",
        "ARXIVID": "2511.20258",
        "COMMENT": "Matches criterion 2 (multi-modal large language models) and is relevant to multi-modal domain generalization. Proposes a new collaborative distillation framework for better modality fusion and generalization.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.19667": {
        "authors": [
            "Istiak Ahmed",
            "Galib Ahmed",
            "K. Shahriar Sanjid",
            "Md. Tanzim Hossain",
            "Md. Nishan Khan",
            "Md. Misbah Khan",
            "Md. Arifur Rahman",
            "Sheikh Anisul Haque",
            "Sharmin Akhtar Rupa",
            "Mohammed Mejbahuddin Mia",
            "Mahmud Hasan Mostofa Kamal",
            "Md. Mostafa Kamal Sarker",
            "M. Monir Uddin"
        ],
        "title": "OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis",
        "abstract": "arXiv:2511.19667v1 Announce Type: new  Abstract: OncoVision is a multimodal AI pipeline that combines mammography images and clinical data for better breast cancer diagnosis. Employing an attention-based encoder-decoder backbone, it jointly segments four ROIs - masses, calcifications, axillary findings, and breast tissues - with state-of-the-art accuracy and robustly predicts ten structured clinical features: mass morphology, calcification type, ACR breast density, and BI-RADS categories. To fuse imaging and clinical insights, we developed two late-fusion strategies. By utilizing complementary multimodal data, late fusion strategies improve diagnostic precision and reduce inter-observer variability. Operationalized as a secure, user-friendly web application, OncoVision produces structured reports with dual-confidence scoring and attention-weighted visualizations for real-time diagnostic support to improve clinician trust and facilitate medical teaching. It can be easily incorporated into the clinic, making screening available in underprivileged areas around the world, such as rural South Asia. Combining accurate segmentation with clinical intuition, OncoVision raises the bar for AI-based mammography, offering a scalable and equitable solution to detect breast cancer at an earlier stage and enhancing treatment through timely interventions.",
        "arxiv_id": "2511.19667",
        "ARXIVID": "2511.19667",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Presents a multimodal AI pipeline for breast cancer diagnosis, integrating mammography and clinical data with attention-based fusion and segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2511.19512": {
        "authors": [
            "Huanning Dong",
            "Yinuo Huang",
            "Fan Li",
            "Ping Kuang"
        ],
        "title": "Single Image to High-Quality 3D Object via Latent Features",
        "abstract": "arXiv:2511.19512v1 Announce Type: new  Abstract: 3D assets are essential in the digital age. While automatic 3D generation, such as image-to-3d, has made significant strides in recent years, it often struggles to achieve fast, detailed, and high-fidelity generation simultaneously. In this work, we introduce LatentDreamer, a novel framework for generating 3D objects from single images. The key to our approach is a pre-trained variational autoencoder that maps 3D geometries to latent features, which greatly reducing the difficulty of 3D generation. Starting from latent features, the pipeline of LatentDreamer generates coarse geometries, refined geometries, and realistic textures sequentially. The 3D objects generated by LatentDreamer exhibit high fidelity to the input images, and the entire generation process can be completed within a short time (typically in 70 seconds). Extensive experiments show that with only a small amount of training, LatentDreamer demonstrates competitive performance compared to contemporary approachs.",
        "arxiv_id": "2511.19512",
        "ARXIVID": "2511.19512",
        "COMMENT": "Related to vision foundation models and generative modeling (single image to 3D object), but not directly about VLLMs/MLLMs or spatial reasoning in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20263": {
        "authors": [
            "Omer Belhasin",
            "Shelly Golan",
            "Ran El-Yaniv",
            "Michael Elad"
        ],
        "title": "Advancing Image Classification with Discrete Diffusion Classification Modeling",
        "abstract": "arXiv:2511.20263v1 Announce Type: new  Abstract: Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .",
        "arxiv_id": "2511.20263",
        "ARXIVID": "2511.20263",
        "COMMENT": "Relevant to generative modeling in vision (diffusion models for classification), but does not directly match any specific criterion (no spatial, VLLM/MLLM, or embodied AI focus).",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2511.19889": {
        "authors": [
            "Zhe Liu",
            "Kai Han",
            "Siqi Ma",
            "Yan Zhu",
            "Jun Chen",
            "Chongwen Lyu",
            "Xinyi Qiu",
            "Chengxuan Qian",
            "Yuqing Song",
            "Yi Liu",
            "Liyuan Tian",
            "Yang Ji",
            "Yuefeng Li"
        ],
        "title": "LiMT: A Multi-task Liver Image Benchmark Dataset",
        "abstract": "arXiv:2511.19889v1 Announce Type: new  Abstract: Computer-aided diagnosis (CAD) technology can assist clinicians in evaluating liver lesions and intervening with treatment in time. Although CAD technology has advanced in recent years, the application scope of existing datasets remains relatively limited, typically supporting only single tasks, which has somewhat constrained the development of CAD technology. To address the above limitation, in this paper, we construct a multi-task liver dataset (LiMT) used for liver and tumor segmentation, multi-label lesion classification, and lesion detection based on arterial phase-enhanced computed tomography (CT), potentially providing an exploratory solution that is able to explore the correlation between tasks and does not need to worry about the heterogeneity between task-specific datasets during training. The dataset includes CT volumes from 150 different cases, comprising four types of liver diseases as well as normal cases. Each volume has been carefully annotated and calibrated by experienced clinicians. This public multi-task dataset may become a valuable resource for the medical imaging research community in the future. In addition, this paper not only provides relevant baseline experimental results but also reviews existing datasets and methods related to liver-related tasks. Our dataset is available at https://drive.google.com/drive/folders/1l9HRK13uaOQTNShf5pwgSz3OTanWjkag?usp=sharing.",
        "arxiv_id": "2511.19889",
        "ARXIVID": "2511.19889",
        "COMMENT": "Matches criterion 3 (embodied AI/new benchmark) in the sense of providing a new multi-task medical imaging benchmark, but not directly simulator-related or spatial intelligence. Still, relevant for vision foundation model applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20045": {
        "authors": [
            "Huijia Zhao",
            "Jie Lu",
            "Yunqing Jiang",
            "Xiao-Ping Lu",
            "Kaichang Di"
        ],
        "title": "History-Augmented Contrastive Meta-Learning for Unsupervised Blind Super-Resolution of Planetary Remote Sensing Images",
        "abstract": "arXiv:2511.20045v1 Announce Type: new  Abstract: Planetary remote sensing images are affected by diverse and unknown degradations caused by imaging environments and hardware constraints. These factors limit image quality and hinder supervised blind super-resolution due to the lack of ground-truth images. This work presents History-Augmented Contrastive Blind Super-Resolution (HACBSR), an unsupervised framework for blind super-resolution that operates without ground-truth images and external kernel priors. HACBSR comprises two components: (1) a contrastive kernel sampling mechanism with kernel similarity control to mitigate distribution bias from Gaussian sampling, and (2) a history-augmented contrastive learning that uses historical models to generate negative samples to enable less greedy optimization and to induce strong convexity without ground-truth. A convergence analysis of the history-augmented contrastive learning is given in the Appendix. To support evaluation in planetary applications, we introduce Ceres-50, a dataset with diverse geological features simulated degradation patterns. Experiments show that HACBSR achieves competitive performance compared with state-of-the-art unsupervised methods across multiple upscaling factors. The code is available at https://github.com/2333repeat/HACBSR, and the dataset is available at https://github.com/2333repeat/Ceres-50.",
        "arxiv_id": "2511.20045",
        "ARXIVID": "2511.20045",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to general interest in computer vision and unsupervised learning. Proposes a new unsupervised blind super-resolution method for planetary images, with a new dataset and contrastive meta-learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.18715": {
        "authors": [
            "Shaoyin Ma",
            "Jie Song",
            "Huiqiong Wang",
            "Li Sun",
            "Mingli Song"
        ],
        "title": "HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions",
        "abstract": "arXiv:2511.18715v1 Announce Type: new  Abstract: Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.",
        "arxiv_id": "2511.18715",
        "ARXIVID": "2511.18715",
        "COMMENT": "Related to multimodal model selection and reasoning, but not directly about VLLMs/MLLMs themselves or spatial/embodied AI; more about LLM agent tool selection.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.20145": {
        "authors": [
            "Wenpei Jiao",
            "Kun Shang",
            "Hui Li",
            "Ke Yan",
            "Jiajin Zhang",
            "Guangjie Yang",
            "Lijuan Guo",
            "Yan Wan",
            "Xing Yang",
            "Dakai Jin",
            "Zhaoheng Xie"
        ],
        "title": "Vision-Language Models for Automated 3D PET/CT Report Generation",
        "abstract": "arXiv:2511.20145v1 Announce Type: new  Abstract: Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.",
        "arxiv_id": "2511.20145",
        "ARXIVID": "2511.20145",
        "COMMENT": "Applies vision-language models to 3D PET/CT report generation, introducing a new dataset and evaluation protocol. While it uses VLMs, the focus is on medical report generation rather than new VLLMs/MLLMs or spatial intelligence in embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.18874": {
        "authors": [
            "Yuzhi Chen",
            "Yuanchang Xie",
            "Lei Zhao",
            "Pan Liu",
            "Yajie Zou",
            "Chen Wang"
        ],
        "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction",
        "abstract": "arXiv:2511.18874v1 Announce Type: new  Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.",
        "arxiv_id": "2511.18874",
        "ARXIVID": "2511.18874",
        "COMMENT": "Presents a new transformer-based architecture for multimodal trajectory prediction, focusing on spatial reasoning and context. Some overlap with criterion 1 (spatial understanding), but not directly about embodied agents or vision-language models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.18793": {
        "authors": [
            "Yejing Wang",
            "Shengyu Zhou",
            "Jinyu Lu",
            "Ziwei Liu",
            "Langming Liu",
            "Maolin Wang",
            "Wenlin Zhang",
            "Feng Li",
            "Wenbo Su",
            "Pengjie Wang",
            "Jian Xu",
            "Xiangyu Zhao"
        ],
        "title": "NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations",
        "abstract": "arXiv:2511.18793v1 Announce Type: new  Abstract: Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.",
        "arxiv_id": "2511.18793",
        "ARXIVID": "2511.18793",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to general interest in generative modeling and LLMs. Focuses on hyperspeed decoding for generative recommendation systems, with some clever engineering for inference speed and hallucination reduction.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.20615": {
        "authors": [
            "Seyede Niloofar Hosseini",
            "Ali Mojibi",
            "Mahdi Mohseni",
            "Navid Arjmand",
            "Alireza Taheri"
        ],
        "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
        "abstract": "arXiv:2511.20615v1 Announce Type: new  Abstract: This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
        "arxiv_id": "2511.20615",
        "ARXIVID": "2511.20615",
        "COMMENT": "Somewhat related to spatial understanding (3D posture prediction), but not focused on embodied agents or new spatial intelligence methods; more about time-series prediction in biomechanics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19752": {
        "authors": [
            "Muchang Bahng",
            "Charlie Berens",
            "Jon Donnelly",
            "Eric Chen",
            "Chaofan Chen",
            "Cynthia Rudin"
        ],
        "title": "What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities",
        "abstract": "arXiv:2511.19752v1 Announce Type: new  Abstract: Species detection is important for monitoring the health of ecosystems and identifying invasive species, serving a crucial role in guiding conservation efforts. Multimodal neural networks have seen increasing use for identifying species to help automate this task, but they have two major drawbacks. First, their black-box nature prevents the interpretability of their decision making process. Second, collecting genetic data is often expensive and requires invasive procedures, often necessitating researchers to capture or kill the target specimen. We address both of these problems by extending prototype networks (ProtoPNets), which are a popular and interpretable alternative to traditional neural networks, to the multimodal, cost-aware setting. We ensemble prototypes from each modality, using an associated weight to determine how much a given prediction relies on each modality. We further introduce methods to identify cases for which we do not need the expensive genetic information to make confident predictions. We demonstrate that our approach can intelligently allocate expensive genetic data for fine-grained distinctions while using abundant image data for clearer visual classifications and achieving comparable accuracy to models that consistently use both modalities.",
        "arxiv_id": "2511.19752",
        "ARXIVID": "2511.19752",
        "COMMENT": "Extends interpretable prototype networks to multimodal, cost-aware settings for species detection. While it involves multimodal learning, it does not introduce new VLLMs/MLLMs or focus on spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19765": {
        "authors": [
            "Ali Torabi",
            "Sanjog Gaihre",
            "Yaqoob Majeed"
        ],
        "title": "Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation",
        "abstract": "arXiv:2511.19765v1 Announce Type: new  Abstract: Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.",
        "arxiv_id": "2511.19765",
        "ARXIVID": "2511.19765",
        "COMMENT": "Primarily about weakly supervised semantic segmentation with transformer improvements. While relevant to computer vision, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}