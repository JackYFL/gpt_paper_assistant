{
    "2602.14941": {
        "authors": [
            "Zun Wang",
            "Han Lin",
            "Jaehong Yoon",
            "Jaemin Cho",
            "Yue Zhang",
            "Mohit Bansal"
        ],
        "title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
        "abstract": "arXiv:2602.14941v1 Announce Type: new  Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.",
        "arxiv_id": "2602.14941",
        "ARXIVID": "2602.14941",
        "COMMENT": "Matches criterion 1 and 4: Introduces AnchorWeave, a memory-augmented video generation framework for world-consistent spatial memory, addressing spatial consistency in video generation with local geometric memories. Strong relevance to spatial understanding and vision foundation model applications.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.14965": {
        "authors": [
            "Qingming Liu",
            "Xinyue Yao",
            "Shuyuan Zhang",
            "Yueci Deng",
            "Guiliang Liu",
            "Zhen Liu",
            "Kui Jia"
        ],
        "title": "PAct: Part-Decomposed Single-View Articulated Object Generation",
        "abstract": "arXiv:2602.14965v1 Announce Type: new  Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.",
        "arxiv_id": "2602.14965",
        "ARXIVID": "2602.14965",
        "COMMENT": "Matches criterion 1 (new method for spatial understanding and articulated object generation for embodied agents) and criterion 3 (novel generative method for articulated 3D assets relevant to embodied AI). The part-centric generative framework is a methodological improvement.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.14837": {
        "authors": [
            "Lorenzo Mur Labadia",
            "Ruben Martinez-Cantin",
            "Jose J. Guerrero",
            "Giovanni M. Farinella",
            "Antonino Furnari"
        ],
        "title": "Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation",
        "abstract": "arXiv:2602.14837v1 Announce Type: new  Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.",
        "arxiv_id": "2602.14837",
        "ARXIVID": "2602.14837",
        "COMMENT": "Matches criterion 1 and 3: Proposes new attention-based architectures and affordance models for short-term object interaction anticipation in egocentric video, with new modules and significant improvements on benchmarks. Focus on spatial intelligence for embodied agents and new methods for embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.13823": {
        "authors": [
            "Haonan Jiang",
            "Yuji Wang",
            "Yongjie Zhu",
            "Xin Lu",
            "Wenyu Qin",
            "Meng Wang",
            "Pengfei Wan",
            "Yansong Tang"
        ],
        "title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
        "abstract": "arXiv:2602.13823v1 Announce Type: new  Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.",
        "arxiv_id": "2602.13823",
        "ARXIVID": "2602.13823",
        "COMMENT": "Matches criterion 2: Proposes a new framework for reasoning-driven multimodal embeddings using MLLMs and reinforcement learning, with novel T-CoT traces and outperforming prior embedding models. Directly relevant to new MLLMs and clever statistical tricks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.13859": {
        "authors": [
            "Max Wolff",
            "Thomas Klein",
            "Evgenia Rusak",
            "Felix Wichmann",
            "Wieland Brendel"
        ],
        "title": "Low-Pass Filtering Improves Behavioral Alignment of Vision Models",
        "abstract": "arXiv:2602.13859v1 Announce Type: new  Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \\emph{generative} -- rather than \\emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.   Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.   We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.",
        "arxiv_id": "2602.13859",
        "ARXIVID": "2602.13859",
        "COMMENT": "Matches criterion 4: Provides surprising empirical results on how low-pass filtering improves behavioral alignment of vision models with humans, with insights into the frequency spectrum and model-vs-human benchmarks. This is a clever statistical/empirical trick with implications for vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.13347": {
        "authors": [
            "Lijun Zhang",
            "Nikhil Chacko",
            "Petter Nilsson",
            "Ruinian Xu",
            "Shantanu Thakar",
            "Bai Lou",
            "Harpreet Sawhney",
            "Zhebin Zhang",
            "Mudit Agrawal",
            "Bhavana Chandrashekhar",
            "Aaron Parness"
        ],
        "title": "Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots",
        "abstract": "arXiv:2602.13347v1 Announce Type: new  Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.",
        "arxiv_id": "2602.13347",
        "ARXIVID": "2602.13347",
        "COMMENT": "Matches criterion 1 and 3: Proposes FOREST, a diffusion-based world model for visual foresight in robotic stow tasks, improving spatial prediction in embodied agents and providing a new method for world modeling in robotic simulation. This is a novel approach to spatial intelligence and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.14857": {
        "authors": [
            "Yixin Zhang",
            "Ziyi Wang",
            "Yiming Rong",
            "Haoxi Wang",
            "Jinling Jiang",
            "Shuang Xu",
            "Haoran Wu",
            "Shiyu Zhou",
            "Bo Xu"
        ],
        "title": "World Models for Policy Refinement in StarCraft II",
        "abstract": "arXiv:2602.14857v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.",
        "arxiv_id": "2602.14857",
        "ARXIVID": "2602.14857",
        "COMMENT": "Matches criterion 3: Proposes StarWM, a world model for policy refinement in StarCraft II, with a new dataset and evaluation framework. Focuses on embodied AI in a complex simulator with a novel world-model-based approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.14879": {
        "authors": [
            "Qingqing Zhu",
            "Qiao Jin",
            "Tejas S. Mathai",
            "Yin Fang",
            "Zhizheng Wang",
            "Yifan Yang",
            "Maame Sarfo-Gyamfi",
            "Benjamin Hou",
            "Ran Gu",
            "Praveen T. S. Balamuralikrishna",
            "Kenneth C. Wang",
            "Ronald M. Summers",
            "Zhiyong Lu"
        ],
        "title": "CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography",
        "abstract": "arXiv:2602.14879v1 Announce Type: new  Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.",
        "arxiv_id": "2602.14879",
        "ARXIVID": "2602.14879",
        "COMMENT": "Matches criterion 3 and 4: Introduces CT-Bench, a new multimodal benchmark for lesion understanding in CT, and evaluates vision-language models and medical CLIP variants. This is a new benchmark and relevant to vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.14721": {
        "authors": [
            "Zikai Xiao",
            "Jianhong Tu",
            "Chuhang Zou",
            "Yuxin Zuo",
            "Zhi Li",
            "Peng Wang",
            "Bowen Yu",
            "Fei Huang",
            "Junyang Lin",
            "Zuozhu Liu"
        ],
        "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
        "abstract": "arXiv:2602.14721v1 Announce Type: new  Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \\textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
        "arxiv_id": "2602.14721",
        "ARXIVID": "2602.14721",
        "COMMENT": "WebWorld is a large-scale open-web simulator for web agent training, with a new benchmark (WebWorld-Bench) and cross-domain generalization. This matches criterion 3 (embodied AI papers on new benchmarks/simulators with novel angles), as it provides a scalable, open-web simulator and evaluation suite.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.14042": {
        "authors": [
            "Kai Guan",
            "Rongyuan Wu",
            "Shuai Li",
            "Wentao Zhu",
            "Wenjun Zeng",
            "Lei Zhang"
        ],
        "title": "Restoration Adaptation for Semantic Segmentation on Low Quality Images",
        "abstract": "arXiv:2602.14042v1 Announce Type: new  Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.",
        "arxiv_id": "2602.14042",
        "ARXIVID": "2602.14042",
        "COMMENT": "Matches criterion 4: Proposes a new method (RASS) for integrating semantic restoration and segmentation, with a novel semantic-constrained restoration model and LoRA-based module merging. Applies to vision foundation models and their robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.14083": {
        "authors": [
            "Weiming Zhang",
            "Jihong Wang",
            "Jiamu Zhou",
            "Qingyao Li",
            "Xinbei Ma",
            "Congmin Zheng",
            "Xingyu Lou",
            "Weiwen Liu",
            "Zhuosheng Zhang",
            "Jun Wang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "title": "Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation",
        "abstract": "arXiv:2602.14083v1 Announce Type: new  Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.",
        "arxiv_id": "2602.14083",
        "ARXIVID": "2602.14083",
        "COMMENT": "Matches criterion 3: Proposes a novel method (Plan-MCTS) for web navigation agents, introducing a new plan-space exploration and abstracted semantic history, which is a novel angle for embodied AI agents in web environments.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.14401": {
        "authors": [
            "Qingqian Yang",
            "Hao Wang",
            "Sai Qian Zhang",
            "Jian Li",
            "Yang Hua",
            "Miao Pan",
            "Tao Song",
            "Zhengwei Qi",
            "Haibing Guan"
        ],
        "title": "pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI",
        "abstract": "arXiv:2602.14401v1 Announce Type: new  Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.",
        "arxiv_id": "2602.14401",
        "ARXIVID": "2602.14401",
        "COMMENT": "Matches criterion 3 (new method for federated vision-language navigation in embodied AI). The structure-aware personalized federated learning for VLN is a novel angle for privacy-preserving spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13305": {
        "authors": [
            "Aydin Ayanzadeh",
            "Prakhar Dixit",
            "Sadia Kamal",
            "Milton Halem"
        ],
        "title": "WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery",
        "abstract": "arXiv:2602.13305v1 Announce Type: new  Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.",
        "arxiv_id": "2602.13305",
        "ARXIVID": "2602.13305",
        "COMMENT": "Matches criterion 2 (MLLMs for satellite imagery and risk assessment) and criterion 4 (application of vision foundation models for real-world monitoring). The integration of YOLOv12 and MLLMs for contextualized risk assessment is a novel application in environmental monitoring.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13507": {
        "authors": [
            "Md Saiful Islam",
            "Ekram Hossain",
            "Abdelrahman Abdelkader",
            "Tariq Adnan",
            "Fazla Rabbi Mashrur",
            "Sooyong Park",
            "Praveen Kumar",
            "Qasim Sudais",
            "Natalia Chunga",
            "Nami Shah",
            "Jan Freyberg",
            "Christopher Kanan",
            "Ruth Schneider",
            "Ehsan Hoque"
        ],
        "title": "Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening",
        "abstract": "arXiv:2602.13507v1 Announce Type: new  Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\\_video\\_benchmarking-A2C5",
        "arxiv_id": "2602.13507",
        "ARXIVID": "2602.13507",
        "COMMENT": "Matches criterion 4: Systematic benchmarking of video foundation models (VFMs) for Parkinson's disease screening, with surprising empirical findings about model-task alignment. Strong application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13588": {
        "authors": [
            "Guanfeng Tang",
            "Hongbo Zhao",
            "Ziwei Long",
            "Jiayao Li",
            "Bohong Xiao",
            "Wei Ye",
            "Hanli Wang",
            "Rui Fan"
        ],
        "title": "Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks",
        "abstract": "arXiv:2602.13588v1 Announce Type: new  Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.",
        "arxiv_id": "2602.13588",
        "ARXIVID": "2602.13588",
        "COMMENT": "Matches criterion 1: Proposes TwInS, a bio-inspired two-stream architecture for joint scene parsing and geometric vision, with novel cross-task adapters and semi-supervised training. Strong focus on spatial understanding and intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.14518": {
        "authors": [
            "Jing Tang",
            "Kun Wang",
            "Haolang Lu",
            "Hongjin Chen",
            "KaiTao Chen",
            "Zhongxiang Sun",
            "Qiankun Li",
            "Lingjuan Lyu",
            "Guoshun Nan",
            "Zhigang Zeng"
        ],
        "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
        "abstract": "arXiv:2602.14518v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.",
        "arxiv_id": "2602.14518",
        "ARXIVID": "2602.14518",
        "COMMENT": "Matches criterion 2: Analyzes knowledge conflict in multimodal large language models (MLLMs) during long-chain reasoning, providing new insights into their internal representations and failure modes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13515": {
        "authors": [
            "Jintao Zhang",
            "Kai Jiang",
            "Chendong Xiang",
            "Weiqi Feng",
            "Yuezhou Hu",
            "Haocheng Xi",
            "Jianfei Chen",
            "Jun Zhu"
        ],
        "title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning",
        "abstract": "arXiv:2602.13515v1 Announce Type: new  Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.",
        "arxiv_id": "2602.13515",
        "ARXIVID": "2602.13515",
        "COMMENT": "Matches criterion 4: Introduces SpargeAttention2, a new trainable sparse attention method for diffusion models, with hybrid masking and distillation fine-tuning. This is a methodological improvement for vision foundation models, especially in generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13555": {
        "authors": [
            "Song Wang",
            "Lingling Li",
            "Marcus Santos",
            "Guanghui Wang"
        ],
        "title": "Privacy-Concealing Cooperative Perception for BEV Scene Segmentation",
        "abstract": "arXiv:2602.13555v1 Announce Type: new  Abstract: Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.",
        "arxiv_id": "2602.13555",
        "ARXIVID": "2602.13555",
        "COMMENT": "Matches criterion 1: Proposes a new privacy-concealing cooperative perception framework for BEV scene segmentation in autonomous driving, with a novel adversarial learning mechanism to hide visual clues while maintaining segmentation performance. This is a methodological improvement to spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13772": {
        "authors": [
            "Xiaoyu Li",
            "Yitao Wu",
            "Xian Wu",
            "Haolin Zhuo",
            "Lijun Zhao",
            "Lining Sun"
        ],
        "title": "Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking",
        "abstract": "arXiv:2602.13772v1 Announce Type: new  Abstract: Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.",
        "arxiv_id": "2602.13772",
        "ARXIVID": "2602.13772",
        "COMMENT": "Offline-Poly is a new offline 3D multi-object tracking framework with a tracking-centric design and a standardized Tracking-by-Tracking paradigm, decoupling offline tracking from upstream detectors. This is relevant to spatial understanding and intelligence in embodied agents (criterion 1), as well as new methods for embodied AI (criterion 3), due to its novel offline tracking approach.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.14186": {
        "authors": [
            "Hongyang Wei",
            "Bin Wen",
            "Yancheng Long",
            "Yankai Yang",
            "Yuhang Hu",
            "Tianke Zhang",
            "Wei Chen",
            "Haonan Fan",
            "Kaiyu Jiang",
            "Jiankang Chen",
            "Changyi Liu",
            "Kaiyu Tang",
            "Haojie Ding",
            "Xiao Yang",
            "Jia Sun",
            "Huaiqing Wang",
            "Zhenyu Yang",
            "Xinyu Wei",
            "Xianglong He",
            "Yangguang Li",
            "Fan Yang",
            "Tingting Gao",
            "Lei Zhang",
            "Guorui Zhou",
            "Han Li"
        ],
        "title": "UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing",
        "abstract": "arXiv:2602.14186v1 Announce Type: new  Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.",
        "arxiv_id": "2602.14186",
        "ARXIVID": "2602.14186",
        "COMMENT": "This paper introduces UniRef-Image-Edit, a multi-modal generation system for scalable and consistent multi-reference image editing, with a new input representation (SELF), a two-stage SFT+RL training, and a novel RL framework (MSGRPO) for multi-reference image generation. This matches criterion 4 (vision foundation models and applications) and is also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.13633": {
        "authors": [
            "Kanggil Park",
            "Yongjun Jeon",
            "Soyoung Lim",
            "Seonmin Park",
            "Jongmin Shin",
            "Jung Yong Kim",
            "Sehyeon An",
            "Jinsoo Rhu",
            "Jongman Kim",
            "Gyu-Seong Choi",
            "Namkee Oh",
            "Kyu-Hwan Jung"
        ],
        "title": "A generalizable foundation model for intraoperative understanding across surgical procedures",
        "abstract": "arXiv:2602.13633v1 Announce Type: new  Abstract: In minimally invasive surgery, clinical decisions depend on real-time visual interpretation, yet intraoperative perception varies substantially across surgeons and procedures. This variability limits consistent assessment, training, and the development of reliable artificial intelligence systems, as most surgical AI models are designed for narrowly defined tasks and do not generalize across procedures or institutions. Here we introduce ZEN, a generalizable foundation model for intraoperative surgical video understanding trained on more than 4 million frames from over 21 procedures using a self-supervised multi-teacher distillation framework. We curated a large and diverse dataset and systematically evaluated multiple representation learning strategies within a unified benchmark. Across 20 downstream tasks and full fine-tuning, frozen-backbone, few-shot and zero-shot settings, ZEN consistently outperforms existing surgical foundation models and demonstrates robust cross-procedure generalization. These results suggest a step toward unified representations for surgical scene understanding and support future applications in intraoperative assistance and surgical training assessment.",
        "arxiv_id": "2602.13633",
        "ARXIVID": "2602.13633",
        "COMMENT": "Matches criterion 4 (vision foundation model for surgical video understanding and generalization across procedures). The ZEN model is a generalizable foundation model for intraoperative understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.14425": {
        "authors": [
            "Yong Li",
            "Yi Ren",
            "Yizhe Zhang",
            "Wenhua Zhang",
            "Tianyi Zhang",
            "Muyun Jiang",
            "Guo-Sen Xie",
            "Cuntai Guan"
        ],
        "title": "Hierarchical Vision-Language Interaction for Facial Action Unit Detection",
        "abstract": "arXiv:2602.14425v1 Announce Type: new  Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.",
        "arxiv_id": "2602.14425",
        "ARXIVID": "2602.14425",
        "COMMENT": "Matches criterion 2 (vision-language models for facial action unit detection) and criterion 4 (cross-modal learning for vision foundation models). The use of hierarchical cross-modal attention and LLM-generated AU descriptions is a methodological improvement.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2602.14376": {
        "authors": [
            "Yuliang Wu",
            "Wei Zhai",
            "Yuxin Cui",
            "Tiesong Zhao",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "Event-based Visual Deformation Measurement",
        "abstract": "arXiv:2602.14376v1 Announce Type: new  Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.",
        "arxiv_id": "2602.14376",
        "ARXIVID": "2602.14376",
        "COMMENT": "Matches criterion 1: Presents a new event-frame fusion framework for visual deformation measurement, with a novel Affine Invariant Simplicial (AIS) framework and a new benchmark dataset. The method improves spatial understanding in dynamic scenes, which is relevant for embodied agents and spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.13372": {
        "authors": [
            "Simon Rosen",
            "Siddarth Singh",
            "Ebenezer Gelo",
            "Helen Sarah Robertson",
            "Ibrahim Suder",
            "Victoria Williams",
            "Benjamin Rosman",
            "Geraud Nangue Tasse",
            "Steven James"
        ],
        "title": "MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents",
        "abstract": "arXiv:2602.13372v1 Announce Type: new  Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.",
        "arxiv_id": "2602.13372",
        "ARXIVID": "2602.13372",
        "COMMENT": "Matches criterion 3: Introduces MoralityGym, a new benchmark for evaluating hierarchical moral alignment in sequential decision-making agents, which is a novel simulator-related benchmark for embodied AI from an ethical decision-making perspective.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.13303": {
        "authors": [
            "Nicolas Bourriez",
            "Alexandre Verine",
            "Auguste Genovesio"
        ],
        "title": "Spectral Collapse in Diffusion Inversion",
        "abstract": "arXiv:2602.13303v1 Announce Type: new  Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.",
        "arxiv_id": "2602.13303",
        "ARXIVID": "2602.13303",
        "COMMENT": "Matches criterion 4: Proposes Orthogonal Variance Guidance (OVG), an inference-time method to address spectral collapse in diffusion inversion for image-to-image translation, improving generative modeling in vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.14234": {
        "authors": [
            "Zheng Chu",
            "Xiao Wang",
            "Jack Hong",
            "Huiming Fan",
            "Yuqi Huang",
            "Yue Yang",
            "Guohai Xu",
            "Chenxiao Zhao",
            "Cheng Xiang",
            "Shengchao Hu",
            "Dongdong Kuang",
            "Ming Liu",
            "Bing Qin",
            "Xing Yu"
        ],
        "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
        "abstract": "arXiv:2602.14234v1 Announce Type: new  Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
        "arxiv_id": "2602.14234",
        "ARXIVID": "2602.14234",
        "COMMENT": "Partially matches criterion 3 (simulator-related benchmark for long-horizon search agents) and criterion 2 (multimodal search agents). The local simulated environment and large-scale trajectory release are relevant for embodied AI benchmarking.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.13693": {
        "authors": [
            "Xin Zhang",
            "Liangxiu Han",
            "Yue Shi",
            "Yalin Zheng",
            "Uazman Alam",
            "Maryam Ferdousi",
            "Rayaz Malik"
        ],
        "title": "A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy",
        "abstract": "arXiv:2602.13693v1 Announce Type: new  Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fr\\'echet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.",
        "arxiv_id": "2602.13693",
        "ARXIVID": "2602.13693",
        "COMMENT": "Matches criterion 4: Presents a new parameter-efficient fine-tuning (WDLoRA) for foundation generative models in medical imaging, enabling multimodal synthesis with clinical guidance. The work is about vision foundation models and their application to medical image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.13658": {
        "authors": [
            "Armin Saadat",
            "Nima Hashemi",
            "Bahar Khodabakhshian",
            "Michael Y. Tsang",
            "Christina Luong",
            "Teresa S. M. Tsang",
            "Purang Abolmaesumi"
        ],
        "title": "Optimizing Point-of-Care Ultrasound Video Acquisition for Probabilistic Multi-Task Heart Failure Detection",
        "abstract": "arXiv:2602.13658v1 Announce Type: new  Abstract: Purpose: Echocardiography with point-of-care ultrasound (POCUS) must support clinical decision-making under tight bedside time and operator-effort constraints. We introduce a personalized data acquisition strategy in which an RL agent, given a partially observed multi-view study, selects the next view to acquire or terminates acquisition to support heart-failure (HF) assessment. Upon termination, a diagnostic model jointly predicts aortic stenosis (AS) severity and left ventricular ejection fraction (LVEF), two key HF biomarkers, and outputs uncertainty, enabling an explicit trade-off between diagnostic performance and acquisition cost. Methods: We model POCUS as a sequential acquisition problem: at each step, a video selector (RL agent) chooses the next view to acquire or terminates acquisition. Upon termination, a shared multi-view transformer performs multi-task inference with two heads, ordinal AS classification, and LVEF regression, and outputs Gaussian predictive distributions yielding ordinal probabilities over AS classes and EF thresholds. These probabilities drive a reward that balances expected diagnostic benefit against acquisition cost, producing patient-specific acquisition pathways. Results: The dataset comprises 12,180 patient-level studies, split into training/validation/test sets (75/15/15). On the 1,820 test studies, our method matches full-study performance while using 32% fewer videos, achieving 77.2% mean balanced accuracy (bACC) across AS severity classification and LVEF estimation, demonstrating robust multi-task performance under acquisition budgets. Conclusion: Patient-tailored, cost-aware acquisition can streamline POCUS workflows while preserving decision quality, producing interpretable scan pathways suited to bedside use. The framework is extensible to additional cardiac endpoints and merits prospective evaluation for clinical integration.",
        "arxiv_id": "2602.13658",
        "ARXIVID": "2602.13658",
        "COMMENT": "Somewhat related to criterion 1: Uses RL agent for sequential view selection in ultrasound, which involves spatial reasoning in a medical embodied setting. Also uses multi-task learning and uncertainty modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.14157": {
        "authors": [
            "Ahmed Ghorbel",
            "Badr Moufad",
            "Navid Bagheri Shouraki",
            "Alain Oliviero Durmus",
            "Thomas Hirtz",
            "Eric Moulines",
            "Jimmy Olsson",
            "Yazid Janati"
        ],
        "title": "When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance",
        "abstract": "arXiv:2602.14157v1 Announce Type: new  Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.",
        "arxiv_id": "2602.14157",
        "ARXIVID": "2602.14157",
        "COMMENT": "This paper studies test-time guidance for diffusion models in image and video editing, showing that VJP-free approximations can match or surpass training-based methods. It is relevant to generative modeling in vision (criterion 4), especially for fast image/video editing with diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.13780": {
        "authors": [
            "Hengtong Shen",
            "Li Yan",
            "Hong Xie",
            "Yaxuan Wei",
            "Xinhao Li",
            "Wenfei Shen",
            "Peixian Lv",
            "Fei Tan"
        ],
        "title": "Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery",
        "abstract": "arXiv:2602.13780v1 Announce Type: new  Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.",
        "arxiv_id": "2602.13780",
        "ARXIVID": "2602.13780",
        "COMMENT": "This paper proposes PerASCD, a semantic change detection method for remote sensing imagery, driven by a remote sensing foundation model (PerA), with a new decoder and loss function. It matches criterion 4 (vision foundation models and applications), especially in remote sensing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.13318": {
        "authors": [
            "Daesik Jang",
            "Morgan Lindsay Heisler",
            "Linzi Xing",
            "Yifei Li",
            "Edward Wang",
            "Ying Xiong",
            "Yong Zhang",
            "Zhenan Fan"
        ],
        "title": "DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing",
        "abstract": "arXiv:2602.13318v1 Announce Type: new  Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .",
        "arxiv_id": "2602.13318",
        "ARXIVID": "2602.13318",
        "COMMENT": "DECKBench is a new benchmark for multi-agent academic slide generation and editing, with a modular multi-agent baseline and a curated dataset. This matches criterion 3 (new benchmarks for embodied AI or multi-agent systems with novel evaluation protocols).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.13930": {
        "authors": [
            "Ruggiero Santeramo",
            "Igor Zubarev",
            "Florian Jug"
        ],
        "title": "MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction",
        "abstract": "arXiv:2602.13930v1 Announce Type: new  Abstract: Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry.   We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy.   We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site.   At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.",
        "arxiv_id": "2602.13930",
        "ARXIVID": "2602.13930",
        "COMMENT": "Somewhat related to criterion 4: Proposes a hybrid vision model (MamaDino) for breast cancer risk prediction, combining ViT and CNNs, and shows that explicit contralateral modeling enables state-of-the-art results with lower-resolution images. Application of vision models, but not a new foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.13880": {
        "authors": [
            "Jiahao Xie",
            "Guangmo Tong"
        ],
        "title": "VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection",
        "abstract": "arXiv:2602.13880v1 Announce Type: new  Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.",
        "arxiv_id": "2602.13880",
        "ARXIVID": "2602.13880",
        "COMMENT": "Somewhat related to criterion 4: Proposes a vision-based framework (VSAL) for graph property detection using adaptive layouts, which is a novel application of vision models to graph problems. Not a foundation model per se, but relevant to vision model applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.13595": {
        "authors": [
            "Henry Han",
            "Xiyang Liu",
            "Xiaodong Wang",
            "Fei Han",
            "Xiaodong Li"
        ],
        "title": "The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning",
        "abstract": "arXiv:2602.13595v1 Announce Type: new  Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's \"smaller-is-better\" heuristic is mathematically counterproductive for complex reasoning tasks.",
        "arxiv_id": "2602.13595",
        "ARXIVID": "2602.13595",
        "COMMENT": "Does not match any specific criterion. Focuses on quantization and scaling laws in multi-hop reasoning, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.13248": {
        "authors": [
            "Ashkan Y. Zadeh",
            "Xiaomeng Li",
            "Andry Rakotonirainy",
            "Ronald Schroeter",
            "Sebastien Glaser",
            "Zishuo Zhu"
        ],
        "title": "X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles",
        "abstract": "arXiv:2602.13248v1 Announce Type: new  Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.   At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.   At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.   The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.",
        "arxiv_id": "2602.13248",
        "ARXIVID": "2602.13248",
        "COMMENT": "Somewhat related to criterion 2 (multi-LLM ensemble for explanation classification), but not a new VLLM/MLLM itself. Focuses on linguistic analysis for AV explanations.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.13873": {
        "authors": [
            "Harris Abdul Majid",
            "Giannis Daras",
            "Francesco Tudisco",
            "Steven McDonagh"
        ],
        "title": "Ambient Physics: Training Neural PDE Solvers with Partial Observations",
        "abstract": "arXiv:2602.13873v1 Announce Type: new  Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish \"truly unobserved\" from \"artificially unobserved\", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\\%$ reduction in average overall error while using 125$\\times$ fewer function evaluations. We also identify a \"one-point transition\": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.",
        "arxiv_id": "2602.13873",
        "ARXIVID": "2602.13873",
        "COMMENT": "Does not match any specific criterion. Presents a new method for learning from partial observations in scientific PDE settings, but not related to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.13691": {
        "authors": [
            "Yu Li",
            "Guangfeng Cai",
            "Shengtian Yang",
            "Han Luo",
            "Shuo Han",
            "Xu He",
            "Dong Li",
            "Lei Feng"
        ],
        "title": "PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning",
        "abstract": "arXiv:2602.13691v1 Announce Type: new  Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.",
        "arxiv_id": "2602.13691",
        "ARXIVID": "2602.13691",
        "COMMENT": "PhGPO introduces a pheromone-guided policy optimization method for long-horizon tool planning in LLM agents, inspired by ant colony optimization. While it is about planning and policy optimization, it does not directly address spatial intelligence, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2602.13980": {
        "authors": [
            "Guojie Liu",
            "Yiqi Wang",
            "Yanfeng Yang",
            "Wenqi Fan",
            "Songlei Jian",
            "Jianfeng Zhang",
            "Jie Yu"
        ],
        "title": "Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking",
        "abstract": "arXiv:2602.13980v1 Announce Type: new  Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\\% in F1 score and 40.7\\% in EM score on QA tasks at the $64\\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\\%.",
        "arxiv_id": "2602.13980",
        "ARXIVID": "2602.13980",
        "COMMENT": "Does not directly match any criterion. Focuses on prompt compression for LLMs, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.13477": {
        "authors": [
            "Akshat Naik",
            "Jay Culligan",
            "Yarin Gal",
            "Philip Torr",
            "Rahaf Aljundi",
            "Alasdair Paren",
            "Adel Bibi"
        ],
        "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage",
        "abstract": "arXiv:2602.13477v1 Announce Type: new  Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \\textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.",
        "arxiv_id": "2602.13477",
        "ARXIVID": "2602.13477",
        "COMMENT": "Does not match any specific criterion. Focuses on security and data leakage in multi-agent LLM systems, not on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.13639": {
        "authors": [
            "Linlin Wang",
            "Tianqing Zhu",
            "Laiqiao Qin",
            "Longxiang Gao",
            "Wanlei Zhou"
        ],
        "title": "Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval",
        "abstract": "arXiv:2602.13639v1 Announce Type: new  Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.",
        "arxiv_id": "2602.13639",
        "ARXIVID": "2602.13639",
        "COMMENT": "Does not directly match any criterion. Focuses on multi-agent LLM systems and adaptive guidance, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.13726": {
        "authors": [
            "Quanjun Li",
            "Weixuan Li",
            "Han Xia",
            "Junhua Zhou",
            "Chi-Man Pun",
            "Xuhang Chen"
        ],
        "title": "RGA-Net: A Vision Enhancement Framework for Robotic Surgical Systems Using Reciprocal Attention Mechanisms",
        "abstract": "arXiv:2602.13726v1 Announce Type: new  Abstract: Robotic surgical systems rely heavily on high-quality visual feedback for precise teleoperation; yet, surgical smoke from energy-based devices significantly degrades endoscopic video feeds, compromising the human-robot interface and surgical outcomes. This paper presents RGA-Net (Reciprocal Gating and Attention-fusion Network), a novel deep learning framework specifically designed for smoke removal in robotic surgery workflows. Our approach addresses the unique challenges of surgical smoke-including dense, non-homogeneous distribution and complex light scattering-through a hierarchical encoder-decoder architecture featuring two key innovations: (1) a Dual-Stream Hybrid Attention (DHA) module that combines shifted window attention with frequency-domain processing to capture both local surgical details and global illumination changes, and (2) an Axis-Decomposed Attention (ADA) module that efficiently processes multi-scale features through factorized attention mechanisms. These components are connected via reciprocal cross-gating blocks that enable bidirectional feature modulation between encoder and decoder pathways. Extensive experiments on the DesmokeData and LSD3K surgical datasets demonstrate that RGA-Net achieves superior performance in restoring visual clarity suitable for robotic surgery integration. Our method enhances the surgeon-robot interface by providing consistently clear visualization, laying a technical foundation for alleviating surgeons' cognitive burden, optimizing operation workflows, and reducing iatrogenic injury risks in minimally invasive procedures. These practical benefits could be further validated through future clinical trials involving surgeon usability assessments. The proposed framework represents a significant step toward more reliable and safer robotic surgical systems through computational vision enhancement.",
        "arxiv_id": "2602.13726",
        "ARXIVID": "2602.13726",
        "COMMENT": "Related to computer vision applications, but does not match any specific criterion. Focuses on vision enhancement for robotic surgery, not on spatial intelligence, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.14451": {
        "authors": [
            "Qianyue Wang",
            "Jinwu Hu",
            "Huanxiang Lin",
            "Bolin Chen",
            "Zhiquan Wen",
            "Yaofo Chen",
            "Yu Rong",
            "Mingkui Tan"
        ],
        "title": "Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning",
        "abstract": "arXiv:2602.14451v1 Announce Type: new  Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.",
        "arxiv_id": "2602.14451",
        "ARXIVID": "2602.14451",
        "COMMENT": "Does not match any specific criterion. Focuses on reasoning efficiency in LLMs, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.13516": {
        "authors": [
            "Jaechul Roh",
            "Eugene Bagdasarian",
            "Hamed Haddadi",
            "Ali Shahin Shamsabadi"
        ],
        "title": "SPILLage: Agentic Oversharing on the Web",
        "abstract": "arXiv:2602.13516v1 Announce Type: new  Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.",
        "arxiv_id": "2602.13516",
        "ARXIVID": "2602.13516",
        "COMMENT": "Does not match any specific criterion. Focuses on privacy and oversharing in LLM-powered web agents, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.13234": {
        "authors": [
            "Mingyang Liao",
            "Yichen Wan",
            "shuchen wu",
            "Chenxi Miao",
            "Xin Shen",
            "Weikang Li",
            "Yang Li",
            "Deguo Xia",
            "Jizhou Huang"
        ],
        "title": "Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents",
        "abstract": "arXiv:2602.13234v1 Announce Type: new  Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.",
        "arxiv_id": "2602.13234",
        "ARXIVID": "2602.13234",
        "COMMENT": "Does not match any specific criterion. Focuses on safety and persona fidelity in LLM-based role-playing agents, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.13530": {
        "authors": [
            "Yiheng Shu",
            "Saisri Padmaja Jonnalagedda",
            "Xiang Gao",
            "Bernal Jim\\'enez Guti\\'errez",
            "Weijian Qi",
            "Kamalika Das",
            "Huan Sun",
            "Yu Su"
        ],
        "title": "REMem: Reasoning with Episodic Memory in Language Agent",
        "abstract": "arXiv:2602.13530v1 Announce Type: new  Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.",
        "arxiv_id": "2602.13530",
        "ARXIVID": "2602.13530",
        "COMMENT": "REMem introduces a framework for reasoning with episodic memory in language agents, with a hybrid memory graph and agentic retriever. While it is about memory and reasoning, it does not directly address spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.13967": {
        "authors": [
            "Ruicheng Zhang",
            "Xinyi Li",
            "Tianyi Xu",
            "Shuhao Zhang",
            "Xiaofei Liao",
            "Hai Jin"
        ],
        "title": "Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs",
        "abstract": "arXiv:2602.13967v1 Announce Type: new  Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.",
        "arxiv_id": "2602.13967",
        "ARXIVID": "2602.13967",
        "COMMENT": "Neuromem presents a testbed for benchmarking external memory modules for LLMs under streaming conditions, decomposing the memory lifecycle and evaluating on several datasets. While it is relevant to memory in LLMs, it does not directly match any of the four criteria, as it is not focused on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.14524": {
        "authors": [
            "Ari Vesalainen",
            "Eetu M\\\"akel\\\"a",
            "Laura Ruotsalainen",
            "Mikko Tolonen"
        ],
        "title": "Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model",
        "abstract": "arXiv:2602.14524v1 Announce Type: new  Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.   While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.",
        "arxiv_id": "2602.14524",
        "ARXIVID": "2602.14524",
        "COMMENT": "This paper compares a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) for historical OCR, analyzing error patterns and architectural biases. It matches criterion 2 (VLLMs) as it evaluates a VLM in a vision-language context, but the focus is on OCR error analysis rather than new VLLM methods or architectures.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    }
}