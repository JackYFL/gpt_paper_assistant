{
    "2509.21733": {
        "authors": [
            "Jiannan Xiang",
            "Yun Zhu",
            "Lei Shu",
            "Maria Wang",
            "Lijun Yu",
            "Gabriel Barcik",
            "James Lyon",
            "Srinivas Sunkara",
            "Jindong Chen"
        ],
        "title": "UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments",
        "abstract": "arXiv:2509.21733v1 Announce Type: new  Abstract: Developing and testing user interfaces (UIs) and training AI agents to interact with them are challenging due to the dynamic and diverse nature of real-world mobile environments. Existing methods often rely on cumbersome physical devices or limited static analysis of screenshots, which hinders scalable testing and the development of intelligent UI agents. We introduce UISim, a novel image-based UI simulator that offers a dynamic and interactive platform for exploring mobile phone environments purely from screen images. Our system employs a two-stage method: given an initial phone screen image and a user action, it first predicts the abstract layout of the next UI state, then synthesizes a new, visually consistent image based on this predicted layout. This approach enables the realistic simulation of UI transitions. UISim provides immediate practical benefits for UI testing, rapid prototyping, and synthetic data generation. Furthermore, its interactive capabilities pave the way for advanced applications, such as UI navigation task planning for AI agents. Our experimental results show that UISim outperforms end-to-end UI generation baselines in generating realistic and coherent subsequent UI states, highlighting its fidelity and potential to streamline UI development and enhance AI agent training.",
        "arxiv_id": "2509.21733",
        "ARXIVID": "2509.21733",
        "COMMENT": "Directly matches criterion 3 (embodied AI, new benchmark/simulator). UISim is a new interactive, image-based UI simulator for dynamic mobile environments, enabling training and evaluation of AI agents for UI navigation and interaction, with a novel two-stage simulation approach.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21998": {
        "authors": [
            "Hanlin Zhu",
            "Tianyu Guo",
            "Song Mei",
            "Stuart Russell",
            "Nikhil Ghosh",
            "Alberto Bietti",
            "Jiantao Jiao"
        ],
        "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments",
        "abstract": "arXiv:2509.21998v1 Announce Type: new  Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.",
        "arxiv_id": "2509.21998",
        "ARXIVID": "2509.21998",
        "COMMENT": "Directly matches criterion 3: introduces GSM-Agent, a new benchmark for agentic reasoning in LLM agents, focusing on tool use and reasoning in controllable environments. The paper also proposes a novel analysis method (agentic reasoning graph) and a test-time scaling method.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21766": {
        "authors": [
            "Haotian Luo",
            "Huaisong Zhang",
            "Xuelin Zhang",
            "Haoyu Wang",
            "Zeyu Qin",
            "Wenjie Lu",
            "Guozheng Ma",
            "Haiying He",
            "Yingsha Xie",
            "Qiyang Zhou",
            "Zixuan Hu",
            "Hongze Mi",
            "Yibo Wang",
            "Naiqiang Tan",
            "Hong Chen",
            "Yi R. Fung",
            "Chun Yuan",
            "Li Shen"
        ],
        "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
        "abstract": "arXiv:2509.21766v1 Announce Type: new  Abstract: Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
        "arxiv_id": "2509.21766",
        "ARXIVID": "2509.21766",
        "COMMENT": "Directly matches criterion 3: introduces UltraHorizon, a new benchmark for evaluating agent capabilities in ultra long-horizon, partially observable scenarios, focusing on sustained reasoning, planning, and tool use. The benchmark exposes gaps in current LLM-agents and provides in-depth analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21760": {
        "authors": [
            "Lan Chen",
            "Yuchao Gu",
            "Qi Mao"
        ],
        "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
        "abstract": "arXiv:2509.21760v1 Announce Type: new  Abstract: Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
        "arxiv_id": "2509.21760",
        "ARXIVID": "2509.21760",
        "COMMENT": "Directly matches criterion 4: proposes UniVid, a framework that unifies vision tasks using pre-trained video generation models, showing generalization across modalities and sources. This is a vision foundation model application with a novel unification angle.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21981": {
        "authors": [
            "Zhimin Wang",
            "Shaokang He",
            "Duo Wu",
            "Jinghe Wang",
            "Linjia Kang",
            "Jing Yu",
            "Zhi Wang"
        ],
        "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
        "abstract": "arXiv:2509.21981v1 Announce Type: new  Abstract: Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
        "arxiv_id": "2509.21981",
        "ARXIVID": "2509.21981",
        "COMMENT": "Directly matches criterion 3: introduces a new framework (CoBel-World) for embodied multi-agent collaboration, focusing on intent inference and belief modeling, and is evaluated on embodied benchmarks. The approach is novel in its explicit modeling of collaborators' mental states and Bayesian-style belief updates via LLM reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22460": {
        "authors": [
            "Shichao Weng",
            "Zhiqiang Wang",
            "Yuhua Zhou",
            "Rui Lu",
            "Ting Liu",
            "Zhiyang Teng",
            "Xiaozhang Liu",
            "Hanmeng Liu"
        ],
        "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation",
        "abstract": "arXiv:2509.22460v1 Announce Type: new  Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large Language Models (MLLMs), requiring not only the joint interpretation of text and diagrams but also iterative visuospatial reasoning. While existing approaches process diagrams as static images, they lack the capacity for dynamic manipulation - a core aspect of human geometric reasoning involving auxiliary line construction and affine transformations. We present GeoSketch, a neural-symbolic framework that recasts geometric reasoning as an interactive perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning module that applies geometric theorems to decide the next deductive step, and (3) a Sketch Action module that executes operations such as drawing auxiliary lines or applying transformations, thereby updating the diagram in a closed loop. To train this agent, we develop a two-stage pipeline: supervised fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement learning with dense, symbolic rewards to enhance robustness and strategic exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a high-quality set of 390 geometry problems requiring auxiliary construction or affine transformations. Experiments on strong MLLM baselines demonstrate that GeoSketch significantly improves stepwise reasoning accuracy and problem-solving success over static perception methods. By unifying hierarchical decision-making, executable visual actions, and symbolic verification, GeoSketch advances multimodal reasoning from static interpretation to dynamic, verifiable interaction, establishing a new foundation for solving complex visuospatial problems.",
        "arxiv_id": "2509.22460",
        "ARXIVID": "2509.22460",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (embodied AI with new benchmark and novel methods). Proposes a neural-symbolic MLLM for geometric multimodal reasoning with dynamic diagram manipulation, introduces a new benchmark, and demonstrates improved reasoning over static methods.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21790": {
        "authors": [
            "Yu Shang",
            "Lei Jin",
            "Yiding Ma",
            "Xin Zhang",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE",
        "abstract": "arXiv:2509.21790v1 Announce Type: new  Abstract: Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.",
        "arxiv_id": "2509.21790",
        "ARXIVID": "2509.21790",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for long-horizon world models in embodied agents) and criterion 4 (vision foundation models, generative modeling for video-based world models). Proposes a hybrid framework for stable long-horizon video generation in embodied manipulation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22407": {
        "authors": [
            "Zhehao Dong",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Yirui Wang",
            "Yang Wang",
            "Yukun Zhou",
            "Boyuan Wang",
            "Chaojun Ni",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang"
        ],
        "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
        "abstract": "arXiv:2509.22407v1 Announce Type: new  Abstract: Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
        "arxiv_id": "2509.22407",
        "ARXIVID": "2509.22407",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for generalization in robot manipulation) and criterion 4 (vision foundation models and generative modeling for data augmentation in embodied agents). Proposes a generative visual transfer framework and a new training strategy for robot manipulation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21991": {
        "authors": [
            "Jewon Lee",
            "Wooksu Shin",
            "Seungmin Yang",
            "Ki-Ung Song",
            "DongUk Lim",
            "Jaeyeon Kim",
            "Tae-Ho Kim",
            "Bo-Kyeong Kim"
        ],
        "title": "ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models",
        "abstract": "arXiv:2509.21991v1 Announce Type: new  Abstract: Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of \"thinking with images\" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage \"coarse-to-fine\" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.",
        "arxiv_id": "2509.21991",
        "ARXIVID": "2509.21991",
        "COMMENT": "Matches criterion 2 and 4. This paper introduces ERGO, a new method for efficient high-resolution visual understanding in vision-language models, using a coarse-to-fine reasoning pipeline and reinforcement learning for region selection. It is a methodological improvement for VLLMs and relates to vision foundation models and their applications.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22404": {
        "authors": [
            "Yiwei Li",
            "Yikang Liu",
            "Jiaqi Guo",
            "Lin Zhao",
            "Zheyuan Zhang",
            "Xiao Chen",
            "Boris Mailhe",
            "Ankush Mukherjee",
            "Terrence Chen",
            "Shanhui Sun"
        ],
        "title": "RAU: Reference-based Anatomical Understanding with Vision Language Models",
        "abstract": "arXiv:2509.22404v1 Announce Type: new  Abstract: Anatomical understanding through deep learning is critical for automatic report generation, intra-operative navigation, and organ localization in medical imaging; however, its progress is constrained by the scarcity of expert-labeled data. A promising remedy is to leverage an annotated reference image to guide the interpretation of an unlabeled target. Although recent vision-language models (VLMs) exhibit non-trivial visual reasoning, their reference-based understanding and fine-grained localization remain limited. We introduce RAU, a framework for reference-based anatomical understanding with VLMs. We first show that a VLM learns to identify anatomical regions through relative spatial reasoning between reference and target images, trained on a moderately sized dataset. We validate this capability through visual question answering (VQA) and bounding box prediction. Next, we demonstrate that the VLM-derived spatial cues can be seamlessly integrated with the fine-grained segmentation capability of SAM2, enabling localization and pixel-level segmentation of small anatomical regions, such as vessel segments. Across two in-distribution and two out-of-distribution datasets, RAU consistently outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding more accurate segmentations and more reliable localization. More importantly, its strong generalization ability makes it scalable to out-of-distribution datasets, a property crucial for medical image applications. To the best of our knowledge, RAU is the first to explore the capability of VLMs for reference-based identification, localization, and segmentation of anatomical structures in medical images. Its promising performance highlights the potential of VLM-driven approaches for anatomical understanding in automated clinical workflows.",
        "arxiv_id": "2509.22404",
        "ARXIVID": "2509.22404",
        "COMMENT": "Matches criterion 1 and 2. This paper introduces RAU, a framework for reference-based anatomical understanding with VLMs, focusing on spatial reasoning between reference and target images and integrating VLM-derived spatial cues with segmentation models. It explores spatial intelligence in VLMs and their application to fine-grained localization and segmentation, which is a novel methodological improvement in spatial understanding for embodied agents (criterion 1) and shows new VLLM capabilities (criterion 2).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21797": {
        "authors": [
            "Yu Shang",
            "Yangcheng Yu",
            "Xin Zhang",
            "Xin Jin",
            "Haisheng Su",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation",
        "abstract": "arXiv:2509.21797v1 Announce Type: new  Abstract: Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.",
        "arxiv_id": "2509.21797",
        "ARXIVID": "2509.21797",
        "COMMENT": "Matches criterion 1 (spatial intelligence for embodied agents) and criterion 3 (new method for embodied planning with mixture-of-world-models, offering novel insights into feature space fusion).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22281": {
        "authors": [
            "Jinkun Hao",
            "Naifu Liang",
            "Zhen Luo",
            "Xudong Xu",
            "Weipeng Zhong",
            "Ran Yi",
            "Yichen Jin",
            "Zhaoyang Lyu",
            "Feng Zheng",
            "Lizhuang Ma",
            "Jiangmiao Pang"
        ],
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
        "abstract": "arXiv:2509.22281v1 Announce Type: new  Abstract: The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at https://mesatask.github.io/",
        "arxiv_id": "2509.22281",
        "ARXIVID": "2509.22281",
        "COMMENT": "Matches criterion 1 (spatial intelligence and reasoning for embodied agents) and criterion 3 (new benchmark and method for embodied AI with a novel angle: task-driven 3D scene generation). Introduces a new dataset and a spatial reasoning chain for tabletop scene generation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.22010": {
        "authors": [
            "Xinyu Zhang",
            "Yuxuan Dong",
            "Lingling Zhang",
            "Chengyou Jia",
            "Zhuohang Dang",
            "Basura Fernando",
            "Jun Liu",
            "Mike Zheng Shou"
        ],
        "title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models",
        "abstract": "arXiv:2509.22010v1 Announce Type: new  Abstract: Despite significant advances in Vision Language Models (VLMs), they remain constrained by the complexity and redundancy of visual input. When images contain large amounts of irrelevant information, VLMs are susceptible to interference, thus generating excessive task-irrelevant reasoning processes or even hallucinations. This limitation stems from their inability to discover and process the required regions during reasoning precisely. To address this limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel training-free approach that enhances VLMs' visual reasoning by emulating human visual cognition. Each Foresight-Focus Thought consists of three stages: (1) Diverse Sample Generation: generates diverse reasoning samples to explore potential reasoning paths, where each sample contains several reasoning steps; (2) Dual Foresight Decoding: rigorously evaluates these samples based on both visual focus and reasoning progression, adding the first step of optimal sample to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual focus toward regions most beneficial for future reasoning, before returning to stage (1) to generate subsequent reasoning samples until reaching the final answer. These stages function iteratively, creating an interdependent cycle where reasoning guides visual focus and visual focus informs subsequent reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL, InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of 3.1-5.8\\% with controllable increasing computational overhead.",
        "arxiv_id": "2509.22010",
        "ARXIVID": "2509.22010",
        "COMMENT": "Directly matches criterion 2: introduces CoFFT, a novel training-free approach to improve visual reasoning in VLMs by iteratively guiding visual focus and reasoning, with empirical improvements on multiple VLMs. Also relevant to criterion 4.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.21651": {
        "authors": [
            "Abhishek Jindal",
            "Dmitry Kalashnikov",
            "Oscar Chang",
            "Divya Garikapati",
            "Anirudha Majumdar",
            "Pierre Sermanet",
            "Vikas Sindhwani"
        ],
        "title": "Can AI Perceive Physical Danger and Intervene?",
        "abstract": "arXiv:2509.21651v1 Announce Type: new  Abstract: When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2",
        "arxiv_id": "2509.21651",
        "ARXIVID": "2509.21651",
        "COMMENT": "Matches criterion 3 (embodied AI benchmark) as it develops a new benchmark for physical safety understanding in embodied AI, using generative models to create photorealistic safety scenarios and evaluating foundation models' safety reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.22228": {
        "authors": [
            "Jun He",
            "Yi Lin",
            "Zilong Huang",
            "Jiacong Yin",
            "Junyan Ye",
            "Yuchuan Zhou",
            "Weijia Li",
            "Xiang Zhang"
        ],
        "title": "UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective",
        "abstract": "arXiv:2509.22228v1 Announce Type: new  Abstract: Urban development impacts over half of the global population, making human-centered understanding of its structural and perceptual changes essential for sustainable development. While Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various domains, existing benchmarks that explore their performance in urban environments remain limited, lacking systematic exploration of temporal evolution and subjective perception of urban environment that aligns with human perception. To address these limitations, we propose UrbanFeel, a comprehensive benchmark designed to evaluate the performance of MLLMs in urban development understanding and subjective environmental perception. UrbanFeel comprises 14.3K carefully constructed visual questions spanning three cognitively progressive dimensions: Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception. We collect multi-temporal single-view and panoramic street-view images from 11 representative cities worldwide, and generate high-quality question-answer pairs through a hybrid pipeline of spatial clustering, rule-based generation, model-assisted prompting, and manual annotation. Through extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5 Pro achieves the best overall performance, with its accuracy approaching human expert levels and narrowing the average gap to just 1.5\\%. Most models perform well on tasks grounded in scene understanding. In particular, some models even surpass human annotators in pixel-level change detection. However, performance drops notably in tasks requiring temporal reasoning over urban development. Additionally, in the subjective perception dimension, several models reach human-level or even higher consistency in evaluating dimension such as beautiful and safety.",
        "arxiv_id": "2509.22228",
        "ARXIVID": "2509.22228",
        "COMMENT": "Strongly matches criterion 3 (embodied AI benchmarks) and criterion 2 (MLLMs). UrbanFeel is a new benchmark for evaluating MLLMs on temporal and perceptual understanding of city scenes, with surprising empirical results on model performance vs. humans.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21388": {
        "authors": [
            "Anton Konushin",
            "Nikita Drozdov",
            "Bulat Gabdullin",
            "Alexey Zakharov",
            "Anna Vorontsova",
            "Danila Rukhovich",
            "Maksim Kolodiazhnyi"
        ],
        "title": "TUN3D: Towards Real-World Scene Understanding from Unposed Images",
        "abstract": "arXiv:2509.21388v1 Announce Type: new  Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor scene understanding. When combined, they enable the creation of a compact yet semantically rich spatial representation of a scene. Existing approaches typically rely on point cloud input, which poses a major limitation since most consumer cameras lack depth sensors and visual-only data remains far more common. We address this issue with TUN3D, the first method that tackles joint layout estimation and 3D object detection in real scans, given multi-view images as input, and does not require ground-truth camera poses or depth supervision. Our approach builds on a lightweight sparse-convolutional backbone and employs two dedicated heads: one for 3D object detection and one for layout estimation, leveraging a novel and effective parametric wall representation. Extensive experiments show that TUN3D achieves state-of-the-art performance across three challenging scene understanding benchmarks: (i) using ground-truth point clouds, (ii) using posed images, and (iii) using unposed images. While performing on par with specialized 3D object detection methods, TUN3D significantly advances layout estimation, setting a new benchmark in holistic indoor scene understanding. Code is available at https://github.com/col14m/tun3d .",
        "arxiv_id": "2509.21388",
        "ARXIVID": "2509.21388",
        "COMMENT": "Strongly matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI/scene understanding). TUN3D proposes a new method for 3D scene understanding from unposed images, advancing layout estimation and 3D object detection without depth or pose supervision.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21997": {
        "authors": [
            "Youxu Shi",
            "Suorong Yang",
            "Dong Liu"
        ],
        "title": "Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors",
        "abstract": "arXiv:2509.21997v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet they remain highly susceptible to hallucinations, producing content that is fluent but inconsistent with visual evidence. Such hallucinations, spanning objects, attributes, and relations, persist even in larger models, while existing mitigation approaches often require additional finetuning, handcrafted priors, or trade-offs that compromise informativeness and scalability. To address this limitation, we propose a training-free, self-supervised method for hallucination mitigation. Our approach introduces a novel hallucination amplification mechanism: a caption is projected into the visual space via a text-to-image model to reveal implicit hallucination signals, serving as a negative anchor, while the original image provides a positive anchor. Leveraging these dual anchors, we edit decoder hidden states by pulling representations toward faithful semantics and pushing them away from hallucination directions. This correction requires no human priors or additional training costs, ensuring both effectiveness and efficiency. Extensive experiments across multiple benchmarks show that our method significantly reduces hallucinations at the object, attribute, and relation levels while largely preserving recall and caption richness, e.g., achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR. Furthermore, results on diverse architectures, including LLaVA-NEXT-7B, Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture generalization. More importantly, when applied to hallucination-free captions, our method introduces almost no side effects, underscoring its robustness and practical plug-and-play applicability. The implementation will be publicly available.",
        "arxiv_id": "2509.21997",
        "ARXIVID": "2509.21997",
        "COMMENT": "Matches criterion 2. This paper proposes a training-free, self-supervised method for hallucination mitigation in MLLMs by editing representations using generative anchors. It is a novel approach to improving VLM/MLLM reliability and is directly relevant to vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21574": {
        "authors": [
            "You Xie",
            "Tianpei Gu",
            "Zenan Li",
            "Chenxu Zhang",
            "Guoxian Song",
            "Xiaochen Zhao",
            "Chao Liang",
            "Jianwen Jiang",
            "Hongyi Xu",
            "Linjie Luo"
        ],
        "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "abstract": "arXiv:2509.21574v1 Announce Type: new  Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
        "arxiv_id": "2509.21574",
        "ARXIVID": "2509.21574",
        "COMMENT": "Matches criterion 2 (new MLLM: unified multimodal human world modeling) and criterion 4 (vision foundation models and applications). Proposes a new architecture for real-time, multimodal digital human agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21799": {
        "authors": [
            "Hongze Mi",
            "Yibo Feng",
            "Wenjie Lu",
            "Yuqi Wang",
            "Jinyuan Li",
            "Song Cao",
            "He Cui",
            "Tengfei Tian",
            "Xuelin Zhang",
            "Haotian Luo",
            "Di Sun",
            "Naiqiang Tan",
            "Gang Pan"
        ],
        "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents",
        "abstract": "arXiv:2509.21799v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.",
        "arxiv_id": "2509.21799",
        "ARXIVID": "2509.21799",
        "COMMENT": "Matches criterion 2 (new MLLM) and criterion 3 (embodied AI, new methods). D-Artemis is a new deliberative cognitive framework for GUI multi-agents, enhancing MLLMs for GUI tasks with a novel cognitive loop and achieving SOTA on major benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21358": {
        "authors": [
            "Jason Jordan",
            "Mohammadreza Akbari Lor",
            "Peter Koulen",
            "Mei-Ling Shyu",
            "Shu-Ching Chen"
        ],
        "title": "MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification",
        "abstract": "arXiv:2509.21358v1 Announce Type: new  Abstract: This study aimed to enhance disease classification accuracy from retinal fundus images by integrating fine-grained image features and global textual context using a novel multimodal deep learning architecture. Existing multimodal large language models (MLLMs) often struggle to capture low-level spatial details critical for diagnosing retinal diseases such as glaucoma, diabetic retinopathy, and retinitis pigmentosa. This model development and validation study was conducted on 1,305 fundus image-text pairs compiled from three public datasets (FIVES, HRF, and StoneRounds), covering acquired and inherited retinal diseases, and evaluated using classification accuracy and F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are patch-wise projected and fused using scaled cross-attention and FiLM-based U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease classification task. MDF-MLLM, with both U-Net and MLLM components fully fine-tuned during training, achieved a significantly higher accuracy of 94%, representing a 56% improvement. Recall and F1-scores improved by as much as 67% and 35% over baseline, respectively. Ablation studies confirmed that the multi-depth fusion approach contributed to substantial gains in spatial reasoning and classification, particularly for inherited diseases with rich clinical text. MDF-MLLM presents a generalizable, interpretable, and modular framework for fundus image classification, outperforming traditional MLLM baselines through multi-scale feature fusion. The architecture holds promise for real-world deployment in clinical decision support systems. Future work will explore synchronized training techniques, a larger pool of diseases for more generalizability, and extending the model for segmentation tasks.",
        "arxiv_id": "2509.21358",
        "ARXIVID": "2509.21358",
        "COMMENT": "Directly matches criterion 2 (new MLLM) and criterion 4 (vision foundation models and applications). Proposes MDF-MLLM, a new multimodal large language model architecture with deep fusion for fundus image classification, showing significant improvements over baselines and novel multi-scale feature fusion.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21552": {
        "authors": [
            "Yu Zhao",
            "Wei-Ning Chen",
            "Huseyin Atahan Inan",
            "Samuel Kessler",
            "Lu Wang",
            "Lukas Wutschitz",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Pasquale Minervini",
            "Saravan Rajmohan",
            "Robert Sim"
        ],
        "title": "Learning GUI Grounding with Spatial Reasoning from Visual Feedback",
        "abstract": "arXiv:2509.21552v1 Announce Type: new  Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate prediction task -- given a natural language instruction, generate on-screen coordinates for actions such as clicks and keystrokes. However, recent Vision Language Models (VLMs) often fail to predict accurate numeric coordinates when processing high-resolution GUI images with complex layouts. To address this issue, we reframe GUI grounding as an \\emph{interactive search task}, where the VLM generates actions to move a cursor in the GUI to locate UI elements. At each step, the model determines the target object, evaluates the spatial relations between the cursor and the target, and moves the cursor closer to the target conditioned on the movement history. In this interactive process, the rendered cursor provides visual feedback to help the model align its predictions with the corresponding on-screen locations. We train our GUI grounding model, GUI-Cursor, using multi-step online reinforcement learning with a dense trajectory-based reward function. Our experimental results show that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\\% \\rightarrow 93.9\\%$) and ScreenSpot-Pro ($26.8\\% \\rightarrow 56.5\\%$). Moreover, we observe that GUI-Cursor learns to solve the problem within two steps for 95\\% of instances and can adaptively conduct more steps on more difficult examples.",
        "arxiv_id": "2509.21552",
        "ARXIVID": "2509.21552",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 2 (VLLMs/MLLMs). Proposes a new interactive, spatially-aware GUI grounding method using visual feedback and reinforcement learning, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21451": {
        "authors": [
            "Abdul Waheed",
            "Zhen Wu",
            "Dareen Alharthi",
            "Seungone Kim",
            "Bhiksha Raj"
        ],
        "title": "VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding",
        "abstract": "arXiv:2509.21451v1 Announce Type: new  Abstract: Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.",
        "arxiv_id": "2509.21451",
        "ARXIVID": "2509.21451",
        "COMMENT": "Matches criterion 2 (new MLLMs as judges for video understanding) and criterion 4 (application of vision foundation models for evaluation). The paper introduces a new MLLM-based judge for video understanding, with empirical insights into the importance of visual input for evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.22570": {
        "authors": [
            "Qi Mao",
            "Tinghan Yang",
            "Jiahao Li",
            "Bin Li",
            "Libiao Jin",
            "Yan Lu"
        ],
        "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
        "abstract": "arXiv:2509.22570v1 Announce Type: new  Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.",
        "arxiv_id": "2509.22570",
        "ARXIVID": "2509.22570",
        "COMMENT": "Matches criterion 2 (new MLLM/VLLM) and criterion 4 (vision foundation models and applications). UniMIC proposes a new token-based multimodal interactive coding framework for efficient communication between edge devices and cloud AI agents, specifically designed for LMMs. The method is evaluated on multiple vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21927": {
        "authors": [
            "Jiahui Wang",
            "Haiyue Zhu",
            "Haoren Guo",
            "Abdullah Al Mamun",
            "Cheng Xiang",
            "Tong Heng Lee"
        ],
        "title": "SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference",
        "abstract": "arXiv:2509.21927v1 Announce Type: new  Abstract: Recent 6D pose estimation methods demonstrate notable performance but still face some practical limitations. For instance, many of them rely heavily on sensor depth, which may fail with challenging surface conditions, such as transparent or highly reflective materials. In the meantime, RGB-based solutions provide less robust matching performance in low-light and texture-less scenes due to the lack of geometry information. Motivated by these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB image as a reference, eliminating the need for costly depth sensors, multi-view image acquisition, or training view synthesis models and neural fields. This enables SingRef6D to remain robust and capable even under resource-limited settings where depth or dense templates are unavailable. Our framework incorporates two key innovations. First, we propose a token-scaler-based fine-tuning mechanism with a novel optimization loss on top of Depth-Anything v2 to enhance its ability to predict accurate depth, even for challenging surfaces. Our results show a 14.41% improvement (in $\\delta_{1.05}$) on REAL275 depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second, benefiting from depth availability, we introduce a depth-aware matching process that effectively integrates spatial relationships within LoFTR, enabling our system to handle matching for challenging materials and lighting conditions. Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light datasets show that our approach surpasses state-of-the-art methods, achieving a 6.1% improvement in average recall.",
        "arxiv_id": "2509.21927",
        "ARXIVID": "2509.21927",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a monocular 6D pose estimation method using a single RGB reference, with a novel depth prediction fine-tuning and depth-aware matching, leveraging Depth-Anything v2 and LoFTR. Strong empirical improvements and robustness in challenging conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21980": {
        "authors": [
            "Zeyu Wang",
            "Baiyu Chen",
            "Kun Yan",
            "Hongjing Piao",
            "Hao Xue",
            "Flora D. Salim",
            "Yuanchun Shi",
            "Yuntao Wang"
        ],
        "title": "Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm",
        "abstract": "arXiv:2509.21980v1 Announce Type: new  Abstract: With the rise in popularity of smart glasses, users' attention has been integrated into Vision-Language Models (VLMs) to streamline multi-modal querying in daily scenarios. However, leveraging gaze data to model users' attention may introduce ambiguity challenges: (1) users' verbal questions become ambiguous by using pronouns or skipping context, (2) humans' gaze patterns can be noisy and exhibit complex spatiotemporal relationships with their spoken questions. Previous works only consider single image as visual modality input, failing to capture the dynamic nature of the user's attention. In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal gaze information to enhance the model's effectiveness in real-world applications. Initially, we analyzed hundreds of querying samples with the gaze modality to demonstrate the noisy nature of users' gaze patterns. We then utilized GPT-4o to design an automatic data synthesis pipeline to generate the GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process to handle noisy gaze patterns. Finally, we designed a heatmap module to incorporate gaze information into cutting-edge VLMs while preserving their pretrained knowledge. We evaluated GLARIFY using a hold-out test set. Experiments demonstrate that GLARIFY significantly outperforms baselines. By robustly aligning VLMs with human attention, GLARIFY paves the way for a usable and intuitive interaction paradigm with a visual assistant.",
        "arxiv_id": "2509.21980",
        "ARXIVID": "2509.21980",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 1 (spatial intelligence on embodied agents) as it introduces a method for integrating spatiotemporal gaze information into VLMs for more intuitive visual assistant interaction, addressing ambiguity in multi-modal querying.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22014": {
        "authors": [
            "Saurav Jha",
            "Stefan K. Ehrlich"
        ],
        "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics",
        "abstract": "arXiv:2509.22014v1 Announce Type: new  Abstract: Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.",
        "arxiv_id": "2509.22014",
        "ARXIVID": "2509.22014",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 1 (spatial intelligence on embodied agents) as it presents a lightweight multimodal reasoning framework for clinical robotics, combining VLMs with structured scene understanding and reasoning for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22631": {
        "authors": [
            "Debargha Ganguly",
            "Sumit Kumar",
            "Ishwar Balappanawar",
            "Weicong Chen",
            "Shashank Kambhatla",
            "Srinivasan Iyengar",
            "Shivkumar Kalyanaraman",
            "Ponnurangam Kumaraguru",
            "Vipin Chaudhary"
        ],
        "title": "LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision",
        "abstract": "arXiv:2509.22631v1 Announce Type: new  Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for deploying robust vision systems, requiring complex trade-offs between data quality, diversity, and cost when researching vast, unlabeled data lakes. We introduce Labeling Copilot, the first data curation deep research agent for computer vision. A central orchestrator agent, powered by a large multimodal language model, uses multi-step reasoning to execute specialized tools across three core capabilities: (1) Calibrated Discovery sources relevant, in-distribution data from large repositories; (2) Controllable Synthesis generates novel data for rare scenarios with robust filtering; and (3) Consensus Annotation produces accurate labels by orchestrating multiple foundation models via a novel consensus mechanism incorporating non-maximum suppression and voting. Our large-scale validation proves the effectiveness of Labeling Copilot's components. The Consensus Annotation module excels at object discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per image-nearly double the 7.4 ground-truth objects-achieving a final annotation mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class imbalance to discover 903 new bounding box categories, expanding its capability to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a 10-million sample scale, features an active learning strategy that is up to 40x more computationally efficient than alternatives with equivalent sample efficiency. These experiments validate that an agentic workflow with optimized, scalable tools provides a robust foundation for curating industrial-scale datasets.",
        "arxiv_id": "2509.22631",
        "ARXIVID": "2509.22631",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Introduces a deep research agent for automated data curation in computer vision, powered by a large multimodal language model and consensus annotation using multiple foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22112": {
        "authors": [
            "Jingrui Ye",
            "Lingting Zhu",
            "Runze Zhang",
            "Zeyu Hu",
            "Yingda Yin",
            "Lanjiong Li",
            "Lequan Yu",
            "Qingmin Liao"
        ],
        "title": "Large Material Gaussian Model for Relightable 3D Generation",
        "abstract": "arXiv:2509.22112v1 Announce Type: new  Abstract: The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
        "arxiv_id": "2509.22112",
        "ARXIVID": "2509.22112",
        "COMMENT": "Matches criterion 4. This paper introduces the Large Material Gaussian Model for relightable 3D generation, leveraging 3D Gaussian Splatting and multiview diffusion, and extends vision foundation models to material property modeling for 3D assets. It is a novel application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22496": {
        "authors": [
            "Ruoyu Chen",
            "Xiaoqing Guo",
            "Kangwei Liu",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Hua Zhang",
            "Xiaochun Cao"
        ],
        "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation",
        "abstract": "arXiv:2509.22496v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
        "arxiv_id": "2509.22496",
        "ARXIVID": "2509.22496",
        "COMMENT": "Matches criterion 2 (analysis and interpretability of MLLMs) and criterion 4 (vision foundation models and their interpretability). Proposes a new framework for explaining token generation in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22258": {
        "authors": [
            "Miao Jing",
            "Mengting Jia",
            "Junling Lin",
            "Zhongxia Shen",
            "Lijun Wang",
            "Yuanyuan Peng",
            "Huan Gao",
            "Mingkun Xu",
            "Shangyang Li"
        ],
        "title": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks",
        "abstract": "arXiv:2509.22258v1 Announce Type: new  Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.",
        "arxiv_id": "2509.22258",
        "ARXIVID": "2509.22258",
        "COMMENT": "Matches criterion 3 (new benchmark for VLMs in medical reasoning, focusing on reasoning beyond classification). Introduces a new, reasoning-intensive benchmark for vision-language models in neurology.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21486": {
        "authors": [
            "Zixuan Wang",
            "Yu Sun",
            "Hongwei Wang",
            "Baoyu Jing",
            "Xiang Shen",
            "Xin Dong",
            "Zhuolin Hao",
            "Hongyu Xiong",
            "Yang Song"
        ],
        "title": "Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation",
        "abstract": "arXiv:2509.21486v1 Announce Type: new  Abstract: Short video platforms are evolving rapidly, making the identification of inappropriate content increasingly critical. Existing approaches typically train separate and small classification models for each type of issue, which requires extensive human-labeled data and lacks cross-issue generalization. We propose a reasoning-enhanced multimodal large language model (MLLM) pretraining paradigm for unified inappropriate content detection. To address the distribution gap between short video content and the original pretraining data of MLLMs, as well as the complex issue definitions, we introduce three targeted pretraining tasks: (1) \\textit{Caption}, to enhance the MLLM's perception of video details; (2) \\textit{Visual Question Answering (VQA)}, to deepen the MLLM's understanding of issue definitions and annotation guidelines; (3) \\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability. Experimental results show that our pretraining approach significantly improves the MLLM's performance in both zero-shot and supervised fine-tuning (SFT) settings. In addition, our pretrained model demonstrates strong generalization capabilities to emergent, previously unseen issues.",
        "arxiv_id": "2509.21486",
        "ARXIVID": "2509.21486",
        "COMMENT": "Matches criterion 2 (new MLLM) and criterion 4 (vision foundation models and applications). Proposes a reasoning-enhanced domain-adaptive pretraining paradigm for MLLMs for short video content moderation, with new pretraining tasks and strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21894": {
        "authors": [
            "Yixiao Liu (College of Computer Science",
            "Sichuan University",
            "China)",
            "Yizhou Yang (College of Computer Science",
            "Sichuan University",
            "China)",
            "Jinwen Li (School of Computer Science and Technology",
            "Xinjiang University",
            "China)",
            "Jun Tao (College of Computer Science",
            "Sichuan University",
            "China)",
            "Ruoyu Li (College of Computer Science",
            "Sichuan University",
            "China)",
            "Xiangkun Wang (College of Computer Science",
            "Sichuan University",
            "China)",
            "Min Zhu (College of Computer Science",
            "Sichuan University",
            "China)",
            "Junlong Cheng (College of Computer Science",
            "Sichuan University",
            "China)"
        ],
        "title": "LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation",
        "abstract": "arXiv:2509.21894v1 Announce Type: new  Abstract: Remote Sensing Change Detection (RSCD) typically identifies changes in land cover or surface conditions by analyzing multi-temporal images. Currently, most deep learning-based methods primarily focus on learning unimodal visual information, while neglecting the rich semantic information provided by multimodal data such as text. To address this limitation, we propose a novel Language-Guided Change Detection model (LG-CD). This model leverages natural language prompts to direct the network's attention to regions of interest, significantly improving the accuracy and robustness of change detection. Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature extractor to capture multi-scale pyramid features from high-resolution to low-resolution across bi-temporal remote sensing images. Subsequently, multi-layer adapters are employed to fine-tune the model for downstream tasks, ensuring its effectiveness in remote sensing change detection. Additionally, we design a Text Fusion Attention Module (TFAM) to align visual and textual information, enabling the model to focus on target change regions using text prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented, which deeply integrates visual and semantic information through a cross-attention mechanism to produce highly accurate change detection masks. Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate that LG-CD consistently outperforms state-of-the-art change detection methods. Furthermore, our approach provides new insights into achieving generalized change detection by leveraging multimodal information.",
        "arxiv_id": "2509.21894",
        "ARXIVID": "2509.21894",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and criterion 2 (MLLMs). Proposes a language-guided change detection model using SAM2 (a vision foundation model) and text prompts for remote sensing, with new modules for vision-language fusion and strong results.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.22186": {
        "authors": [
            "Junbo Niu",
            "Zheng Liu",
            "Zhuangcheng Gu",
            "Bin Wang",
            "Linke Ouyang",
            "Zhiyuan Zhao",
            "Tao Chu",
            "Tianyao He",
            "Fan Wu",
            "Qintong Zhang",
            "Zhenjiang Jin",
            "Guang Liang",
            "Rui Zhang",
            "Wenzheng Zhang",
            "Yuan Qu",
            "Zhifei Ren",
            "Yuefeng Sun",
            "Yuanhong Zheng",
            "Dongsheng Ma",
            "Zirui Tang",
            "Boyu Niu",
            "Ziyang Miao",
            "Hejun Dong",
            "Siyi Qian",
            "Junyuan Zhang",
            "Jingzhou Chen",
            "Fangdong Wang",
            "Xiaomeng Zhao",
            "Liqun Wei",
            "Wei Li",
            "Shasha Wang",
            "Ruiliang Xu",
            "Yuanyuan Cao",
            "Lu Chen",
            "Qianqian Wu",
            "Huaiyu Gu",
            "Lindong Lu",
            "Keming Wang",
            "Dechen Lin",
            "Guanlin Shen",
            "Xuanhe Zhou",
            "Linfeng Zhang",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Jiaqi Wang",
            "Bo Zhang",
            "Lei Bai",
            "Pei Chu",
            "Weijia Li",
            "Jiang Wu",
            "Lijun Wu",
            "Zhenxiang Li",
            "Guangyu Wang",
            "Zhongying Tu",
            "Chao Xu",
            "Kai Chen",
            "Yu Qiao",
            "Bowen Zhou",
            "Dahua Lin",
            "Wentao Zhang",
            "Conghui He"
        ],
        "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing",
        "abstract": "arXiv:2509.22186v1 Announce Type: new  Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
        "arxiv_id": "2509.22186",
        "ARXIVID": "2509.22186",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new vision-language model for document parsing with a decoupled, efficient architecture and strong benchmark results.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21573": {
        "authors": [
            "Boyi Chen",
            "Zhangyu Wang",
            "Fabian Deuser",
            "Johann Maximilian Zollner",
            "Martin Werner"
        ],
        "title": "Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms",
        "abstract": "arXiv:2509.21573v1 Announce Type: new  Abstract: Accurate and robust image-based geo-localization at a global scale is challenging due to diverse environments, visually ambiguous scenes, and the lack of distinctive landmarks in many regions. While contrastive learning methods show promising performance by aligning features between street-view images and corresponding locations, they neglect the underlying spatial dependency in the geographic space. As a result, they fail to address the issue of false negatives -- image pairs that are both visually and geographically similar but labeled as negatives, and struggle to effectively distinguish hard negatives, which are visually similar but geographically distant. To address this issue, we propose a novel spatially regularized contrastive learning strategy that integrates a semivariogram, which is a geostatistical tool for modeling how spatial correlation changes with distance. We fit the semivariogram by relating the distance of images in feature space to their geographical distance, capturing the expected visual content in a spatial correlation. With the fitted semivariogram, we define the expected visual dissimilarity at a given spatial distance as reference to identify hard negatives and false negatives. We integrate this strategy into GeoCLIP and evaluate it on the OSV5M dataset, demonstrating that explicitly modeling spatial priors improves image-based geo-localization performance, particularly at finer granularity.",
        "arxiv_id": "2509.21573",
        "ARXIVID": "2509.21573",
        "COMMENT": "Matches criterion 1 (spatial understanding) as it introduces a spatially regularized contrastive learning strategy for geolocalization, explicitly modeling spatial priors using semivariograms.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.22415": {
        "authors": [
            "Jiawei Liang",
            "Ruoyu Chen",
            "Xianghao Jiao",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Zheng Hu",
            "Xiaochun Cao"
        ],
        "title": "Explaining multimodal LLMs via intra-modal token interactions",
        "abstract": "arXiv:2509.22415v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \\textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.",
        "arxiv_id": "2509.22415",
        "ARXIVID": "2509.22415",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) as it proposes new interpretability methods for MLLMs, focusing on intra-modal token interactions and providing new insights into model behavior.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.21787": {
        "authors": [
            "Dwip Dalal",
            "Gautam Vashishtha",
            "Anku Ranui",
            "Aishwarya Reganti",
            "Parth Patwa",
            "Mohd Sarique",
            "Chandan Gupta",
            "Keshav Nath",
            "Viswanatha Reddy",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das",
            "Amit Sheth",
            "Asif Ekbal"
        ],
        "title": "DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images",
        "abstract": "arXiv:2509.21787v1 Announce Type: new  Abstract: The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.",
        "arxiv_id": "2509.21787",
        "ARXIVID": "2509.21787",
        "COMMENT": "This paper introduces DeHate, a stable diffusion-based multimodal approach for mitigating hate speech in images, and presents a new VLM (DeHater) for dehatification tasks. This matches criterion 2 (shows new VLLMs/MLLMs) and is relevant to vision-language models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.22262": {
        "authors": [
            "Yujian Yuan",
            "Changjie Wu",
            "Xinyuan Chang",
            "Sijin Wang",
            "Hang Zhang",
            "Shiyi Liang",
            "Shuang Zeng",
            "Mu Xu"
        ],
        "title": "UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data",
        "abstract": "arXiv:2509.22262v1 Announce Type: new  Abstract: Large-scale map construction is foundational for critical applications such as autonomous driving and navigation systems. Traditional large-scale map construction approaches mainly rely on costly and inefficient special data collection vehicles and labor-intensive annotation processes. While existing satellite-based methods have demonstrated promising potential in enhancing the efficiency and coverage of map construction, they exhibit two major limitations: (1) inherent drawbacks of satellite data (e.g., occlusions, outdatedness) and (2) inefficient vectorization from perception-based methods, resulting in discontinuous and rough roads that require extensive post-processing. This paper presents a novel generative framework, UniMapGen, for large-scale map construction, offering three key innovations: (1) representing lane lines as \\textbf{discrete sequence} and establishing an iterative strategy to generate more complete and smooth map vectors than traditional perception-based methods. (2) proposing a flexible architecture that supports \\textbf{multi-modal} inputs, enabling dynamic selection among BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3) developing a \\textbf{state update} strategy for global continuity and consistency of the constructed large-scale map. UniMapGen achieves state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen can infer occluded roads and predict roads missing from dataset annotations. Our code will be released.",
        "arxiv_id": "2509.22262",
        "ARXIVID": "2509.22262",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a generative, multi-modal framework for large-scale map construction, with novel representation and inference strategies.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.22485": {
        "authors": [
            "Guohui Zhang",
            "Hu Yu",
            "Xiaoxiao Ma",
            "JingHao Zhang",
            "Yaning Pan",
            "Mingde Yao",
            "Jie Xiao",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "title": "Group Critical-token Policy Optimization for Autoregressive Image Generation",
        "abstract": "arXiv:2509.22485v1 Announce Type: new  Abstract: Recent studies have extended Reinforcement Learning with Verifiable Rewards (RLVR) to autoregressive (AR) visual generation and achieved promising progress. However, existing methods typically apply uniform optimization across all image tokens, while the varying contributions of different image tokens for RLVR's training remain unexplored. In fact, the key obstacle lies in how to identify more critical image tokens during AR generation and implement effective token-wise optimization for them. To tackle this challenge, we propose $\\textbf{G}$roup $\\textbf{C}$ritical-token $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{GCPO}$), which facilitates effective policy optimization on critical tokens. We identify the critical tokens in RLVR-based AR generation from three perspectives, specifically: $\\textbf{(1)}$ Causal dependency: early tokens fundamentally determine the later tokens and final image effect due to unidirectional dependency; $\\textbf{(2)}$ Entropy-induced spatial structure: tokens with high entropy gradients correspond to image structure and bridges distinct visual regions; $\\textbf{(3)}$ RLVR-focused token diversity: tokens with low visual similarity across a group of sampled images contribute to richer token-level diversity. For these identified critical tokens, we further introduce a dynamic token-wise advantage weight to encourage exploration, based on confidence divergence between the policy model and reference model. By leveraging 30\\% of the image tokens, GCPO achieves better performance than GRPO with full tokens. Extensive experiments on multiple text-to-image benchmarks for both AR models and unified multimodal models demonstrate the effectiveness of GCPO for AR visual generation.",
        "arxiv_id": "2509.22485",
        "ARXIVID": "2509.22485",
        "COMMENT": "This paper proposes a new reinforcement learning method (GCPO) for autoregressive image generation, focusing on token-wise optimization. It is relevant to criterion 4 (vision foundation models and applications) and also to generative modeling in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.21375": {
        "authors": [
            "Aleksa Jelaca",
            "Ying Jiao",
            "Chang Tian",
            "Marie-Francine Moens"
        ],
        "title": "Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis",
        "abstract": "arXiv:2509.21375v1 Announce Type: new  Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.",
        "arxiv_id": "2509.21375",
        "ARXIVID": "2509.21375",
        "COMMENT": "This paper proposes a new prompt engineering framework for counterfactual text-to-image synthesis, constructing a new dataset and improving controllability. It is relevant to criterion 4 (vision foundation models and applications), especially in generative modeling and prompt engineering for VLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.22524": {
        "authors": [
            "Alexandra Gomez-Villa",
            "Pablo Hern\\'andez-C\\'amara",
            "Muhammad Atif Butt",
            "Valero Laparra",
            "Jesus Malo",
            "Javier Vazquez-Corral"
        ],
        "title": "Color Names in Vision-Language Models",
        "abstract": "arXiv:2509.22524v1 Announce Type: new  Abstract: Color serves as a fundamental dimension of human visual perception and a primary means of communicating about objects and scenes. As vision-language models (VLMs) become increasingly prevalent, understanding whether they name colors like humans is crucial for effective human-AI interaction. We present the first systematic evaluation of color naming capabilities across VLMs, replicating classic color naming methodologies using 957 color samples across five representative models. Our results show that while VLMs achieve high accuracy on prototypical colors from classical studies, performance drops significantly on expanded, non-prototypical color sets. We identify 21 common color terms that consistently emerge across all models, revealing two distinct approaches: constrained models using predominantly basic terms versus expansive models employing systematic lightness modifiers. Cross-linguistic analysis across nine languages demonstrates severe training imbalances favoring English and Chinese, with hue serving as the primary driver of color naming decisions. Finally, ablation studies reveal that language model architecture significantly influences color naming independent of visual processing capabilities.",
        "arxiv_id": "2509.22524",
        "ARXIVID": "2509.22524",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and their applications), as it systematically evaluates color naming in VLMs, providing empirical insights into their perceptual capabilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22063": {
        "authors": [
            "Chao Huang",
            "Susan Liang",
            "Yapeng Tian",
            "Anurag Kumar",
            "Chenliang Xu"
        ],
        "title": "High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling",
        "abstract": "arXiv:2509.22063v1 Announce Type: new  Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.",
        "arxiv_id": "2509.22063",
        "ARXIVID": "2509.22063",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and your friend's interest in generative modeling in multi-modal learning. Proposes a diffusion-based audio-visual separation framework using generative models, but not directly a VLLM/MLLM.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.21750": {
        "authors": [
            "Yu Li",
            "Da Chang",
            "Xi Xiao"
        ],
        "title": "KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields",
        "abstract": "arXiv:2509.21750v1 Announce Type: new  Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.",
        "arxiv_id": "2509.21750",
        "ARXIVID": "2509.21750",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it extends SAM (Segment Anything Model) with anatomical knowledge for medical image segmentation. Also relevant to criterion 1 (spatial understanding) due to anatomical priors and boundary refinement.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22244": {
        "authors": [
            "Junyi Wu",
            "Zhiteng Li",
            "Haotong Qin",
            "Xiaohong Liu",
            "Linghe Kong",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing",
        "abstract": "arXiv:2509.22244v1 Announce Type: new  Abstract: Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
        "arxiv_id": "2509.22244",
        "ARXIVID": "2509.22244",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Presents a novel, efficient image editing framework using diffusion models with new spatial attention mechanisms.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22400": {
        "authors": [
            "Xinhao Zhong",
            "Yimin Zhou",
            "Zhiqi Zhang",
            "Junhao Li",
            "Yi Sun",
            "Bin Chen",
            "Shu-Tao Xia",
            "Ke Xu"
        ],
        "title": "Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models",
        "abstract": "arXiv:2509.22400v1 Announce Type: new  Abstract: The rapid progress of visual autoregressive (VAR) models has brought new opportunities for text-to-image generation, but also heightened safety concerns. Existing concept erasure techniques, primarily designed for diffusion models, fail to generalize to VARs due to their next-scale token prediction paradigm. In this paper, we first propose a novel VAR Erasure framework VARE that enables stable concept erasure in VAR models by leveraging auxiliary visual tokens to reduce fine-tuning intensity. Building upon this, we introduce S-VARE, a novel and effective concept erasure method designed for VAR, which incorporates a filtered cross entropy loss to precisely identify and minimally adjust unsafe visual tokens, along with a preservation loss to maintain semantic fidelity, addressing the issues such as language drift and reduced diversity introduce by na\\\"ive fine-tuning. Extensive experiments demonstrate that our approach achieves surgical concept erasure while preserving generation quality, thereby closing the safety gap in autoregressive text-to-image generation by earlier methods.",
        "arxiv_id": "2509.22400",
        "ARXIVID": "2509.22400",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications). Proposes a new method for surgical concept erasure in visual autoregressive models (VARs), addressing safety in text-to-image generation. Focus is on model safety rather than new spatial or embodied intelligence.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.21351": {
        "authors": [
            "Valentin Samokhin",
            "Boris Shirokikh",
            "Mikhail Goncharov",
            "Dmitriy Umerenkov",
            "Maksim Bobrin",
            "Ivan Oseledets",
            "Dmitry Dylov",
            "Mikhail Belyaev"
        ],
        "title": "Random Direct Preference Optimization for Radiography Report Generation",
        "abstract": "arXiv:2509.21351v1 Announce Type: new  Abstract: Radiography Report Generation (RRG) has gained significant attention in medical image analysis as a promising tool for alleviating the growing workload of radiologists. However, despite numerous advancements, existing methods have yet to achieve the quality required for deployment in real-world clinical settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated remarkable progress in the general domain by adopting training strategies originally designed for Large Language Models (LLMs), such as alignment techniques. In this paper, we introduce a model-agnostic framework to enhance RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages random contrastive sampling to construct training pairs, eliminating the need for reward models or human preference annotations. Experiments on supplementing three state-of-the-art models with our Random DPO show that our method improves clinical performance metrics by up to 5%, without requiring any additional training data.",
        "arxiv_id": "2509.21351",
        "ARXIVID": "2509.21351",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) as it proposes a model-agnostic framework for improving radiography report generation using DPO, leveraging VLMs and alignment techniques. Also relevant for clever statistical tricks (random contrastive sampling).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.21420": {
        "authors": [
            "Jian Liu",
            "Chunshi Wang",
            "Song Guo",
            "Haohan Weng",
            "Zhen Zhou",
            "Zhiqi Li",
            "Jiaao Yu",
            "Yiling Zhu",
            "Jing Xu",
            "Biwen Lei",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "title": "QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models",
        "abstract": "arXiv:2509.21420v1 Announce Type: new  Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of professional 3D content creation. However, existing generative models generate quad meshes by first generating triangle meshes and then merging triangles into quadrilaterals with some specific rules, which typically produces quad meshes with poor topology. In this paper, we introduce QuadGPT, the first autoregressive framework for generating quadrilateral meshes in an end-to-end manner. QuadGPT formulates this as a sequence prediction paradigm, distinguished by two key innovations: a unified tokenization method to handle mixed topologies of triangles and quadrilaterals, and a specialized Reinforcement Learning fine-tuning method tDPO for better generation quality. Extensive experiments demonstrate that QuadGPT significantly surpasses previous triangle-to-quad conversion pipelines in both geometric accuracy and topological quality. Our work establishes a new benchmark for native quad-mesh generation and showcases the power of combining large-scale autoregressive models with topology-aware RL refinement for creating structured 3D assets.",
        "arxiv_id": "2509.21420",
        "ARXIVID": "2509.21420",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and generative modeling in 3D, but not directly about spatial intelligence in embodied agents or VLLMs/MLLMs. Novel autoregressive model for quad mesh generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.21774": {
        "authors": [
            "Yuxin Liu",
            "Fei Wang",
            "Kun Li",
            "Yiqi Nie",
            "Junjie Chen",
            "Yanyan Wei",
            "Zhangling Duan",
            "Zhaohong Jia"
        ],
        "title": "Training-Free Multimodal Deepfake Detection via Graph Reasoning",
        "abstract": "arXiv:2509.21774v1 Announce Type: new  Abstract: Multimodal deepfake detection (MDD) aims to uncover manipulations across visual, textual, and auditory modalities, thereby reinforcing the reliability of modern information systems. Although large vision-language models (LVLMs) exhibit strong multimodal reasoning, their effectiveness in MDD is limited by challenges in capturing subtle forgery cues, resolving cross-modal inconsistencies, and performing task-aligned retrieval. To this end, we propose Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a training-free framework for MDD. GASP-ICL employs a pipeline to preserve semantic relevance while injecting task-aware knowledge into LVLMs. We leverage an MDD-adapted feature extractor to retrieve aligned image-text pairs and build a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer (GSTAS) to capture cross-sample relations and propagate query-aligned signals, producing discriminative exemplars. This enables precise selection of semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust MDD. Experiments on four forgery types show that GASP-ICL surpasses strong baselines, delivering gains without LVLM fine-tuning.",
        "arxiv_id": "2509.21774",
        "ARXIVID": "2509.21774",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) as it proposes a new training-free framework leveraging LVLMs for multimodal deepfake detection, and introduces a novel graph-based reasoning module to enhance LVLMs' performance on this task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.22283": {
        "authors": [
            "Michael Jungo",
            "Andreas Fischer"
        ],
        "title": "Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models",
        "abstract": "arXiv:2509.22283v1 Announce Type: new  Abstract: Rule-based reinforcement learning has been gaining popularity ever since DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In the domain of document analysis, reinforcement learning is not as prevalent, even though many downstream tasks may benefit from the emerging properties of reinforcement learning, particularly the enhanced reason capabilities. We study the effects of rule-based reinforcement learning with the task of Document Image Classification which is one of the most commonly studied downstream tasks in document analysis. We find that reinforcement learning tends to have better generalisation capabilities to out-of-distritbution data, which we examine in three different scenarios, namely out-of-distribution images, unseen classes and different modalities. Our code is available at https://github.com/jungomi/vision-finetune.",
        "arxiv_id": "2509.22283",
        "ARXIVID": "2509.22283",
        "COMMENT": "This paper uses vision-language models (VLMs) for document image classification and explores rule-based reinforcement learning for better generalization. It is relevant to criterion 2 (VLLMs/MLLMs) and also touches on criterion 4 (vision foundation models and applications), but the main focus is on document analysis rather than spatial intelligence or embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21862": {
        "authors": [
            "So Kuroki",
            "Yingtao Tian",
            "Kou Misaki",
            "Takashi Ikegami",
            "Takuya Akiba",
            "Yujin Tang"
        ],
        "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
        "abstract": "arXiv:2509.21862v1 Announce Type: new  Abstract: The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
        "arxiv_id": "2509.21862",
        "ARXIVID": "2509.21862",
        "COMMENT": "Some relevance to criterion 3 (embodied AI, agent-based modeling), but focus is on LLM-driven agent architectures and not specifically spatial or embodied intelligence. Interesting for multi-agent LLM research.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21896": {
        "authors": [
            "Minfeng Zhu",
            "Zi Wang",
            "Sizhe Ji",
            "Zhengtong Du",
            "Junming Ke",
            "Xiao Deng",
            "Zanlang Yin",
            "Xiuqi Huang",
            "Heyu Wang",
            "Wei Chen"
        ],
        "title": "GenesisGeo: Technical Report",
        "abstract": "arXiv:2509.21896v1 Announce Type: new  Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We have open-sourced a large-scale geometry dataset of 21.8 million geometric problems, over 3 million of which contain auxiliary constructions. Specially, we significantly accelerate the symbolic deduction engine DDARN by 120x through theorem matching, combined with a C++ implementation of its core components. Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold medal level) with a dual-model ensemble.",
        "arxiv_id": "2509.21896",
        "ARXIVID": "2509.21896",
        "COMMENT": "Related to spatial reasoning (criterion 1) in the context of automated theorem proving in geometry, but not focused on embodied agents or spatial intelligence in embodied AI. More about symbolic/neuro-symbolic reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.22085": {
        "authors": [
            "Hadar Peer",
            "Eyal Weiss",
            "Ron Alterovitz",
            "Oren Salzman"
        ],
        "title": "Generalizing Multi-Objective Search via Objective-Aggregation Functions",
        "abstract": "arXiv:2509.22085v1 Announce Type: new  Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world robotic systems need to simultaneously balance multiple, often conflicting objectives. Recent works explore complex interactions between objectives, leading to problem formulations that do not allow the usage of out-of-the-box state-of-the-art MOS algorithms. In this paper, we suggest a generalized problem formulation that optimizes solution objectives via aggregation functions of hidden (search) objectives. We show that our formulation supports the application of standard MOS algorithms, necessitating only to properly extend several core operations to reflect the specific aggregation functions employed. We demonstrate our approach in several diverse robotics planning problems, spanning motion-planning for navigation, manipulation and planning fr medical systems under obstacle uncertainty as well as inspection planning, and route planning with different road types. We solve the problems using state-of-the-art MOS algorithms after properly extending their core operations, and provide empirical evidence that they outperform by orders of magnitude the vanilla versions of the algorithms applied to the same problems but without objective aggregation.",
        "arxiv_id": "2509.22085",
        "ARXIVID": "2509.22085",
        "COMMENT": "Somewhat related to criterion 3 (embodied AI, new methods/benchmarks) as it discusses generalizing multi-objective search in robotics, but does not focus on spatial intelligence or embodied agents in the sense of perception or vision-language models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.21360": {
        "authors": [
            "Xingkai Peng",
            "Jun Jiang",
            "Meng Tong",
            "Shuai Li",
            "Weiming Zhang",
            "Nenghai Yu",
            "Kejiang Chen"
        ],
        "title": "Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models",
        "abstract": "arXiv:2509.21360v1 Announce Type: new  Abstract: Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.",
        "arxiv_id": "2509.21360",
        "ARXIVID": "2509.21360",
        "COMMENT": "Somewhat related to criterion 2 (VLLMs/MLLMs) as it discusses attacks on text-to-image models and uses visual language models for adversarial prompt generation, but the focus is on safety/jailbreak attacks rather than new model architectures or methods.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.22622": {
        "authors": [
            "Shuai Yang",
            "Wei Huang",
            "Ruihang Chu",
            "Yicheng Xiao",
            "Yuyang Zhao",
            "Xianbang Wang",
            "Muyang Li",
            "Enze Xie",
            "Yingcong Chen",
            "Yao Lu",
            "Song Han",
            "Yukang Chen"
        ],
        "title": "LongLive: Real-time Interactive Long Video Generation",
        "abstract": "arXiv:2509.22622v1 Announce Type: new  Abstract: We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.",
        "arxiv_id": "2509.22622",
        "ARXIVID": "2509.22622",
        "COMMENT": "This paper presents LongLive, a new framework for real-time, interactive long video generation. While it is relevant to generative modeling and computer vision, it does not focus on spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about video generation efficiency and interactivity.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.22044": {
        "authors": [
            "Ziqi Wang",
            "Boye Niu",
            "Zhongli Li",
            "Linghui Meng",
            "Jing Liu",
            "Zhi Zheng",
            "Tong Xu",
            "Hua Wu",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning",
        "abstract": "arXiv:2509.22044v1 Announce Type: new  Abstract: Recent Large Reasoning Models have achieved significant improvements in complex task-solving capabilities by allocating more computation at the inference stage with a \"thinking longer\" paradigm. Even as the foundational reasoning capabilities of models advance rapidly, the persistent gap between a model's performance in a single attempt and its latent potential, often revealed only across multiple solution paths, starkly highlights the disparity between its realized and inherent capabilities. To address this, we present A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge the gap between a model's potential and its actual performance. In this framework, an \"explorer\" model first generates potential solutions in parallel through repeated sampling. Subsequently,a \"synthesizer\" model integrates these references for a more refined, second stage of reasoning. This two-stage process allows computation to be scaled orthogonally to existing sequential methods. Our work makes two key innovations: First, we present A2R as a plug-and-play parallel reasoning framework that explicitly enhances a model's capabilities on complex questions. For example, using our framework, the Qwen3-8B-distill model achieves a 75% performance improvement compared to its self-consistency baseline. Second, through a systematic analysis of the explorer and synthesizer roles, we identify an effective asymmetric scaling paradigm. This insight leads to A2R-Efficient, a \"small-to-big\" variant that combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration surpasses the average performance of a monolithic Qwen3-32B model at a nearly 30% lower cost. Collectively, these results show that A2R is not only a performance-boosting framework but also an efficient and practical solution for real-world applications.",
        "arxiv_id": "2509.22044",
        "ARXIVID": "2509.22044",
        "COMMENT": "This paper proposes a two-stage reasoning framework (A2R) for parallel reasoning in large reasoning models. While it is relevant to reasoning in LLMs, it does not directly address spatial intelligence, embodied agents, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21918": {
        "authors": [
            "Hong Mo",
            "Xiong Zhang",
            "Tengfei Shi",
            "Zhongbo Wu"
        ],
        "title": "Multi-View Crowd Counting With Self-Supervised Learning",
        "abstract": "arXiv:2509.21918v1 Announce Type: new  Abstract: Multi-view counting (MVC) methods have attracted significant research attention and stimulated remarkable progress in recent years. Despite their success, most MVC methods have focused on improving performance by following the fully supervised learning (FSL) paradigm, which often requires large amounts of annotated data. In this work, we propose SSLCounter, a novel self-supervised learning (SSL) framework for MVC that leverages neural volumetric rendering to alleviate the reliance on large-scale annotated datasets. SSLCounter learns an implicit representation w.r.t. the scene, enabling the reconstruction of continuous geometry shape and the complex, view-dependent appearance of their 2D projections via differential neural rendering. Owing to its inherent flexibility, the key idea of our method can be seamlessly integrated into exsiting frameworks. Notably, extensive experiments demonstrate that SSLCounter not only demonstrates state-of-the-art performances but also delivers competitive performance with only using 70% proportion of training data, showcasing its superior data efficiency across multiple MVC benchmarks.",
        "arxiv_id": "2509.21918",
        "ARXIVID": "2509.21918",
        "COMMENT": "This paper presents a self-supervised learning framework for multi-view crowd counting using neural volumetric rendering. While it is a vision paper with some generative modeling, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22150": {
        "authors": [
            "Zhiqiang Tian",
            "Weigang Li",
            "Junwei Hu",
            "Chunhua Deng"
        ],
        "title": "Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions",
        "abstract": "arXiv:2509.22150v1 Announce Type: new  Abstract: Classification tasks in 3D point clouds often assume that class events \\replaced{are }{follow }independent and identically distributed (IID), although this assumption destroys the correlation between classes. This \\replaced{study }{paper }proposes a classification strategy, \\textbf{J}oint \\textbf{G}raph \\textbf{E}ntropy \\textbf{K}nowledge \\textbf{D}istillation (JGEKD), suitable for non-independent and identically distributed 3D point cloud data, \\replaced{which }{the strategy } achieves knowledge transfer of class correlations through knowledge distillation by constructing a loss function based on joint graph entropy. First\\deleted{ly}, we employ joint graphs to capture add{the }hidden relationships between classes\\replaced{ and}{,} implement knowledge distillation to train our model by calculating the entropy of add{add }graph.\\replaced{ Subsequently}{ Then}, to handle 3D point clouds \\deleted{that is }invariant to spatial transformations, we construct \\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge distillation and teacher-knowledge distillation, to facilitate information transfer between different transformation forms of the same data. \\replaced{In addition}{ Additionally}, we use the above framework to achieve knowledge transfer between point clouds and their corrupted forms, and increase the robustness against corruption of model. Extensive experiments on ScanObject, ModelNet40, ScanntV2\\_cls and ModelNet-C demonstrate that the proposed strategy can achieve competitive results.",
        "arxiv_id": "2509.22150",
        "ARXIVID": "2509.22150",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it proposes a new knowledge distillation method for point cloud classification, but not directly about spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.21394": {
        "authors": [
            "Qiyu Ma",
            "Wanli Ni",
            "Zhijin Qin"
        ],
        "title": "Large AI Model-Enabled Generative Semantic Communications for Image Transmission",
        "abstract": "arXiv:2509.21394v1 Announce Type: new  Abstract: The rapid development of generative artificial intelligence (AI) has introduced significant opportunities for enhancing the efficiency and accuracy of image transmission within semantic communication systems. Despite these advancements, existing methodologies often neglect the difference in importance of different regions of the image, potentially compromising the reconstruction quality of visually critical content. To address this issue, we introduce an innovative generative semantic communication system that refines semantic granularity by segmenting images into key and non-key regions. Key regions, which contain essential visual information, are processed using an image oriented semantic encoder, while non-key regions are efficiently compressed through an image-to-text modeling approach. Additionally, to mitigate the substantial storage and computational demands posed by large AI models, the proposed system employs a lightweight deployment strategy incorporating model quantization and low-rank adaptation fine-tuning techniques, significantly boosting resource utilization without sacrificing performance. Simulation results demonstrate that the proposed system outperforms traditional methods in terms of both semantic fidelity and visual quality, thereby affirming its effectiveness for image transmission tasks.",
        "arxiv_id": "2509.21394",
        "ARXIVID": "2509.21394",
        "COMMENT": "Somewhat related to criterion 2 (VLLMs/MLLMs) as it discusses large AI models for generative semantic communication, but the focus is on image transmission rather than vision-language reasoning or spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.21365": {
        "authors": [
            "Zhicheng Du",
            "Qingyang Shi",
            "Jiasheng Lu",
            "Yingshan Liang",
            "Xinyu Zhang",
            "Yiran Wang",
            "Peiwu Qin"
        ],
        "title": "MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation",
        "abstract": "arXiv:2509.21365v1 Announce Type: new  Abstract: The multimodal relevance metric is usually borrowed from the embedding ability of pretrained contrastive learning models for bimodal data, which is used to evaluate the correlation between cross-modal data (e.g., CLIP). However, the commonly used evaluation metrics are only suitable for the associated analysis between two modalities, which greatly limits the evaluation of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation metric for the relevance of multiple modalities (N modalities, N>=3) via multimodal joint representation for the first time. The ability of multimodal joint representation to integrate multiple modalities into the same latent space can accurately represent different modalities at one scale, providing support for fair relevance scoring. Extensive experiments have shown that MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves as a more reliable metric for evaluating similarity on large-scale multimodal datasets and multimodal model performance evaluation.",
        "arxiv_id": "2509.21365",
        "ARXIVID": "2509.21365",
        "COMMENT": "This paper introduces MAJORScore, a new metric for evaluating multimodal relevance via joint representation. It is relevant to multi-modal learning and evaluation, but does not present a new VLLM/MLLM, spatial intelligence, embodied AI, or vision foundation model. It is a statistical trick for evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21633": {
        "authors": [
            "Georgios Chochlakis",
            "Jackson Trager",
            "Vedant Jhaveri",
            "Nikhil Ravichandran",
            "Alexandros Potamianos",
            "Shrikanth Narayanan"
        ],
        "title": "Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries",
        "abstract": "arXiv:2509.21633v1 Announce Type: new  Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or fuzzy multi-label classification that quantify semantic relatedness between predicted and gold labels. Unlike the conventional F1 metrics that treat semantically related predictions as complete failures, Semantic F1 incorporates a label similarity matrix to compute soft precision-like and recall-like scores, from which the Semantic F1 scores are derived. Unlike existing similarity-based metrics, our novel two-step precision-recall formulation enables the comparison of label sets of arbitrary sizes without discarding labels or forcing matches between dissimilar labels. By granting partial credit for semantically related but nonidentical labels, Semantic F1 better reflects the realities of domains marked by human disagreement or fuzzy category boundaries. In this way, it provides fairer evaluations: it recognizes that categories overlap, that annotators disagree, and that downstream decisions based on similar predictions lead to similar outcomes. Through theoretical justification and extensive empirical validation on synthetic and real data, we show that Semantic F1 demonstrates greater interpretability and ecological validity. Because it requires only a domain-appropriate similarity matrix, which is robust to misspecification, and not a rigid ontology, it is applicable across tasks and modalities.",
        "arxiv_id": "2509.21633",
        "ARXIVID": "2509.21633",
        "COMMENT": "This paper proposes a new evaluation metric (Semantic F1) for fuzzy multi-label classification, which is a statistical trick for evaluation. While interesting, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21743": {
        "authors": [
            "Ammar Ahmed",
            "Azal Ahmad Khan",
            "Ayaan Ahmad",
            "Sheng Di",
            "Zirui Liu",
            "Ali Anwar"
        ],
        "title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts",
        "abstract": "arXiv:2509.21743v1 Announce Type: new  Abstract: Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.",
        "arxiv_id": "2509.21743",
        "ARXIVID": "2509.21743",
        "COMMENT": "This paper proposes a new method (Retrieval-of-Thought) for efficient reasoning in large reasoning models by reusing prior reasoning steps. While it is a methodological improvement in reasoning, it does not focus on spatial understanding, VLLMs/MLLMs, embodied agents, or vision foundation models. It is more about language model efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22132": {
        "authors": [
            "Jingjing Lu",
            "Huilong Pi",
            "Yunchuan Qin",
            "Zhuo Tang",
            "Ruihui Li"
        ],
        "title": "Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud",
        "abstract": "arXiv:2509.22132v1 Announce Type: new  Abstract: Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.",
        "arxiv_id": "2509.22132",
        "ARXIVID": "2509.22132",
        "COMMENT": "Some relevance to criterion 1 (spatial understanding in 3D point clouds), but the focus is on self-supervised point cloud completion, not embodied agents or spatial intelligence in agents.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.22613": {
        "authors": [
            "Siwei Wang",
            "Yifei Shen",
            "Haoran Sun",
            "Shi Feng",
            "Shang-Hua Teng",
            "Li Dong",
            "Yaru Hao",
            "Wei Chen"
        ],
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
        "abstract": "arXiv:2509.22613v1 Announce Type: new  Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
        "arxiv_id": "2509.22613",
        "ARXIVID": "2509.22613",
        "COMMENT": "Not directly related to any criterion. Theoretical analysis of RL for LLM planning, with some application to Blocksworld (a classic planning benchmark), but not focused on spatial intelligence, embodied agents, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21783": {
        "authors": [
            "Tianci Wu",
            "Guangming Zhu",
            "Jiang Lu",
            "Siyuan Wang",
            "Ning Wang",
            "Nuoye Xiong",
            "Zhang Liang"
        ],
        "title": "Prompt-guided Representation Disentanglement for Action Recognition",
        "abstract": "arXiv:2509.21783v1 Announce Type: new  Abstract: Action recognition is a fundamental task in video understanding. Existing methods typically extract unified features to process all actions in one video, which makes it challenging to model the interactions between different objects in multi-action scenarios. To alleviate this issue, we explore disentangling any specified actions from complex scenes as an effective solution. In this paper, we propose Prompt-guided Disentangled Representation for Action Recognition (ProDA), a novel framework that disentangles any specified actions from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs) and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural Network (GPNN) in generating action-specific representations. Furthermore, we design a video-adapted GPNN that aggregates information using dynamic weights. Experiments in video action recognition demonstrate the effectiveness of our approach when compared with the state-of-the-art methods. Our code can be found in https://github.com/iamsnaping/ProDA.git",
        "arxiv_id": "2509.21783",
        "ARXIVID": "2509.21783",
        "COMMENT": "Somewhat relevant to general computer vision and action recognition, but does not match any specific criterion (not spatial intelligence on embodied agents, not VLLM/MLLM, not a new benchmark or simulator, not vision foundation model).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21715": {
        "authors": [
            "Xu Yang",
            "Gady Agam"
        ],
        "title": "Motion-Aware Transformer for Multi-Object Tracking",
        "abstract": "arXiv:2509.21715v1 Announce Type: new  Abstract: Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.",
        "arxiv_id": "2509.21715",
        "ARXIVID": "2509.21715",
        "COMMENT": "Somewhat relevant to general computer vision interests (multi-object tracking), but does not match any specific criterion (not spatial intelligence on embodied agents, not VLLM/MLLM, not a new benchmark or simulator, not vision foundation model).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21825": {
        "authors": [
            "Jaehyun Nam",
            "Jinsung Yoon",
            "Jiefeng Chen",
            "Jinwoo Shin",
            "Tomas Pfister"
        ],
        "title": "DS-STAR: Data Science Agent via Iterative Planning and Verification",
        "abstract": "arXiv:2509.21825v1 Announce Type: new  Abstract: Data science, which transforms raw data into actionable insights, is critical for data-driven decision-making. However, these tasks are often complex, involving steps for exploring multiple data sources and synthesizing findings to deliver insightful answers. While large language models (LLMs) show significant promise in automating this process, they often struggle with heterogeneous data formats and generate sub-optimal analysis plans, as verifying plan sufficiency is inherently difficult without ground-truth labels for such open-ended tasks. To overcome these limitations, we introduce DS-STAR, a novel data science agent. Specifically, DS-STAR makes three key contributions: (1) a data file analysis module that automatically explores and extracts context from diverse data formats, including unstructured types; (2) a verification step where an LLM-based judge evaluates the sufficiency of the analysis plan at each stage; and (3) a sequential planning mechanism that starts with a simple, executable plan and iteratively refines it based on the DS-STAR's feedback until its sufficiency is verified. This iterative refinement allows DS-STAR to reliably navigate complex analyses involving diverse data sources. Our experiments show that DS-STAR achieves state-of-the-art performance across three challenging benchmarks: DABStep, KramaBench, and DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks that require processing multiple data files with heterogeneous formats.",
        "arxiv_id": "2509.21825",
        "ARXIVID": "2509.21825",
        "COMMENT": "Does not directly match any specific criterion; focuses on data science agents and iterative planning/verification, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21385": {
        "authors": [
            "Eric Enouen",
            "Sainyam Galhotra"
        ],
        "title": "Debugging Concept Bottleneck Models through Removal and Retraining",
        "abstract": "arXiv:2509.21385v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to predict the final task label, enabling domain experts to not only validate the CBM's predictions, but also intervene on incorrect concepts at test time. However, these interventions fail to address systemic misalignment between the CBM and the expert's reasoning, such as when the model learns shortcuts from biased data. To address this, we present a general interpretable debugging framework for CBMs that follows a two-step process of Removal and Retraining. In the Removal step, experts use concept explanations to identify and remove any undesired concepts. In the Retraining step, we introduce CBDebug, a novel method that leverages the interpretability of CBMs as a bridge for converting concept-level user feedback into sample-level auxiliary labels. These labels are then used to apply supervised bias mitigation and targeted augmentation, reducing the model's reliance on undesired concepts. We evaluate our framework with both real and automated expert feedback, and find that CBDebug significantly outperforms prior retraining methods across multiple CBM architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious correlations.",
        "arxiv_id": "2509.21385",
        "ARXIVID": "2509.21385",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it discusses interpretability and debugging in concept bottleneck models, but not directly about spatial intelligence, VLLMs/MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22365": {
        "authors": [
            "Defan Chen",
            "Yaohua Hu",
            "Luchan Zhang"
        ],
        "title": "HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network for UAV Photography",
        "abstract": "arXiv:2509.22365v1 Announce Type: new  Abstract: The real-time detection of small objects in complex scenes, such as the unmanned aerial vehicle (UAV) photography captured by drones, has dual challenges of detecting small targets (<32 pixels) and maintaining real-time efficiency on resource-constrained platforms. While YOLO-series detectors have achieved remarkable success in real-time large object detection, they suffer from significantly higher false negative rates for drone-based detection where small objects dominate, compared to large object scenarios. This paper proposes HierLight-YOLO, a hierarchical feature fusion and lightweight model that enhances the real-time detection of small objects, based on the YOLOv8 architecture. We propose the Hierarchical Extended Path Aggregation Network (HEPAN), a multi-scale feature fusion method through hierarchical cross-level connections, enhancing the small object detection accuracy. HierLight-YOLO includes two innovative lightweight modules: Inverted Residual Depthwise Convolution Block (IRDCB) and Lightweight Downsample (LDown) module, which significantly reduce the model's parameters and computational complexity without sacrificing detection capabilities. Small object detection head is designed to further enhance spatial resolution and feature fusion to tackle the tiny object (4 pixels) detection. Comparison experiments and ablation studies on the VisDrone2019 benchmark demonstrate state-of-the-art performance of HierLight-YOLO.",
        "arxiv_id": "2509.22365",
        "ARXIVID": "2509.22365",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) as it proposes a lightweight object detection model for UAV imagery, but does not introduce a new foundation model or multi-modal reasoning.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.21549": {
        "authors": [
            "Dongkyu Cho",
            "Amy B. Z. Zhang",
            "Bilel Fehri",
            "Sheng Wang",
            "Rumi Chunara",
            "Rui Song",
            "Hengrui Cai"
        ],
        "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
        "abstract": "arXiv:2509.21549v1 Announce Type: new  Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
        "arxiv_id": "2509.21549",
        "ARXIVID": "2509.21549",
        "COMMENT": "This paper introduces a new method for aligning reasoning paths in LLMs using decision pivots. It is not focused on spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about language model reasoning verification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}