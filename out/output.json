{
    "2506.17707": {
        "authors": [
            "Jihyun Kim",
            "Junho Park",
            "Kyeongbo Kong",
            "Suk-Ju Kang"
        ],
        "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models",
        "abstract": "arXiv:2506.17707v1 Announce Type: new  Abstract: We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room's each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in generating and editing 3D room meshes, and prove our framework's superiority to an existing model quantitatively and qualitatively. Project page is available in https://jihyun0510.github.io/Programmable_Room_Page/.",
        "arxiv_id": "2506.17707",
        "ARXIVID": "2506.17707",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a framework (Programmable-Room) for generating and editing 3D room meshes using LLMs, focusing on spatial intelligence and embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.18385": {
        "authors": [
            "Nianchen Deng",
            "Lixin Gu",
            "Shenglong Ye",
            "Yinan He",
            "Zhe Chen",
            "Songze Li",
            "Haomin Wang",
            "Xingguang Wei",
            "Tianshuo Yang",
            "Min Dou",
            "Tong He",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Yi Wang",
            "Botian Shi",
            "Yanting Zhang",
            "Jifeng Dai",
            "Yu Qiao",
            "Hongjie Zhang",
            "Wenhai Wang"
        ],
        "title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models",
        "abstract": "arXiv:2506.18385v1 Announce Type: new  Abstract: Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.",
        "arxiv_id": "2506.18385",
        "ARXIVID": "2506.18385",
        "COMMENT": "Matches criterion 1 as it introduces a new dataset and benchmark for spatial reasoning in vision-language models, which is directly related to spatial understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.17873": {
        "authors": [
            "Guankun Wang",
            "Wenjin Mo",
            "Junyi Wang",
            "Long Bai",
            "Kun Yuan",
            "Ming Hu",
            "Jinlin Wu",
            "Junjun He",
            "Yiming Huang",
            "Nicolas Padoy",
            "Zhen Lei",
            "Hongbin Liu",
            "Nassir Navab",
            "Hongliang Ren"
        ],
        "title": "SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model",
        "abstract": "arXiv:2506.17873v1 Announce Type: new  Abstract: Recent advances in Multimodal Large Language Models have demonstrated great potential in the medical domain, facilitating users to understand surgical scenes and procedures. Beyond image-based methods, the exploration of Video Large Language Models (Vid-LLMs) has emerged as a promising avenue for capturing the complex sequences of information involved in surgery. However, there is still a lack of Vid-LLMs specialized for fine-grained surgical video understanding tasks, which is crucial for analyzing specific processes or details within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K dataset which consists of over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Furthermore, we introduce the StageFocus mechanism which is a two-stage framework performing the multi-grained, progressive understanding of surgical videos. We also develop the Multi-frequency Fusion Attention to effectively integrate low and high-frequency visual tokens, ensuring the retention of critical information. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing complex procedural contexts.",
        "arxiv_id": "2506.17873",
        "ARXIVID": "2506.17873",
        "COMMENT": "Matches criteria 2 as it introduces SurgVidLM, a video large language model (Vid-LLM) for fine-grained surgical video understanding, which is a novel application of multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.18862": {
        "authors": [
            "Zhongbin Guo",
            "Yuhao Wang",
            "Ping Jian",
            "Xinyue Chen",
            "Wei Peng",
            "Ertai E"
        ],
        "title": "TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting",
        "abstract": "arXiv:2506.18862v1 Announce Type: new  Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding.",
        "arxiv_id": "2506.18862",
        "ARXIVID": "2506.18862",
        "COMMENT": "Matches criteria 2 and 4 as it introduces a new multimodal large language model (MLLM) for spatio-temporal understanding and forecasting, with novel mechanisms like Semantic-Fused Control Injection.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.17667": {
        "authors": [
            "Lintao Wang",
            "Encheng Su",
            "Jiaqi Liu",
            "Pengze Li",
            "Peng Xia",
            "Jiabei Xiao",
            "Wenlong Zhang",
            "Xinnan Dai",
            "Xi Chen",
            "Yuan Meng",
            "Mingyu Ding",
            "Lei Bai",
            "Wanli Ouyang",
            "Shixiang Tang",
            "Aoran Wang",
            "Xinzhu Ma"
        ],
        "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models",
        "abstract": "arXiv:2506.17667v1 Announce Type: new  Abstract: Physics problem-solving is a challenging domain for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments. To this end, we present PhysUniBench, a large-scale multimodal benchmark designed to evaluate and improve the reasoning capabilities of multimodal large language models (MLLMs) specifically on undergraduate-level physics problems. PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmark's construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels. Through extensive experiments, we observe that current state-of-the-art models encounter substantial challenges in physics reasoning. For example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding. The benchmark and evaluation scripts are available at https://prismax-team.github.io/PhysUniBenchmark/.",
        "arxiv_id": "2506.17667",
        "ARXIVID": "2506.17667",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (PhysUniBench) for evaluating multimodal models on physics reasoning, focusing on a novel angle of multimodal understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.18871": {
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "abstract": "arXiv:2506.18871v1 Announce Type: new  Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
        "arxiv_id": "2506.18871",
        "ARXIVID": "2506.18871",
        "COMMENT": "Matches criterion 2 as it introduces OmniGen2, a new multimodal generative model for diverse tasks, including text-to-image and image editing.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.17561": {
        "authors": [
            "Chongkai Gao",
            "Zixuan Liu",
            "Zhenghao Chi",
            "Junshan Huang",
            "Xin Fei",
            "Yiwen Hou",
            "Yuxuan Zhang",
            "Yudi Lin",
            "Zhirui Fang",
            "Zeyu Jiang",
            "Lin Shao"
        ],
        "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
        "abstract": "arXiv:2506.17561v1 Announce Type: new  Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.",
        "arxiv_id": "2506.17561",
        "ARXIVID": "2506.17561",
        "COMMENT": "Matches criterion 3 as it introduces a unified architecture for vision-language-action models and evaluates planning paradigms, which is related to embodied AI and new methods.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.18234": {
        "authors": [
            "Yue Li",
            "Meng Tian",
            "Dechang Zhu",
            "Jiangtong Zhu",
            "Zhenyu Lin",
            "Zhiwei Xiong",
            "Xinhai Zhao"
        ],
        "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
        "abstract": "arXiv:2506.18234v1 Announce Type: new  Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications.",
        "arxiv_id": "2506.18234",
        "ARXIVID": "2506.18234",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for reasoning and planning in vision-language models for autonomous driving, which is related to embodied AI and new methods.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.18901": {
        "authors": [
            "Wenqiang Sun",
            "Fangyun Wei",
            "Jinjing Zhao",
            "Xi Chen",
            "Zilong Chen",
            "Hongyang Zhang",
            "Jun Zhang",
            "Yan Lu"
        ],
        "title": "From Virtual Games to Real-World Play",
        "abstract": "arXiv:2506.18901v1 Announce Type: new  Abstract: We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/",
        "arxiv_id": "2506.18901",
        "ARXIVID": "2506.18901",
        "COMMENT": "This paper closely matches criterion 3 as it introduces RealPlay, a novel real-world game engine that bridges virtual and real-world scenarios, which is relevant to embodied AI and simulators. It also demonstrates surprising generalization capabilities, which could be of interest.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18839": {
        "authors": [
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Vidit Goel",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Avalon Vinella",
            "Michael Vasilkovsky",
            "Ivan Skorokhodov",
            "Vladislav Shakhrai",
            "Sergey Korolev",
            "Sergey Tulyakov",
            "Peter Wonka"
        ],
        "title": "4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation",
        "abstract": "arXiv:2506.18839v1 Announce Type: new  Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid of video frames and 3D Gaussian particles for each time step using a feed-forward architecture. Our architecture has two main components, a 4D video model and a 4D reconstruction model. In the first part, we analyze current 4D video diffusion architectures that perform spatial and temporal attention either sequentially or in parallel within a two-stream design. We highlight the limitations of existing approaches and introduce a novel fused architecture that performs spatial and temporal attention within a single layer. The key to our method is a sparse attention pattern, where tokens attend to others in the same frame, at the same timestamp, or from the same viewpoint. In the second part, we extend existing 3D reconstruction algorithms by introducing a Gaussian head, a camera token replacement algorithm, and additional dynamic layers and training. Overall, we establish a new state of the art for 4D generation, improving both visual quality and reconstruction capability.",
        "arxiv_id": "2506.18839",
        "ARXIVID": "2506.18839",
        "COMMENT": "Matches criterion 4 as it introduces a novel 4D video and reconstruction model for spatio-temporal grid generation, which is related to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18701": {
        "authors": [
            "Yifan Zhang",
            "Chunli Peng",
            "Boyang Wang",
            "Puyi Wang",
            "Qingcheng Zhu",
            "Fei Kang",
            "Biao Jiang",
            "Zedong Gao",
            "Eric Li",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "title": "Matrix-Game: Interactive World Foundation Model",
        "abstract": "arXiv:2506.18701v1 Announce Type: new  Abstract: We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
        "arxiv_id": "2506.18701",
        "ARXIVID": "2506.18701",
        "COMMENT": "Matches criteria 4 as it introduces Matrix-Game, a foundation model for interactive world generation in Minecraft, with a focus on controllability and visual quality.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18164": {
        "authors": [
            "Varun Belagali",
            "Pierre Marza",
            "Srikar Yellapragada",
            "Zilinghan Li",
            "Tarak Nath Nandi",
            "Ravi K Madduri",
            "Joel Saltz",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou",
            "Dimitris Samaras"
        ],
        "title": "CDG-MAE: Learning Correspondences from Diffusion Generated Views",
        "abstract": "arXiv:2506.18164v1 Announce Type: new  Abstract: Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches.",
        "arxiv_id": "2506.18164",
        "ARXIVID": "2506.18164",
        "COMMENT": "Matches criteria 4 as it introduces a novel self-supervised method (CDG-MAE) leveraging diffusion models for dense correspondence learning, which is relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18372": {
        "authors": [
            "Hieu Nguyen",
            "Phuc-Tan Nguyen",
            "Thien-Phuc Tran",
            "Minh-Quang Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding",
        "abstract": "arXiv:2506.18372v1 Announce Type: new  Abstract: We introduce OpenEvents V1, a large-scale benchmark dataset aimed at advancing event-centric vision-language understanding. Unlike conventional image captioning and retrieval datasets that emphasize surface-level descriptions, OpenEvents V1 focuses on contextual and temporal grounding through two primary tasks: (1) generating rich, event-aware image captions and (2) retrieving event-relevant images based on narrative-style textual queries. The dataset contains over 200,000 news articles and 400,000 associated images sourced from CNN and The Guardian, spanning diverse domains and time periods. We provide extensive baseline results and standardized evaluation protocols for both tasks. OpenEvents V1 establishes a robust foundation for developing multimodal models capable of deep reasoning over complex real-world events. The dataset is available at https://ltnghia.github.io/eventa/openevents-v1",
        "arxiv_id": "2506.18372",
        "ARXIVID": "2506.18372",
        "COMMENT": "Matches criterion 3 as it introduces OpenEvents V1, a new benchmark dataset for multimodal event grounding, focusing on contextual and temporal grounding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17901": {
        "authors": [
            "Yixuan Wu",
            "Yang Zhang",
            "Jian Wu",
            "Philip Torr",
            "Jindong Gu"
        ],
        "title": "PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs",
        "abstract": "arXiv:2506.17901v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such as image captioning and visual question answering. However, they often suffer from over-reliance on spurious correlations, primarily due to linguistic priors that distract the model from leveraging actual visual information. To address these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment framework designed to enhance the visual understanding capabilities and mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal grounding module for both visual grounding, which identifies the referred object in the image, and textual grounding, which generates the rationale for the final answer, ensuring that outputs are anchored in both visual and textual evidence. To mitigate the hallucinations, we introduce a negative rejection mechanism in the visual grounding module to distinguish grounded entities from non-existent objects influenced by linguistic biases. On the textual grounding side, we propose a selective reasoning mechanism that adjusts the model's reasoning strategy based on query complexity. Extensive evaluations are conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench showing significant improvements in fine-grained visual understanding and hallucination suppression.",
        "arxiv_id": "2506.17901",
        "ARXIVID": "2506.17901",
        "COMMENT": "Matches criterion 2 as it introduces a new framework (MMGrounded-PostAlign) to enhance visual understanding and mitigate hallucinations in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.18084": {
        "authors": [
            "Wenzhuo Liu",
            "Yicheng Qiao",
            "Zhen Wang",
            "Qiannan Guo",
            "Zilong Chen",
            "Meihua Zhou",
            "Xinran Li",
            "Letian Wang",
            "Zhiwei Li",
            "Huaping Liu",
            "Wenshuo Wang"
        ],
        "title": "TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving",
        "abstract": "arXiv:2506.18084v1 Announce Type: new  Abstract: Multi-task learning (MTL) can advance assistive driving by exploring inter-task correlations through shared representations. However, existing methods face two critical limitations: single-modality constraints limiting comprehensive scene understanding and inefficient architectures impeding real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient Multimodal Multi-task Learning), a novel framework that jointly optimizes driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition through a two-stage architecture. The first component, the mamba-based multi-view temporal-spatial feature extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal scanning mechanism and global-local spatial attention to efficiently extract low-cost temporal-spatial features from multi-view sequential images. The second component, the MTL-based gated multimodal feature integrator (MGMI), employs task-specific multi-gating modules to adaptively highlight the most relevant modality features for each task, effectively alleviating the negative transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model achieves state-of-the-art accuracy across all four tasks, maintaining a lightweight architecture with fewer than 6 million parameters and delivering an impressive 142.32 FPS inference speed. Rigorous ablation studies further validate the effectiveness of the proposed framework and the independent contributions of each module. The code is available on https://github.com/Wenzhuo-Liu/TEM3-Learning.",
        "arxiv_id": "2506.18084",
        "ARXIVID": "2506.18084",
        "COMMENT": "Matches criterion 3 as it proposes a novel multimodal multi-task learning framework for assistive driving, which is related to embodied AI and new methods.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17705": {
        "authors": [
            "Bo Pan",
            "Yang Chen",
            "Yingwei Pan",
            "Ting Yao",
            "Wei Chen",
            "Tao Mei"
        ],
        "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models",
        "abstract": "arXiv:2506.17705v1 Announce Type: new  Abstract: Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app.",
        "arxiv_id": "2506.17705",
        "ARXIVID": "2506.17705",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model (video diffusion model) and its application to perpetual view generation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.18463": {
        "authors": [
            "Sophia Sirko-Galouchenko",
            "Spyros Gidaris",
            "Antonin Vobecky",
            "Andrei Bursuc",
            "Nicolas Thome"
        ],
        "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations",
        "abstract": "arXiv:2506.18463v1 Announce Type: new  Abstract: We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP",
        "arxiv_id": "2506.18463",
        "ARXIVID": "2506.18463",
        "COMMENT": "Matches criterion 4 as it introduces a novel unsupervised post-training method (DIP) for improving dense image representations in pretrained vision encoders.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.18902": {
        "authors": [
            "Michael G\\\"unther",
            "Saba Sturua",
            "Mohammad Kalim Akram",
            "Isabelle Mohr",
            "Andrei Ungureanu",
            "Sedigheh Eslami",
            "Scott Martens",
            "Bo Wang",
            "Nan Wang",
            "Han Xiao"
        ],
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "abstract": "arXiv:2506.18902v1 Announce Type: new  Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "arxiv_id": "2506.18902",
        "ARXIVID": "2506.18902",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal embedding model (jina-embeddings-v4) for cross-modal tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.18890": {
        "authors": [
            "Ziqiao Ma",
            "Xuweiyi Chen",
            "Shoubin Yu",
            "Sai Bi",
            "Kai Zhang",
            "Chen Ziwen",
            "Sihan Xu",
            "Jianing Yang",
            "Zexiang Xu",
            "Kalyan Sunkavalli",
            "Mohit Bansal",
            "Joyce Chai",
            "Hao Tan"
        ],
        "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time",
        "abstract": "arXiv:2506.18890v1 Announce Type: new  Abstract: Can we scale 4D pretraining to learn general space-time representations that reconstruct an object from a few views at some times to any view at any time? We provide an affirmative answer with 4D-LRM, the first large-scale 4D reconstruction model that takes input from unconstrained views and timestamps and renders arbitrary novel view-time combinations. Unlike prior 4D approaches, e.g., optimization-based, geometry-based, or generative, that struggle with efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time representation and directly predicts per-pixel 4D Gaussian primitives from posed image tokens across time, enabling fast, high-quality rendering at, in principle, infinite frame rate. Our results demonstrate that scaling spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We show that 4D-LRM generalizes to novel objects, interpolates across time, and handles diverse camera setups. It reconstructs 24-frame sequences in one forward pass with less than 1.5 seconds on a single A100 GPU.",
        "arxiv_id": "2506.18890",
        "ARXIVID": "2506.18890",
        "COMMENT": "Matches criterion 4 as it introduces a large-scale 4D reconstruction model, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.17545": {
        "authors": [
            "Zhihao Yuan",
            "Shuyi Jiang",
            "Chun-Mei Feng",
            "Yaolun Zhang",
            "Shuguang Cui",
            "Zhen Li",
            "Na Zhao"
        ],
        "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations",
        "abstract": "arXiv:2506.17545v1 Announce Type: new  Abstract: Currently, utilizing large language models to understand the 3D world is becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output bounding boxes or textual answers without revealing how those decisions are made, and they still rely on pre-trained 3D detectors to supply object proposals. We introduce Scene-R1, a video-grounded framework that learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline. In the temporal grounding stage, we explicitly reason about the video and select the video snippets most relevant to an open-ended query. In the subsequent image grounding stage, we analyze the image and predict the 2D bounding box. After that, we track the object using SAM2 to produce pixel-accurate masks in RGB frames, and project them back into 3D, thereby eliminating the need for 3D detector-based proposals while capturing fine geometry and material cues. Scene-R1 can also adapt to the 3D visual question answering task to answer free-form questions directly from video. Our training pipeline only needs task-level 2D boxes or textual labels without dense 3D point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales. These results show that reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding.",
        "arxiv_id": "2506.17545",
        "ARXIVID": "2506.17545",
        "COMMENT": "Matches criterion 1 as it introduces a video-grounded framework for 3D scene reasoning without 3D annotations.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.18369": {
        "authors": [
            "Yeongtak Oh",
            "Jisoo Mok",
            "Dohyun Chung",
            "Juhyeon Shin",
            "Sangha Park",
            "Johan Barthelemy",
            "Sungroh Yoon"
        ],
        "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models",
        "abstract": "arXiv:2506.18369v1 Announce Type: new  Abstract: Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task.",
        "arxiv_id": "2506.18369",
        "ARXIVID": "2506.18369",
        "COMMENT": "Matches criterion 2 as it proposes a reinforcement learning-based post-training framework for multi-modal large language models (MLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.18246": {
        "authors": [
            "Xiangzhao Hao",
            "Kuan Zhu",
            "Hongyu Guo",
            "Haiyun Guo",
            "Ming Tang",
            "JinQiao Wang"
        ],
        "title": "Referring Expression Instance Retrieval and A Strong End-to-End Baseline",
        "abstract": "arXiv:2506.18246v1 Announce Type: new  Abstract: Natural language querying of visual content underpins many vision-language tasks, typically categorized by text granularity and visual search scope. Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions, while Referring Expression Comprehension (REC) localizes objects using fine-grained expressions within a single image. However, real-world scenarios often require both instance-level retrieval and localization across large galleries -- tasks where TIR lacks precision and REC lacks scalability. To address this gap, we propose a new task: Referring Expression Instance Retrieval (REIR), which jointly supports instance-level retrieval and localization. We introduce REIRCOCO, a large-scale benchmark constructed by prompting vision-language models to generate fine-grained expressions for MSCOCO and RefCOCO instances. We also present a baseline method, CLARE, featuring a dual-stream architecture with a Mix of Relation Experts (MORE) module for capturing inter-instance relationships. CLARE integrates object detection and REC pretraining with Contrastive Language-Instance Alignment (CLIA) for end-to-end optimization. Experiments show that CLARE achieves state-of-the-art performance on REIR and generalizes well to TIR and REC, highlighting its effectiveness and versatility.",
        "arxiv_id": "2506.18246",
        "ARXIVID": "2506.18246",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (REIRCOCO) and a novel method (CLARE) for instance-level retrieval and localization.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.18268": {
        "authors": [
            "Yu Liu",
            "Yangtao Meng",
            "Xianfei Pan",
            "Jie Jiang",
            "Changhao Chen"
        ],
        "title": "ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments",
        "abstract": "arXiv:2506.18268v1 Announce Type: new  Abstract: Thermal cameras capture environmental data through heat emission, a fundamentally different mechanism compared to visible light cameras, which rely on pinhole imaging. As a result, traditional visual relocalization methods designed for visible light images are not directly applicable to thermal images. Despite significant advancements in deep learning for camera relocalization, approaches specifically tailored for thermal camera-based relocalization remain underexplored. To address this gap, we introduce ThermalLoc, a novel end-to-end deep learning method for thermal image relocalization. ThermalLoc effectively extracts both local and global features from thermal images by integrating EfficientNet with Transformers, and performs absolute pose regression using two MLP networks. We evaluated ThermalLoc on both the publicly available thermal-odometry dataset and our own dataset. The results demonstrate that ThermalLoc outperforms existing representative methods employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet, and RobustLoc, achieving superior accuracy and robustness.",
        "arxiv_id": "2506.18268",
        "ARXIVID": "2506.18268",
        "COMMENT": "Matches criterion 3 as it discusses a novel method for thermal camera relocalization, which could be relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17792": {
        "authors": [
            "Alexandros Evangelidis",
            "Gricel V\\'azquez",
            "Simos Gerasimou"
        ],
        "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition",
        "abstract": "arXiv:2506.17792v1 Announce Type: new  Abstract: Software-intensive systems, such as software product lines and robotics, utilise Markov decision processes (MDPs) to capture uncertainty and analyse sequential decision-making problems. Despite the usefulness of conventional policy synthesis methods, they fail to scale to large state spaces. Our approach addresses this issue and accelerates policy synthesis in large MDPs by dynamically refining the MDP and iteratively selecting the most fragile MDP regions for refinement. This iterative procedure offers a balance between accuracy and efficiency, as refinement occurs only when necessary. Through a comprehensive empirical evaluation comprising diverse case studies and MDPs up to 1M states, we demonstrate significant performance improvements yielded by our approach compared to the leading probabilistic model checker PRISM (up to 2x), thus offering a very competitive solution for real-world policy synthesis tasks in larger MDPs.",
        "arxiv_id": "2506.17792",
        "ARXIVID": "2506.17792",
        "COMMENT": "Matches criterion 3 as it discusses a novel method for policy synthesis in large MDPs, which could be relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18140": {
        "authors": [
            "Ruinan Jin",
            "Gexin Huang",
            "Xinwei Shen",
            "Qiong Zhang",
            "Yan Shuo Tan",
            "Xiaoxiao Li"
        ],
        "title": "See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis",
        "abstract": "arXiv:2506.18140v1 Announce Type: new  Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that mimic normal anatomy and exhibit significant inter-patient variability. Clinicians routinely employ comparative reasoning-using reference images from healthy controls or previous patient examinations-to discern subtle yet diagnostically critical abnormalities. However, existing medical vision-language models (VLMs) focus primarily on single-image or single-series analyses and lack explicit mechanisms for comparative reasoning. Conversely, general-purpose VLMs demonstrate strong multi-image comparative reasoning capabilities but lack essential medical-domain knowledge to identify nuanced clinical differences. This work aims to bridge this gap by exploring clinically-inspired comparative analysis within VLMs, leveraging reference images to enhance diagnostic accuracy. Through extensive empirical analysis, we show that providing general-purpose VLMs with query and normative matched reference images, accompanied by clinically-informed comparative prompts, significantly improves diagnostic outcomes compared to single-image baselines, especially after supervised finetuning (SFT). Our contributions highlight the clinical relevance of comparative analysis introduce novel strategies for leveraging reference images in VLMs, empirically demonstrate enhanced performance across multiple medical visual question answering (VQA) tasks, and provide theoretical insights into the efficacy of comparative image analysis in medical diagnosis.",
        "arxiv_id": "2506.18140",
        "ARXIVID": "2506.18140",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) for comparative medical diagnosis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17958": {
        "authors": [
            "Xiangyuan Peng",
            "Miao Tang",
            "Huawei Sun",
            "Bierzynski Kay",
            "Lorenzo Servadei",
            "Robert Wille"
        ],
        "title": "ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty",
        "abstract": "arXiv:2506.17958v1 Announce Type: new  Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While LiDAR provides rich spatial information, 4D radar offers velocity measurement and remains robust under adverse conditions. As a result, increasing studies have focused on the 4D radar-LiDAR fusion method to enhance the perception. However, the misalignment between different modalities is often overlooked. To address this challenge and leverage the strengths of both modalities, we propose a LiDAR detection framework enhanced by 4D radar motion status and cross-modal uncertainty. The object movement information from 4D radar is first captured using a Dynamic Motion-Aware Encoding module during feature extraction to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties of bounding boxes are estimated to mitigate the cross-modal misalignment and refine the final LiDAR predictions. Extensive experiments on the View-of-Delft (VoD) dataset highlight the effectiveness of our method, achieving state-of-the-art performance with the mAP of 74.89% in the entire area and 88.70% within the driving corridor while maintaining a real-time inference speed of 30.02 FPS.",
        "arxiv_id": "2506.17958",
        "ARXIVID": "2506.17958",
        "COMMENT": "Matches criterion 3 as it discusses a novel method for LiDAR and 4D radar fusion in autonomous driving, focusing on cross-modal uncertainty.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18866": {
        "authors": [
            "Qijun Gan",
            "Ruizi Yang",
            "Jianke Zhu",
            "Shaofei Xue",
            "Steven Hoi"
        ],
        "title": "OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation",
        "abstract": "arXiv:2506.18866v1 Announce Type: new  Abstract: Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.",
        "arxiv_id": "2506.18866",
        "ARXIVID": "2506.18866",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model for audio-driven full-body video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17645": {
        "authors": [
            "Shih-Wen Liu",
            "Hsuan-Yu Fan",
            "Wei-Ta Chu",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning",
        "abstract": "arXiv:2506.17645v1 Announce Type: new  Abstract: Automating medical report generation from histopathology images is a critical challenge requiring effective visual representations and domain-specific knowledge. Inspired by the common practices of human experts, we propose an in-context learning framework called PathGenIC that integrates context derived from the training set with a multimodal in-context learning (ICL) mechanism. Our method dynamically retrieves semantically similar whole slide image (WSI)-report pairs and incorporates adaptive feedback to enhance contextual relevance and generation quality. Evaluated on the HistGen benchmark, the framework achieves state-of-the-art results, with significant improvements across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across diverse report lengths and disease categories. By maximizing training data utility and bridging vision and language with ICL, our work offers a solution for AI-driven histopathology reporting, setting a strong foundation for future advancements in multimodal clinical applications.",
        "arxiv_id": "2506.17645",
        "ARXIVID": "2506.17645",
        "COMMENT": "Matches criterion 2 as it discusses a vision-language model (VLM) with multimodal in-context learning for medical report generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18527": {
        "authors": [
            "JiaKui Hu",
            "Yuxiao Yang",
            "Jialun Liu",
            "Jinbo Wu",
            "Chen Zhao",
            "Yanye Lu"
        ],
        "title": "Auto-Regressively Generating Multi-View Consistent Images",
        "abstract": "arXiv:2506.18527v1 Announce Type: new  Abstract: Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the \"Shuffle View\" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at https://github.com/MILab-PKU/MVAR.",
        "arxiv_id": "2506.18527",
        "ARXIVID": "2506.18527",
        "COMMENT": "Matches criterion 4 as it discusses multi-view image generation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18856": {
        "authors": [
            "Kuanning Wang",
            "Yuqian Fu",
            "Tianyu Wang",
            "Yanwei Fu",
            "Longfei Liang",
            "Yu-Gang Jiang",
            "Xiangyang Xue"
        ],
        "title": "RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base",
        "abstract": "arXiv:2506.18856v1 Announce Type: new  Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present RAG-6DPose, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points; 2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints. Supplementary material is available on our project website: https://sressers.github.io/RAG-6DPose .",
        "arxiv_id": "2506.18856",
        "ARXIVID": "2506.18856",
        "COMMENT": "Matches criterion 3 as it introduces a novel retrieval-augmented method (RAG-6DPose) for 6D pose estimation, which is relevant to embodied AI and benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18204": {
        "authors": [
            "Youjie Zhou",
            "Guofeng Mei",
            "Yiming Wang",
            "Yi Wan",
            "Fabio Poiesi"
        ],
        "title": "Multimodal Fusion SLAM with Fourier Attention",
        "abstract": "arXiv:2506.18204v1 Announce Type: new  Abstract: Visual SLAM is particularly challenging in environments affected by noise, varying lighting conditions, and darkness. Learning-based optical flow algorithms can leverage multiple modalities to address these challenges, but traditional optical flow-based visual SLAM approaches often require significant computational resources.To overcome this limitation, we propose FMF-SLAM, an efficient multimodal fusion SLAM method that utilizes fast Fourier transform (FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel Fourier-based self-attention and cross-attention mechanism to extract features from RGB and depth signals. We further enhance the interaction of multimodal features by incorporating multi-scale knowledge distillation across modalities. We also demonstrate the practical feasibility of FMF-SLAM in real-world scenarios with real time performance by integrating it with a security robot by fusing with a global positioning module GNSS-RTK and global Bundle Adjustment. Our approach is validated using video sequences from TUM, TartanAir, and our real-world datasets, showcasing state-of-the-art performance under noisy, varying lighting, and dark conditions.Our code and datasets are available at https://github.com/youjie-zhou/FMF-SLAM.git.",
        "arxiv_id": "2506.18204",
        "ARXIVID": "2506.18204",
        "COMMENT": "Matches criterion 3 as it introduces a new multimodal SLAM method with novel Fourier-based attention mechanisms, which is relevant to embodied AI and simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18071": {
        "authors": [
            "Jisheng Dang",
            "Huilin Song",
            "Junbin Xiao",
            "Bimei Wang",
            "Han Peng",
            "Haoxuan Li",
            "Xun Yang",
            "Meng Wang",
            "Tat-Seng Chua"
        ],
        "title": "MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering",
        "abstract": "arXiv:2506.18071v1 Announce Type: new  Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning textual answers with explicit visual evidence. However, modern multimodal models often rely on linguistic priors and spurious correlations, resulting in poorly grounded predictions. In this work, we propose MUPA, a cooperative MUlti-Path Agentic approach that unifies video grounding, question answering, answer reflection and aggregation to tackle Grounded VideoQA. MUPA features three distinct reasoning paths on the interplay of grounding and QA agents in different chronological orders, along with a dedicated reflection agent to judge and aggregate the multi-path results to accomplish consistent QA and grounding. This design markedly improves grounding fidelity without sacrificing answer accuracy. Despite using only 2B parameters, our method outperforms all 7B-scale competitors. When scaled to 7B parameters, MUPA establishes new state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy video-language understanding. Our code is available in https://github.com/longmalongma/MUPA.",
        "arxiv_id": "2506.18071",
        "ARXIVID": "2506.18071",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for grounded video question answering, which involves embodied AI reasoning and grounding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17500": {
        "authors": [
            "Julio Silva-Rodr\\'iguez",
            "Fereshteh Shakeri",
            "Houda Bahig",
            "Jose Dolz",
            "Ismail Ben Ayed"
        ],
        "title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation",
        "abstract": "arXiv:2506.17500v1 Announce Type: new  Abstract: Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of high-performing solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyper-parameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios.",
        "arxiv_id": "2506.17500",
        "ARXIVID": "2506.17500",
        "COMMENT": "Matches criterion 2 as it discusses vision-language models (VLMs) and their adaptation in medical image analysis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17912": {
        "authors": [
            "Chuhao Jin",
            "Haosen Li",
            "Bingzi Zhang",
            "Che Liu",
            "Xiting Wang",
            "Ruihua Song",
            "Wenbing Huang",
            "Ying Qin",
            "Fuzheng Zhang",
            "Di Zhang"
        ],
        "title": "PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis",
        "abstract": "arXiv:2506.17912v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.",
        "arxiv_id": "2506.17912",
        "ARXIVID": "2506.17912",
        "COMMENT": "Matches criterion 2 as it proposes a new LLM-based framework for text-to-motion synthesis, which is a multimodal generation task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18564": {
        "authors": [
            "Xuanyu Zhang",
            "Weiqi Li",
            "Shijie Zhao",
            "Junlin Li",
            "Li Zhang",
            "Jian Zhang"
        ],
        "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning",
        "abstract": "arXiv:2506.18564v1 Announce Type: new  Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of powerful text-to-video generation models. Despite these successes, evaluating the quality of AIGC-generated videos remains challenging due to limited generalization, lack of temporal awareness, heavy reliance on large-scale annotated datasets, and the lack of effective interaction with generation models. Most current approaches rely on supervised finetuning of vision-language models (VLMs), which often require large-scale annotated datasets and tend to decouple understanding and generation. To address these shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for AIGC video quality assessment. Our approach features: (1) a progressive video quality learning scheme that combines image quality warm-up, general task-specific temporal learning, and joint optimization with the video generation model; (2) the design of multi-dimension scoring rewards, preference comparison rewards, and temporal modeling rewards to enhance both generalization and specialization in video quality evaluation. Extensive experiments demonstrate that VQ-Insight consistently outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring, bringing significant improvements for video generation tasks.",
        "arxiv_id": "2506.18564",
        "ARXIVID": "2506.18564",
        "COMMENT": "Matches criterion 2 as it proposes a novel reasoning-style vision-language model (VLM) framework for video quality assessment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17514": {
        "authors": [
            "Ninareh Mehrabi",
            "Tharindu Kumarage",
            "Kai-Wei Chang",
            "Aram Galstyan",
            "Rahul Gupta"
        ],
        "title": "Kaleidoscopic Teaming in Multi Agent Simulations",
        "abstract": "arXiv:2506.17514v1 Announce Type: new  Abstract: Warning: This paper contains content that may be inappropriate or offensive.   AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases.",
        "arxiv_id": "2506.17514",
        "ARXIVID": "2506.17514",
        "COMMENT": "Matches criterion 3 as it introduces a new framework (kaleidoscopic teaming) for evaluating safety in multi-agent simulations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18575": {
        "authors": [
            "Kaifeng Sheng",
            "Zheng Zhou",
            "Yingliang Peng",
            "Qianwei Wang"
        ],
        "title": "2D Triangle Splatting for Direct Differentiable Mesh Training",
        "abstract": "arXiv:2506.18575v1 Announce Type: new  Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a powerful method for reconstructing high-fidelity 3D scenes from multi-view images. While it offers improvements over NeRF-based methods, this representation still encounters challenges with rendering speed and advanced rendering effects, such as relighting and shadow rendering, compared to mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a novel method that replaces 3D Gaussian primitives with 2D triangle facelets. This representation naturally forms a discrete mesh-like structure while retaining the benefits of continuous volumetric modeling. By incorporating a compactness parameter into the triangle primitives, we enable direct training of photorealistic meshes. Our experimental results demonstrate that our triangle-based method, in its vanilla version (without compactness tuning), achieves higher fidelity compared to state-of-the-art Gaussian-based methods. Furthermore, our approach produces reconstructed meshes with superior visual quality compared to existing mesh reconstruction methods.",
        "arxiv_id": "2506.18575",
        "ARXIVID": "2506.18575",
        "COMMENT": "This paper introduces a novel differentiable rendering method for mesh training, which is tangentially related to vision foundation models but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2506.18348": {
        "authors": [
            "Weilun Yu",
            "Shixiang Tang",
            "Yonggui Huang",
            "Nanqing Dong",
            "Li Fan",
            "Honggang Qi",
            "Wei Liu",
            "Xiaoli Diao",
            "Xi Chen",
            "Wanli Ouyang"
        ],
        "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team",
        "abstract": "arXiv:2506.18348v1 Announce Type: new  Abstract: Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.",
        "arxiv_id": "2506.18348",
        "ARXIVID": "2506.18348",
        "COMMENT": "This paper does not directly match any of the specific criteria. It focuses on a multi-agent framework for LLM-based autonomous research, which is tangentially related to multi-agent systems but not specifically about spatial intelligence, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18493": {
        "authors": [
            "Trong-Vu Hoang",
            "Quang-Binh Nguyen",
            "Thanh-Toan Do",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation",
        "abstract": "arXiv:2506.18493v1 Announce Type: new  Abstract: Customizing image generation remains a core challenge in controllable image synthesis. For single-concept generation, maintaining both identity preservation and prompt alignment is challenging. In multi-concept scenarios, relying solely on a prompt without additional conditions like layout boxes or semantic masks, often leads to identity loss and concept omission. In this paper, we introduce ShowFlow, a comprehensive framework designed to tackle these challenges. We propose ShowFlow-S for single-concept image generation, and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. Building on this foundation, ShowFlow-M directly reuses the learned models from ShowFlow-S to support multi-concept generation without extra conditions, incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout consistency strategy as the plug-and-play module. Extensive experiments and user studies validate ShowFlow's effectiveness, highlighting its potential in real-world applications like advertising and virtual dressing.",
        "arxiv_id": "2506.18493",
        "ARXIVID": "2506.18493",
        "COMMENT": "This paper focuses on controllable image synthesis and multi-concept generation, which is tangentially related to vision foundation models but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17788": {
        "authors": [
            "Shahab Rahimirad",
            "Guven Gergerli",
            "Lucia Romero",
            "Angela Qian",
            "Matthew Lyle Olson",
            "Simon Stepputtis",
            "Joseph Campbell"
        ],
        "title": "Bayesian Social Deduction with Graph-Informed Language Models",
        "abstract": "arXiv:2506.17788v1 Announce Type: new  Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial observations of other agents - remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves competitive performance with much larger models in Agent-Agent play and, notably, is the first language agent to defeat human players in a controlled study - achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents, which can be found at https://camp-lab-purdue.github.io/bayesian-social-deduction/",
        "arxiv_id": "2506.17788",
        "ARXIVID": "2506.17788",
        "COMMENT": "This paper does not match any of the specific criteria but explores social reasoning in LLMs, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18438": {
        "authors": [
            "Dinh-Khoi Vo",
            "Thanh-Toan Do",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing",
        "abstract": "arXiv:2506.18438v1 Announce Type: new  Abstract: Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to preserve and independently control the object and background effectively. This ensures that the objects' shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques.",
        "arxiv_id": "2506.18438",
        "ARXIVID": "2506.18438",
        "COMMENT": "Does not match any specific criteria but focuses on zero-shot real image editing using diffusion models, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17597": {
        "authors": [
            "Pengyu Kan",
            "Craig Jones",
            "Kenichi Oishi"
        ],
        "title": "OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor",
        "abstract": "arXiv:2506.17597v1 Announce Type: new  Abstract: Purpose: To develop an age prediction model which is interpretable and robust to demographic and technological variances in brain MRI scans. Materials and Methods: We propose a transformer-based architecture that leverages self-supervised pre-training on large-scale datasets. Our model processes pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates brain volumetric information. By introducing a stem architecture, we reduce the conventional quadratic complexity of transformer models to linear complexity, enabling scalability for high-dimensional MRI data. We trained our model on ADNI2 $\\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the North America, with an 8:1:1 split for train, validation and test. Then, we validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia. Results: We achieved an MAE of 3.65 years on ADNI2 $\\&$ 3 and OASIS3 test set and a high generalizability of MAE of 3.54 years on AIBL. There was a notable increase in brain age gap (BAG) across cognitive groups, with mean of 0.15 years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12 years ([5.82, 6.43]) in AD. Additionally, significant negative correlation between BAG and cognitive scores was observed, with correlation coefficient of -0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based feature attribution highlighted ventricles and white matter structures as key regions influenced by brain aging. Conclusion: Our model effectively fused information from different views and volumetric information to achieve state-of-the-art brain age prediction accuracy, improved generalizability and interpretability with association to neurodegenerative disorders.",
        "arxiv_id": "2506.17597",
        "ARXIVID": "2506.17597",
        "COMMENT": "Does not match any specific criteria but focuses on brain age prediction using transformer-based models, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18544": {
        "authors": [
            "Muhao Xu",
            "Xueying Zhou",
            "Xizhan Gao",
            "Weiye Song",
            "Guang Feng",
            "Sijie Niu"
        ],
        "title": "Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection",
        "abstract": "arXiv:2506.18544v1 Announce Type: new  Abstract: Recently, detecting logical anomalies is becoming a more challenging task compared to detecting structural ones. Existing encoder decoder based methods typically compress inputs into low-dimensional bottlenecks on the assumption that the compression process can effectively suppress the transmission of logical anomalies to the decoder. However, logical anomalies present a particular difficulty because, while their local features often resemble normal semantics, their global semantics deviate significantly from normal patterns. Thanks to the generalisation capabilities inherent in neural networks, these abnormal semantic features can propagate through low-dimensional bottlenecks. This ultimately allows the decoder to reconstruct anomalous images with misleading fidelity. To tackle the above challenge, we propose a novel normality prior guided multi-semantic fusion network for unsupervised anomaly detection. Instead of feeding the compressed bottlenecks to the decoder directly, we introduce the multi-semantic features of normal samples into the reconstruction process. To this end, we first extract abstract global semantics of normal cases by a pre-trained vision-language network, then the learnable semantic codebooks are constructed to store representative feature vectors of normal samples by vector quantisation. Finally, the above multi-semantic features are fused and employed as input to the decoder to guide the reconstruction of anomalies to approximate normality. Extensive experiments are conducted to validate the effectiveness of our proposed method, and it achieves the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in pixel-sPRO and 2.6% in image-AUROC. The source code is available at https://github.com/Xmh-L/NPGMF.",
        "arxiv_id": "2506.18544",
        "ARXIVID": "2506.18544",
        "COMMENT": "Does not match any specific criteria but focuses on unsupervised anomaly detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18586": {
        "authors": [
            "Zijie Yang",
            "Qiji Zhou",
            "Fang Guo",
            "Sijie Zhang",
            "Yexun Xi",
            "Jinglei Nie",
            "Yudian Zhu",
            "Liping Huang",
            "Chou Wu",
            "Yonghe Xia",
            "Xiaoyu Ma",
            "Yingming Pu",
            "Panzhong Lu",
            "Junshu Pan",
            "Mingtao Chen",
            "Tiannan Guo",
            "Yanmei Dou",
            "Hongyu Chen",
            "Anping Zeng",
            "Jiaxing Huang",
            "Tian Xu",
            "Yue Zhang"
        ],
        "title": "Airalogy: AI-empowered universal data digitization for research automation",
        "abstract": "arXiv:2506.18586v1 Announce Type: new  Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.",
        "arxiv_id": "2506.18586",
        "ARXIVID": "2506.18586",
        "COMMENT": "Does not match any specific criteria but discusses a multidisciplinary AI platform for research data digitization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18346": {
        "authors": [
            "Tongshun Zhang",
            "Pingping Liu",
            "Mengen Cai",
            "Zijian Zhang",
            "Yubing Lu",
            "Qiuzhan Zhou"
        ],
        "title": "BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement",
        "abstract": "arXiv:2506.18346v1 Announce Type: new  Abstract: Current low-light image enhancement (LLIE) methods face significant limitations in simultaneously improving brightness while preserving semantic consistency, fine details, and computational efficiency. With the emergence of state-space models, particularly Mamba, image restoration has achieved remarkable performance, yet existing visual Mamba approaches flatten 2D images into 1D token sequences using fixed scanning rules, critically limiting interactions between distant tokens with causal relationships and constraining their ability to capture meaningful long-range dependencies. To address these fundamental limitations, we propose BSMamba, a novel visual Mamba architecture comprising two specially designed components: Brightness Mamba and Semantic Mamba. The Brightness Mamba revolutionizes token interaction patterns by prioritizing connections between distant tokens with similar brightness levels, effectively addressing the challenge of brightness restoration in LLIE tasks through brightness-guided selective attention. Complementing this, the Semantic Mamba establishes priority interactions between tokens sharing similar semantic meanings, allowing the model to maintain contextual consistency by connecting semantically related regions across the image, thus preserving the hierarchical nature of image semantics during enhancement. By intelligently modeling tokens based on brightness and semantic similarity rather than arbitrary scanning patterns, BSMamba transcends the constraints of conventional token sequencing while adhering to the principles of causal modeling. Extensive experiments demonstrate that BSMamba achieves state-of-the-art performance in LLIE while preserving semantic consistency.",
        "arxiv_id": "2506.18346",
        "ARXIVID": "2506.18346",
        "COMMENT": "Does not match any specific criterion but is related to low-light image enhancement, which is tangentially relevant to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18658": {
        "authors": [
            "Ling Zhang",
            "Boxiang Yun",
            "Qingli Li",
            "Yan Wang"
        ],
        "title": "Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation",
        "abstract": "arXiv:2506.18658v1 Announce Type: new  Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces two key challenges: (1) lack of semantic content in visual features and (2) inherent information redundancy in WSIs. To address these issues, we propose a novel Historical Report Guided \\textbf{Bi}-modal Concurrent Learning Framework for Pathology Report \\textbf{Gen}eration (BiGen) emulating pathologists' diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to provide rich semantic content, which retrieves WSI-relevant knowledge from pre-built medical knowledge bank by matching high-attention patches and (2) A bi-modal concurrent learning strategy instantiated via a learnable visual token and a learnable textual token to dynamically extract key visual features and retrieved knowledge, where weight-shared layers enable cross-modal alignment between visual features and knowledge features. Our multi-modal decoder integrates both modals for comprehensive diagnostic reports generation. Experiments on the PathText (BRCA) dataset demonstrate our framework's superiority, achieving state-of-the-art performance with 7.4\\% relative improvement in NLP metrics and 19.1\\% enhancement in classification metrics for Her-2 prediction versus existing methods. Ablation studies validate the necessity of our proposed modules, highlighting our method's ability to provide WSI-relevant rich semantic content and suppress information redundancy in WSIs. Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.",
        "arxiv_id": "2506.18658",
        "ARXIVID": "2506.18658",
        "COMMENT": "Does not match any specific criterion but is related to multimodal learning for pathology report generation, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17585": {
        "authors": [
            "Yukun Huang",
            "Sanxing Chen",
            "Jian Pei",
            "Manzil Zaheer",
            "Bhuwan Dhingra"
        ],
        "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models",
        "abstract": "arXiv:2506.17585v1 Announce Type: new  Abstract: Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining--without test-time retrieval--by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count.",
        "arxiv_id": "2506.17585",
        "ARXIVID": "2506.17585",
        "COMMENT": "Does not match any specific criterion but is related to language modeling and retrieval-free knowledge attribution, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18135": {
        "authors": [
            "Zijun Chen",
            "Zhanpeng Zhou",
            "Bo Zhang",
            "Weinan Zhang",
            "Xi Sun",
            "Junchi Yan"
        ],
        "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
        "abstract": "arXiv:2506.18135v1 Announce Type: new  Abstract: Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood. In this work, we delve into the mechanism behind model merging from a representation perspective. Our analysis reveals that model merging achieves multi-task abilities through two key capabilities: i) distinguishing samples from different tasks, and ii) adapting to the corresponding expert model for each sample. These two capabilities allow the merged model to retain task-specific expertise, enabling efficient multi-task adaptation. Building on these insights, we propose \\texttt{SE-Merging}, a self-enhanced model merging framework that leverages these two characteristics to dynamically identify the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. Notably, \\texttt{SE-Merging} achieves dynamic model merging without additional training. Extensive experiments demonstrate that \\texttt{SE-Merging} achieves significant performance improvements while remaining compatible with existing model merging techniques.",
        "arxiv_id": "2506.18135",
        "ARXIVID": "2506.18135",
        "COMMENT": "Does not match any specific criteria but discusses model merging techniques, which is tangentially related to general machine learning but not the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17558": {
        "authors": [
            "Jake Levi",
            "Mark van der Wilk"
        ],
        "title": "SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference",
        "abstract": "arXiv:2506.17558v1 Announce Type: new  Abstract: Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are \\emph{designed} to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model \\emph{actually} learns to infer part-whole hierarchies, as claimed. To address this difficulty, we present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision.",
        "arxiv_id": "2506.17558",
        "ARXIVID": "2506.17558",
        "COMMENT": "Does not match any specific criteria but discusses part-whole hierarchical inference in computer vision, which is tangentially related to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18042": {
        "authors": [
            "Dongdong Meng",
            "Sheng Li",
            "Hao Wu",
            "Suqing Tian",
            "Wenjun Ma",
            "Guoping Wang",
            "Xueqing Yan"
        ],
        "title": "CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images",
        "abstract": "arXiv:2506.18042v1 Announce Type: new  Abstract: Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.",
        "arxiv_id": "2506.18042",
        "ARXIVID": "2506.18042",
        "COMMENT": "Does not match any specific criterion but is related to weakly-supervised learning in medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18134": {
        "authors": [
            "Quan Zhou",
            "Gan Luo",
            "Qiang Hu",
            "Qingyong Zhang",
            "Jinhua Zhang",
            "Yinjiao Tian",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection",
        "abstract": "arXiv:2506.18134v1 Announce Type: new  Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, detector-confusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at https://github.com/Huster-Hq/DADA.",
        "arxiv_id": "2506.18134",
        "ARXIVID": "2506.18134",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18019": {
        "authors": [
            "Yuanchen Bei",
            "Weizhi Zhang",
            "Siwen Wang",
            "Weizhi Chen",
            "Sheng Zhou",
            "Hao Chen",
            "Yong Li",
            "Jiajun Bu",
            "Shirui Pan",
            "Yizhou Yu",
            "Irwin King",
            "Fakhri Karray",
            "Philip S. Yu"
        ],
        "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
        "abstract": "arXiv:2506.18019v1 Announce Type: new  Abstract: AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.",
        "arxiv_id": "2506.18019",
        "ARXIVID": "2506.18019",
        "COMMENT": "This survey paper discusses the integration of graphs with AI agents, which is tangentially related to embodied AI but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18096": {
        "authors": [
            "Yuxuan Huang",
            "Yihang Chen",
            "Haozheng Zhang",
            "Kang Li",
            "Meng Fang",
            "Linyi Yang",
            "Xiaoguang Li",
            "Lifeng Shang",
            "Songcen Xu",
            "Jianye Hao",
            "Kun Shao",
            "Jun Wang"
        ],
        "title": "Deep Research Agents: A Systematic Examination And Roadmap",
        "abstract": "arXiv:2506.18096v1 Announce Type: new  Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
        "arxiv_id": "2506.18096",
        "ARXIVID": "2506.18096",
        "COMMENT": "Does not match any specific criteria. Focuses on deep research agents and their architectures, which is unrelated to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17633": {
        "authors": [
            "Xiang Fang",
            "Arvind Easwaran",
            "Blaise Genest"
        ],
        "title": "Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection",
        "abstract": "arXiv:2506.17633v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many IID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where {Only a few {\\em labeled ID} samples are available.} Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID {\\em image samples}, we leverage CLIP, connecting text with images, engineering learnable ID and OOD {\\em textual prompts}. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works.",
        "arxiv_id": "2506.17633",
        "ARXIVID": "2506.17633",
        "COMMENT": "Does not match any specific criteria. Focuses on few-shot out-of-distribution detection, which is unrelated to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18520": {
        "authors": [
            "JiaKui Hu",
            "Zhengjian Yao",
            "Lujia Jin",
            "Hangzhou He",
            "Yanye Lu"
        ],
        "title": "Enhancing Image Restoration Transformer via Adaptive Translation Equivariance",
        "abstract": "arXiv:2506.18520v1 Announce Type: new  Abstract: Translation equivariance is a fundamental inductive bias in image restoration, ensuring that translated inputs produce translated outputs. Attention mechanisms in modern restoration transformers undermine this property, adversely impacting both training convergence and generalization. To alleviate this issue, we propose two key strategies for incorporating translation equivariance: slide indexing and component stacking. Slide indexing maintains operator responses at fixed positions, with sliding window attention being a notable example, while component stacking enables the arrangement of translation-equivariant operators in parallel or sequentially, thereby building complex architectures while preserving translation equivariance. However, these strategies still create a dilemma in model design between the high computational cost of self-attention and the fixed receptive field associated with sliding window attention. To address this, we develop an adaptive sliding indexing mechanism to efficiently select key-value pairs for each query, which are then concatenated in parallel with globally aggregated key-value pairs. The designed network, called the Translation Equivariance Adaptive Transformer (TEAFormer), is assessed across a variety of image restoration tasks. The results highlight its superiority in terms of effectiveness, training convergence, and generalization.",
        "arxiv_id": "2506.18520",
        "ARXIVID": "2506.18520",
        "COMMENT": "Does not match any specific criteria. Focuses on translation equivariance in image restoration transformers, which is unrelated to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17969": {
        "authors": [
            "Chenyue Song",
            "Chen Hui",
            "Wei Zhang",
            "Haiqi Zhu",
            "Shaohui Liu",
            "Hong Huang",
            "Feng Jiang"
        ],
        "title": "BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP",
        "abstract": "arXiv:2506.17969v1 Announce Type: new  Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of images based on human subjective perception. Existing methods generally combine multiscale features to achieve high performance, but most rely on straightforward linear fusion of these features, which may not adequately capture the impact of distortions on semantic content. To address this, we propose a bottom-up image quality assessment approach based on the Contrastive Language-Image Pre-training (CLIP, a recently proposed model that aligns images and text in a shared feature space), named BPCLIP, which progressively extracts the impact of low-level distortions on high-level semantics. Specifically, we utilize an encoder to extract multiscale features from the input image and introduce a bottom-up multiscale cross attention module designed to capture the relationships between shallow and deep features. In addition, by incorporating 40 image quality adjectives across six distinct dimensions, we enable the pre-trained CLIP text encoder to generate representations of the intrinsic quality of the image, thereby strengthening the connection between image quality perception and human language. Our method achieves superior results on most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while demonstrating greater robustness.",
        "arxiv_id": "2506.17969",
        "ARXIVID": "2506.17969",
        "COMMENT": "Does not match any specific criteria but focuses on image quality assessment using CLIP, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17931": {
        "authors": [
            "Ravi Kant Gupta",
            "Shounak Das",
            "Amit Sethi"
        ],
        "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset",
        "abstract": "arXiv:2506.17931v1 Announce Type: new  Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for natural images. A commonly-used objective for UDA schemes is to enhance domain alignment in representation space even if there is a domain shift in the input space. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. Our approach has two main features. Firstly, its neural architecture uses the deep structure of ResNet and the effective separation of scales of feature pyramidal network (FPN) to work with both content and style features. Secondly, it uses a combination of a novel loss function and judiciously selected existing loss functions to train the network architecture. This tailored combination is designed to address challenges inherent to natural images, such as scale, noise, and style shifts, that occur on top of a multi-modal (multi-class) distribution. The combined loss function not only enhances model accuracy and robustness on the target domain but also speeds up training convergence. Our proposed UDA scheme generalizes better than state-of-the-art for CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and comaparable for DomainNet dataset.",
        "arxiv_id": "2506.17931",
        "ARXIVID": "2506.17931",
        "COMMENT": "Does not match any specific criteria but focuses on domain adaptation for natural images, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18070": {
        "authors": [
            "Hangzhou He",
            "Jiachen Tang",
            "Lei Zhu",
            "Kaiwen Li",
            "Yanye Lu"
        ],
        "title": "Training-free Test-time Improvement for Explainable Medical Image Classification",
        "abstract": "arXiv:2506.18070v1 Announce Type: new  Abstract: Deep learning-based medical image classification techniques are rapidly advancing in medical image analysis, making it crucial to develop accurate and trustworthy models that can be efficiently deployed across diverse clinical scenarios. Concept Bottleneck Models (CBMs), which first predict a set of explainable concepts from images and then perform classification based on these concepts, are increasingly being adopted for explainable medical image classification. However, the inherent explainability of CBMs introduces new challenges when deploying trained models to new environments. Variations in imaging protocols and staining methods may induce concept-level shifts, such as alterations in color distribution and scale. Furthermore, since CBM training requires explicit concept annotations, fine-tuning models solely with image-level labels could compromise concept prediction accuracy and faithfulness - a critical limitation given the high cost of acquiring expert-annotated concept labels in medical domains. To address these challenges, we propose a training-free confusion concept identification strategy. By leveraging minimal new data (e.g., 4 images per class) with only image-level labels, our approach enhances out-of-domain performance without sacrificing source domain accuracy through two key operations: masking misactivated confounding concepts and amplifying under-activated discriminative concepts. The efficacy of our method is validated on both skin and white blood cell images. Our code is available at: https://github.com/riverback/TF-TTI-XMed.",
        "arxiv_id": "2506.18070",
        "ARXIVID": "2506.18070",
        "COMMENT": "Does not match any specific criteria but is related to explainable medical image classification, which is tangentially relevant to your friend's general interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17562": {
        "authors": [
            "Haoxuan Che",
            "Haibo Jin",
            "Zhengrui Guo",
            "Yi Lin",
            "Cheng Jin",
            "Hao Chen"
        ],
        "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning",
        "abstract": "arXiv:2506.17562v1 Announce Type: new  Abstract: LLMs have demonstrated significant potential in Medical Report Generation (MRG), yet their development requires large amounts of medical image-report pairs, which are commonly scattered across multiple centers. Centralizing these data is exceptionally challenging due to privacy regulations, thereby impeding model development and broader adoption of LLM-driven MRG models. To address this challenge, we present FedMRG, the first framework that leverages Federated Learning (FL) to enable privacy-preserving, multi-center development of LLM-driven MRG models, specifically designed to overcome the critical challenge of communication-efficient LLM training under multi-modal data heterogeneity. To start with, our framework tackles the fundamental challenge of communication overhead in FL-LLM tuning by employing low-rank factorization to efficiently decompose parameter updates, significantly reducing gradient transmission costs and making LLM-driven MRG feasible in bandwidth-constrained FL settings. Furthermore, we observed the dual heterogeneity in MRG under the FL scenario: varying image characteristics across medical centers, as well as diverse reporting styles and terminology preferences. To address this, we further enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder, coupled with diagnosis-driven prompts, which capture both globally generalizable and locally distinctive features while maintaining diagnostic accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder that harmonizes generic and specialized adapters to address variations in reporting styles and terminology. Through extensive evaluation of our established FL-MRG benchmark, we demonstrate the generalizability and adaptability of FedMRG, underscoring its potential in harnessing multi-center data and generating clinically accurate reports while maintaining communication efficiency.",
        "arxiv_id": "2506.17562",
        "ARXIVID": "2506.17562",
        "COMMENT": "Does not match any specific criteria but discusses federated learning for medical report generation using LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17503": {
        "authors": [
            "Julio Silva-Rodr\\'iguez",
            "Ismail Ben Ayed",
            "Jose Dolz"
        ],
        "title": "Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction",
        "abstract": "arXiv:2506.17503v1 Announce Type: new  Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities and are being increasingly adopted for data-efficient image classification. Despite its growing popularity, its reliability aspect remains largely unexplored. This work explores the split conformal prediction (SCP) framework to provide trustworthiness guarantees when transferring such models based on a small labeled calibration set. Despite its potential, the generalist nature of the VLMs' pre-training could negatively affect the properties of the predicted conformal sets for specific tasks. While common practice in transfer learning for discriminative purposes involves an adaptation stage, we observe that deploying such a solution for conformal purposes is suboptimal since adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP. To address this issue, we propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data. We present comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores. Our framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees.",
        "arxiv_id": "2506.17503",
        "ARXIVID": "2506.17503",
        "COMMENT": "Does not match any specific criteria but explores trustworthy few-shot transfer of medical vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17374": {
        "authors": [
            "Muhammad Tayyab Khan",
            "Lequn Chen",
            "Zane Yong",
            "Jun Ming Tan",
            "Wenhe Feng",
            "Seung Ki Moon"
        ],
        "title": "From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge",
        "abstract": "arXiv:2506.17374v1 Announce Type: new  Abstract: Efficient and accurate extraction of key information from 2D engineering drawings is essential for advancing digital manufacturing workflows. Such information includes geometric dimensioning and tolerancing (GD&T), measures, material specifications, and textual annotations. Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs. These limitations result in incomplete and unreliable outputs. To address these challenges, we propose a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser. Our structured pipeline applies YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB) patches, which are then parsed into structured outputs using a fine-tuned, lightweight vision-language model (VLM). We curate a dataset of 1,367 2D mechanical drawings annotated across nine key categories. YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2. Both models are lightweight and well-suited for specialized industrial tasks under limited computational overhead. Following fine-tuning of both models on the curated dataset of image patches paired with structured annotation labels, a comparative experiment is conducted to evaluate parsing performance across four key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a case study demonstrates how the extracted structured information supports downstream manufacturing tasks such as process and tool selection, showcasing the practical utility of the proposed framework in modernizing 2D drawing interpretation.",
        "arxiv_id": "2506.17374",
        "ARXIVID": "2506.17374",
        "COMMENT": "Does not match any specific criteria but discusses a hybrid vision-language framework for parsing 2D engineering drawings.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18325": {
        "authors": [
            "Yu Xie",
            "Chengjie Zeng",
            "Lingyun Zhang",
            "Yanwei Fu"
        ],
        "title": "NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation",
        "abstract": "arXiv:2506.18325v1 Announce Type: new  Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by \"jailbreak\" attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.",
        "arxiv_id": "2506.18325",
        "ARXIVID": "2506.18325",
        "COMMENT": "Does not match any specific criteria but focuses on prompt sanitization for safe text-to-image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18028": {
        "authors": [
            "Junjian Li",
            "Hulin Kuang",
            "Jin Liu",
            "Hailin Yue",
            "Mengshen He",
            "Jianxin Wang"
        ],
        "title": "MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis",
        "abstract": "arXiv:2506.18028v1 Announce Type: new  Abstract: Multiple instance learning (MIL) has shown significant promise in histopathology whole slide image (WSI) analysis for cancer diagnosis and prognosis. However, the inherent spatial heterogeneity of WSIs presents critical challenges, as morphologically similar tissue types are often dispersed across distant anatomical regions. Conventional MIL methods struggle to model these scattered tissue distributions and capture cross-regional spatial interactions effectively. To address these limitations, we propose a novel Multiple instance learning framework with Context-Aware Clustering (MiCo), designed to enhance cross-regional intra-tissue correlations and strengthen inter-tissue semantic associations in WSIs. MiCo begins by clustering instances to distill discriminative morphological patterns, with cluster centroids serving as semantic anchors. To enhance cross-regional intra-tissue correlations, MiCo employs a Cluster Route module, which dynamically links instances of the same tissue type across distant regions via feature similarity. These semantic anchors act as contextual hubs, propagating semantic relationships to refine instance-level representations. To eliminate semantic fragmentation and strengthen inter-tissue semantic associations, MiCo integrates a Cluster Reducer module, which consolidates redundant anchors while enhancing information exchange between distinct semantic groups. Extensive experiments on two challenging tasks across nine large-scale public cancer datasets demonstrate the effectiveness of MiCo, showcasing its superiority over state-of-the-art methods. The code is available at https://github.com/junjianli106/MiCo.",
        "arxiv_id": "2506.18028",
        "ARXIVID": "2506.18028",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.17733": {
        "authors": [
            "Mengqi Lei",
            "Siqi Li",
            "Yihong Wu",
            "Han Hu",
            "You Zhou",
            "Xinhu Zheng",
            "Guiguang Ding",
            "Shaoyi Du",
            "Zongze Wu",
            "Yue Gao"
        ],
        "title": "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception",
        "abstract": "arXiv:2506.17733v1 Announce Type: new  Abstract: The YOLO series models reign supreme in real-time object detection due to their superior accuracy and computational efficiency. However, both the convolutional architectures of YOLO11 and earlier versions and the area-based self-attention mechanism introduced in YOLOv12 are limited to local information aggregation and pairwise correlation modeling, lacking the capability to capture global multi-to-multi high-order correlations, which limits detection performance in complex scenarios. In this paper, we propose YOLOv13, an accurate and lightweight object detector. To address the above-mentioned challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement (HyperACE) mechanism that adaptively exploits latent high-order correlations and overcomes the limitation of previous methods that are restricted to pairwise correlation modeling based on hypergraph computation, achieving efficient global cross-location and cross-scale feature fusion and enhancement. Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD) paradigm based on HyperACE, which effectively achieves fine-grained information flow and representation synergy within the entire network by distributing correlation-enhanced features to the full pipeline. Finally, we propose to leverage depthwise separable convolutions to replace vanilla large-kernel convolutions, and design a series of blocks that significantly reduce parameters and computational complexity without sacrificing performance. We conduct extensive experiments on the widely used MS COCO benchmark, and the experimental results demonstrate that our method achieves state-of-the-art performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N improves mAP by 3.0\\% over YOLO11-N and by 1.5\\% over YOLOv12-N. The code and models of our YOLOv13 model are available at: https://github.com/iMoonLab/yolov13.",
        "arxiv_id": "2506.17733",
        "ARXIVID": "2506.17733",
        "COMMENT": "Does not match any specific criteria. Focuses on object detection improvements in YOLOv13, which is unrelated to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.18682": {
        "authors": [
            "Imad Ali Shah",
            "Jiarong Li",
            "Tim Brophy",
            "Martin Glavin",
            "Edward Jones",
            "Enda Ward",
            "Brian Deegan"
        ],
        "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios",
        "abstract": "arXiv:2506.18682v1 Announce Type: new  Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly in challenging weather and lighting conditions. However, efficiently processing its high-dimensional spectral data remains a significant challenge. This paper introduces a Multi-scale Spectral Attention Module (MSAM) that enhances spectral feature extraction through three parallel 1D convolutions with varying kernel sizes between 1 to 11, coupled with an adaptive feature aggregation mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our proposed UNet-MSAM achieves significant improvements in semantic segmentation performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and Hyperspectral City v2. Our comprehensive experiments demonstrate that with minimal computational overhead (on average 0.02% in parameters and 0.82% GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets. Through extensive ablation studies, we have established that multi-scale kernel combinations perform better than single-scale configurations. These findings demonstrate the potential of HSI processing for AD and provide valuable insights into designing robust, multi-scale spectral feature extractors for real-world applications.",
        "arxiv_id": "2506.18682",
        "ARXIVID": "2506.18682",
        "COMMENT": "Does not match any specific criteria. Focuses on hyperspectral segmentation for autonomous driving, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}