{
    "2509.15333": {
        "authors": [
            "Yulin Wang",
            "Yang Yue",
            "Yang Yue",
            "Huanqian Wang",
            "Haojun Jiang",
            "Yizeng Han",
            "Zanlin Ni",
            "Yifan Pu",
            "Minglei Shi",
            "Rui Lu",
            "Qisen Yang",
            "Andrew Zhao",
            "Zhuofan Xia",
            "Shiji Song",
            "Gao Huang"
        ],
        "title": "Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception",
        "abstract": "arXiv:2509.15333v1 Announce Type: new  Abstract: Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.",
        "arxiv_id": "2509.15333",
        "ARXIVID": "2509.15333",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial intelligence and spatial understanding in vision models, especially for embodied agents). AdaptiveNN introduces a general framework for active, adaptive vision, with reinforcement learning for sequential fixation, and is evaluated on language-driven embodied AI tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2509.15772": {
        "authors": [
            "Weimin Bai",
            "Yubo Li",
            "Weijian Luo",
            "Wenzheng Chen",
            "He Sun"
        ],
        "title": "Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation",
        "abstract": "arXiv:2509.15772v1 Announce Type: new  Abstract: Score Distillation Sampling (SDS) enables high-quality text-to-3D generation by supervising 3D models through the denoising of multi-view 2D renderings, using a pretrained text-to-image diffusion model to align with the input prompt and ensure 3D consistency. However, existing SDS-based methods face two fundamental limitations: (1) their reliance on CLIP-style text encoders leads to coarse semantic alignment and struggles with fine-grained prompts; and (2) 2D diffusion priors lack explicit 3D spatial constraints, resulting in geometric inconsistencies and inaccurate object relationships in multi-object scenes. To address these challenges, we propose VLM3D, a novel text-to-3D generation framework that integrates large vision-language models (VLMs) into the SDS pipeline as differentiable semantic and spatial priors. Unlike standard text-to-image diffusion priors, VLMs leverage rich language-grounded supervision that enables fine-grained prompt alignment. Moreover, their inherent vision language modeling provides strong spatial understanding, which significantly enhances 3D consistency for single-object generation and improves relational reasoning in multi-object scenes. We instantiate VLM3D based on the open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark. Experiments across diverse objects and complex scenes show that VLM3D significantly outperforms prior SDS-based methods in semantic fidelity, geometric coherence, and spatial correctness.",
        "arxiv_id": "2509.15772",
        "ARXIVID": "2509.15772",
        "COMMENT": "Matches criteria 1 and 2: Proposes a new method for spatial understanding in text-to-3D generation by integrating VLLMs as differentiable semantic and spatial rewards. Also introduces a novel use of VLLMs in generative modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.16197": {
        "authors": [
            "Yanghao Li",
            "Rui Qian",
            "Bowen Pan",
            "Haotian Zhang",
            "Haoshuo Huang",
            "Bowen Zhang",
            "Jialing Tong",
            "Haoxuan You",
            "Xianzhi Du",
            "Zhe Gan",
            "Hyunjik Kim",
            "Chao Jia",
            "Zhenbang Wang",
            "Yinfei Yang",
            "Mingfei Gao",
            "Zi-Yi Dou",
            "Wenze Hu",
            "Chang Gao",
            "Dongxu Li",
            "Philipp Dufter",
            "Zirui Wang",
            "Guoli Yin",
            "Zhengdong Zhang",
            "Chen Chen",
            "Yang Zhao",
            "Ruoming Pang",
            "Zhifeng Chen"
        ],
        "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer",
        "abstract": "arXiv:2509.16197v1 Announce Type: new  Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.",
        "arxiv_id": "2509.16197",
        "ARXIVID": "2509.16197",
        "COMMENT": "Matches criterion 2 and 4: Introduces MANZANO, a unified multimodal large language model (VLLM/MLLM) with a hybrid vision tokenizer, capable of both image understanding and generation. Shows state-of-the-art results and a novel architecture for joint vision-language modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.15536": {
        "authors": [
            "Sen Wang",
            "Jingyi Tian",
            "Le Wang",
            "Zhimin Liao",
            "Jiayi Li",
            "Huaiyi Dong",
            "Kun Xia",
            "Sanping Zhou",
            "Wei Tang",
            "Hua Gang"
        ],
        "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models",
        "abstract": "arXiv:2509.15536v1 Announce Type: new  Abstract: World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \\textbf{S}cale-wise \\textbf{A}utoregression with \\textbf{M}otion \\textbf{P}r\\textbf{O}mpt (\\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.",
        "arxiv_id": "2509.15536",
        "ARXIVID": "2509.15536",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding and spatial intelligence on embodied agents) and criterion 3 (novel world model for embodied agents with new spatial/motion modeling and benchmarking). The paper introduces a hybrid world model (SAMPO) with scale-wise autoregression, motion prompts, and spatial attention, specifically for action-conditioned video prediction and model-based control in embodied settings.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.15871": {
        "authors": [
            "Liwei Liao",
            "Xufeng Li",
            "Xiaoyun Zheng",
            "Boning Liu",
            "Feng Gao",
            "Ronggang Wang"
        ],
        "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval",
        "abstract": "arXiv:2509.15871v1 Announce Type: new  Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.",
        "arxiv_id": "2509.15871",
        "ARXIVID": "2509.15871",
        "COMMENT": "This paper introduces a zero-shot visual grounding framework for 3D Gaussian Splatting, transforming 3D visual grounding into a 2D retrieval task. It is highly relevant to criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel embodied AI methods), with a novel approach to 3DVG without per-scene training.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.15566": {
        "authors": [
            "Shaojie Zhang",
            "Ruoceng Zhang",
            "Pei Fu",
            "Shaokang Wang",
            "Jiahui Yang",
            "Xin Du",
            "Shiqi Cui",
            "Bin Qin",
            "Ying Huang",
            "Zhenbo Luo",
            "Jian Luan"
        ],
        "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
        "abstract": "arXiv:2509.15566v1 Announce Type: new  Abstract: In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
        "arxiv_id": "2509.15566",
        "ARXIVID": "2509.15566",
        "COMMENT": "Matches criterion 3: Proposes a novel, brain-inspired embodied AI agent for GUI interaction, with new reward mechanisms and annotation pipelines, focusing on aspects of human-like reasoning previously ignored.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15293": {
        "authors": [
            "Dinura Dissanayake",
            "Ahmed Heakl",
            "Omkar Thawakar",
            "Noor Ahsan",
            "Ritesh Thawkar",
            "Ketan More",
            "Jean Lahoud",
            "Rao Anwer",
            "Hisham Cholakkal",
            "Ivan Laptev",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "title": "How Good are Foundation Models in Step-by-Step Embodied Reasoning?",
        "abstract": "arXiv:2509.15293v1 Announce Type: new  Abstract: Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.",
        "arxiv_id": "2509.15293",
        "ARXIVID": "2509.15293",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI, focusing on step-by-step embodied reasoning with foundation models). The FoMER benchmark evaluates LMMs' reasoning in embodied environments, providing new empirical insights and a novel evaluation framework.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15596": {
        "authors": [
            "Gui Wang",
            "Yang Wennuo",
            "Xusen Ma",
            "Zehao Zhong",
            "Zhuoru Wu",
            "Ende Wu",
            "Rong Qu",
            "Wooi Ping Cheah",
            "Jianfeng Ren",
            "Linlin Shen"
        ],
        "title": "EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery",
        "abstract": "arXiv:2509.15596v1 Announce Type: new  Abstract: MLLMs (Multimodal Large Language Models) have showcased remarkable capabilities, but their performance in high-stakes, domain-specific scenarios like surgical settings, remains largely under-explored. To address this gap, we develop \\textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery analysis, grounded in structured clinical knowledge to evaluate cognition across \\textit{Perception}, \\textit{Comprehension} and \\textit{Reasoning}. EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover 1048 fine-grained attributes for multi-view perception, medical knowledge graph of more than 25k triplets for comprehension, and four clinically grounded reasoning tasks. The rich annotations facilitate in-depth cognitive analysis, simulating how surgeons perceive visual cues and combine them with domain knowledge to make decisions, thus greatly improving models' cognitive ability. In particular, \\textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B, achieves the highest accuracy on MCQs for \\textit{Perception} among compared models and outperforms open-source models in \\textit{Comprehension} and \\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals the limitations of existing MLLMs in surgical cognition and lays the foundation for benchmarking and enhancing clinical reliability of surgical video understanding models.",
        "arxiv_id": "2509.15596",
        "ARXIVID": "2509.15596",
        "COMMENT": "Matches criterion 2 (shows new MLLMs) and criterion 3 (builds a new benchmark for embodied/clinical video understanding). EyePCR is a large-scale benchmark for MLLMs in ophthalmic surgery, with a domain-adapted MLLM and detailed analysis of model performance in perception, comprehension, and reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.16127": {
        "authors": [
            "Yi-Fan Zhang",
            "Haihua Yang",
            "Huanyu Zhang",
            "Yang Shi",
            "Zezhou Chen",
            "Haochen Tian",
            "Chaoyou Fu",
            "Haotian Wang",
            "Kai Wu",
            "Bo Cui",
            "Xu Wang",
            "Jianfei Pan",
            "Haotian Wang",
            "Zhang Zhang",
            "Liang Wang"
        ],
        "title": "BaseReward: A Strong Baseline for Multimodal Reward Model",
        "abstract": "arXiv:2509.16127v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \\textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \\textit{reward head architecture}, \\textit{training strategies}, \\textit{data curation} (covering over ten multimodal and text-only preference datasets), \\textit{backbone model} and \\textit{model scale}, and \\textit{ensemble methods}.   Based on these experimental insights, we introduce \\textbf{BaseReward}, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.",
        "arxiv_id": "2509.16127",
        "ARXIVID": "2509.16127",
        "COMMENT": "Matches criterion 2 (new MLLMs and VLLMs, specifically a new multimodal reward model for aligning MLLMs with human preferences). BaseReward provides a systematic recipe and a new SOTA baseline for multimodal reward modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15693": {
        "authors": [
            "Cristian Sbrolli",
            "Matteo Matteucci"
        ],
        "title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions",
        "abstract": "arXiv:2509.15693v1 Announce Type: new  Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForge's compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions.",
        "arxiv_id": "2509.15693",
        "ARXIVID": "2509.15693",
        "COMMENT": "Matches criterion 1 (spatial understanding via structured scene composition) and criterion 4 (vision foundation models, compositional augmentation for 3D-text alignment). SceneForge augments 3D-text contrastive learning with explicit spatial relations, improving spatial reasoning and generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.15532": {
        "authors": [
            "Xianhang Ye",
            "Yiqing Li",
            "Wei Dai",
            "Miancan Liu",
            "Ziyuan Chen",
            "Zhangye Han",
            "Hongbo Min",
            "Jinkui Ren",
            "Xiantao Zhang",
            "Wen Yang",
            "Zhi Jin"
        ],
        "title": "GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents",
        "abstract": "arXiv:2509.15532v1 Announce Type: new  Abstract: Existing GUI grounding methods often struggle with fine-grained localization in high-resolution screenshots. To address this, we propose GUI-ARP, a novel framework that enables adaptive multi-stage inference. Equipped with the proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC), GUI-ARP dynamically exploits visual attention for cropping task-relevant regions and adapts its inference strategy, performing a single-stage inference for simple cases and a multi-stage analysis for more complex scenarios. This is achieved through a two-phase training pipeline that integrates supervised fine-tuning with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate that the proposed GUI-ARP achieves state-of-the-art performance on challenging GUI grounding benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.",
        "arxiv_id": "2509.15532",
        "ARXIVID": "2509.15532",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for GUI agents) and criterion 3 (embodied AI, new methods for GUI grounding with adaptive region perception). The use of adaptive multi-stage inference and reinforcement fine-tuning for GUI agents is a novel approach to spatial understanding in embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15886": {
        "authors": [
            "Paul Julius K\\\"uhn",
            "Duc Anh Nguyen",
            "Arjan Kuijper",
            "Holger Graf",
            "Dieter Fellner",
            "Saptarshi Neil Sinha"
        ],
        "title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation",
        "abstract": "arXiv:2509.15886v1 Announce Type: new  Abstract: Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.",
        "arxiv_id": "2509.15886",
        "ARXIVID": "2509.15886",
        "COMMENT": "This paper adapts the SAM2 visual foundation model for LiDAR point cloud segmentation in the range view, introducing architectural modifications for 3D perception. It matches criterion 4 (vision foundation models and applications) and is also relevant to spatial understanding (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15553": {
        "authors": [
            "Tian Lan",
            "Yiming Zheng",
            "Jianxin Yin"
        ],
        "title": "Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification",
        "abstract": "arXiv:2509.15553v1 Announce Type: new  Abstract: Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious \"Layer $12$\" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal \"image-text\"$\\times$\"block-timestep\" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\\% mAP on MS-COCO-enhanced and 45.7\\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at https://github.com/lt-0123/Diff-Feat.",
        "arxiv_id": "2509.15553",
        "ARXIVID": "2509.15553",
        "COMMENT": "This paper introduces Diff-Feat, a framework that extracts and fuses intermediate features from pre-trained diffusion-Transformer models for images and text, achieving state-of-the-art results in multi-label classification. It is relevant to criterion 2 (multi-modal large language models) and criterion 4 (vision foundation models and applications), with a clever empirical analysis of feature extraction.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.15704": {
        "authors": [
            "Yuxuan Liang",
            "Xu Li",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance",
        "abstract": "arXiv:2509.15704v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have significantly advanced multimodal understanding but still struggle with efficiently processing high-resolution images. Recent approaches partition high-resolution images into multiple sub-images, dramatically increasing the number of visual tokens and causing exponential computational overhead during inference. To address these limitations, we propose a training-free token pruning strategy, Pyramid Token Pruning (PTP), that integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided importance. Inspired by human visual attention mechanisms, PTP selectively retains more tokens from visually salient regions and further leverages textual instructions to pinpoint tokens most relevant to specific multimodal tasks. Extensive experiments across 13 diverse benchmarks demonstrate that our method substantially reduces computational overhead and inference latency with minimal performance loss.",
        "arxiv_id": "2509.15704",
        "ARXIVID": "2509.15704",
        "COMMENT": "Matches criterion 4: Proposes a new, training-free token pruning method for large vision-language models, improving efficiency for high-resolution images and leveraging vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15250": {
        "authors": [
            "Wenda Qin",
            "Andrea Burns",
            "Bryan A. Plummer",
            "Margrit Betke"
        ],
        "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
        "abstract": "arXiv:2509.15250v1 Announce Type: new  Abstract: Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.",
        "arxiv_id": "2509.15250",
        "ARXIVID": "2509.15250",
        "COMMENT": "Matches criterion 3: Proposes a novel, tuning-free token pruning method for embodied agents in vision-and-language navigation, addressing VLN-specific challenges overlooked by prior work.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16054": {
        "authors": [
            "Jihua Peng",
            "Qianxiong Xu",
            "Yichen Liu",
            "Chenxi Liu",
            "Cheng Long",
            "Rui Zhao",
            "Ziyue Li"
        ],
        "title": "Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model",
        "abstract": "arXiv:2509.16054v1 Announce Type: new  Abstract: Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level  token and multiple cluster-specific  tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the  token and  tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the  token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.",
        "arxiv_id": "2509.16054",
        "ARXIVID": "2509.16054",
        "COMMENT": "Matches criterion 2: Introduces a new MLLM-based framework for group activity detection, expanding MLLM vocabulary and integrating multimodal reasoning for video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16132": {
        "authors": [
            "Carter Sifferman",
            "Yiquan Li",
            "Yiming Li",
            "Fangzhou Mu",
            "Michael Gleicher",
            "Mohit Gupta",
            "Yin Li"
        ],
        "title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels",
        "abstract": "arXiv:2509.16132v1 Announce Type: new  Abstract: We aim to recover the geometry of 3D parametric scenes using very few depth measurements from low-cost, commercially available time-of-flight sensors. These sensors offer very low spatial resolution (i.e., a single pixel), but image a wide field-of-view per pixel and capture detailed time-of-flight data in the form of time-resolved photon counts. This time-of-flight data encodes rich scene information and thus enables recovery of simple scenes from sparse measurements. We investigate the feasibility of using a distributed set of few measurements (e.g., as few as 15 pixels) to recover the geometry of simple parametric scenes with a strong prior, such as estimating the 6D pose of a known object. To achieve this, we design a method that utilizes both feed-forward prediction to infer scene parameters, and differentiable rendering within an analysis-by-synthesis framework to refine the scene parameter estimate. We develop hardware prototypes and demonstrate that our method effectively recovers object pose given an untextured 3D model in both simulations and controlled real-world captures, and show promising initial results for other parametric scenes. We additionally conduct experiments to explore the limits and capabilities of our imaging solution.",
        "arxiv_id": "2509.16132",
        "ARXIVID": "2509.16132",
        "COMMENT": "Matches criterion 1: Presents a new method for recovering 3D parametric scenes from very sparse time-of-flight data, using a combination of feed-forward prediction and differentiable rendering. This is a methodological improvement in spatial understanding for embodied agents or robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15602": {
        "authors": [
            "Zhongyuan Bao",
            "Lejun Zhang"
        ],
        "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
        "abstract": "arXiv:2509.15602v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks at rally and stroke levels and includes 2,500 human-verified questions. Evaluating 16 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results reveal substantial shortcomings and yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.",
        "arxiv_id": "2509.15602",
        "ARXIVID": "2509.15602",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI/video understanding with a novel angle: tennis rally understanding for MLLMs). TennisTV is a new, comprehensive benchmark for evaluating MLLMs on fine-grained, high-frequency sports video understanding, revealing new empirical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15751": {
        "authors": [
            "Zhengyang Yu",
            "Arthur Aubret",
            "Chen Yu",
            "Jochen Triesch"
        ],
        "title": "Simulated Cortical Magnification Supports Self-Supervised Object Learning",
        "abstract": "arXiv:2509.15751v1 Announce Type: new  Abstract: Recent self-supervised learning models simulate the development of semantic object representations by training on visual experience similar to that of toddlers. However, these models ignore the foveated nature of human vision with high/low resolution in the center/periphery of the visual field. Here, we investigate the role of this varying resolution in the development of object representations. We leverage two datasets of egocentric videos that capture the visual experience of humans during interactions with objects. We apply models of human foveation and cortical magnification to modify these inputs, such that the visual content becomes less distinct towards the periphery. The resulting sequences are used to train two bio-inspired self-supervised learning models that implement a time-based learning objective. Our results show that modeling aspects of foveated vision improves the quality of the learned object representations in this setting. Our analysis suggests that this improvement comes from making objects appear bigger and inducing a better trade-off between central and peripheral visual information. Overall, this work takes a step towards making models of humans' learning of visual representations more realistic and performant.",
        "arxiv_id": "2509.15751",
        "ARXIVID": "2509.15751",
        "COMMENT": "Matches criterion 1 (methodological improvement to spatial understanding in embodied/self-supervised agents). The paper investigates the effect of foveated vision and cortical magnification on self-supervised object learning, providing a novel bio-inspired approach to spatial representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15962": {
        "authors": [
            "Sander Schildermans",
            "Chang Tian",
            "Ying Jiao",
            "Marie-Francine Moens"
        ],
        "title": "Structured Information for Improving Spatial Relationships in Text-to-Image Generation",
        "abstract": "arXiv:2509.15962v1 Announce Type: new  Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing spatial relationships described in natural language prompts remains a major challenge. Prior efforts have addressed this issue through prompt optimization, spatially grounded generation, and semantic refinement. This work introduces a lightweight approach that augments prompts with tuple-based structured information, using a fine-tuned language model for automatic conversion and seamless integration into T2I pipelines. Experimental results demonstrate substantial improvements in spatial accuracy, without compromising overall image quality as measured by Inception Score. Furthermore, the automatically generated tuples exhibit quality comparable to human-crafted tuples. This structured information provides a practical and portable solution to enhance spatial relationships in T2I generation, addressing a key limitation of current large-scale generative systems.",
        "arxiv_id": "2509.15962",
        "ARXIVID": "2509.15962",
        "COMMENT": "Matches criterion 1 (spatial understanding in generative models) and criterion 4 (improving spatial relationships in text-to-image generation, a key limitation of current vision foundation models). The method augments prompts with structured information to improve spatial accuracy in T2I.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15271": {
        "authors": [
            "Sebastian Ray Mason",
            "Anders Gj{\\o}lbye",
            "Phillip Chavarria H{\\o}jbjerg",
            "Lenka T\\v{e}tkov\\'a",
            "Lars Kai Hansen"
        ],
        "title": "Large Vision Models Can Solve Mental Rotation Problems",
        "abstract": "arXiv:2509.15271v1 Announce Type: new  Abstract: Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.",
        "arxiv_id": "2509.15271",
        "ARXIVID": "2509.15271",
        "COMMENT": "Matches criterion 1 (spatial intelligence, specifically mental rotation and spatial reasoning in vision models). The paper systematically evaluates vision transformers on mental rotation tasks, providing empirical insights into spatial reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16098": {
        "authors": [
            "Jinyuan Qu",
            "Hongyang Li",
            "Xingyu Chen",
            "Shilong Liu",
            "Yukai Shi",
            "Tianhe Ren",
            "Ruitao Jing",
            "Lei Zhang"
        ],
        "title": "SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features",
        "abstract": "arXiv:2509.16098v1 Announce Type: new  Abstract: In this paper, we present SegDINO3D, a novel Transformer encoder-decoder framework for 3D instance segmentation. As 3D training data is generally not as sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D representation from a pre-trained 2D detection model, including both image-level and object-level features, for improving 3D representation. SegDINO3D takes both a point cloud and its associated 2D images as input. In the encoder stage, it first enriches each 3D point by retrieving 2D image features from its corresponding image views and then leverages a 3D encoder for 3D context fusion. In the decoder stage, it formulates 3D object queries as 3D anchor boxes and performs cross-attention from 3D queries to 2D object queries obtained from 2D images using the 2D detection model. These 2D object queries serve as a compact object-level representation of 2D images, effectively avoiding the challenge of keeping thousands of image feature maps in the memory while faithfully preserving the knowledge of the pre-trained 2D model. The introducing of 3D box queries also enables the model to modulate cross-attention using the predicted boxes for more precise querying. SegDINO3D achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D instance segmentation benchmarks. Notably, on the challenging ScanNet200 dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP on the validation and hidden test sets, respectively, demonstrating its superiority.",
        "arxiv_id": "2509.16098",
        "ARXIVID": "2509.16098",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 1 (leveraging 2D vision foundation models for 3D spatial understanding). The method fuses 2D and 3D features for 3D instance segmentation, which is a strong application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15237": {
        "authors": [
            "Di Wen",
            "Kunyu Peng",
            "Junwei Zheng",
            "Yufan Chen",
            "Yitain Shi",
            "Jiale Wei",
            "Ruiping Liu",
            "Kailun Yang",
            "Rainer Stiefelhagen"
        ],
        "title": "MICA: Multi-Agent Industrial Coordination Assistant",
        "abstract": "arXiv:2509.15237v1 Announce Type: new  Abstract: Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.",
        "arxiv_id": "2509.15237",
        "ARXIVID": "2509.15237",
        "COMMENT": "This paper introduces MICA, a multi-agent industrial coordination assistant with perception-grounded and speech-interactive capabilities, and a new benchmark for multi-agent coordination. It matches criterion 3 (embodied AI, new benchmark/methods), especially with its focus on real-world deployment and evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15472": {
        "authors": [
            "Zhenghao Zhao",
            "Haoxuan Wang",
            "Junyi Wu",
            "Yuzhang Shang",
            "Gaowen Liu",
            "Yan Yan"
        ],
        "title": "Efficient Multimodal Dataset Distillation via Generative Models",
        "abstract": "arXiv:2509.15472v1 Announce Type: new  Abstract: Dataset distillation aims to synthesize a small dataset from a large dataset, enabling the model trained on it to perform well on the original dataset. With the blooming of large language models and multimodal large language models, the importance of multimodal datasets, particularly image-text datasets, has grown significantly. However, existing multimodal dataset distillation methods are constrained by the Matching Training Trajectories algorithm, which significantly increases the computing resource requirement, and takes days to process the distillation. In this work, we introduce EDGE, a generative distillation method for efficient multimodal dataset distillation. Specifically, we identify two key challenges of distilling multimodal datasets with generative models: 1) The lack of correlation between generated images and captions. 2) The lack of diversity among generated samples. To address the aforementioned issues, we propose a novel generative model training workflow with a bi-directional contrastive loss and a diversity loss. Furthermore, we propose a caption synthesis strategy to further improve text-to-image retrieval performance by introducing more text information. Our method is evaluated on Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and efficiency compared to existing approaches. Notably, our method achieves results 18x faster than the state-of-the-art method.",
        "arxiv_id": "2509.15472",
        "ARXIVID": "2509.15472",
        "COMMENT": "This paper proposes a new generative model-based method for efficient multimodal dataset distillation, addressing key challenges in image-text data. It is directly relevant to criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications), as it leverages generative models for multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15330": {
        "authors": [
            "Min Zhang",
            "Bo Jiang",
            "Jie Zhou",
            "Yimeng Liu",
            "Xin Lin"
        ],
        "title": "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization",
        "abstract": "arXiv:2509.15330v1 Announce Type: new  Abstract: Recent advances in pre-training vision-language models (VLMs), e.g., contrastive language-image pre-training (CLIP) methods, have shown great potential in learning out-of-distribution (OOD) representations. Despite showing competitive performance, the prompt-based CLIP methods still suffer from: i) inaccurate text descriptions, which leads to degraded accuracy and robustness, and poses a challenge for zero-shot CLIP methods. ii) limited vision-language embedding alignment, which significantly affects the generalization performance. To tackle the above issues, this paper proposes a novel Conditional Domain prompt Learning (CoDoL) method, which utilizes readily-available domain information to form prompts and improves the vision-language embedding alignment for improving OOD generalization. To capture both instance-specific and domain-specific information, we further propose a lightweight Domain Meta Network (DMN) to generate input-conditional tokens for images in each domain. Extensive experiments on four OOD benchmarks (PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed CoDoL in terms of improving the vision-language embedding alignment as well as the out-of-distribution generalization performance.",
        "arxiv_id": "2509.15330",
        "ARXIVID": "2509.15330",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). CoDoL proposes a new prompt learning method for out-of-distribution generalization in vision-language models (CLIP), improving vision-language embedding alignment and robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.16119": {
        "authors": [
            "Weiyi Xiong",
            "Bing Zhu",
            "Tao Huang",
            "Zewei Zheng"
        ],
        "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars",
        "abstract": "arXiv:2509.16119v1 Announce Type: new  Abstract: 4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.",
        "arxiv_id": "2509.16119",
        "ARXIVID": "2509.16119",
        "COMMENT": "This paper introduces a new Gaussian-based 3D detector for 4D automotive radars, with novel encoding and loss functions for efficient and accurate 3D object detection. It is relevant to embodied AI and spatial understanding (criteria 1, 3), especially with its focus on real-time deployment and new methods for BEV feature extraction.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.15270": {
        "authors": [
            "Emanuele Ricco",
            "Elia Onofri",
            "Lorenzo Cima",
            "Stefano Cresci",
            "Roberto Di Pietro"
        ],
        "title": "PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images",
        "abstract": "arXiv:2509.15270v1 Announce Type: new  Abstract: A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.",
        "arxiv_id": "2509.15270",
        "ARXIVID": "2509.15270",
        "COMMENT": "Related to criterion 4: Proposes PRISM, a frequency-domain fingerprinting method for attribution of AI-generated images, relevant to generative vision models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.15234": {
        "authors": [
            "Hanbin Ko",
            "Gihun Cho",
            "Inhyeok Baek",
            "Donguk Kim",
            "Joonbeom Koo",
            "Changi Kim",
            "Dongheon Lee",
            "Chang Min Park"
        ],
        "title": "Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays",
        "abstract": "arXiv:2509.15234v1 Announce Type: new  Abstract: Vision-language pretraining has advanced image-text alignment, yet progress in radiology remains constrained by the heterogeneity of clinical reports, including abbreviations, impression-only notes, and stylistic variability. Unlike general-domain settings where more data often leads to better performance, naively scaling to large collections of noisy reports can plateau or even degrade model learning. We ask whether large language model (LLM) encoders can provide robust clinical representations that transfer across diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR, a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a dual-tower framework that couples this encoder with a vision backbone. LLM2VEC4CXR improves clinical text understanding over BERT-based baselines, handles abbreviations and style variation, and achieves strong clinical alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to boost retrieval accuracy and clinically oriented scores, with stronger cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M CXR studies from public and private sources with heterogeneous and noisy reports, our models demonstrate that robustness -- not scale alone -- is the key to effective multimodal learning. We release models to support further research in medical image-text representation learning.",
        "arxiv_id": "2509.15234",
        "ARXIVID": "2509.15234",
        "COMMENT": "Matches criterion 2: Proposes a domain-adapted LLM encoder for chest X-ray reports and a dual-tower vision-language model for medical image-text retrieval. Focuses on robustness in multimodal learning for medical vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16170": {
        "authors": [
            "Xiaoqi Zhao",
            "Youwei Pang",
            "Chenyang Yu",
            "Lihe Zhang",
            "Huchuan Lu",
            "Shijian Lu",
            "Georges El Fakhri",
            "Xiaofeng Liu"
        ],
        "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation",
        "abstract": "arXiv:2509.16170v1 Announce Type: new  Abstract: Multi-modal image segmentation faces real-world deployment challenges from incomplete/corrupted modalities degrading performance. While existing methods address training-inference modality gaps via specialized per-combination models, they introduce high deployment costs by requiring exhaustive model subsets and model-modality matching. In this work, we propose a unified modality-relax segmentation network (UniMRSeg) through hierarchical self-supervised compensation (HSSC). Our approach hierarchically bridges representation gaps between complete and incomplete modalities across input, feature and output levels. % First, we adopt modality reconstruction with the hybrid shuffled-masking augmentation, encouraging the model to learn the intrinsic modality characteristics and generate meaningful representations for missing modalities through cross-modal fusion. % Next, modality-invariant contrastive learning implicitly compensates the feature space distance among incomplete-complete modality pairs. Furthermore, the proposed lightweight reverse attention adapter explicitly compensates for the weak perceptual semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid consistency constraint to ensure stable prediction under all modality combinations without large performance fluctuations. Without bells and whistles, UniMRSeg significantly outperforms the state-of-the-art methods under diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D semantic segmentation, RGB-D/T salient object segmentation. The code will be released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.",
        "arxiv_id": "2509.16170",
        "ARXIVID": "2509.16170",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). UniMRSeg proposes a unified segmentation network for multi-modal images, addressing missing modality scenarios and leveraging self-supervised learning, which is relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16011": {
        "authors": [
            "Xiwei Liu",
            "Yulong Li",
            "Yichen Li",
            "Xinlin Zhuang",
            "Haolin Yang",
            "Huifa Li",
            "Imran Razzak"
        ],
        "title": "Towards Robust Visual Continual Learning with Multi-Prototype Supervision",
        "abstract": "arXiv:2509.16011v1 Announce Type: new  Abstract: Language-guided supervision, which utilizes a frozen semantic target from a Pretrained Language Model (PLM), has emerged as a promising paradigm for visual Continual Learning (CL). However, relying on a single target introduces two critical limitations: 1) semantic ambiguity, where a polysemous category name results in conflicting visual representations, and 2) intra-class visual diversity, where a single prototype fails to capture the rich variety of visual appearances within a class. To this end, we propose MuproCL, a novel framework that replaces the single target with multiple, context-aware prototypes. Specifically, we employ a lightweight LLM agent to perform category disambiguation and visual-modal expansion to generate a robust set of semantic prototypes. A LogSumExp aggregation mechanism allows the vision model to adaptively align with the most relevant prototype for a given image. Extensive experiments across various CL baselines demonstrate that MuproCL consistently enhances performance and robustness, establishing a more effective path for language-guided continual learning.",
        "arxiv_id": "2509.16011",
        "ARXIVID": "2509.16011",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications, specifically language-guided continual learning with multi-prototype supervision). The method uses LLMs to generate multiple semantic prototypes for robust continual learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15750": {
        "authors": [
            "Han Ye",
            "Haofu Wang",
            "Yunchi Zhang",
            "Jiangjian Xiao",
            "Yuqiang Jin",
            "Jinyuan Liu",
            "Wen-An Zhang",
            "Uladzislau Sychou",
            "Alexander Tuzikov",
            "Vladislav Sobolevskii",
            "Valerii Zakharov",
            "Boris Sokolov",
            "Minglei Fu"
        ],
        "title": "FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion",
        "abstract": "arXiv:2509.15750v1 Announce Type: new  Abstract: Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.",
        "arxiv_id": "2509.15750",
        "ARXIVID": "2509.15750",
        "COMMENT": "Matches criterion 4 (application of vision foundation models, specifically SAM, to floorplan reconstruction). FloorSAM uses the Segment Anything Model for robust room segmentation from point clouds, showing a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15243": {
        "authors": [
            "Muhammad Imran",
            "Yugyung Lee"
        ],
        "title": "Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models",
        "abstract": "arXiv:2509.15243v1 Announce Type: new  Abstract: Recent advances in vision-language models have significantly expanded the frontiers of automated image analysis. However, applying these models in safety-critical contexts remains challenging due to the complex relationships between objects, subtle visual cues, and the heightened demand for transparency and reliability. This paper presents the Multi-Modal Explainable Learning (MMEL) framework, designed to enhance the interpretability of vision-language models while maintaining high performance. Building upon prior work in gradient-based explanations for transformer architectures (Grad-eclip), MMEL introduces a novel Hierarchical Semantic Relationship Module that enhances model interpretability through multi-scale feature processing, adaptive attention weighting, and cross-modal alignment. Our approach processes features at multiple semantic levels to capture relationships between image regions at different granularities, applying learnable layer-specific weights to balance contributions across the model's depth. This results in more comprehensive visual explanations that highlight both primary objects and their contextual relationships with improved precision. Through extensive experiments on standard datasets, we demonstrate that by incorporating semantic relationship information into gradient-based attribution maps, MMEL produces more focused and contextually aware visualizations that better reflect how vision-language models process complex scenes. The MMEL framework generalizes across various domains, offering valuable insights into model decisions for applications requiring high interpretability and reliability.",
        "arxiv_id": "2509.15243",
        "ARXIVID": "2509.15243",
        "COMMENT": "This paper proposes a new framework for multi-modal interpretability in vision-language models, introducing a hierarchical semantic relationship module for better localization and explanations. It is relevant to criterion 2 (VLLMs) and criterion 4 (vision foundation models and applications), with a focus on interpretability.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15706": {
        "authors": [
            "Chi Yang",
            "Fu Wang",
            "Xiaofei Yang",
            "Hao Huang",
            "Weijia Cao",
            "Xiaowen Chu"
        ],
        "title": "SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark",
        "abstract": "arXiv:2509.15706v1 Announce Type: new  Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as they directly affect radiative transfer and precipitation processes. In this study, we present a benchmark dataset and a baseline framework for transforming multimodal satellite observations into detailed 3D cloud phase structures, aiming toward operational cloud phase profile retrieval and future integration with NWP systems to improve cloud microphysics parameterization. The multimodal observations consist of (1) high--spatiotemporal--resolution, multi-band visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites, and (2) accurate vertical cloud phase profiles from spaceborne lidar (CALIOP\\slash CALIPSO) and radar (CPR\\slash CloudSat). The dataset consists of synchronized image--profile pairs across diverse cloud regimes, defining a supervised learning task: given VIS/TIR patches, predict the corresponding 3D cloud phase structure. We adopt SGMAGNet as the main model and compare it with several baseline architectures, including UNet variants and SegNet, all designed to capture multi-scale spatial patterns. Model performance is evaluated using standard classification metrics, including Precision, Recall, F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior performance in cloud phase reconstruction, particularly in complex multi-layer and boundary transition regions. Quantitatively, SGMAGNet attains a Precision of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617, significantly outperforming all baselines across these key metrics.",
        "arxiv_id": "2509.15706",
        "ARXIVID": "2509.15706",
        "COMMENT": "This paper presents a new benchmark dataset and baseline model for reconstructing 3D cloud phase structures from multimodal satellite data. It is relevant to criterion 3 (new benchmark in embodied AI/simulators), though the application is in weather prediction rather than robotics or embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.16163": {
        "authors": [
            "Het Patel",
            "Muzammil Allie",
            "Qian Zhang",
            "Jia Chen",
            "Evangelos E. Papalexakis"
        ],
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "abstract": "arXiv:2509.16163v1 Announce Type: new  Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3\\% performance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On COCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ($\\alpha=0.1-0.2$) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.",
        "arxiv_id": "2509.16163",
        "ARXIVID": "2509.16163",
        "COMMENT": "This paper proposes a lightweight, plug-and-play tensor decomposition defense for vision-language models against adversarial attacks. It is relevant to criterion 2 (VLLMs), with a novel statistical trick for robustness.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15257": {
        "authors": [
            "Silpa Vadakkeeveetil Sreelatha",
            "Sauradip Nag",
            "Muhammad Awais",
            "Serge Belongie",
            "Anjan Dutta"
        ],
        "title": "RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation",
        "abstract": "arXiv:2509.15257v1 Announce Type: new  Abstract: The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.",
        "arxiv_id": "2509.15257",
        "ARXIVID": "2509.15257",
        "COMMENT": "Somewhat related to criterion 4: Proposes a new framework for responsible text-to-image generation using diffusion models, with a dual-module bottleneck transformation. Focuses on fairness and safety in generative vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15987": {
        "authors": [
            "Aur\\'elien Cecille",
            "Stefan Duffner",
            "Franck Davoine",
            "R\\'emi Agier",
            "Thibault Neveu"
        ],
        "title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation",
        "abstract": "arXiv:2509.15987v1 Announce Type: new  Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.",
        "arxiv_id": "2509.15987",
        "ARXIVID": "2509.15987",
        "COMMENT": "Somewhat related to criterion 1 (improving spatial understanding in depth estimation), but the focus is on self-supervised depth estimation and sharper object boundaries, not embodied agents or spatial intelligence per se.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.16058": {
        "authors": [
            "Krati Saxena",
            "Federico Jurado Ruiz",
            "Guido Manzi",
            "Dianbo Liu",
            "Alex Lamb"
        ],
        "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers",
        "abstract": "arXiv:2509.16058v1 Announce Type: new  Abstract: Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.",
        "arxiv_id": "2509.16058",
        "ARXIVID": "2509.16058",
        "COMMENT": "Relevant to vision transformers and attention mechanisms, but does not directly match any specific criterion (not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in application).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.15573": {
        "authors": [
            "Shilong Bao",
            "Qianqian Xu",
            "Feiran Li",
            "Boyu Han",
            "Zhiyong Yang",
            "Xiaochun Cao",
            "Qingming Huang"
        ],
        "title": "Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach",
        "abstract": "arXiv:2509.15573v1 Announce Type: new  Abstract: This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at https://github.com/Ferry-Li/SI-SOD.",
        "arxiv_id": "2509.15573",
        "ARXIVID": "2509.15573",
        "COMMENT": "Somewhat related to criterion 4: Proposes a size-invariant evaluation and optimization framework for salient object detection, which is a vision task, but not directly about vision foundation models or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.16022": {
        "authors": [
            "Xihong Yang",
            "Siwei Wang",
            "Jiaqi Jin",
            "Fangdi Wang",
            "Tianrui Liu",
            "Yueming Jin",
            "Xinwang Liu",
            "En Zhu",
            "Kunlun He"
        ],
        "title": "Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence",
        "abstract": "arXiv:2509.16022v1 Announce Type: new  Abstract: Multi-view clustering (MVC) aims to explore the common clustering structure across multiple views. Many existing MVC methods heavily rely on the assumption of view consistency, where alignments for corresponding samples across different views are ordered in advance. However, real-world scenarios often present a challenge as only partial data is consistently aligned across different views, restricting the overall clustering performance. In this work, we consider the model performance decreasing phenomenon caused by data order shift (i.e., from fully to partially aligned) as a generalized multi-view clustering problem. To tackle this problem, we design a causal multi-view clustering network, termed CauMVC. We adopt a causal modeling approach to understand multi-view clustering procedure. To be specific, we formulate the partially aligned data as an intervention and multi-view clustering with partially aligned data as an post-intervention inference. However, obtaining invariant features directly can be challenging. Thus, we design a Variational Auto-Encoder for causal learning by incorporating an encoder from existing information to estimate the invariant features. Moreover, a decoder is designed to perform the post-intervention inference. Lastly, we design a contrastive regularizer to capture sample correlations. To the best of our knowledge, this paper is the first work to deal generalized multi-view clustering via causal learning. Empirical experiments on both fully and partially aligned data illustrate the strong generalization and effectiveness of CauMVC.",
        "arxiv_id": "2509.16022",
        "ARXIVID": "2509.16022",
        "COMMENT": "This paper presents a causal learning approach for generalized deep multi-view clustering with partially aligned cross-view correspondence. While it is related to multi-modal learning, it does not directly match any of the specific criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.16091": {
        "authors": [
            "Shen Cheng",
            "Haipeng Li",
            "Haibin Huang",
            "Xiaohong Liu",
            "Shuaicheng Liu"
        ],
        "title": "Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising",
        "abstract": "arXiv:2509.16091v1 Announce Type: new  Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised framework for real-world image denoising. Our approach addresses two major challenges: the limitations of blind-spot networks (BSNs), which often sacrifice local detail and introduce pixel discontinuities due to spatial independence assumptions, and the difficulty of adapting diffusion models to self-supervised denoising. We propose a dual-branch diffusion framework that combines a BSN-based diffusion branch, generating semi-clean images, with a conventional diffusion branch that captures underlying noise distributions. To enable effective training without paired data, we use the BSN-based branch to guide the sampling process, capturing noise structure while preserving local details. Extensive experiments on the SIDD and DND datasets demonstrate state-of-the-art performance, establishing our method as a highly effective self-supervised solution for real-world denoising. Code and pre-trained models are released at: https://github.com/Sumching/BSGD.",
        "arxiv_id": "2509.16091",
        "ARXIVID": "2509.16091",
        "COMMENT": "Relevant to computer vision and generative modeling, but does not match any specific criterion (no spatial intelligence, VLLM/MLLM, embodied AI, or vision foundation model focus).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15470": {
        "authors": [
            "Thomas Z. Li",
            "Aravind R. Krishnan",
            "Lianrui Zuo",
            "John M. Still",
            "Kim L. Sandler",
            "Fabien Maldonado",
            "Thomas A. Lasko",
            "Bennett A. Landman"
        ],
        "title": "Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture",
        "abstract": "arXiv:2509.15470v1 Announce Type: new  Abstract: The development of multimodal models for pulmonary nodule diagnosis is limited by the scarcity of labeled data and the tendency for these models to overfit on the training distribution. In this work, we leverage self-supervised learning from longitudinal and multimodal archives to address these challenges. We curate an unlabeled set of patients with CT scans and linked electronic health records from our home institution to power joint embedding predictive architecture (JEPA) pretraining. After supervised finetuning, we show that our approach outperforms an unregularized multimodal model and imaging-only model in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC), but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC). We develop a synthetic environment that characterizes the context in which JEPA may underperform. This work innovates an approach that leverages unlabeled multimodal medical archives to improve predictive models and demonstrates its advantages and limitations in pulmonary nodule diagnosis.",
        "arxiv_id": "2509.15470",
        "ARXIVID": "2509.15470",
        "COMMENT": "Somewhat related to criterion 4: Uses self-supervised learning for multimodal (imaging and clinical) data, with a joint-embedding predictive architecture. Focuses on medical applications, not general vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.15791": {
        "authors": [
            "Tan Pan",
            "Kaiyu Guo",
            "Dongli Xu",
            "Zhaorui Tan",
            "Chen Jiang",
            "Deshu Chen",
            "Xin Guo",
            "Brian C. Lovell",
            "Limei Han",
            "Yuan Cheng",
            "Mahsa Baktashmotlagh"
        ],
        "title": "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization",
        "abstract": "arXiv:2509.15791v1 Announce Type: new  Abstract: The generalization ability of deep learning has been extensively studied in supervised settings, yet it remains less explored in unsupervised scenarios. Recently, the Unsupervised Domain Generalization (UDG) task has been proposed to enhance the generalization of models trained with prevalent unsupervised learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the challenge of distinguishing semantics from variations without category labels. Although some recent methods have employed domain labels to tackle this issue, such domain labels are often unavailable in real-world contexts. In this paper, we address these limitations by formalizing UDG as the task of learning a Minimal Sufficient Semantic Representation: a representation that (i) preserves all semantic information shared across augmented views (sufficiency), and (ii) maximally removes information irrelevant to semantics (minimality). We theoretically ground these objectives from the perspective of information theory, demonstrating that optimizing representations to achieve sufficiency and minimality directly reduces out-of-distribution risk. Practically, we implement this optimization through Minimal-Sufficient UDG (MS-UDG), a learnable model by integrating (a) an InfoNCE-based objective to achieve sufficiency; (b) two complementary components to promote minimality: a novel semantic-variation disentanglement loss and a reconstruction-based mechanism for capturing adequate variation. Empirically, MS-UDG sets a new state-of-the-art on popular unsupervised domain-generalization benchmarks, consistently outperforming existing SSL and UDG methods, without category or domain labels during representation learning.",
        "arxiv_id": "2509.15791",
        "ARXIVID": "2509.15791",
        "COMMENT": "Does not match any specific criterion. The paper is about unsupervised domain generalization and semantic representation learning, which is tangential to vision foundation models but not directly about VLLMs, MLLMs, embodied AI, or spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15242": {
        "authors": [
            "Jaydeep Rade",
            "Md Hasibul Hasan Hasib",
            "Meric Ozturk",
            "Baboucarr Faal",
            "Sheng Yang",
            "Dipali G. Sashital",
            "Vincenzo Venditti",
            "Baoyu Chen",
            "Soumik Sarkar",
            "Adarsh Krishnamurthy",
            "Anwesha Sarkar"
        ],
        "title": "ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images",
        "abstract": "arXiv:2509.15242v1 Announce Type: new  Abstract: AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues. Experimental techniques like Cryo-EM are accurate but costly and time-consuming. We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction. However, generating a large-scale AFM imaging data set sufficient to train deep learning models is impractical. Therefore, we developed a virtual AFM framework that simulates the imaging process and generated a dataset of ~542,000 proteins with multi-view synthetic AFM images. We train a conditional diffusion model to synthesize novel views from unposed inputs and an instance-specific Neural Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D protein structures achieve an average Chamfer Distance within the AFM imaging resolution, reflecting high structural fidelity. Our method is extensively validated on experimental AFM images of various PCs, demonstrating strong potential for accurate, cost-effective protein complex structure prediction and rapid iterative validation using AFM experiments.",
        "arxiv_id": "2509.15242",
        "ARXIVID": "2509.15242",
        "COMMENT": "Does not match any specific criterion. While it uses diffusion models and NeRFs for 3D protein structure reconstruction, it is focused on protein complexes and AFM imaging, not on embodied agents, VLLMs/MLLMs, or vision foundation models in a general sense.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15578": {
        "authors": [
            "Shanghong Li",
            "Chiam Wen Qi Ruth",
            "Hong Xu",
            "Fang Liu"
        ],
        "title": "Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion",
        "abstract": "arXiv:2509.15578v1 Announce Type: new  Abstract: The rapid proliferation of short video platforms has necessitated advanced methods for detecting fake news. This need arises from the widespread influence and ease of sharing misinformation, which can lead to significant societal harm. Current methods often struggle with the dynamic and multimodal nature of short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel multimodal framework that integrates video, audio, and text data to evaluate the authenticity of short video content. HFN introduces a Decision Network that dynamically adjusts modality weights during inference and a Weighted Multi-Modal Feature Fusion module to ensure robust performance even with incomplete data. Additionally, we contribute a comprehensive dataset VESV (VEracity on Short Videos) specifically designed for short video fake news detection. Experiments conducted on the FakeTT and newly collected VESV datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over state-of-the-art methods. This work establishes a robust solution capable of effectively identifying fake news in the complex landscape of short video platforms, paving the way for more reliable and comprehensive approaches in combating misinformation.",
        "arxiv_id": "2509.15578",
        "ARXIVID": "2509.15578",
        "COMMENT": "Presents a new multimodal framework for fake news detection in short videos, but does not introduce a new VLLM/MLLM or vision foundation model, nor is it about spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16095": {
        "authors": [
            "Yi Xu",
            "Yun Fu"
        ],
        "title": "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports",
        "abstract": "arXiv:2509.16095v1 Announce Type: new  Abstract: Trajectory prediction in multi-agent sports scenarios is inherently challenging due to the structural heterogeneity across agent roles (e.g., players vs. ball) and dynamic distribution gaps across different sports domains. Existing unified frameworks often fail to capture these structured distributional shifts, resulting in suboptimal generalization across roles and domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework that explicitly addresses both intra-domain and inter-domain distribution discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and Domain-Aware Adapter to conditionally adjust latent representations based on agent identity and domain context. Additionally, we introduce a Hierarchical Contrastive Learning objective, which separately supervises role-sensitive and domain-aware representations to encourage disentangled latent structures without introducing optimization conflict. Experiments on three diverse sports datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness of our adaptive design, achieving strong performance in both unified and cross-domain trajectory prediction settings.",
        "arxiv_id": "2509.16095",
        "ARXIVID": "2509.16095",
        "COMMENT": "Related to general multi-agent modeling, but does not directly match any criterion. Focuses on trajectory modeling in sports with role- and domain-aware adaptation, but not specifically spatial intelligence for embodied agents or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15645": {
        "authors": [
            "Donghyun Lee",
            "Dawoon Jeong",
            "Jae W. Lee",
            "Hongil Yoon"
        ],
        "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading",
        "abstract": "arXiv:2509.15645v1 Announce Type: new  Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
        "arxiv_id": "2509.15645",
        "ARXIVID": "2509.15645",
        "COMMENT": "Does not match any specific criterion. The paper is about system-level optimizations for 3D Gaussian Splatting training, which is relevant to graphics and rendering, but not to embodied AI, VLLMs/MLLMs, or vision foundation models in the context of spatial intelligence or multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15957": {
        "authors": [
            "Kanato Masayoshi",
            "Masahiro Hashimoto",
            "Ryoichi Yokoyama",
            "Naoki Toda",
            "Yoshifumi Uwamino",
            "Shogo Fukuda",
            "Ho Namkoong",
            "Masahiro Jinzaki"
        ],
        "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol",
        "abstract": "arXiv:2509.15957v1 Announce Type: new  Abstract: Background: Large language models (LLMs) show promise in medicine, but their deployment in hospitals is limited by restricted access to electronic health record (EHR) systems. The Model Context Protocol (MCP) enables integration between LLMs and external tools.   Objective: To evaluate whether an LLM connected to an EHR database via MCP can autonomously retrieve clinically relevant information in a real hospital setting.   Methods: We developed EHR-MCP, a framework of custom MCP tools integrated with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct agent to interact with it. Six tasks were tested, derived from use cases of the infection control team (ICT). Eight patients discussed at ICT conferences were retrospectively analyzed. Agreement with physician-generated gold standards was measured.   Results: The LLM consistently selected and executed the correct MCP tools. Except for two tasks, all tasks achieved near-perfect accuracy. Performance was lower in the complex task requiring time-dependent calculations. Most errors arose from incorrect arguments or misinterpretation of tool results. Responses from EHR-MCP were reliable, though long and repetitive data risked exceeding the context window.   Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks while highlighting challenges in complex ones. EHR-MCP provides an infrastructure for secure, consistent data access and may serve as a foundation for hospital AI agents. Future work should extend beyond retrieval to reasoning, generation, and clinical impact assessment, paving the way for effective integration of generative AI into clinical practice.",
        "arxiv_id": "2509.15957",
        "ARXIVID": "2509.15957",
        "COMMENT": "Does not match any specific criteria. Focuses on LLMs for clinical information retrieval, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}