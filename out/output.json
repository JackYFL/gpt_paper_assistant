{
    "2509.08826": {
        "authors": [
            "Jie Wu",
            "Yu Gao",
            "Zilyu Ye",
            "Ming Li",
            "Liang Li",
            "Hanzhong Guo",
            "Jie Liu",
            "Zeyue Xue",
            "Xiaoxia Hou",
            "Wei Liu",
            "Yan Zeng",
            "Weilin Huang"
        ],
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "abstract": "arXiv:2509.08826v1 Announce Type: new  Abstract: Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "arxiv_id": "2509.08826",
        "ARXIVID": "2509.08826",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications) and criterion 2 (addresses reward modeling for VLMs in visual generation, with a novel scalable reward paradigm and empirical insights into reward hacking).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.08016": {
        "authors": [
            "Hyungjin Chung",
            "Hyelin Nam",
            "Jiyeon Kim",
            "Hyojun Go",
            "Byeongjun Park",
            "Junho Kim",
            "Joonseok Lee",
            "Seongsu Ha",
            "Byung-Hoon Kim"
        ],
        "title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs",
        "abstract": "arXiv:2509.08016v1 Announce Type: new  Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.",
        "arxiv_id": "2509.08016",
        "ARXIVID": "2509.08016",
        "COMMENT": "Matches criterion 2 (new inference-time method for VideoLLMs, a type of VLLM) and criterion 4 (method for improving vision foundation models' temporal reasoning). The VPS method is a clever statistical trick for scaling input without increasing context window.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.08500": {
        "authors": [
            "Kechen Jiao",
            "Zhirui Fang",
            "Jiahao Liu",
            "Bei Li",
            "Qifan Wang",
            "Xinyu Liu",
            "Junhao Ruan",
            "Zhongjian Qiao",
            "Yifan Zhu",
            "Yaxin Xu",
            "Jingang Wang",
            "Xiu Li"
        ],
        "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making",
        "abstract": "arXiv:2509.08500v1 Announce Type: new  Abstract: Using effective generalization capabilities of vision language models (VLMs) in context-specific dynamic tasks for embodied artificial intelligence remains a significant challenge. Although supervised fine-tuned models can better align with the real physical world, they still exhibit sluggish responses and hallucination issues in dynamically changing environments, necessitating further alignment. Existing post-SFT methods, reliant on reinforcement learning and chain-of-thought (CoT) approaches, are constrained by sparse rewards and action-only optimization, resulting in low sample efficiency, poor consistency, and model degradation. To address these issues, this paper proposes Thought-Centric Preference Optimization (TCPO) for effective embodied decision-making. Specifically, TCPO introduces a stepwise preference-based optimization approach, transforming sparse reward signals into richer step sample pairs. It emphasizes the alignment of the model's intermediate reasoning process, mitigating the problem of model degradation. Moreover, by incorporating Action Policy Consistency Constraint (APC), it further imposes consistency constraints on the model output. Experiments in the ALFWorld environment demonstrate an average success rate of 26.67%, achieving a 6% improvement over RL4VLM and validating the effectiveness of our approach in mitigating model degradation after fine-tuning. These results highlight the potential of integrating preference-based learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents.",
        "arxiv_id": "2509.08500",
        "ARXIVID": "2509.08500",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding and decision-making in embodied agents) and criterion 3 (novel method for embodied AI, focusing on preference-based learning and chain-of-thought alignment, which is a novel angle).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.08777": {
        "authors": [
            "Eric Slyman",
            "Mehrab Tanjim",
            "Kushal Kafle",
            "Stefan Lee"
        ],
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "abstract": "arXiv:2509.08777v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "arxiv_id": "2509.08777",
        "ARXIVID": "2509.08777",
        "COMMENT": "Directly matches criterion 2 (shows new MLLM/VLLM) as it proposes a new multimodal Bayesian prompt ensemble method for calibrating MLLMs as judges in text-to-image evaluation. Also relevant to criterion 4 (vision foundation models and applications) due to its focus on evaluation of vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.08538": {
        "authors": [
            "Garry Yang",
            "Zizhe Chen",
            "Man Hon Wong",
            "Haoyu Lei",
            "Yongqiang Chen",
            "Zhenguo Li",
            "Kaiwen Zhou",
            "James Cheng"
        ],
        "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models",
        "abstract": "arXiv:2509.08538v1 Announce Type: new  Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large Language Models (LLMs) and vision modules by integrating temporal information to better understand dynamic video content. Despite their progress, LVMs are prone to hallucinations-producing inaccurate or irrelevant descriptions. Current benchmarks for video hallucination depend heavily on manual categorization of video content, neglecting the perception-based processes through which humans naturally interpret videos. We introduce MESH, a benchmark designed to evaluate hallucinations in LVMs systematically. MESH uses a Question-Answering framework with binary and multi-choice formats incorporating target and trap instances. It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding. We demonstrate that MESH offers an effective and comprehensive approach for identifying hallucinations in videos. Our evaluations show that while LVMs excel at recognizing basic objects and features, their susceptibility to hallucinations increases markedly when handling fine details or aligning multiple actions involving various subjects in longer videos.",
        "arxiv_id": "2509.08538",
        "ARXIVID": "2509.08538",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/methods) as it introduces MESH, a new benchmark for measuring hallucinations in large video models, focusing on human-like video understanding and error analysis, which is a novel angle in LVM evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.08519": {
        "authors": [
            "Liyang Chen",
            "Tianxiang Ma",
            "Jiawei Liu",
            "Bingchuan Li",
            "Zhuowei Chen",
            "Lijie Liu",
            "Xu He",
            "Gen Li",
            "Qian He",
            "Zhiyong Wu"
        ],
        "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning",
        "abstract": "arXiv:2509.08519v1 Announce Type: new  Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.",
        "arxiv_id": "2509.08519",
        "ARXIVID": "2509.08519",
        "COMMENT": "Matches criterion 4: introduces HuMo, a unified human-centric video generation framework with collaborative multi-modal conditioning (text, image, audio), leveraging vision foundation models and generative modeling in multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.08764": {
        "authors": [
            "Lena Wild",
            "Rafael Valencia",
            "Patric Jensfelt"
        ],
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "abstract": "arXiv:2509.08764v1 Announce Type: new  Abstract: Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "arxiv_id": "2509.08764",
        "ARXIVID": "2509.08764",
        "COMMENT": "Matches criterion 3: introduces ArgoTweak, a new benchmark dataset and framework for self-updating HD maps with realistic priors, addressing sim2real gap in mapping for embodied agents. Also relevant to spatial intelligence in embodied agents (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.08311": {
        "authors": [
            "Rongsheng Wang",
            "Fenghe Tang",
            "Qingsong Yao",
            "Rui Yan",
            "Xu Zhang",
            "Zhen Huang",
            "Haoran Lai",
            "Zhiyang He",
            "Xiaodong Tao",
            "Zihang Jiang",
            "Shaohua Kevin Zhou"
        ],
        "title": "SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training",
        "abstract": "arXiv:2509.08311v1 Announce Type: new  Abstract: Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Experimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at https://github.com/ToniChopp/SimCroP.",
        "arxiv_id": "2509.08311",
        "ARXIVID": "2509.08311",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes a new multi-modal vision-language pre-training framework (SimCroP) for radiograph representation learning, with novel similarity-driven alignment and cross-granularity fusion. Also relevant to criterion 4 (vision foundation models and applications) as it applies vision-language models to medical imaging.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07996": {
        "authors": [
            "Lingdong Kong",
            "Wesley Yang",
            "Jianbiao Mei",
            "Youquan Liu",
            "Ao Liang",
            "Dekai Zhu",
            "Dongyue Lu",
            "Wei Yin",
            "Xiaotao Hu",
            "Mingkai Jia",
            "Junyuan Deng",
            "Kaiwen Zhang",
            "Yang Wu",
            "Tianyi Yan",
            "Shenyuan Gao",
            "Song Wang",
            "Linfeng Li",
            "Liang Pan",
            "Yong Liu",
            "Jianke Zhu",
            "Wei Tsang Ooi",
            "Steven C. H. Hoi",
            "Ziwei Liu"
        ],
        "title": "3D and 4D World Modeling: A Survey",
        "abstract": "arXiv:2509.07996v1 Announce Type: new  Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey",
        "arxiv_id": "2509.07996",
        "ARXIVID": "2509.07996",
        "COMMENT": "Matches criterion 4 (survey of 3D/4D world modeling, including vision foundation models and their applications). Provides a comprehensive taxonomy and summary, which is valuable for understanding the field.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08388": {
        "authors": [
            "Dubing Chen",
            "Huan Zheng",
            "Yucheng Zhou",
            "Xianfei Li",
            "Wenlong Liao",
            "Tao He",
            "Pai Peng",
            "Jianbing Shen"
        ],
        "title": "Semantic Causality-Aware Vision-Based 3D Occupancy Prediction",
        "abstract": "arXiv:2509.08388v1 Announce Type: new  Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.",
        "arxiv_id": "2509.08388",
        "ARXIVID": "2509.08388",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding) as it proposes a new causal loss for end-to-end 2D-to-3D semantic occupancy prediction, improving spatial reasoning and robustness in 3D vision tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08502": {
        "authors": [
            "Piyush Bagad",
            "Andrew Zisserman"
        ],
        "title": "Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening",
        "abstract": "arXiv:2509.08502v1 Announce Type: new  Abstract: Our objective is to develop compact video representations that are sensitive to visual change over time. To measure such time-sensitivity, we introduce a new task: chiral action recognition, where one needs to distinguish between a pair of temporally opposite actions, such as \"opening vs. closing a door\", \"approaching vs. moving away from something\", \"folding vs. unfolding paper\", etc. Such actions (i) occur frequently in everyday life, (ii) require understanding of simple visual change over time (in object state, size, spatial position, count . . . ), and (iii) are known to be poorly represented by many video embeddings. Our goal is to build time aware video representations which offer linear separability between these chiral pairs. To that end, we propose a self-supervised adaptation recipe to inject time-sensitivity into a sequence of frozen image features. Our model is based on an auto-encoder with a latent space with inductive bias inspired by perceptual straightening. We show that this results in a compact but time-sensitive video representation for the proposed task across three datasets: Something-Something, EPIC-Kitchens, and Charade. Our method (i) outperforms much larger video models pre-trained on large-scale video datasets, and (ii) leads to an improvement in classification performance on standard benchmarks when combined with these existing models.",
        "arxiv_id": "2509.08502",
        "ARXIVID": "2509.08502",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) as it introduces a new self-supervised method for time-aware video representation learning, focusing on chiral action recognition, which is closely related to spatial and temporal understanding in video.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08621": {
        "authors": [
            "Xinwei Long",
            "Kai Tian",
            "Peng Xu",
            "Guoli Jia",
            "Jingxuan Li",
            "Sa Yang",
            "Yihua Shao",
            "Kaiyan Zhang",
            "Che Jiang",
            "Hao Xu",
            "Yang Liu",
            "Jiaheng Ma",
            "Bowen Zhou"
        ],
        "title": "AdsQA: Towards Advertisement Video Understanding",
        "abstract": "arXiv:2509.08621v1 Announce Type: new  Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos' traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin.",
        "arxiv_id": "2509.08621",
        "ARXIVID": "2509.08621",
        "COMMENT": "Matches criterion 2: introduces AdsQA, a new challenging video QA benchmark for evaluating LLMs (including VLLMs) on advertisement video understanding, and benchmarks 14 LLMs including a new RL-based model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08828": {
        "authors": [
            "David Stotko",
            "Reinhard Klein"
        ],
        "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video",
        "abstract": "arXiv:2509.08828v1 Announce Type: new  Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
        "arxiv_id": "2509.08828",
        "ARXIVID": "2509.08828",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a novel method for 3D reconstruction and appearance estimation of fabrics from monocular video using differentiable physical simulation, which is a significant application in computer vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.08628": {
        "authors": [
            "Xuqin Wang",
            "Tao Wu",
            "Yanfeng Zhang",
            "Lu Liu",
            "Dong Wang",
            "Mingwei Sun",
            "Yongliang Wang",
            "Niclas Zeller",
            "Daniel Cremers"
        ],
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation",
        "abstract": "arXiv:2509.08628v1 Announce Type: new  Abstract: Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "arxiv_id": "2509.08628",
        "ARXIVID": "2509.08628",
        "COMMENT": "Relevant to criterion 4: proposes a new diffusion-based method for semi-supervised domain translation, with multi-modal conditioning (depth, segmentation, style). The method is a novel generative modeling approach in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.08338": {
        "authors": [
            "Jihyun Moon",
            "Charmgil Hong"
        ],
        "title": "Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis",
        "abstract": "arXiv:2509.08338v1 Announce Type: new  Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving patient outcomes. While convolutional neural networks (CNNs) have shown promise in dermoscopic image analysis, they often neglect clinical metadata and require extensive preprocessing. Vision-language models (VLMs) offer a multimodal alternative but struggle to capture clinical specificity when trained on general-domain data. To address this, we propose a retrieval-augmented VLM framework that incorporates semantically similar patient cases into the diagnostic prompt. Our method enables informed predictions without fine-tuning and significantly improves classification accuracy and error correction over conventional baselines. These results demonstrate that retrieval-augmented prompting provides a robust strategy for clinical decision support.",
        "arxiv_id": "2509.08338",
        "ARXIVID": "2509.08338",
        "COMMENT": "Matches criterion 2 (applies VLMs to multimodal medical diagnosis with retrieval-augmented prompting, which is a novel application of VLLMs/MLLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08489": {
        "authors": [
            "Kaleem Ahmad"
        ],
        "title": "Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation",
        "abstract": "arXiv:2509.08489v1 Announce Type: new  Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.",
        "arxiv_id": "2509.08489",
        "ARXIVID": "2509.08489",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it presents a unified, prompt-driven pipeline for image analysis using modern vision and multimodal models, with practical insights for reliability and transparency.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08024": {
        "authors": [
            "Lata Pangtey",
            "Omkar Kabde",
            "Shahid Shafi Dar",
            "Nagendra Kumar"
        ],
        "title": "Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change",
        "abstract": "arXiv:2509.08024v1 Announce Type: new  Abstract: With the rapid proliferation of information across digital platforms, stance detection has emerged as a pivotal challenge in social media analysis. While most of the existing approaches focus solely on textual data, real-world social media content increasingly combines text with visual elements creating a need for advanced multimodal methods. To address this gap, we propose a multimodal stance detection framework that integrates textual and visual information through a hierarchical fusion approach. Our method first employs a Large Language Model to retrieve stance-relevant summaries from source text, while a domain-aware image caption generator interprets visual content in the context of the target topic. These modalities are then jointly modeled along with the reply text, through a specialized transformer module that captures interactions between the texts and images. The proposed modality fusion framework integrates diverse modalities to facilitate robust stance classification. We evaluate our approach on the MultiClimate dataset, a benchmark for climate change-related stance detection containing aligned video frames and transcripts. We achieve accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%, respectively, outperforming existing state-of-the-art approaches.",
        "arxiv_id": "2509.08024",
        "ARXIVID": "2509.08024",
        "COMMENT": "Matches criterion 2 (shows new MLLM/VLLM) as it proposes a multimodal stance detection framework integrating LLMs and image captioning for climate change stance detection. Also relevant to criterion 4 (vision foundation models and applications) due to the use of vision-language models for real-world multimodal analysis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08436": {
        "authors": [
            "Xia Yue",
            "Anfeng Liu",
            "Ning Chen",
            "Chenjia Huang",
            "Hui Liu",
            "Zhou Huang",
            "Leyuan Fang"
        ],
        "title": "Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time",
        "abstract": "arXiv:2509.08436v1 Announce Type: new  Abstract: Hyperspectral image (HSI) classification models are highly sensitive to distribution shifts caused by various real-world degradations such as noise, blur, compression, and atmospheric effects. To address this challenge, we propose HyperTTA, a unified framework designed to enhance model robustness under diverse degradation conditions. Specifically, we first construct a multi-degradation hyperspectral dataset that systematically simulates nine representative types of degradations, providing a comprehensive benchmark for robust classification evaluation. Based on this, we design a spectral-spatial transformer classifier (SSTC) enhanced with a multi-level receptive field mechanism and label smoothing regularization to jointly capture multi-scale spatial context and improve generalization. Furthermore, HyperTTA incorporates a lightweight test-time adaptation (TTA) strategy, the confidence-aware entropy-minimized LayerNorm adapter (CELA), which updates only the affine parameters of LayerNorm layers by minimizing prediction entropy on high-confidence unlabeled target samples. This confidence-aware adaptation prevents unreliable updates from noisy predictions, enabling robust and dynamic adaptation without access to source data or target annotations. Extensive experiments on two benchmark datasets demonstrate that HyperTTA outperforms existing baselines across a wide range of degradation scenarios, validating the effectiveness of both its classification backbone and the proposed TTA scheme. Code will be made available publicly.",
        "arxiv_id": "2509.08436",
        "ARXIVID": "2509.08436",
        "COMMENT": "Relevant to criterion 4: proposes HyperTTA, a robust hyperspectral image classification framework with a new test-time adaptation method and a new multi-degradation benchmark. Involves vision foundation models and robust adaptation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.08458": {
        "authors": [
            "Yujie Zhu",
            "Xinyi Zhang",
            "Yekai Lu",
            "Guang Yang",
            "Faming Fang",
            "Guixu Zhang"
        ],
        "title": "First-order State Space Model for Lightweight Image Super-resolution",
        "abstract": "arXiv:2509.08458v1 Announce Type: new  Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.",
        "arxiv_id": "2509.08458",
        "ARXIVID": "2509.08458",
        "COMMENT": "Relevant to criterion 4: proposes a new first-order state space model (FSSM) for lightweight image super-resolution, improving vision foundation models (Mamba-based) for vision tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.08442": {
        "authors": [
            "Ivan Stoyanov",
            "Fabian Bongratz",
            "Christian Wachinger"
        ],
        "title": "Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting",
        "abstract": "arXiv:2509.08442v1 Announce Type: new  Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.",
        "arxiv_id": "2509.08442",
        "ARXIVID": "2509.08442",
        "COMMENT": "Partially matches criterion 1 (methodological improvements to spatial understanding): Introduces a new diffusion model (SBDM) for forecasting cortical thickness on non-Euclidean geometry, with a novel spherical U-Net and cross-attention. While not on embodied agents, it is a methodological advance in spatial modeling on complex surfaces.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08785": {
        "authors": [
            "Anup Tuladhar",
            "Araz Minhas",
            "Adam Kirton",
            "Eli Kinney-Lang"
        ],
        "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making",
        "abstract": "arXiv:2509.08785v1 Announce Type: new  Abstract: We present a preliminary experimental platform that explores how narrative elements might shape AI decision-making by combining reinforcement learning (RL) with language model reasoning. While AI systems can now both make decisions and engage in narrative reasoning, these capabilities have mostly been studied separately. Our platform attempts to bridge this gap using a dual-system architecture to examine how narrative frameworks could influence reward-based learning. The system comprises a reinforcement learning policy that suggests actions based on past experience, and a language model that processes these suggestions through different narrative frameworks to guide decisions. This setup enables initial experimentation with narrative elements while maintaining consistent environment and reward structures. We implement this architecture in a configurable gridworld environment, where agents receive both policy suggestions and information about their surroundings. The platform's modular design facilitates controlled testing of environmental complexity, narrative parameters, and the interaction between reinforcement learning and narrative-based decisions. Our logging system captures basic decision metrics, from RL policy values to language model reasoning to action selection patterns. While preliminary, this implementation provides a foundation for studying how different narrative frameworks might affect reward-based decisions and exploring potential interactions between optimization-based learning and symbolic reasoning in AI systems.",
        "arxiv_id": "2509.08785",
        "ARXIVID": "2509.08785",
        "COMMENT": "Somewhat relevant to criterion 3 (embodied AI, new benchmark/methods) as it introduces a new experimental platform for studying the influence of narrative frameworks on RL agents, but is more focused on RL and language model interaction than spatial intelligence or embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08571": {
        "authors": [
            "Bayu Adhi Tama",
            "Homayra Alam",
            "Mostafa Cham",
            "Omar Faruque",
            "Jianwu Wang",
            "Vandana Janeja"
        ],
        "title": "Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data",
        "abstract": "arXiv:2509.08571v1 Announce Type: new  Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level projections, but radar observations are sparse and uneven. We introduce GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built from surface observables (elevation, velocity, mass balance) are augmented with gradient features and polynomial trends to capture both local variability and broad structure. To handle data gaps, we employ a hybrid loss that combines confidence-weighted radar supervision with dynamically balanced regularization. Applied to three Greenland subregions, GraphTopoNet outperforms interpolation, convolutional, and graph-based baselines, reducing error by up to 60 percent while preserving fine-scale glacial features. The resulting bed maps improve reliability for operational modeling, supporting agencies engaged in climate forecasting and policy. More broadly, GraphTopoNet shows how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale.",
        "arxiv_id": "2509.08571",
        "ARXIVID": "2509.08571",
        "COMMENT": "Relevant to criterion 4: proposes GraphTopoNet, a graph-learning framework for mapping Greenland's bed topography from sparse data, using uncertainty-aware graph learning. Applies vision/graph foundation models to geospatial data.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08289": {
        "authors": [
            "Yuelin Guo",
            "Haoyu He",
            "Zhiyuan Chen",
            "Zitong Huang",
            "Renhao Lu",
            "Lu Shi",
            "Zejun Wang",
            "Weizhe Zhang"
        ],
        "title": "Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection",
        "abstract": "arXiv:2509.08289v1 Announce Type: new  Abstract: Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.",
        "arxiv_id": "2509.08289",
        "ARXIVID": "2509.08289",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it proposes improvements in weakly supervised object detection using heatmap-guided proposal selection, but does not introduce a new foundation model or a major new application.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.08694": {
        "authors": [
            "Zhen Tian",
            "Christos Anagnostopoulos",
            "Qiyuan Wang",
            "Zhiwei Gao"
        ],
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework",
        "abstract": "arXiv:2509.08694v1 Announce Type: new  Abstract: Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "arxiv_id": "2509.08694",
        "ARXIVID": "2509.08694",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications) as it proposes a new robust U-Net framework for coastal water segmentation using multi-modal constraints and HSV color space, but does not introduce a new foundation model or a major new application area.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.08583": {
        "authors": [
            "Jinhan Li",
            "Haoyang He",
            "Lei Xie",
            "Jiangning Zhang"
        ],
        "title": "EfficientIML: Efficient High-Resolution Image Manipulation Localization",
        "abstract": "arXiv:2509.08583v1 Announce Type: new  Abstract: With imaging devices delivering ever-higher resolutions and the emerging diffusion-based forgery methods, current detectors trained only on traditional datasets (with splicing, copy-moving and object removal forgeries) lack exposure to this new manipulation type. To address this, we propose a novel high-resolution SIF dataset of 1200+ diffusion-generated manipulations with semantically extracted masks. However, this also imposes a challenge on existing methods, as they face significant computational resource constraints due to their prohibitive computational complexities. Therefore, we propose a novel EfficientIML model with a lightweight, three-stage EfficientRWKV backbone. EfficientRWKV's hybrid state-space and attention network captures global context and local details in parallel, while a multi-scale supervision strategy enforces consistency across hierarchical predictions. Extensive evaluations on our dataset and standard benchmarks demonstrate that our approach outperforms ViT-based and other SOTA lightweight baselines in localization performance, FLOPs and inference speed, underscoring its suitability for real-time forensic applications.",
        "arxiv_id": "2509.08583",
        "ARXIVID": "2509.08583",
        "COMMENT": "Does not match any specific criterion. Focuses on image manipulation localization and efficient architectures, which is generally relevant to computer vision but not to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08151": {
        "authors": [
            "Botao Zhu",
            "Jeslyn Wang",
            "Dusit Niyato",
            "Xianbin Wang"
        ],
        "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI",
        "abstract": "arXiv:2509.08151v1 Announce Type: new  Abstract: Accurate trustworthiness evaluation of potential collaborating devices is essential for the effective execution of complex computing tasks. This evaluation process involves collecting diverse trust-related data from potential collaborators, including historical performance and available resources, for collaborator selection. However, when each task owner independently assesses all collaborators' trustworthiness, frequent data exchange, complex reasoning, and dynamic situation changes can result in significant overhead and deteriorated trust evaluation. To overcome these challenges, we propose a task-specific trust semantics distillation (2TSD) model based on a large AI model (LAM)-driven teacher-student agent architecture. The teacher agent is deployed on a server with powerful computational capabilities and an augmented memory module dedicated to multidimensional trust-related data collection, task-specific trust semantics extraction, and task-collaborator matching analysis. Upon receiving task-specific requests from device-side student agents, the teacher agent transfers the trust semantics of potential collaborators to the student agents, enabling rapid and accurate collaborator selection. Experimental results demonstrate that the proposed 2TSD model can reduce collaborator evaluation time, decrease device resource consumption, and improve the accuracy of collaborator selection.",
        "arxiv_id": "2509.08151",
        "ARXIVID": "2509.08151",
        "COMMENT": "Does not match any specific criterion. Focuses on trust semantics distillation for collaborator selection in agentic AI, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08580": {
        "authors": [
            "Mathilde Monvoisin",
            "Louise Piecuch",
            "Blanche Texier",
            "C\\'edric H\\'emon",
            "Ana\\\"is Barateau",
            "J\\'er\\'emie Huet",
            "Antoine Nordez",
            "Anne-Sophie Boureau",
            "Jean-Claude Nunes",
            "Diana Mateus"
        ],
        "title": "Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation",
        "abstract": "arXiv:2509.08580v1 Announce Type: new  Abstract: The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia.",
        "arxiv_id": "2509.08580",
        "ARXIVID": "2509.08580",
        "COMMENT": "Does not match any specific criterion. Focuses on medical image segmentation with implicit shape priors, not spatial intelligence for embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08392": {
        "authors": [
            "Cuong Nguyen",
            "Dung T. Tran",
            "Hong Nguyen",
            "Xuan-Vu Phan",
            "Nam-Phong Nguyen"
        ],
        "title": "VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring",
        "abstract": "arXiv:2509.08392v1 Announce Type: new  Abstract: In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\\%, reduces NMSE by around 50\\%, and enhances SSIM by 1\\%, while requiring only a marginal increase of roughly 1\\% in parameters.",
        "arxiv_id": "2509.08392",
        "ARXIVID": "2509.08392",
        "COMMENT": "Does not match any specific criterion. Focuses on autoencoder-based denoising and deblurring for license plates, which is a specific vision application.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.08205": {
        "authors": [
            "Jingjing Liu",
            "Yinchao Han",
            "Xianchao Xiu",
            "Jianhua Zhang",
            "Wanquan Liu"
        ],
        "title": "Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection",
        "abstract": "arXiv:2509.08205v1 Announce Type: new  Abstract: Infrared small target detection (ISTD) is one of the key techniques in image processing. Although deep unfolding networks (DUNs) have demonstrated promising performance in ISTD due to their model interpretability and data adaptability, existing methods still face significant challenges in parameter lightweightness and noise robustness. In this regard, we propose a highly lightweight framework based on robust principal component analysis (RPCA) called L-RPCANet. Technically, a hierarchical bottleneck structure is constructed to reduce and increase the channel dimension in the single-channel input infrared image to achieve channel-wise feature refinement, with bottleneck layers designed in each module to extract features. This reduces the number of channels in feature extraction and improves the lightweightness of network parameters. Furthermore, a noise reduction module is embedded to enhance the robustness against complex noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a channel attention mechanism to focus on the varying importance of different features across channels, thereby achieving excellent performance while maintaining both lightweightness and robustness. Extensive experiments on the ISTD datasets validate the superiority of our proposed method compared with state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code will be available at https://github.com/xianchaoxiu/L-RPCANet.",
        "arxiv_id": "2509.08205",
        "ARXIVID": "2509.08205",
        "COMMENT": "Does not match any specific criterion. Focuses on lightweight deep unfolding networks for infrared small target detection, which is a specific application in image processing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}