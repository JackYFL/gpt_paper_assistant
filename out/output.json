{
    "2510.03230": {
        "authors": [
            "Suyuchen Wang",
            "Tianyu Zhang",
            "Ahmed Masry",
            "Christopher Pal",
            "Spandana Gella",
            "Bang Liu",
            "Perouz Taslakian"
        ],
        "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
        "abstract": "arXiv:2510.03230v1 Announce Type: new  Abstract: GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.",
        "arxiv_id": "2510.03230",
        "ARXIVID": "2510.03230",
        "COMMENT": "Matches criterion 1 (new methodological improvement to spatial understanding for embodied agents via explicit position-to-coordinate mapping in GUI grounding). Also relevant to criterion 4 (vision foundation models applied to GUI automation).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.02898": {
        "authors": [
            "Lorenzo Bianchi",
            "Giacomo Pacini",
            "Fabio Carrara",
            "Nicola Messina",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
        "abstract": "arXiv:2510.02898v1 Announce Type: new  Abstract: Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present \\frameworkName{}, a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
        "arxiv_id": "2510.02898",
        "ARXIVID": "2510.02898",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in captioning via patch-centric paradigm) and criterion 4 (vision foundation models, e.g., DINO, and their application to region-based captioning). Also introduces a new trace captioning task, which is a novel angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.03198": {
        "authors": [
            "Junchao Huang",
            "Xinting Hu",
            "Boyao Han",
            "Shaoshuai Shi",
            "Zhuotao Tian",
            "Tianyu He",
            "Li Jiang"
        ],
        "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft",
        "abstract": "arXiv:2510.03198v1 Announce Type: new  Abstract: Autoregressive video diffusion models have proved effective for world modeling and interactive scene generation, with Minecraft gameplay as a representative application. To faithfully simulate play, a model must generate natural content while exploring new scenes and preserve spatial consistency when revisiting explored areas. Under limited computation budgets, it must compress and exploit historical cues within a finite context window, which exposes a trade-off: Temporal-only memory lacks long-term spatial consistency, whereas adding spatial memory strengthens consistency but may degrade new scene generation quality when the model over-relies on insufficient spatial context. We present Memory Forcing, a learning framework that pairs training protocols with a geometry-indexed spatial memory. Hybrid Training exposes distinct gameplay regimes, guiding the model to rely on temporal memory during exploration and incorporate spatial memory for revisits. Chained Forward Training extends autoregressive training with model rollouts, where chained predictions create larger pose variations and encourage reliance on spatial memory for maintaining consistency. Point-to-Frame Retrieval efficiently retrieves history by mapping currently visible points to their source frames, while Incremental 3D Reconstruction maintains and updates an explicit 3D cache. Extensive experiments demonstrate that Memory Forcing achieves superior long-term spatial consistency and generative quality across diverse environments, while maintaining computational efficiency for extended sequences.",
        "arxiv_id": "2510.03198",
        "ARXIVID": "2510.03198",
        "COMMENT": "Matches criterion 1 (spatial intelligence and spatial memory in embodied agents) and criterion 3 (new methods for embodied AI in simulators). Proposes Memory Forcing, a framework for spatio-temporal memory in Minecraft scene generation, with novel training protocols and memory mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.02912": {
        "authors": [
            "Xin Zou",
            "Di Lu",
            "Yizhou Wang",
            "Yibo Yan",
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual Holistic Context Retention",
        "abstract": "arXiv:2510.02912v1 Announce Type: new  Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs) suffer from considerable computational overhead due to their reliance on massive visual tokens. Recent studies have explored token pruning to alleviate this problem, which typically uses text-vision cross-attention or [\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this work, we identify a critical limitation of such attention-first pruning approaches, i.e., they tend to preserve semantically similar tokens, resulting in pronounced performance drops under high pruning ratios. To this end, we propose {HoloV}, a simple yet effective, plug-and-play visual token pruning framework for efficient inference. Distinct from previous attention-first schemes, HoloV rethinks token retention from a holistic perspective. By adaptively distributing the pruning budget across different spatial crops, HoloV ensures that the retained tokens capture the global visual context rather than isolated salient features. This strategy minimizes representational collapse and maintains task-relevant information even under aggressive pruning. Experimental results demonstrate that our HoloV achieves superior performance across various tasks, MLLM architectures, and pruning ratios compared to SOTA methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the original performance after pruning 88.9\\% of visual tokens, achieving superior efficiency-accuracy trade-offs.",
        "arxiv_id": "2510.02912",
        "ARXIVID": "2510.02912",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes HoloV, a new visual token pruning framework for MLLMs, with strong empirical results and a novel holistic approach to token retention.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.02750": {
        "authors": [
            "Lihua Zhou",
            "Mao Ye",
            "Shuaifeng Li",
            "Nianxin Li",
            "Jinlin Wu",
            "Xiatian Zhu",
            "Lei Deng",
            "Hongbin Liu",
            "Jiebo Luo",
            "Zhen Lei"
        ],
        "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models",
        "abstract": "arXiv:2510.02750v1 Announce Type: new  Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved remarkable success in object recognition and detection. However, their performance often degrades under real-world distribution shifts. Test-time adaptation (TTA) aims to mitigate this issue by adapting models during inference. Existing methods either rely on computationally expensive backpropagation, which hinders real-time deployment, or focus solely on likelihood adaptation, which overlooks the critical role of the prior. Our prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for object recognition by introducing a training-free framework that incorporates adaptive priors. Building upon this foundation, we now present Bayesian Class Adaptation plus (BCA+), a unified, training-free framework for TTA for both object recognition and detection. BCA+ introduces a dynamic cache that adaptively stores and updates class embeddings, spatial scales (for detection), and, crucially, adaptive class priors derived from historical predictions. We formulate adaptation as a Bayesian inference problem, where final predictions are generated by fusing the initial VLM output with a cache-based prediction. This cache-based prediction combines a dynamically updated likelihood (measuring feature and scale similarity) and a prior (reflecting the evolving class distribution). This dual-adaptation mechanism, coupled with uncertainty-guided fusion, enables BCA+ to correct both the model's semantic understanding and its contextual confidence. As a training-free method requiring no backpropagation, BCA+ is highly efficient. Extensive experiments demonstrate that BCA+ achieves state-of-the-art performance on both recognition and detection benchmarks.",
        "arxiv_id": "2510.02750",
        "ARXIVID": "2510.02750",
        "COMMENT": "Matches criterion 2 (VLLMs, e.g., CLIP, Grounding DINO) and criterion 4 (vision foundation models and their adaptation). Proposes a new Bayesian test-time adaptation method for object recognition and detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.03117": {
        "authors": [
            "Kaisi Guan",
            "Xihua Wang",
            "Zhengfeng Lai",
            "Xin Cheng",
            "Peng Zhang",
            "XiaoJiang Liu",
            "Ruihua Song",
            "Meng Cao"
        ],
        "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction",
        "abstract": "arXiv:2510.03117v1 Announce Type: new  Abstract: This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.",
        "arxiv_id": "2510.03117",
        "ARXIVID": "2510.03117",
        "COMMENT": "Matches criterion 2 (multi-modal large language models for text-to-sounding-video generation) and criterion 4 (vision foundation models in multi-modal generation). Proposes new methods for cross-modal interaction and conditioning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02987": {
        "authors": [
            "Juntong Wang",
            "Huiyu Duan",
            "Jiarui Wang",
            "Ziheng Jia",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency",
        "abstract": "arXiv:2510.02987v1 Announce Type: new  Abstract: With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation. To address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.",
        "arxiv_id": "2510.02987",
        "ARXIVID": "2510.02987",
        "COMMENT": "Matches criterion 4 (vision foundation models and their evaluation) and partially criterion 2 (evaluates VLLMs for text-to-image alignment, introduces a new benchmark and metric for long-prompt T2I evaluation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02732": {
        "authors": [
            "Jianing Chen",
            "Zehao Li",
            "Yujun Cai",
            "Hao Jiang",
            "Shuqin Gao",
            "Honglong Zhao",
            "Tianlu Mao",
            "Yucheng Zhang"
        ],
        "title": "From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting",
        "abstract": "arXiv:2510.02732v1 Announce Type: new  Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the ambiguity inferring 3D motion from limited views and computational demands of modeling temporally varying scenes. While recent sparse control methods alleviate computation by reducing millions of Gaussians to thousands of control points, they suffer from a critical limitation: they allocate points purely by geometry, leading to static redundancy and dynamic insufficiency. We propose a motion-adaptive framework that aligns control density with motion complexity. Leveraging semantic and motion priors from vision foundation models, we establish patch-token-node correspondences and apply motion-adaptive compression to concentrate control points in dynamic regions while suppressing redundancy in static backgrounds. Our approach achieves flexible representational density adaptation through iterative voxelization and motion tendency scoring, directly addressing the fundamental mismatch between control point allocation and motion complexity. To capture temporal evolution, we introduce spline-based trajectory parameterization initialized by 2D tracklets, replacing MLP-based deformation fields to achieve smoother motion representation and more stable optimization. Extensive experiments demonstrate significant improvements in reconstruction quality and efficiency over existing state-of-the-art methods.",
        "arxiv_id": "2510.02732",
        "ARXIVID": "2510.02732",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 1 (spatial understanding in dynamic 3D reconstruction). Uses vision foundation models for semantic and motion priors in dynamic 3D Gaussian splatting, and proposes a novel motion-adaptive control framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02778": {
        "authors": [
            "Xian Zhang",
            "Zexi Wu",
            "Zinuo Li",
            "Hongming Xu",
            "Luqi Gong",
            "Farid Boussaid",
            "Naoufel Werghi",
            "Mohammed Bennamoun"
        ],
        "title": "AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding",
        "abstract": "arXiv:2510.02778v1 Announce Type: new  Abstract: Understanding long-form videos remains a significant challenge for vision--language models (VLMs) due to their extensive temporal length and high information density. Most current multimodal large language models (MLLMs) rely on uniform sampling, which often overlooks critical moments, leading to incorrect responses to queries. In parallel, many keyframe selection approaches impose rigid temporal spacing: once a frame is chosen, an exclusion window suppresses adjacent timestamps to reduce redundancy. While effective at limiting overlap, this strategy frequently misses short, fine-grained cues near important events. Other methods instead emphasize visual diversity but neglect query relevance. We propose AdaRD-Key, a training-free keyframe sampling module for query-driven long-form video understanding. AdaRD-Key maximizes a unified Relevance--Diversity Max-Volume (RD-MV) objective, combining a query-conditioned relevance score with a log-determinant diversity component to yield informative yet non-redundant frames. To handle broad queries with weak alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating mechanism; when the relevance distribution indicates weak alignment, the method seamlessly shifts into a diversity-only mode, enhancing coverage without additional supervision. Our pipeline is training-free, computationally efficient (running in real time on a single GPU), and compatible with existing VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and Video-MME demonstrate state-of-the-art performance, particularly on long-form videos. Code available at https://github.com/Xian867/AdaRD-Key.",
        "arxiv_id": "2510.02778",
        "ARXIVID": "2510.02778",
        "COMMENT": "Matches criterion 2 (MLLMs) and criterion 4 (vision foundation models and applications). Proposes AdaRD-Key, a training-free, plug-and-play keyframe sampling module for long-form video understanding, improving VLMs' performance on long videos.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02418": {
        "authors": [
            "Sagnik Anupam",
            "Davis Brown",
            "Shuo Li",
            "Eric Wong",
            "Hamed Hassani",
            "Osbert Bastani"
        ],
        "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
        "abstract": "arXiv:2510.02418v1 Announce Type: new  Abstract: LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about captcha resolution. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.",
        "arxiv_id": "2510.02418",
        "ARXIVID": "2510.02418",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI agents, specifically web navigation agents). Introduces BrowserArena, a live open-web evaluation platform for LLM agents, with a novel benchmarking methodology and analysis of agent failure modes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.03153": {
        "authors": [
            "Hima Jacob Leven Suprabha",
            "Laxmi Nag Laxminarayan Nagesh",
            "Ajith Nair",
            "Alvin Reuben Amal Selvaster",
            "Ayan Khan",
            "Raghuram Damarla",
            "Sanju Hannah Samuel",
            "Sreenithi Saravana Perumal",
            "Titouan Puech",
            "Venkataramireddy Marella",
            "Vishal Sonar",
            "Alessandro Suglia",
            "Oliver Lemon"
        ],
        "title": "Improving Cooperation in Collaborative Embodied AI",
        "abstract": "arXiv:2510.03153v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.",
        "arxiv_id": "2510.03153",
        "ARXIVID": "2510.03153",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for collaborative embodied agents). Explores prompt engineering and LLM integration for improving multi-agent collaboration in embodied AI, with empirical results and speech integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.02922": {
        "authors": [
            "Daphne Tsolissou",
            "Theofanis Ganitidis",
            "Konstantinos Mitsis",
            "Stergios CHristodoulidis",
            "Maria Vakalopoulou",
            "Konstantina Nikita"
        ],
        "title": "Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights",
        "abstract": "arXiv:2510.02922v1 Announce Type: new  Abstract: Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.",
        "arxiv_id": "2510.02922",
        "ARXIVID": "2510.02922",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Benchmarks and adapts large vision-language models for multimodal medical risk stratification, with empirical insights into their limitations and improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.03191": {
        "authors": [
            "Denis Zavadski",
            "Nikita Philip Tatsch",
            "Carsten Rother"
        ],
        "title": "Product-Quantised Image Representation for High-Quality Image Synthesis",
        "abstract": "arXiv:2510.03191v1 Announce Type: new  Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.",
        "arxiv_id": "2510.03191",
        "ARXIVID": "2510.03191",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces PQGAN, a product-quantised image autoencoder that improves over VQGAN, with novel findings on quantisation and integration with diffusion models for efficient image synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.03206": {
        "authors": [
            "Cai Zhou",
            "Chenxiao Yang",
            "Yi Hu",
            "Chenyu Wang",
            "Chubin Zhang",
            "Muhan Zhang",
            "Lester Mackey",
            "Tommi Jaakkola",
            "Stephen Bates",
            "Dinghuai Zhang"
        ],
        "title": "Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner",
        "abstract": "arXiv:2510.03206v1 Announce Type: new  Abstract: Diffusion language models, especially masked discrete diffusion models, have achieved great success recently. While there are some theoretical and primary empirical results showing the advantages of latent reasoning with looped transformers or continuous chain-of-thoughts, continuous diffusion models typically underperform their discrete counterparts. In this paper, we argue that diffusion language models do not necessarily need to be in the discrete space. In particular, we prove that continuous diffusion models have stronger expressivity than discrete diffusions and looped transformers. We attribute the contradiction between the theoretical expressiveness and empirical performance to their practical trainability: while continuous diffusion provides intermediate supervision that looped transformers lack, they introduce additional difficulty decoding tokens into the discrete token space from the continuous representation space. We therefore propose Coevolutionary Continuous Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process on the union of a continuous representation space and a discrete token space, leveraging a single model to simultaneously denoise in the joint space. By combining two modalities, CCDD is expressive with rich semantics in the latent space, as well as good trainability and sample quality with the help of explicit discrete tokens. We also propose effective architectures and advanced training/sampling techniques for CCDD, which reveals strong empirical performance in extensive language modeling experiments on real-world tasks.",
        "arxiv_id": "2510.03206",
        "ARXIVID": "2510.03206",
        "COMMENT": "Matches criterion 2 (new methodological improvements in diffusion language models with a multimodal joint process). The paper proposes a novel coevolutionary continuous-discrete diffusion process for language modeling, with advanced training and sampling techniques, and discusses latent reasoning in diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.02617": {
        "authors": [
            "Beijia Lu",
            "Ziyi Chen",
            "Jing Xiao",
            "Jun-Yan Zhu"
        ],
        "title": "Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation",
        "abstract": "arXiv:2510.02617v1 Announce Type: new  Abstract: Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.",
        "arxiv_id": "2510.02617",
        "ARXIVID": "2510.02617",
        "COMMENT": "Matches criterion 1 (methodological improvement for spatial understanding in embodied agents via input-aware sparse attention for co-speech video generation). Also relevant to criterion 4 (vision foundation models in generative modeling).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.02601": {
        "authors": [
            "Patrick Rim",
            "Kun He",
            "Kevin Harris",
            "Braden Copple",
            "Shangchen Han",
            "Sizhe An",
            "Ivan Shugurov",
            "Tomas Hodan",
            "He Wen",
            "Xu Xie"
        ],
        "title": "Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig",
        "abstract": "arXiv:2510.02601v1 Announce Type: new  Abstract: Accurate 3D tracking of hands and their interactions with the world in unconstrained settings remains a significant challenge for egocentric computer vision. With few exceptions, existing datasets are predominantly captured in controlled lab setups, limiting environmental diversity and model generalization. To address this, we introduce a novel marker-less multi-camera system designed to capture precise 3D hands and objects, which allows for nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a lightweight, back-mounted capture rig with eight exocentric cameras, and a user-worn Meta Quest 3 headset, which contributes two egocentric views. We design an ego-exo tracking pipeline to generate accurate 3D hand pose ground truth from this system, and rigorously evaluate its quality. By collecting an annotated dataset featuring synchronized multi-view images and precise 3D hand poses, we demonstrate the capability of our approach to significantly reduce the trade-off between environmental realism and 3D annotation accuracy.",
        "arxiv_id": "2510.02601",
        "ARXIVID": "2510.02601",
        "COMMENT": "Partially matches criterion 3 (embodied AI, new benchmark/dataset for 3D hand tracking in the wild). Introduces a new multi-camera rig and dataset for egocentric and exocentric 3D hand tracking, improving realism and annotation accuracy.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.02589": {
        "authors": [
            "Yunqi Huang",
            "Nishith Chennakeshava",
            "Alexis Carras",
            "Vladislav Neverov",
            "Wei Liu",
            "Aske Plaat",
            "Yingjie Fan"
        ],
        "title": "A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem",
        "abstract": "arXiv:2510.02589v1 Announce Type: new  Abstract: Container stowage planning (CSPP) is a critical component of maritime transportation and terminal operations, directly affecting supply chain efficiency. Owing to its complexity, CSPP has traditionally relied on human expertise. While reinforcement learning (RL) has recently been applied to CSPP, systematic benchmark comparisons across different algorithms remain limited. To address this gap, we develop a Gym environment that captures the fundamental features of CSPP and extend it to include crane scheduling in both multi-agent and single-agent formulations. Within this framework, we evaluate five RL algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying complexity. The results reveal distinct performance gaps with increasing complexity, underscoring the importance of algorithm choice and problem formulation for CSPP. Overall, this paper benchmarks multiple RL methods for CSPP while providing a reusable Gym environment with crane scheduling, thus offering a foundation for future research and practical deployment in maritime logistics.",
        "arxiv_id": "2510.02589",
        "ARXIVID": "2510.02589",
        "COMMENT": "Matches criterion 3 (embodied AI benchmark/simulator). Introduces a new Gym environment for container stowage planning and benchmarks RL algorithms, providing a reusable simulator for future research.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.02423": {
        "authors": [
            "Hang Wu",
            "Yujun Cai",
            "Haonan Ge",
            "Hongkai Chen",
            "Ming-Hsuan Yang",
            "Yiwei Wang"
        ],
        "title": "RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation",
        "abstract": "arXiv:2510.02423v1 Announce Type: new  Abstract: Cinematography understanding refers to the ability to recognize not only the visual content of a scene but also the cinematic techniques that shape narrative meaning. This capability is attracting increasing attention, as it enhances multimodal understanding in real-world applications and underpins coherent content creation in film and media. As the most comprehensive benchmark for this task, ShotBench spans a wide range of cinematic concepts and VQA-style evaluations, with ShotVL achieving state-of-the-art results on it. However, our analysis reveals that ambiguous option design in ShotBench and ShotVL's shortcomings in reasoning consistency and instruction adherence undermine evaluation reliability, limiting fair comparison and hindering future progress. To overcome these issues, we systematically refine ShotBench through consistent option restructuring, conduct the first critical analysis of ShotVL's reasoning behavior, and introduce an extended evaluation protocol that jointly assesses task accuracy and core model competencies. These efforts lead to RefineShot, a refined and expanded benchmark that enables more reliable assessment and fosters future advances in cinematography understanding.",
        "arxiv_id": "2510.02423",
        "ARXIVID": "2510.02423",
        "COMMENT": "Matches criterion 3 (new benchmark for vision-language models in cinematography understanding). Refines and expands the ShotBench benchmark, introduces new evaluation protocols, and analyzes reasoning behavior in VLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.02722": {
        "authors": [
            "Junyu Shi",
            "Yong Sun",
            "Zhiyuan Zhang",
            "Lijiang Liu",
            "Zhengjie Zhang",
            "Yuxin He",
            "Qiang Nie"
        ],
        "title": "MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context",
        "abstract": "arXiv:2510.02722v1 Announce Type: new  Abstract: Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\\% on HumanML3D and 34.6\\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at https://github.com/JunyuShi02/MoGIC",
        "arxiv_id": "2510.02722",
        "ARXIVID": "2510.02722",
        "COMMENT": "Matches criterion 2 (multi-modal large language models) and is relevant to generative modeling in multi-modal learning. Proposes MoGIC, a framework for motion generation that integrates intention modeling and visual priors, with a new benchmark and mixture-of-attention mechanism.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.03152": {
        "authors": [
            "Anantajit Subrahmanya",
            "Chandrakanth Gudavalli",
            "Connor Levenson",
            "Umang Garg",
            "B. S. Manjunath"
        ],
        "title": "ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories",
        "abstract": "arXiv:2510.03152v1 Announce Type: new  Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.",
        "arxiv_id": "2510.03152",
        "ARXIVID": "2510.03152",
        "COMMENT": "Partially matches criterion 1 (spatial understanding) by introducing a novel probabilistic topological model for simulating spatiotemporal trajectories, but not directly focused on embodied agents or vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.03122": {
        "authors": [
            "Shiyi Zhang",
            "Dong Liang",
            "Hairong Zheng",
            "Yihang Zhou"
        ],
        "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion",
        "abstract": "arXiv:2510.03122v1 Announce Type: new  Abstract: The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.",
        "arxiv_id": "2510.03122",
        "ARXIVID": "2510.03122",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). The paper uses CLIP-guided diffusion for reconstructing images from brain activity, leveraging hierarchical vision representations and foundation models for improved performance.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.03049": {
        "authors": [
            "Ruotong Liao",
            "Guowen Huang",
            "Qing Cheng",
            "Thomas Seidl",
            "Daniel Cremers",
            "Volker Tresp"
        ],
        "title": "When and Where do Events Switch in Multi-Event Video Generation?",
        "abstract": "arXiv:2510.03049v1 Announce Type: new  Abstract: Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.",
        "arxiv_id": "2510.03049",
        "ARXIVID": "2510.03049",
        "COMMENT": "Matches criterion 4 (vision foundation models and their applications) and is relevant to generative modeling in multi-modal learning. The paper introduces a new prompt suite for multi-event text-to-video generation and provides empirical insights into event transition control in T2V models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.02815": {
        "authors": [
            "Feng Yuan",
            "Yifan Gao",
            "Yuehua Ye",
            "Haoyue Li",
            "Xin Gao"
        ],
        "title": "Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis",
        "abstract": "arXiv:2510.02815v1 Announce Type: new  Abstract: Cross-modal medical image synthesis research focuses on reconstructing missing imaging modalities from available ones to support clinical diagnosis. Driven by clinical necessities for flexible modality reconstruction, we explore K to N medical generation, where three critical challenges emerge: How can we model the heterogeneous contributions of different modalities to various target tasks? How can we ensure fusion quality control to prevent degradation from noisy information? How can we maintain modality identity consistency in multi-output generation? Driven by these clinical necessities, and drawing inspiration from SAM2's sequential frame paradigm and clinicians' progressive workflow of incrementally adding and selectively integrating multi-modal information, we treat multi-modal medical data as sequential frames with quality-driven selection mechanisms. Our key idea is to \"learn\" adaptive weights for each modality-task pair and \"memorize\" beneficial fusion patterns through progressive enhancement. To achieve this, we design three collaborative modules: PreWeightNet for global contribution assessment, ThresholdNet for adaptive filtering, and EffiWeightNet for effective weight computation. Meanwhile, to maintain modality identity consistency, we propose the Causal Modality Identity Module (CMIM) that establishes causal constraints between generated images and target modality descriptions using vision-language modeling. Extensive experimental results demonstrate that our proposed Med-K2N outperforms state-of-the-art methods by significant margins on multiple benchmarks. Source code is available.",
        "arxiv_id": "2510.02815",
        "ARXIVID": "2510.02815",
        "COMMENT": "Partially matches criterion 4 (vision foundation models and applications) by proposing a new method for multi-modal medical image synthesis with vision-language modeling, but not directly about VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.03006": {
        "authors": [
            "Sara Mobsite",
            "Renaud Hostache",
            "Laure Berti Equille",
            "Emmanuel Roux",
            "Joris Guerin"
        ],
        "title": "Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources",
        "abstract": "arXiv:2510.03006v1 Announce Type: new  Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on labeled satellite data. However, most existing Sentinel-2 datasets are cloud-free, which limits their usefulness in tropical regions where clouds are common. To properly evaluate the extent of this problem, we developed a cloud injection algorithm that simulates realistic cloud cover, allowing us to test how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed optical imagery. We also tackle the issue of losing spatial and/or spectral details during encoder downsampling in deep networks. To mitigate this loss, we propose a lightweight method that injects Normalized Difference Indices (NDIs) into the final decoding layers, enabling the model to retain key spatial features with minimal additional computation. Injecting NDIs enhanced land cover segmentation performance on the DFC2020 dataset, yielding improvements of 1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under cloud-covered conditions, incorporating Sentinel-1 data led to significant performance gains across all models compared to using optical data alone, highlighting the effectiveness of radar-optical fusion in challenging atmospheric scenarios.",
        "arxiv_id": "2510.03006",
        "ARXIVID": "2510.03006",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications), as it discusses robustness in land cover segmentation and proposes a new method for cloud injection and radar-optical fusion. However, it is more application-focused and less about foundational model advances.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2510.03194": {
        "authors": [
            "Zichen Chen",
            "Jiefeng Chen",
            "Sercan \\\"O. Arik",
            "Misha Sra",
            "Tomas Pfister",
            "Jinsung Yoon"
        ],
        "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
        "abstract": "arXiv:2510.03194v1 Announce Type: new  Abstract: Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.",
        "arxiv_id": "2510.03194",
        "ARXIVID": "2510.03194",
        "COMMENT": "Does not directly match any criterion. Focuses on collaborative agentic systems for data visualization, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.03161": {
        "authors": [
            "Qing Huang",
            "Zhipei Xu",
            "Xuanyu Zhang",
            "Jian Zhang"
        ],
        "title": "UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization",
        "abstract": "arXiv:2510.03161v1 Announce Type: new  Abstract: With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.",
        "arxiv_id": "2510.03161",
        "ARXIVID": "2510.03161",
        "COMMENT": "Does not directly match any of the criteria. Focuses on forgery image detection and localization using a multi-agent framework, but not on spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}