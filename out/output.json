{
    "2509.10441": {
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "abstract": "arXiv:2509.10441v1 Announce Type: new  Abstract: Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
        "arxiv_id": "2509.10441",
        "ARXIVID": "2509.10441",
        "COMMENT": "Matches criterion 4: This paper proposes InfGen, a new paradigm for scalable, resolution-agnostic image synthesis using diffusion models, enabling arbitrary resolution image generation with reduced computational cost. It is a significant methodological improvement for vision foundation models and generative modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.10312": {
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "abstract": "arXiv:2509.10312v1 Announce Type: new  Abstract: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.",
        "arxiv_id": "2509.10312",
        "ARXIVID": "2509.10312",
        "COMMENT": "Matches criterion 4: This paper introduces a novel acceleration method for diffusion transformers (a type of vision foundation model) using cluster-driven feature caching, which is applicable to any diffusion transformer and improves efficiency for image and video generation. This is a clever statistical trick for vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.10408": {
        "authors": [
            "Iacopo Curti",
            "Pierluigi Zama Ramirez",
            "Alioscia Petrelli",
            "Luigi Di Stefano"
        ],
        "title": "Multimodal SAM-adapter for Semantic Segmentation",
        "abstract": "arXiv:2509.10408v1 Announce Type: new  Abstract: Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.",
        "arxiv_id": "2509.10408",
        "ARXIVID": "2509.10408",
        "COMMENT": "Matches criterion 4: This paper extends the Segment Anything Model (SAM), a vision foundation model, for multimodal semantic segmentation by fusing RGB and auxiliary modalities. It proposes a novel adapter network for multimodal adaptation and demonstrates state-of-the-art results, making it highly relevant for vision foundation model applications.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.09958": {
        "authors": [
            "Jeffrey Liu",
            "Rongbin Hu"
        ],
        "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification",
        "abstract": "arXiv:2509.09958v1 Announce Type: new  Abstract: Referring Expression Comprehension (REC) is usually addressed with task-trained grounding models. We show that a zero-shot workflow, without any REC-specific training, can achieve competitive or superior performance. Our approach reformulates REC as box-wise visual-language verification: given proposals from a COCO-clean generic detector (YOLO-World), a general-purpose VLM independently answers True/False queries for each region. This simple procedure reduces cross-box interference, supports abstention and multiple matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our method not only surpasses a zero-shot GroundingDINO baseline but also exceeds reported results for GroundingDINO trained on REC and GroundingDINO+CRG. Controlled studies with identical proposals confirm that verification significantly outperforms selection-based prompting, and results hold with open VLMs. Overall, we show that workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance.",
        "arxiv_id": "2509.09958",
        "ARXIVID": "2509.09958",
        "COMMENT": "This paper proposes a zero-shot referring expression comprehension method using visual-language models for true/false verification, outperforming task-trained models. It matches criterion 2 (shows new VLLMs/MLLMs) and is highly relevant for vision-language modeling and surprising empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.09730": {
        "authors": [
            "Kaikai Zhao",
            "Zhaoxiang Liu",
            "Peng Wang",
            "Xin Wang",
            "Zhicheng Ma",
            "Yajun Xu",
            "Wenjing Zhang",
            "Yibing Nan",
            "Kai Wang",
            "Shiguo Lian"
        ],
        "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance",
        "abstract": "arXiv:2509.09730v1 Announce Type: new  Abstract: General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs, addressing five critical ITS tasks: object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source, providing high-value resources to advance both ITS and LMM research.",
        "arxiv_id": "2509.09730",
        "ARXIVID": "2509.09730",
        "COMMENT": "This paper introduces MITS, a large-scale multimodal benchmark for intelligent traffic surveillance, and demonstrates its impact on LMMs. It matches criterion 3 (new embodied AI benchmarks with a novel domain) and criterion 2 (shows new VLLMs/MLLMs in a specific application). Highly relevant for vision-language and multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.09737": {
        "authors": [
            "Klemen Kotar",
            "Wanhee Lee",
            "Rahul Venkatesh",
            "Honglin Chen",
            "Daniel Bear",
            "Jared Watrous",
            "Simon Kim",
            "Khai Loong Aw",
            "Lilian Naing Chen",
            "Stefan Stojanov",
            "Kevin Feigelis",
            "Imran Thobani",
            "Alex Durango",
            "Khaled Jedoui",
            "Atlas Kazemian",
            "Dan Yamins"
        ],
        "title": "World Modeling with Probabilistic Structure Integration",
        "abstract": "arXiv:2509.09737v1 Announce Type: new  Abstract: We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful \"intermediate structures\", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.",
        "arxiv_id": "2509.09737",
        "ARXIVID": "2509.09737",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces Probabilistic Structure Integration (PSI), a large-scale world model trained on internet video, capable of extracting and integrating diverse structures (optical flow, depth, segmentation) and supporting flexible prompting. This is a significant advance in world modeling and multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.10026": {
        "authors": [
            "Jing Huang",
            "Zhiya Tan",
            "Shutao Gong",
            "Fanwei Zeng",
            "Jianshu Li"
        ],
        "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA",
        "abstract": "arXiv:2509.10026v1 Announce Type: new  Abstract: As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A/B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: \\href{https://github.com/HJNVR/LaV-CoT}",
        "arxiv_id": "2509.10026",
        "ARXIVID": "2509.10026",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes LaV-CoT, a new framework for multilingual VQA with interpretable visual chain-of-thought reasoning and multi-aspect reward optimization, outperforming strong baselines and proprietary models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.10005": {
        "authors": [
            "Xiaodong Guo",
            "Tong Liu",
            "Yike Li",
            "Zi'ang Lin",
            "Zhihong Deng"
        ],
        "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion",
        "abstract": "arXiv:2509.10005v1 Announce Type: new  Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental perception of autonomous platforms in challenging conditions. Prevailing models employ encoders pre-trained on RGB images to extract features from both RGB and infrared inputs, and design additional modules to achieve cross-modal feature fusion. This results in limited thermal feature extraction and suboptimal cross-modal fusion, while the redundant encoders further compromises the model's real-time efficiency. To address the above issues, we propose TUNI, with an RGB-T encoder consisting of multiple stacked blocks that simultaneously perform multi-modal feature extraction and cross-modal fusion. By leveraging large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder learns to integrate feature extraction and fusion in a unified manner. By slimming down the thermal branch, the encoder achieves a more compact architecture. Moreover, we introduce an RGB-T local module to strengthen the encoder's capacity for cross-modal local feature fusion. The RGB-T local module employs adaptive cosine similarity to selectively emphasize salient consistent and distinct local features across RGB-T modalities. Experimental results show that TUNI achieves competitive performance with state-of-the-art models on FMB, PST900 and CART, with fewer parameters and lower computational cost. Meanwhile, it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its real-time capability in deployment. Codes are available at https://github.com/xiaodonguo/TUNI.",
        "arxiv_id": "2509.10005",
        "ARXIVID": "2509.10005",
        "COMMENT": "Matches criterion 4: This paper proposes TUNI, a real-time RGB-T semantic segmentation model with unified multi-modal feature extraction and fusion, leveraging large-scale pre-training. It is relevant to vision foundation models and their application to multi-modal perception.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10423": {
        "authors": [
            "Cameron Reid",
            "Wael Hafez",
            "Amirhossein Nazeri"
        ],
        "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
        "abstract": "arXiv:2509.10423v1 Announce Type: new  Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face degradation from sensor faults, actuator wear, and environmental shifts, yet lack intrinsic mechanisms to detect and diagnose these failures. We present an information-theoretic framework that reveals both the fundamental dynamics of RL and provides practical methods for diagnosing deployment-time anomalies. Through analysis of state-action mutual information patterns in a robotic control task, we first demonstrate that successful learning exhibits characteristic information signatures: mutual information between states and actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing state entropy, indicating that agents develop increasingly selective attention to task-relevant patterns. Intriguingly, states, actions and next states joint mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during early learning before declining as the agent specializes suggesting a transition from broad exploration to efficient exploitation. More immediately actionable, we show that information metrics can differentially diagnose system failures: observation-space, i.e., states noise (sensor faults) produces broad collapses across all information channels with pronounced drops in state-action coupling, while action-space noise (actuator faults) selectively disrupts action-outcome predictability while preserving state-action relationships. This differential diagnostic capability demonstrated through controlled perturbation experiments enables precise fault localization without architectural modifications or performance degradation. By establishing information patterns as both signatures of learning and diagnostic for system health, we provide the foundation for adaptive RL systems capable of autonomous fault detection and policy adjustment based on information-theoretic principles.",
        "arxiv_id": "2509.10423",
        "ARXIVID": "2509.10423",
        "COMMENT": "This paper uses mutual information to analyze and diagnose RL agent policy coherence and system failures, providing actionable insights for embodied agents. It matches criterion 1 (methodological improvements to spatial understanding/spatial intelligence on embodied agents) and offers a novel information-theoretic diagnostic tool.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10140": {
        "authors": [
            "Yifan Chang",
            "Jie Qin",
            "Limeng Qiao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Lin Ma",
            "Xingang Wang"
        ],
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
        "abstract": "arXiv:2509.10140v1 Announce Type: new  Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.",
        "arxiv_id": "2509.10140",
        "ARXIVID": "2509.10140",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method (VQBridge) for scalable, stable vector-quantized networks with 100% codebook utilization, improving tokenizers for image generation and showing strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10260": {
        "authors": [
            "Jia Wang",
            "Jie Hu",
            "Xiaoqi Ma",
            "Hanghang Ma",
            "Yanbing Zeng",
            "Xiaoming Wei"
        ],
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation",
        "abstract": "arXiv:2509.10260v1 Announce Type: new  Abstract: Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.",
        "arxiv_id": "2509.10260",
        "ARXIVID": "2509.10260",
        "COMMENT": "Matches criterion 3 (new benchmark for T2I artifact assessment) and criterion 4 (vision foundation models and their evaluation). Introduces a large-scale dataset, a VLM-based assessor, and a new automated benchmark for T2I models, with surprising empirical findings about artifact prevalence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09732": {
        "authors": [
            "Sary Elmansoury",
            "Islam Mesabah",
            "Gerrit Gro{\\ss}mann",
            "Peter Neigel",
            "Raj Bhalwankar",
            "Daniel Kondermann",
            "Sebastian J. Vollmer"
        ],
        "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs",
        "abstract": "arXiv:2509.09732v1 Announce Type: new  Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but their performance on fine-grained tasks and large hierarchical label spaces is understudied. This paper investigates whether structured, tree-based reasoning can enhance VLM performance. We introduce a framework that decomposes classification into interpretable decisions using decision trees and evaluates it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the model achieves 98.2% accuracy in understanding the tree knowledge, tree-based reasoning consistently underperforms standard zero-shot prompting. We also explore enhancing the tree prompts with LLM-generated classes and image descriptions to improve alignment. The added description enhances the performance of the tree-based and zero-shot methods. Our findings highlight limitations of structured reasoning in visual classification and offer insights for designing more interpretable VLM systems.",
        "arxiv_id": "2509.09732",
        "ARXIVID": "2509.09732",
        "COMMENT": "Matches criterion 2: This paper investigates tree-based reasoning in vision-language models (VLMs), decomposing visual classification into interpretable decisions. It provides empirical insights into the limitations of structured reasoning in VLMs, which is of interest for understanding VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.10105": {
        "authors": [
            "Young-rok Cha",
            "Jeongho Ju",
            "SunYoung Park",
            "Jong-Hyeon Lee",
            "Younghyun Yu",
            "Youngjune Kim"
        ],
        "title": "VARCO-VISION-2.0 Technical Report",
        "abstract": "arXiv:2509.10105v1 Announce Type: new  Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model (VLM) for Korean and English with improved capabilities compared to the previous model VARCO-VISION-14B. The model supports multi-image understanding for complex inputs such as documents, charts, and tables, and delivers layoutaware OCR by predicting both textual content and its spatial location. Trained with a four-stage curriculum with memory-efficient techniques, the model achieves enhanced multimodal alignment, while preserving core language abilities and improving safety via preference optimization. Extensive benchmark evaluations demonstrate strong spatial grounding and competitive results for both languages, with the 14B model achieving 8th place on the OpenCompass VLM leaderboard among models of comparable scale. Alongside the 14B-scale model, we release a 1.7B version optimized for on-device deployment. We believe these models advance the development of bilingual VLMs and their practical applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a full-scale 14B model and a lightweight 1.7B model.",
        "arxiv_id": "2509.10105",
        "ARXIVID": "2509.10105",
        "COMMENT": "Matches criterion 2 (new VLLM: bilingual vision-language model with spatial grounding and layout-aware OCR). Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.10058": {
        "authors": [
            "Sung-Lin Tsai",
            "Bo-Lun Huang",
            "Yu Ting Shen",
            "Cheng Yu Yeo",
            "Chiang Tseng",
            "Bo-Kai Ruan",
            "Wen-Sheng Lien",
            "Hong-Han Shuai"
        ],
        "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation",
        "abstract": "arXiv:2509.10058v1 Announce Type: new  Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.",
        "arxiv_id": "2509.10058",
        "ARXIVID": "2509.10058",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a training-free framework for improving color alignment in T2I diffusion models using LLMs and color space reasoning, with clever statistical tricks for prompt disambiguation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10344": {
        "authors": [
            "Yuexi Du",
            "Lihui Chen",
            "Nicha C. Dvornek"
        ],
        "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
        "abstract": "arXiv:2509.10344v1 Announce Type: new  Abstract: Mammography screening is an essential tool for early detection of breast cancer. The speed and accuracy of mammography interpretation have the potential to be improved with deep learning methods. However, the development of a foundation visual language model (VLM) is hindered by limited data and domain differences between natural and medical images. Existing mammography VLMs, adapted from natural images, often ignore domain-specific characteristics, such as multi-view relationships in mammography. Unlike radiologists who analyze both views together to process ipsilateral correspondence, current methods treat them as independent images or do not properly model the multi-view correspondence learning, losing critical geometric context and resulting in suboptimal prediction. We propose GLAM: Global and Local Alignment for Multi-view mammography for VLM pretraining using geometry guidance. By leveraging the prior knowledge about the multi-view imaging process of mammograms, our model learns local cross-view alignments and fine-grained local features through joint global and local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED [14], one of the largest open mammography datasets, our model outperforms baselines across multiple datasets under different settings.",
        "arxiv_id": "2509.10344",
        "ARXIVID": "2509.10344",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and partially criterion 1 (spatial understanding in multi-view mammography). Proposes a geometry-guided VLM pretraining method that leverages multi-view relationships, which is a novel angle for medical imaging VLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10156": {
        "authors": [
            "Goker Erdogan",
            "Nikhil Parthasarathy",
            "Catalin Ionescu",
            "Drew Hudson",
            "Alexander Lerchner",
            "Andrew Zisserman",
            "Mehdi Sajjadi",
            "Joao Carreira"
        ],
        "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing",
        "abstract": "arXiv:2509.10156v1 Announce Type: new  Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from \"representation collapse\". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.",
        "arxiv_id": "2509.10156",
        "ARXIVID": "2509.10156",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces LayerLock, a new self-supervised learning approach for visual foundation models, with progressive freezing and latent prediction, applied to large ViT models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10278": {
        "authors": [
            "Vidit Vidit",
            "Pavel Korshunov",
            "Amir Mohammadi",
            "Christophe Ecabert",
            "Ketan Kotwal",
            "S\\'ebastien Marcel"
        ],
        "title": "Detecting Text Manipulation in Images using Vision Language Models",
        "abstract": "arXiv:2509.10278v1 Announce Type: new  Abstract: Recent works have shown the effectiveness of Large Vision Language Models (VLMs or LVLMs) in image manipulation detection. However, text manipulation detection is largely missing in these studies. We bridge this knowledge gap by analyzing closed- and open-source VLMs on different text manipulation datasets. Our results suggest that open-source models are getting closer, but still behind closed-source ones like GPT- 4o. Additionally, we benchmark image manipulation detection-specific VLMs for text manipulation detection and show that they suffer from the generalization problem. We benchmark VLMs for manipulations done on in-the-wild scene texts and on fantasy ID cards, where the latter mimic a challenging real-world misuse.",
        "arxiv_id": "2509.10278",
        "ARXIVID": "2509.10278",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). The paper benchmarks VLMs for text manipulation detection, a novel application area, and compares open- and closed-source models, including GPT-4o.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.09946": {
        "authors": [
            "Vu-Minh Le",
            "Thao-Anh Tran",
            "Duc Huy Do",
            "Xuan Canh Do",
            "Huong Ninh",
            "Hai Tran"
        ],
        "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
        "abstract": "arXiv:2509.09946v1 Announce Type: new  Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision task for automating large-scale surveillance. With camera calibration and depth information, the targets in the scene can be projected into 3D space, offering unparalleled levels of automatic perception of a 3D environment. However, tracking in the 3D space requires replacing all 2D tracking components from the ground up, which may be infeasible for existing MTMC systems. In this paper, we present an approach for extending any online 2D multi-camera tracking system into 3D space by utilizing depth information to reconstruct a target in point-cloud space, and recovering its 3D box through clustering and yaw refinement following tracking. We also introduced an enhanced online data association mechanism that leverages the target's local ID consistency to assign global IDs across frames. The proposed framework is evaluated on the 2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the leaderboard.",
        "arxiv_id": "2509.09946",
        "ARXIVID": "2509.09946",
        "COMMENT": "Matches criterion 3: This paper presents a new method for online 3D multi-camera perception, extending 2D tracking systems into 3D using depth-based aggregation. It is relevant to embodied AI and spatial understanding, especially in the context of building new benchmarks or methods for 3D perception in simulators.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10122": {
        "authors": [
            "Zongliang Wu",
            "Siming Zheng",
            "Peng-Tao Jiang",
            "Xin Yuan"
        ],
        "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
        "abstract": "arXiv:2509.10122v1 Announce Type: new  Abstract: Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.",
        "arxiv_id": "2509.10122",
        "ARXIVID": "2509.10122",
        "COMMENT": "This paper introduces a new diffusion-based method for real-world image super-resolution with a visual prompt injection module, which is a novel application of vision foundation models. It matches criterion 4 (vision foundation models and applications) and may be of interest for its generative modeling and prompt-based control.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.09790": {
        "authors": [
            "Yuxuan Li",
            "Victor Zhong"
        ],
        "title": "How well can LLMs provide planning feedback in grounded environments?",
        "abstract": "arXiv:2509.09790v1 Announce Type: new  Abstract: Learning to plan in grounded environments typically requires carefully designed reward functions or high-quality annotated demonstrations. Recent works show that pretrained foundation models, such as large language models (LLMs) and vision language models (VLMs), capture background knowledge helpful for planning, which reduces the amount of reward design and demonstrations needed for policy learning. We evaluate how well LLMs and VLMs provide feedback across symbolic, language, and continuous control environments. We consider prominent types of feedback for planning including binary feedback, preference feedback, action advising, goal advising, and delta action feedback. We also consider inference methods that impact feedback performance, including in-context learning, chain-of-thought, and access to environment dynamics. We find that foundation models can provide diverse high-quality feedback across domains. Moreover, larger and reasoning models consistently provide more accurate feedback, exhibit less bias, and benefit more from enhanced inference methods. Finally, feedback quality degrades for environments with complex dynamics or continuous state spaces and action spaces.",
        "arxiv_id": "2509.09790",
        "ARXIVID": "2509.09790",
        "COMMENT": "Matches criterion 3 (embodied AI: evaluates LLM/VLM feedback for planning in grounded environments, with surprising empirical results about feedback quality and inference methods).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09859": {
        "authors": [
            "Razvan Stefanescu",
            "Ethan Oh",
            "Ruben Vazquez",
            "Chris Mesterharm",
            "Constantin Serban",
            "Ritu Chadha"
        ],
        "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector",
        "abstract": "arXiv:2509.09859v1 Announce Type: new  Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and acoustic signals for robust real-life UAV object detection. Our approach fuses visual and acoustic features in a unified object detector model relying on the Deformable DETR and Wav2Vec2 architectures, achieving strong performance under challenging environmental conditions. Our work leverage the existing Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more than 7,500 synchronized images and audio segments. We show how the acoustic information is used to improve the performance of the Deformable DETR object detector on the real ARDrone dataset. We developed, trained and tested four different fusion configurations based on a gated mechanism, linear layer, MLP and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi resolution feature mappings of the Deformable DETR and enhance the object detection performance over all drones dimensions. The best performer is the gated fusion approach, which improves the mAP of the Deformable DETR object detector on our in-distribution and out-of-distribution ARDrone datasets by 11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9. The mAP scores for medium and large drones are also enhanced, with overall gains across all drone sizes ranging from 3.27% to 5.84%.",
        "arxiv_id": "2509.09859",
        "ARXIVID": "2509.09859",
        "COMMENT": "Matches criterion 2 (new MLLM: multi-modal drone detector combining vision and audio) and is relevant to multi-modal learning. Proposes a new fusion approach and demonstrates strong empirical improvements.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10250": {
        "authors": [
            "Haozhen Yan",
            "Yan Hong",
            "Suning Lang",
            "Jiahui Zhan",
            "Yikun Ji",
            "Yujie Gao",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Jianfu Zhang"
        ],
        "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection",
        "abstract": "arXiv:2509.10250v1 Announce Type: new  Abstract: With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.",
        "arxiv_id": "2509.10250",
        "ARXIVID": "2509.10250",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new training framework (GAMMA) for generalizable AI-generated image detection, with multi-task supervision and manipulation-augmented training, and demonstrates robustness to new generative models like GPT-4o.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10241": {
        "authors": [
            "Elias De Smijter",
            "Renaud Detry",
            "Christophe De Vleeschouwer"
        ],
        "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints",
        "abstract": "arXiv:2509.10241v1 Announce Type: new  Abstract: We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
        "arxiv_id": "2509.10241",
        "ARXIVID": "2509.10241",
        "COMMENT": "This paper systematically compares implicit and explicit 3D reconstruction methods for space robotics, focusing on geometric accuracy and representation efficiency. While it is relevant to spatial understanding (criterion 1), it does not propose a new method for embodied agents or a new benchmark, but provides insightful empirical results.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10388": {
        "authors": [
            "Zeqing Leo Yuan",
            "Mani Ramanagopal",
            "Aswin C. Sankaranarayanan",
            "Srinivasa G. Narasimhan"
        ],
        "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition",
        "abstract": "arXiv:2509.10388v1 Announce Type: new  Abstract: Decomposing an image into its intrinsic photometric factors--shading and reflectance--is a long-standing challenge due to the lack of extensive ground-truth data for real-world scenes. Recent methods rely on synthetic data or sparse annotations for limited indoor and even fewer outdoor scenes. We introduce a novel training-free approach for intrinsic image decomposition using only a pair of visible and thermal images. We leverage the principle that light not reflected from an opaque surface is absorbed and detected as heat by a thermal camera. This allows us to relate the ordinalities between visible and thermal image intensities to the ordinalities of shading and reflectance, which can densely self-supervise an optimizing neural network to recover shading and reflectance. We perform quantitative evaluations with known reflectance and shading under natural and artificial lighting, and qualitative experiments across diverse outdoor scenes. The results demonstrate superior performance over recent learning-based models and point toward a scalable path to curating real-world ordinal supervision, previously infeasible via manual labeling.",
        "arxiv_id": "2509.10388",
        "ARXIVID": "2509.10388",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it proposes a training-free method for intrinsic image decomposition using visible and thermal images, leveraging physical principles for self-supervision. Not directly about foundation models, but a novel application of multi-modal vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09977": {
        "authors": [
            "Siying Liu",
            "Zikai Wang",
            "Hanle Zheng",
            "Yifan Hu",
            "Xilin Wang",
            "Qingkai Yang",
            "Jibin Wu",
            "Hao Guo",
            "Lei Deng"
        ],
        "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking",
        "abstract": "arXiv:2509.09977v1 Announce Type: new  Abstract: RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped with \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.",
        "arxiv_id": "2509.09977",
        "ARXIVID": "2509.09977",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models), as it proposes a hybrid ANN-SNN tracker for RGB-Event tracking, with a novel ISTA adapter for feature fusion. The focus is on tracking, not general spatial intelligence or embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10359": {
        "authors": [
            "Matteo Trippodo",
            "Federico Becattini",
            "Lorenzo Seidenari"
        ],
        "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
        "abstract": "arXiv:2509.10359v1 Announce Type: new  Abstract: Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.",
        "arxiv_id": "2509.10359",
        "ARXIVID": "2509.10359",
        "COMMENT": "This paper proposes a novel adversarial attack on text-to-image editing models, introducing new evaluation metrics for spatial layout disruption. It is related to spatial understanding in generative models, but focuses on adversarial robustness rather than embodied agents or spatial intelligence per se. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.10341": {
        "authors": [
            "Botond Fazekas",
            "Thomas Pinetz",
            "Guilherme Aresta",
            "Taha Emre",
            "Hrvoje Bogunovic"
        ],
        "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT",
        "abstract": "arXiv:2509.10341v1 Announce Type: new  Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.",
        "arxiv_id": "2509.10341",
        "ARXIVID": "2509.10341",
        "COMMENT": "This paper introduces a gamma-based diffusion model for denoising retinal OCT images. While it uses generative modeling (diffusion models), it is focused on a specific medical imaging application and does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.10006": {
        "authors": [
            "Masaki Akiba",
            "Shumpei Takezaki",
            "Daichi Haraguchi",
            "Seiichi Uchida"
        ],
        "title": "Few-Part-Shot Font Generation",
        "abstract": "arXiv:2509.10006v1 Announce Type: new  Abstract: This paper proposes a novel model of few-part-shot font generation, which designs an entire font based on a set of partial design elements, i.e., partial shapes. Unlike conventional few-shot font generation, which requires entire character shapes for a couple of character classes, our approach only needs partial shapes as input. The proposed model not only improves the efficiency of font creation but also provides insights into how partial design details influence the entire structure of the individual characters.",
        "arxiv_id": "2509.10006",
        "ARXIVID": "2509.10006",
        "COMMENT": "This paper proposes a few-part-shot font generation model, which is a generative vision model but not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. General interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09919": {
        "authors": [
            "Franklin Yiu",
            "Mohan Lu",
            "Nina Li",
            "Kevin Joseph",
            "Tianxu Zhang",
            "Julian Togelius",
            "Timothy Merino",
            "Sam Earle"
        ],
        "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments",
        "abstract": "arXiv:2509.09919v1 Announce Type: new  Abstract: Procedural content generation often requires satisfying both designer-specified objectives and adjacency constraints implicitly imposed by the underlying tile set. To address the challenges of jointly optimizing both constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a Markov Decision Process (MDP), enabling external optimization algorithms to focus exclusively on objective maximization while leveraging WFC's propagation mechanism to enforce constraint satisfaction. We empirically compare optimizing this MDP to traditional evolutionary approaches that jointly optimize global metrics and local tile placement. Across multiple domains with various difficulties, we find that joint optimization not only struggles as task complexity increases, but consistently underperforms relative to optimization over the WFC-MDP, underscoring the advantages of decoupling local constraint satisfaction from global objective optimization.",
        "arxiv_id": "2509.09919",
        "ARXIVID": "2509.09919",
        "COMMENT": "Somewhat related to criterion 3 (embodied AI, new benchmarks/methods) as it reformulates WaveFunctionCollapse for procedural environment generation using an MDP, but not directly about embodied agents or simulators. Interesting for generative modeling and procedural content generation.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.10054": {
        "authors": [
            "Hailong Yang",
            "Mingxian Gu",
            "Jianqi Wang",
            "Guanjin Wang",
            "Zhaohong Deng"
        ],
        "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph",
        "abstract": "arXiv:2509.10054v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans with complex, real-world tasks. However, MAS still face challenges in effective task planning when handling highly complex tasks with uncertainty, often resulting in misleading or incorrect outputs that hinder task execution. To address this, we propose XAgents, a unified multi-agent cooperative framework built on a multipolar task processing graph and IF-THEN rules. XAgents uses the multipolar task processing graph to enable dynamic task planning and handle task uncertainty. During subtask processing, it integrates domain-specific IF-THEN rules to constrain agent behaviors, while global rules enhance inter-agent collaboration. We evaluate the performance of XAgents across three distinct datasets, demonstrating that it consistently surpasses state-of-the-art single-agent and multi-agent approaches in both knowledge-typed and logic-typed question-answering tasks. The codes for XAgents are available at: https://github.com/AGI-FHBC/XAgents.",
        "arxiv_id": "2509.10054",
        "ARXIVID": "2509.10054",
        "COMMENT": "This paper proposes a multi-agent cooperation framework using LLMs and IF-THEN rules. While it is about multi-agent systems and LLMs, it does not focus on spatial intelligence, embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about logic and planning in multi-agent LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09935": {
        "authors": [
            "Chirayu Agrawal",
            "Snehasis Mukherjee"
        ],
        "title": "SCoDA: Self-supervised Continual Domain Adaptation",
        "abstract": "arXiv:2509.09935v1 Announce Type: new  Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a model to a target domain without access to the data of the source domain. Prevailing methods typically start with a source model pre-trained with full supervision and distill the knowledge by aligning instance-level features. However, these approaches, relying on cosine similarity over L2-normalized feature vectors, inadvertently discard crucial geometric information about the latent manifold of the source model. We introduce Self-supervised Continual Domain Adaptation (SCoDA) to address these limitations. We make two key departures from standard practice: first, we avoid the reliance on supervised pre-training by initializing the proposed framework with a teacher model pre-trained entirely via self-supervision (SSL). Second, we adapt the principle of geometric manifold alignment to the SFDA setting. The student is trained with a composite objective combining instance-level feature matching with a Space Similarity Loss. To combat catastrophic forgetting, the teacher's parameters are updated via an Exponential Moving Average (EMA) of the student's parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA significantly outperforms state-of-the-art SFDA methods.",
        "arxiv_id": "2509.09935",
        "ARXIVID": "2509.09935",
        "COMMENT": "This paper proposes a new self-supervised continual domain adaptation method, with a geometric manifold alignment approach. While it is relevant to spatial understanding and representation learning, it does not focus on embodied agents or spatial intelligence in embodied settings (criterion 1), nor does it introduce a new VLLM/MLLM (criterion 2), nor is it about embodied AI benchmarks or simulators (criterion 3), nor vision foundation models (criterion 4).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10407": {
        "authors": [
            "Xiem HoangVan",
            "Dang BuiDinh",
            "Sang NguyenQuang",
            "Wen-Hsiao Peng"
        ],
        "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards",
        "abstract": "arXiv:2509.10407v1 Announce Type: new  Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC. While deep learning based CVQE has driven significant progress, existing surveys still suffer from limitations: lack of systematic classification linking methods to specific standards and artifacts, insufficient comparative analysis of architectural paradigms across coding types, and underdeveloped benchmarking practices. To address these gaps, this paper presents three key contributions. First, it introduces a novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization. Second, it proposes a unified benchmarking framework integrating modern compression protocols and standard test sequences for fair multi-criteria evaluation. Third, it provides a systematic analysis of the critical trade-offs between reconstruction performance and computational complexity observed in state-of-the-art methods and highlighting promising directions for future research. This comprehensive review aims to establish a foundation for consistent assessment and informed model selection in CVQE research and deployment.",
        "arxiv_id": "2509.10407",
        "ARXIVID": "2509.10407",
        "COMMENT": "This is a survey and benchmarking paper on compressed video quality enhancement, with a new taxonomy and benchmarking framework. While it is relevant to vision, it does not match the specific criteria (spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10018": {
        "authors": [
            "Hailong Yang",
            "Renhuo Zhao",
            "Guanjin Wang",
            "Zhaohong Deng"
        ],
        "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method",
        "abstract": "arXiv:2509.10018v1 Announce Type: new  Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents exhibit exceptional abilities in understanding and generating natural language, facilitating human-like collaboration and information transmission in LLM-based Multi-Agent System (MAS). High-performance LLMs are often hosted on remote servers in public spaces. When tasks involve privacy data, MAS cannot securely utilize these LLMs without implementing privacy-preserving mechanisms. To address this challenge, we propose a General Anonymizing Multi-Agent system (GAMA), which divides the agents' workspace into private and public spaces and protects privacy through the anonymizing mechanism. In the private space, agents handle sensitive data, while in the public space, only anonymized data is utilized. GAMA incorporates two key modules to mitigate semantic loss caused by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The results demonstrate that GAMA has superior performance compared to the state-of-the-art models. To further assess its privacy-preserving capabilities, we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy Preservation. The final results highlight GAMA's exceptional effectiveness in both task processing and privacy preservation.",
        "arxiv_id": "2509.10018",
        "ARXIVID": "2509.10018",
        "COMMENT": "This paper presents a privacy-preserving multi-agent system using LLMs, with domain-rule and logic-based enhancements. It is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. General interest in multi-agent systems and privacy, but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09785": {
        "authors": [
            "Moslem Yazdanpanah",
            "Ali Bahri",
            "Mehrdad Noori",
            "Sahar Dastani",
            "Gustavo Adolfo Vargas Hakim",
            "David Osowiechi",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging",
        "abstract": "arXiv:2509.09785v1 Announce Type: new  Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation caused by distribution shifts in 3D point cloud classification. In this work, we introduce Token Purging (PG), a novel backpropagation-free approach that removes tokens highly affected by domain shifts before they reach attention layers. Unlike existing TTA methods, PG operates at the token level, ensuring robust adaptation without iterative updates. We propose two variants: PG-SP, which leverages source statistics, and PG-SF, a fully source-free version relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of +10.3\\% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is 12.4 times faster and 5.5 times more memory efficient than our baseline, making it suitable for real-world deployment. Code is available at \\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}",
        "arxiv_id": "2509.09785",
        "ARXIVID": "2509.09785",
        "COMMENT": "Not directly matching any criterion, but relevant to general interest in clever statistical tricks and adaptation. Proposes a backpropagation-free test-time adaptation method for point cloud classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09722": {
        "authors": [
            "Taylor Archibald",
            "Tony Martinez"
        ],
        "title": "Improving MLLM Historical Record Extraction with Test-Time Image",
        "abstract": "arXiv:2509.09722v1 Announce Type: new  Abstract: We present a novel ensemble framework that stabilizes LLM based text extraction from noisy historical documents. We transcribe multiple augmented variants of each image with Gemini 2.0 Flash and fuse these outputs with a custom Needleman Wunsch style aligner that yields both a consensus transcription and a confidence score. We present a new dataset of 622 Pennsylvania death records, and demonstrate our method improves transcription accuracy by 4 percentage points relative to a single shot baseline. We find that padding and blurring are the most useful for improving accuracy, while grid warp perturbations are best for separating high and low confidence cases. The approach is simple, scalable, and immediately deployable to other document collections and transcription models.",
        "arxiv_id": "2509.09722",
        "ARXIVID": "2509.09722",
        "COMMENT": "Somewhat related to criterion 2 (MLLMs) as it uses Gemini 2.0 Flash for text extraction from images, but the focus is on an ensemble framework for historical document transcription, not on new VLLMs/MLLMs or their core methodology.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.10114": {
        "authors": [
            "MohammadAli Hamidi",
            "Hadi Amirpour",
            "Luigi Atzori",
            "Christian Timmerer"
        ],
        "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss",
        "abstract": "arXiv:2509.10114v1 Announce Type: new  Abstract: Face image quality assessment (FIQA) plays a critical role in face recognition and verification systems, especially in uncontrolled, real-world environments. Although several methods have been proposed, general-purpose no-reference image quality assessment techniques often fail to capture face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be computationally intensive, limiting their practical applicability. We propose a lightweight and efficient method for FIQA, designed for the perceptual evaluation of face images in the wild. Our approach integrates an ensemble of two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2, with prediction-level fusion via simple averaging. To enhance alignment with human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss), combining mean squared error (MSE) with a Pearson correlation regularizer. Our method achieves a strong balance between accuracy and computational cost, making it suitable for real-world deployment. Experiments on the VQualA FIQA benchmark demonstrate that our model achieves a Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894, remaining within competition efficiency constraints.",
        "arxiv_id": "2509.10114",
        "ARXIVID": "2509.10114",
        "COMMENT": "This paper proposes a lightweight ensemble for face image quality assessment with a correlation-aware loss. While it is a vision paper, it does not match any of the specific criteria (spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}