{
    "2508.16201": {
        "authors": [
            "Yicheng Ji",
            "Jun Zhang",
            "Heming Xia",
            "Jinpeng Chen",
            "Lidan Shou",
            "Gang Chen",
            "Huan Li"
        ],
        "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
        "abstract": "arXiv:2508.16201v1 Announce Type: new  Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B.",
        "arxiv_id": "2508.16201",
        "ARXIVID": "2508.16201",
        "COMMENT": "This paper introduces SpecVLM, a speculative decoding framework for video LLMs (Vid-LLMs), with a novel token pruning strategy for efficient video understanding. It directly matches criterion 2 (new VLLMs/MLLMs) and is highly relevant for multi-modal generative modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.16463": {
        "authors": [
            "Aniello Panariello",
            "Emanuele Frascaroli",
            "Pietro Buzzega",
            "Lorenzo Bonicelli",
            "Angelo Porrello",
            "Simone Calderara"
        ],
        "title": "Modular Embedding Recomposition for Incremental Learning",
        "abstract": "arXiv:2508.16463v1 Announce Type: new  Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.",
        "arxiv_id": "2508.16463",
        "ARXIVID": "2508.16463",
        "COMMENT": "Matches criterion 2: Proposes a new method to enhance zero-shot capabilities of Vision-Language Models (VLMs) via modular embedding recomposition, with extensive evaluation on incremental learning protocols.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.16030": {
        "authors": [
            "Jinyue Song",
            "Hansol Ku",
            "Jayneel Vora",
            "Nelson Lee",
            "Ahmad Kamari",
            "Prasant Mohapatra",
            "Parth Pathak"
        ],
        "title": "CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars",
        "abstract": "arXiv:2508.16030v1 Announce Type: new  Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse, noisy point clouds constrain 3-D object detection. We therefore release CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and GPS streams from multiple vehicles across diverse manoeuvres. Built on this data, we propose a unified cooperative-perception framework with middle- and late-fusion options. Its baseline network employs a multi-branch PointNet-style encoder enhanced with self-attention to fuse spatial, Doppler, and intensity cues into a common latent space, which a decoder converts into 3-D bounding boxes and per-point depth confidence. Experiments show that middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the first reproducible benchmark for multi-vehicle FMCW-radar perception and demonstrates that affordable radar sharing markedly improves detection robustness. Dataset and code are publicly available to encourage further research.",
        "arxiv_id": "2508.16030",
        "ARXIVID": "2508.16030",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI/simulators). Introduces CoVeRaP, a new multi-vehicle radar-camera-GPS dataset and a cooperative perception framework for robust 3D object detection, establishing a reproducible benchmark for multi-vehicle radar perception.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.16239": {
        "authors": [
            "Nan wang",
            "Zhiyi Xia",
            "Yiming Li",
            "Shi Tang",
            "Zuxin Fan",
            "Xi Fang",
            "Haoyi Tao",
            "Xiaochen Cai",
            "Guolin Ke",
            "Linfeng Zhang",
            "Yanhui Hong"
        ],
        "title": "UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation",
        "abstract": "arXiv:2508.16239v1 Announce Type: new  Abstract: Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.",
        "arxiv_id": "2508.16239",
        "ARXIVID": "2508.16239",
        "COMMENT": "This paper introduces UniEM-3M, a large-scale multimodal electron micrograph dataset, a new generative model, and a benchmark for microstructural segmentation and generation. It matches criterion 4 (vision foundation models and applications) and criterion 3 (new benchmark for a novel domain).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.16069": {
        "authors": [
            "Qifeng Liu",
            "Dawei Zhao",
            "Yabo Dong",
            "Linzhi Shang",
            "Liang Xiao",
            "Juan Wang",
            "Kunkong Zhao",
            "Dongming Lu",
            "Qi Zhu"
        ],
        "title": "A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection",
        "abstract": "arXiv:2508.16069v1 Announce Type: new  Abstract: Recent advances in point cloud object detection have increasingly adopted Transformer-based and State Space Models (SSMs), demonstrating strong performance. However, voxelbased representations in these models require strict consistency in input and output dimensions due to their serialized processing, which limits the spatial diffusion capability typically offered by convolutional operations. This limitation significantly affects detection accuracy. Inspired by CNN-based object detection architectures, we propose a novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and diffusion in point cloud data. VDM is composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. To ensure computational efficiency, the output feature maps are downsampled to one-fourth of the original input resolution. VDM serves two primary functions: (1) diffusing foreground voxel features through sparse 3D convolutions to enrich spatial context, and (2) aggregating fine-grained spatial information to strengthen voxelwise feature representation. The enhanced voxel features produced by VDM can be seamlessly integrated into mainstream Transformer- or SSM-based detection models for accurate object classification and localization, highlighting the generalizability of our method. We evaluate VDM on several benchmark datasets by embedding it into both Transformerbased and SSM-based models. Experimental results show that our approach consistently improves detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new stateof-the-art performance across all datasets. Our code will be made publicly available.",
        "arxiv_id": "2508.16069",
        "ARXIVID": "2508.16069",
        "COMMENT": "This paper introduces a Voxel Diffusion Module for point cloud 3D object detection, improving spatial diffusion and representation. It is a methodological improvement for spatial understanding in embodied agents (criterion 1), and is highly relevant for embodied AI research.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.16158": {
        "authors": [
            "Haodong He",
            "Yancheng Bai",
            "Rui Lan",
            "Xu Duan",
            "Lei Sun",
            "Xiangxiang Chu",
            "Gui-Song Xia"
        ],
        "title": "RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution",
        "abstract": "arXiv:2508.16158v1 Announce Type: new  Abstract: The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.",
        "arxiv_id": "2508.16158",
        "ARXIVID": "2508.16158",
        "COMMENT": "RAGSR proposes a regional attention mechanism for super-resolution using vision-language models and text-to-image diffusion models. It is a methodological improvement in spatial understanding (criterion 1) and leverages VLMs (criterion 2), with a novel approach to fine-grained regional control.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.15904": {
        "authors": [
            "Dexuan He",
            "Xiao Zhou",
            "Wenbin Guan",
            "Liyuan Zhang",
            "Xiaoman Zhang",
            "Sinuo Xu",
            "Ge Wang",
            "Lifeng Wang",
            "Xiaojun Yuan",
            "Xin Sun",
            "Yanfeng Wang",
            "Kun Sun",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "title": "Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping",
        "abstract": "arXiv:2508.15904v1 Announce Type: new  Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic challenges due to limited expert availability-especially in pediatric oncology, where they represent over 70% of cases. While pathology vision-language (VL) foundation models show promising zero-shot capabilities for common cancer subtyping, their clinical performance for rare cancers remains limited. Existing multi-instance learning (MIL) methods rely only on visual features, overlooking cross-modal knowledge and compromising interpretability critical for rare cancer diagnosis. To address this limitation, we propose PathPT, a novel framework that fully exploits the potential of vision-language pathology foundation models through spatially-aware visual aggregation and task-specific prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision into fine-grained tile-level guidance by leveraging the zero-shot capabilities of VL models, thereby preserving localization on cancerous regions and enabling cross-modal reasoning through prompts aligned with histopathological semantics. We benchmark PathPT on eight rare cancer datasets(four adult and four pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer datasets, evaluating four state-of-the-art VL models and four MIL frameworks under three few-shot settings. Results show that PathPT consistently delivers superior performance, achieving substantial gains in subtyping accuracy and cancerous region grounding ability. This work advances AI-assisted diagnosis for rare cancers, offering a scalable solution for improving subtyping accuracy in settings with limited access to specialized expertise.",
        "arxiv_id": "2508.15904",
        "ARXIVID": "2508.15904",
        "COMMENT": "Matches criterion 4: Applies vision-language foundation models to pathology, with a novel prompt-tuning and spatial aggregation method for rare cancer subtyping. Also relevant to criterion 2 as it benchmarks multiple VL models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16512": {
        "authors": [
            "Chun-Peng Chang",
            "Chen-Yu Wang",
            "Julian Schmidt",
            "Holger Caesar",
            "Alain Pagani"
        ],
        "title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation",
        "abstract": "arXiv:2508.16512v1 Announce Type: new  Abstract: Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called \"world models\". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.",
        "arxiv_id": "2508.16512",
        "ARXIVID": "2508.16512",
        "COMMENT": "Matches criterion 3: Embodied AI for driving simulation, investigates trade-offs in video generation for driving simulators and proposes continual learning strategies to balance visual fidelity and spatial accuracy. Also relevant to criterion 1: spatial understanding in video generation for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16465": {
        "authors": [
            "Anilkumar Swamy",
            "Vincent Leroy",
            "Philippe Weinzaepfel",
            "Jean-S\\'ebastien Franco",
            "Gr\\'egory Rogez"
        ],
        "title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images",
        "abstract": "arXiv:2508.16465v1 Announce Type: new  Abstract: Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.",
        "arxiv_id": "2508.16465",
        "ARXIVID": "2508.16465",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding for embodied agents). Proposes a keypoint-free, object-agnostic hand-object 3D reconstruction method from monocular RGB, improving spatial understanding in human-robot interaction and AR/VR.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16271": {
        "authors": [
            "Yi Xu",
            "Yesheng Zhang",
            "jiajia Liu",
            "Jingdong Chen"
        ],
        "title": "Structuring GUI Elements through Vision Language Models: Towards Action Space Generation",
        "abstract": "arXiv:2508.16271v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction. In this paper we focus on the application of MLLMs in the field of graphical user interface (GUI) elements structuring, where they assist in processing user instructions based on screen contents. Despite the promise of MLLMs, their performance in precisely generating UI element coordinates, a critical aspect of GUI understanding, is hindered by the nature of next-token prediction training. This challenge arises from the semantic void surrounding numerical UI coordinates in language representation spaces, necessitating a substantial and diverse dataset to bolster visual module capabilities. To address these limitations, we introduce an IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our approach involves a novel pipeline for IoU-based coordinate sampling to augment the training data, which considers the proximity to ground truth coordinates. This data augmentation strategy is then employed to fine-tune MLLMs under the IAML paradigm, which is designed to mitigate the exposure bias problem inherent in traditional maximum likelihood estimation. Through extensive experiments, we demonstrate the superior performance of our IAML training approach over traditional training paradigms.",
        "arxiv_id": "2508.16271",
        "ARXIVID": "2508.16271",
        "COMMENT": "Matches criterion 2 (new MLLM methods for GUI element structuring and action space generation). Proposes a novel IoU-Augmented Maximum Likelihood (IAML) training paradigm for MLLMs to improve spatial understanding of UI elements, which is a methodological improvement in multi-modal models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16159": {
        "authors": [
            "Jiaqi Ma",
            "Guo-Sen Xie",
            "Fang Zhao",
            "Zechao Li"
        ],
        "title": "Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation",
        "abstract": "arXiv:2508.16159v1 Announce Type: new  Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\\% improvement on Pascal-5\\textsuperscript{i} and a 9.7\\% improvement on COCO-20\\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.",
        "arxiv_id": "2508.16159",
        "ARXIVID": "2508.16159",
        "COMMENT": "This paper proposes a novel network for weakly-supervised few-shot segmentation, introducing heterogeneous visual aggregation and transfer modules, and leveraging CLIP for multimodal generalization. It is a methodological improvement in spatial understanding (criterion 1) and uses vision-language models (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16230": {
        "authors": [
            "Jiahao Chen",
            "Zhiyong Ma",
            "Wenbiao Du",
            "Qingyuan Chuai"
        ],
        "title": "FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing",
        "abstract": "arXiv:2508.16230v1 Announce Type: new  Abstract: Multi-modal creative writing (MMCW) aims to produce illustrated articles. Unlike common multi-modal generative (MMG) tasks such as storytelling or caption generation, MMCW is an entirely new and more abstract challenge where textual and visual contexts are not strictly related to each other. Existing methods for related tasks can be forcibly migrated to this track, but they require specific modality inputs or costly training, and often suffer from semantic inconsistencies between modalities. Therefore, the main challenge lies in economically performing MMCW with flexible interactive patterns, where the semantics between the modalities of the output are more aligned. In this work, we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE promotes creativity and emphasizes the unification between modalities by proposing the modality semantic alignment gating (msaGate) to restrict the textual input. Besides, an attention-based cross-modality fusion is proposed to augment the input features for semantic enhancement. The modality semantic creative direct preference optimization (mscDPO) within FlexMUSE is designed by extending the rejected samples to facilitate the writing creativity. Moreover, to advance the MMCW, we expose a dataset called ArtMUSE which contains with around 3k calibrated text-image pairs. FlexMUSE achieves promising results, demonstrating its consistency, creativity and coherence.",
        "arxiv_id": "2508.16230",
        "ARXIVID": "2508.16230",
        "COMMENT": "FlexMUSE is a new multi-modal generative model for creative writing, with a focus on semantic alignment between modalities. It introduces a new dataset and several novel mechanisms for multi-modal generation, matching criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.15930": {
        "authors": [
            "Jiahao Li",
            "Jiancheng Pan",
            "Yuze Sun",
            "Xiaomeng Huang"
        ],
        "title": "Semantic-Aware Ship Detection with Vision-Language Integration",
        "abstract": "arXiv:2508.15930v1 Announce Type: new  Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging applications, such as maritime activity monitoring, shipping logistics, and environmental studies. However, existing methods often struggle to capture fine-grained semantic information, limiting their effectiveness in complex scenarios. To address these challenges, we propose a novel detection framework that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture fine-grained ship attributes. We evaluate our framework through three well-defined tasks, providing a comprehensive analysis of its performance and demonstrating its effectiveness in advancing SASD from multiple perspectives.",
        "arxiv_id": "2508.15930",
        "ARXIVID": "2508.15930",
        "COMMENT": "This paper proposes a novel vision-language model for semantic-aware ship detection in remote sensing imagery, introducing a new dataset and framework. It matches criterion 2 (new VLLMs/MLLMs) and is also relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.16272": {
        "authors": [
            "Yu Meng",
            "Ligao Deng",
            "Zhihao Xi",
            "Jiansheng Chen",
            "Jingbo Chen",
            "Anzhi Yue",
            "Diyou Liu",
            "Kai Li",
            "Chenhao Wang",
            "Kaiyu Li",
            "Yupeng Deng",
            "Xian Sun"
        ],
        "title": "IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization",
        "abstract": "arXiv:2508.16272v1 Announce Type: new  Abstract: With the enhancement of remote sensing image resolution and the rapid advancement of deep learning, land cover mapping is transitioning from pixel-level segmentation to object-based vector modeling. This shift demands more from deep learning models, requiring precise object boundaries and topological consistency. However, existing datasets face three main challenges: limited class annotations, small data scale, and lack of spatial structural information. To overcome these issues, we introduce IRSAMap, the first global remote sensing dataset for large-scale, high-resolution, multi-feature land cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive vector annotation system with over 1.8 million instances of 10 typical objects (e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an intelligent annotation workflow combining manual and AI-based methods to improve efficiency and consistency; 3) global coverage across 79 regions in six continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks like pixel-level classification, building outline extraction, road centerline extraction, and panoramic segmentation. IRSAMap provides a standardized benchmark for the shift from pixel-based to object-based approaches, advancing geographic feature automation and collaborative modeling. It is valuable for global geographic information updates and digital twin construction. The dataset is publicly available at https://github.com/ucas-dlg/IRSAMap",
        "arxiv_id": "2508.16272",
        "ARXIVID": "2508.16272",
        "COMMENT": "IRSAMap introduces a new large-scale, high-resolution remote sensing dataset for land cover vector mapping. This is a new benchmark for spatial understanding and could be relevant for embodied AI (criterion 3), especially for spatial intelligence and simulator-related research.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.16016": {
        "authors": [
            "Aykut Sirma",
            "Angelos Plastropoulos",
            "Argyrios Zolotas",
            "Gilbert Tang"
        ],
        "title": "DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions",
        "abstract": "arXiv:2508.16016v1 Announce Type: new  Abstract: Recent advancements in computer vision and deep learning have enhanced disaster-response capabilities, particularly in the rapid assessment of earthquake-affected urban environments. Timely identification of accessible entry points and structural obstacles is essential for effective search-and-rescue (SAR) operations. To address this need, we introduce DRespNeT, a high-resolution dataset specifically developed for aerial instance segmentation of post-earthquake structural environments. Unlike existing datasets, which rely heavily on satellite imagery or coarse semantic labeling, DRespNeT provides detailed polygon-level instance segmentation annotations derived from high-definition (1080p) aerial footage captured in disaster zones, including the 2023 Turkiye earthquake and other impacted regions. The dataset comprises 28 operationally critical classes, including structurally compromised buildings, access points such as doors, windows, and gaps, multiple debris levels, rescue personnel, vehicles, and civilian visibility. A distinctive feature of DRespNeT is its fine-grained annotation detail, enabling differentiation between accessible and obstructed areas, thereby improving operational planning and response efficiency. Performance evaluations using YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate significant gains in real-time situational awareness and decision-making. Our optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27 FPS on an RTX-4090 GPU for multi-target detection, meeting real-time operational requirements. The dataset and models support SAR teams and robotic systems, providing a foundation for enhancing human-robot collaboration, streamlining emergency response, and improving survivor outcomes.",
        "arxiv_id": "2508.16016",
        "ARXIVID": "2508.16016",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications) and partially to criterion 3 (new dataset and method for embodied AI in disaster response). Introduces a new UAV dataset and a YOLOv8-based model for aerial instance segmentation in SAR missions, with fine-grained spatial annotations.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.15960": {
        "authors": [
            "Zhenhao Guo",
            "Rachit Saluja",
            "Tianyuan Yao",
            "Quan Liu",
            "Yuankai Huo",
            "Benjamin Liechty",
            "David J. Pisapia",
            "Kenji Ikemura",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "title": "Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification",
        "abstract": "arXiv:2508.15960v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown considerable potential in digital pathology, yet their effectiveness remains limited for fine-grained, disease-specific classification tasks such as distinguishing between glomerular subtypes. The subtle morphological variations among these subtypes, combined with the difficulty of aligning visual patterns with precise clinical terminology, make automated diagnosis in renal pathology particularly challenging. In this work, we explore how large pretrained VLMs can be effectively adapted to perform fine-grained glomerular classification, even in scenarios where only a small number of labeled examples are available. In this work, we introduce Glo-VLMs, a systematic framework designed to explore the adaptation of VLMs to fine-grained glomerular classification in data-constrained settings. Our approach leverages curated pathology images alongside clinical text prompts to facilitate joint image-text representation learning for nuanced renal pathology subtypes. By assessing various VLMs architectures and adaptation strategies under a few-shot learning paradigm, we explore how both the choice of method and the amount of labeled data impact model performance in clinically relevant scenarios. To ensure a fair comparison, we evaluate all models using standardized multi-class metrics, aiming to clarify the practical requirements and potential of large pretrained models for specialized clinical research applications. As a result, fine-tuning the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with only 8 shots per class, demonstrating that even with highly limited supervision, foundation models can be effectively adapted for fine-grained medical image classification.",
        "arxiv_id": "2508.15960",
        "ARXIVID": "2508.15960",
        "COMMENT": "This paper explores adapting large pretrained vision-language models (VLMs) for fine-grained glomerular classification in pathology, especially in few-shot settings. It matches criterion 4 (vision foundation models and applications) and is relevant for empirical insights into VLM adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.15986": {
        "authors": [
            "Jerry Cao-Xue",
            "Tien Comlekoglu",
            "Keyi Xue",
            "Guanliang Wang",
            "Jiang Li",
            "Gordon Laurie"
        ],
        "title": "Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset",
        "abstract": "arXiv:2508.15986v1 Announce Type: new  Abstract: The development of multi-label deep learning models for retinal disease classification is often hindered by the scarcity of large, expertly annotated clinical datasets due to patient privacy concerns and high costs. The recent release of SynFundus-1M, a high-fidelity synthetic dataset with over one million fundus images, presents a novel opportunity to overcome these barriers. To establish a foundational performance benchmark for this new resource, we developed an end-to-end deep learning pipeline, training six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a meta-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a macro-average Area Under the Receiver Operating Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated strong generalization to three diverse, real-world clinical datasets, achieving an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset. This work provides a robust baseline for future research on large-scale synthetic datasets and establishes that models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images, offering a viable pathway to accelerate the development of comprehensive AI systems in ophthalmology.",
        "arxiv_id": "2508.15986",
        "ARXIVID": "2508.15986",
        "COMMENT": "This paper benchmarks modern architectures and a meta-ensemble for multi-label retinal disease classification on a large synthetic dataset, including the use of the RETFound foundation model. It is relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.16317": {
        "authors": [
            "Nedyalko Prisadnikov",
            "Danda Pani Paudel",
            "Yuqian Fu",
            "Luc Van Gool"
        ],
        "title": "Vision encoders should be image size agnostic and task driven",
        "abstract": "arXiv:2508.16317v1 Announce Type: new  Abstract: This position paper argues that the next generation of vision encoders should be image size agnostic and task driven. The source of our inspiration is biological. Not a structural aspect of biological vision, but a behavioral trait -- efficiency. We focus on a couple of ways in which vision in nature is efficient, but modern vision encoders not. We -- humans and animals -- deal with vast quantities of visual data, and need to be smart where we focus our limited energy -- it depends on the task. It is our belief that vision encoders should be dynamic and the computational complexity should depend on the task at hand rather than the size of the image. We, also, provide concrete first steps towards our vision -- a proof-of-concept solution for image classification. Despite classification being not very representative for what we are trying to achieve, it shows that our approach is feasible and promising.",
        "arxiv_id": "2508.16317",
        "ARXIVID": "2508.16317",
        "COMMENT": "This position paper argues for image size agnostic and task-driven vision encoders, with a proof-of-concept. It is a methodological improvement in vision foundation models (criterion 4), and could inspire new directions in spatial intelligence.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.15946": {
        "authors": [
            "Angela Zhu",
            "Christian Lange",
            "Max Hamilton"
        ],
        "title": "Investigating Different Geo Priors for Image Classification",
        "abstract": "arXiv:2508.15946v1 Announce Type: new  Abstract: Species distribution models encode spatial patterns of species occurrence making them effective priors for vision-based species classification when location information is available. In this study, we evaluate various SINR (Spatial Implicit Neural Representations) models as a geographical prior for visual classification of species from iNaturalist observations. We explore the impact of different model configurations and adjust how we handle predictions for species not included in Geo Prior training. Our analysis reveals factors that contribute to the effectiveness of these models as Geo Priors, factors that may differ from making accurate range maps.",
        "arxiv_id": "2508.15946",
        "ARXIVID": "2508.15946",
        "COMMENT": "Matches criterion 1: Investigates spatial priors (geo priors) for image classification, which is a methodological improvement for spatial understanding in vision models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2508.16397": {
        "authors": [
            "Yong Zhang",
            "Cunjian Chen",
            "Qiang Gao",
            "Yi Wang",
            "Bin Fang"
        ],
        "title": "A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection",
        "abstract": "arXiv:2508.16397v1 Announce Type: new  Abstract: Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.",
        "arxiv_id": "2508.16397",
        "ARXIVID": "2508.16397",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications). Proposes a lightweight, real-time network for steel surface defect detection with novel multiscale feature interaction modules.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16282": {
        "authors": [
            "Khai Duc Minh Tran",
            "Hoa Van Nguyen",
            "Aimuni Binti Muhammad Rawi",
            "Hareeshrao Athinarayanarao",
            "Ba-Ngu Vo"
        ],
        "title": "Robust Small Methane Plume Segmentation in Satellite Imagery",
        "abstract": "arXiv:2508.16282v1 Announce Type: new  Abstract: This paper tackles the challenging problem of detecting methane plumes, a potent greenhouse gas, using Sentinel-2 imagery. This contributes to the mitigation of rapid climate change. We propose a novel deep learning solution based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement techniques (Varon ratio and Sanchez regression) to optimise input features for heightened sensitivity. A key achievement is the ability to detect small plumes down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing traditional methods limited to larger plumes. Experiments show our approach achieves a 78.39% F1-score on the validation set, demonstrating superior performance in sensitivity and precision over existing remote sensing techniques for automated methane monitoring, especially for small plumes.",
        "arxiv_id": "2508.16282",
        "ARXIVID": "2508.16282",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications). Proposes a novel deep learning method for methane plume segmentation in satellite imagery, with improved sensitivity for small plumes.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16172": {
        "authors": [
            "Kai Hu",
            "Parfait Atchade-Adelomou",
            "Carlo Adornetto",
            "Adrian Mora-Carrero",
            "Luis Alonso-Pastor",
            "Ariel Noyman",
            "Yubo Liu",
            "Kent Larson"
        ],
        "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain",
        "abstract": "arXiv:2508.16172v1 Announce Type: new  Abstract: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.",
        "arxiv_id": "2508.16172",
        "ARXIVID": "2508.16172",
        "COMMENT": "Somewhat related to criterion 3 (embodied AI, new methods for agent behavior modeling). Proposes a novel method (Preference Chain) integrating Graph RAG with LLMs for simulating human mobility behavior, but not directly about spatial intelligence or vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16524": {
        "authors": [
            "Xuan Zhang",
            "Zhijian Zhou",
            "Weidi Xu",
            "Yanting Miao",
            "Chao Qu",
            "Yuan Qi"
        ],
        "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning",
        "abstract": "arXiv:2508.16524v1 Announce Type: new  Abstract: Enabling neural networks to learn complex logical constraints and fulfill symbolic reasoning is a critical challenge. Bridging this gap often requires guiding the neural network's output distribution to move closer to the symbolic constraints. While diffusion models have shown remarkable generative capability across various domains, we employ the powerful architecture to perform neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline adopts a two-stage training strategy: the first stage focuses on cultivating basic reasoning abilities, while the second emphasizes systematic learning of logical constraints. To impose hard constraints on neural outputs in the second stage, we formulate the diffusion reasoner as a Markov decision process and innovatively fine-tune it with an improved proximal policy optimization algorithm. We utilize a rule-based reward signal derived from the logical consistency of neural outputs and adopt a flexible strategy to optimize the diffusion reasoner's policy. We evaluate our methodology on some classical symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and preference learning. Experimental results demonstrate that our approach achieves outstanding accuracy and logical consistency among neural networks.",
        "arxiv_id": "2508.16524",
        "ARXIVID": "2508.16524",
        "COMMENT": "This paper introduces a constraints-guided diffusion reasoner for neuro-symbolic learning, using diffusion models for logical reasoning tasks. While it is a novel use of generative models for reasoning, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.16212": {
        "authors": [
            "Huanpeng Chu",
            "Wei Wu",
            "Guanyu Fen",
            "Yutao Zhang"
        ],
        "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
        "abstract": "arXiv:2508.16212v1 Announce Type: new  Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.",
        "arxiv_id": "2508.16212",
        "ARXIVID": "2508.16212",
        "COMMENT": "This paper proposes a training-free cache reuse method for diffusion Transformer models, which is a methodological improvement for generative modeling. It is relevant to generative modeling in multi-modal learning (criterion 4), but does not directly address spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16213": {
        "authors": [
            "Kaiyuan Ji",
            "Yijin Guo",
            "Zicheng Zhang",
            "Xiangyang Zhu",
            "Yuan Tian",
            "Ning Liu",
            "Guangtao Zhai"
        ],
        "title": "MedOmni-45{\\deg}: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine",
        "abstract": "arXiv:2508.16213v1 Announce Type: new  Abstract: With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",
        "arxiv_id": "2508.16213",
        "ARXIVID": "2508.16213",
        "COMMENT": "This paper introduces a new benchmark for reasoning-oriented LLMs in medicine, focusing on safety-performance trade-offs and reasoning vulnerabilities. It is a new benchmark (criterion 3), but not for embodied AI or simulators; it is more about LLMs in the medical domain.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.16072": {
        "authors": [
            "Zizhen Li",
            "Chuanhao Li",
            "Yibin Wang",
            "Qi Chen",
            "Diping Song",
            "Yukang Feng",
            "Jianwen Sun",
            "Jiaxin Ai",
            "Fanrui Zhang",
            "Mingzhu Sun",
            "Kaipeng Zhang"
        ],
        "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles",
        "abstract": "arXiv:2508.16072v1 Announce Type: new  Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction.",
        "arxiv_id": "2508.16072",
        "ARXIVID": "2508.16072",
        "COMMENT": "Does not directly match any criterion. Focuses on LLMs and individualized reasoning in social deduction games, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16140": {
        "authors": [
            "Jincheng Li",
            "Danyang Dong",
            "Menglin Zheng",
            "Jingbo Zhang",
            "Yueqin Hang",
            "Lichi Zhang",
            "Lili Zhao"
        ],
        "title": "High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection",
        "abstract": "arXiv:2508.16140v1 Announce Type: new  Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test (TCT) images is a critical component in the development of intelligent computer-aided diagnostic systems. However, existing algorithms typically fail to effectively model the correlations of visual features, while these spatial correlation features actually contain critical diagnostic information. Furthermore, no detection algorithm has the ability to integrate inter-correlation features of cells with intra-discriminative features of cells, lacking a fusion strategy for the end-to-end detection model. In this work, we propose a hypergraph-based cell detection network that effectively fuses different types of features, combining spatial correlation features and deep discriminative features. Specifically, we use a Multi-level Fusion Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation module (CLFFS-HC), to integrate mixed features. Finally, we conducted experiments on three publicly available datasets, and the results demonstrate that our method significantly improves the performance of cervical abnormal cell detection.",
        "arxiv_id": "2508.16140",
        "ARXIVID": "2508.16140",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications). Proposes a hypergraph-based feature fusion network for abnormal cell detection, but not directly about spatial intelligence, VLLMs/MLLMs, or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.16279": {
        "authors": [
            "Dawei Gao",
            "Zitao Li",
            "Yuexiang Xie",
            "Weirui Kuang",
            "Liuyi Yao",
            "Bingchen Qian",
            "Zhijian Ma",
            "Yue Cui",
            "Haohao Luo",
            "Shen Li",
            "Lu Yi",
            "Yi Yu",
            "Shiqi He",
            "Zhiling Luo",
            "Wenmeng Zhou",
            "Zhicheng Zhang",
            "Xuguang He",
            "Ziqian Chen",
            "Weikai Liao",
            "Farruh Isakulovich Kushnazarov",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications",
        "abstract": "arXiv:2508.16279v1 Announce Type: new  Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.",
        "arxiv_id": "2508.16279",
        "ARXIVID": "2508.16279",
        "COMMENT": "Somewhat related to criterion 3 (embodied AI frameworks), as it introduces a developer-centric framework for agentic applications, but does not focus on spatial intelligence or vision-language models. More about agent infrastructure and tool use.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.15943": {
        "authors": [
            "Riccardo Andreoni",
            "Andrei Buliga",
            "Alessandro Daniele",
            "Chiara Ghidini",
            "Marco Montali",
            "Massimiliano Ronzani"
        ],
        "title": "T-ILR: a Neurosymbolic Integration for LTLf",
        "abstract": "arXiv:2508.15943v1 Announce Type: new  Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep learning architectures have demonstrated promising results in static domains. However, methods to handle temporal logic specifications remain underexplored. The only existing approach relies on an explicit representation of a finite-state automaton corresponding to the temporal specification. Instead, we aim at proposing a neurosymbolic framework designed to incorporate temporal logic specifications, expressed in Linear Temporal Logic over finite traces (LTLf), directly into deep learning architectures for sequence-based tasks. We extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging the recent introduction of fuzzy LTLf interpretations. We name this proposed method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an existing benchmark for temporal neurosymbolic architectures, consisting of the classification of image sequences in the presence of temporal knowledge. The results demonstrate improved accuracy and computational efficiency compared to the state-of-the-art method.",
        "arxiv_id": "2508.15943",
        "ARXIVID": "2508.15943",
        "COMMENT": "This paper introduces a neurosymbolic framework for integrating temporal logic into deep learning, which is interesting for sequence-based tasks but does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16089": {
        "authors": [
            "Sun Weikai",
            "Song Shijie",
            "Chi Wenjie"
        ],
        "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network",
        "abstract": "arXiv:2508.16089v1 Announce Type: new  Abstract: Although diffusion model has made good progress in the field of image generation, GAN\\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\\cite{liu2021comparing}, SSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset is 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\\% with INJK With strong cross-task capability.",
        "arxiv_id": "2508.16089",
        "ARXIVID": "2508.16089",
        "COMMENT": "This paper proposes a new GAN architecture with several novel mechanisms for image generation. While it is relevant to generative modeling, it does not directly address spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16138": {
        "authors": [
            "Hao Tang",
            "Rongxi Yi",
            "Lei Li",
            "Kaiyi Cao",
            "Jiapeng Zhao",
            "Yihan Xiao",
            "Minghai Shi",
            "Peng Yuan",
            "Yan Xi",
            "Hui Tang",
            "Wei Li",
            "Zhan Wu",
            "Yixin Zhou"
        ],
        "title": "4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration",
        "abstract": "arXiv:2508.16138v1 Announce Type: new  Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic, weight-bearing joint motion. Functional evaluation, particularly after surgical intervention, requires four-dimensional (4D) imaging, but current methods are limited by excessive radiation exposure or incomplete spatial information from 2D techniques. We propose an integrated 4D joint analysis platform that combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a programmable, gantry-free trajectory optimized for upright scanning; (2) a hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using deep learning-based preprocessing, 3D-2D projection, and iterative optimization; and (3) a clinically validated framework for quantitative kinematic assessment. In simulation studies, the method achieved sub-voxel accuracy (0.235 mm) with a 99.18 percent success rate, outperforming conventional and state-of-the-art registration approaches. Clinical evaluation further demonstrated accurate quantification of tibial plateau motion and medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D CBCT platform enables fast, accurate, and low-dose dynamic joint imaging, offering new opportunities for biomechanical research, precision diagnostics, and personalized orthopedic care.",
        "arxiv_id": "2508.16138",
        "ARXIVID": "2508.16138",
        "COMMENT": "This paper presents a 4D imaging platform for dynamic joint assessment using X-ray and 2D-3D registration. While it is a novel imaging method, it does not directly address any of the listed criteria or the main interests in vision-language or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16291": {
        "authors": [
            "Fengshun Wang",
            "Qiurui Wang",
            "Peilin Zhao"
        ],
        "title": "Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment",
        "abstract": "arXiv:2508.16291v1 Announce Type: new  Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations in figure skating demand precise assessment of athletic actions and artistic interpretation, respectively. Existing methods face three major challenges. Firstly, video and audio cues are regarded as common features for both TES and PCS predictions in previous works without considering the prior evaluation criterion of figure skating. Secondly, action elements in competitions are separated in time, TES should be derived from each element's score, but existing methods try to give an overall TES prediction without evaluating each action element. Thirdly, lengthy competition videos make it difficult and inefficient to handle long-range contexts. To address these challenges, we propose a two-stream Mamba pyramid network that aligns with actual judging criteria to predict TES and PCS by separating visual-feature based TES evaluation stream from audio-visual-feature based PCS evaluation stream. In the PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee that video-based features remain unaffected when assessing TES, and enhance PCS estimation by fusing visual and auditory cues across each contextual level of the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and TES head we proposed effectively address the challenges of localizing and evaluating action elements with various temporal scales and give score predictions. With Mamba's superior ability to capture long-range dependencies and its linear computational complexity, our method is ideal for handling lengthy figure skating videos. Comprehensive experimentation demonstrates that our framework attains state-of-the-art performance on the FineFS benchmark. Our source code is available at https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.",
        "arxiv_id": "2508.16291",
        "ARXIVID": "2508.16291",
        "COMMENT": "Does not directly match any criterion. Focuses on long-range action representation in figure skating assessment, with a two-stream network, but not on embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16124": {
        "authors": [
            "Savvas Karatsiolis",
            "Andreas Kamilaris"
        ],
        "title": "Domain Adaptation via Feature Refinement",
        "abstract": "arXiv:2508.16124v1 Announce Type: new  Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet effective framework for unsupervised domain adaptation under distribution shift. The proposed method synergistically combines three key components: adaptation of Batch Normalization statistics using unlabeled target data, feature distillation from a source-trained model and hypothesis transfer. By aligning feature distributions at the statistical and representational levels, DAFR2 produces robust and domain-invariant feature spaces that generalize across similar domains without requiring target labels, complex architectures or sophisticated training objectives. Extensive experiments on benchmark datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C, demonstrate that the proposed algorithm outperforms prior methods in robustness to corruption. Theoretical and empirical analyses further reveal that our method achieves improved feature alignment, increased mutual information between the domains and reduced sensitivity to input perturbations.",
        "arxiv_id": "2508.16124",
        "ARXIVID": "2508.16124",
        "COMMENT": "This paper proposes a new method for unsupervised domain adaptation via feature refinement, focusing on feature alignment and robustness. While it is relevant to computer vision and machine learning, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16057": {
        "authors": [
            "Sijie Yang",
            "Binyu Lei",
            "Filip Biljecki"
        ],
        "title": "Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework",
        "abstract": "arXiv:2508.16057v1 Announce Type: new  Abstract: Ensuring liveability and comfort is one of the fundamental objectives of urban planning. Numerous studies have employed computational methods to assess and quantify factors related to urban comfort such as greenery coverage, thermal comfort, and walkability. However, a clear definition of urban comfort and its comprehensive evaluation framework remain elusive. Our research explores the theoretical interpretations and methodologies for assessing urban comfort within digital planning, emphasising three key dimensions: multidimensional analysis, data support, and AI assistance.",
        "arxiv_id": "2508.16057",
        "ARXIVID": "2508.16057",
        "COMMENT": "Does not match any specific criterion. Focuses on urban comfort assessment using AI, but not on spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.16062": {
        "authors": [
            "Ziqi Li",
            "Abderraouf Amrani",
            "Shri Rai",
            "Hamid Laga"
        ],
        "title": "Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals",
        "abstract": "arXiv:2508.16062v1 Announce Type: new  Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a long-standing problem, which has a wide range of applications, from biology, livestock management, and animal conservation and welfare to content creation in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally, 3D models of real animals are obtained using 3D scanners. These, however, are intrusive, often prohibitively expensive, and difficult to deploy in the natural environment of the animals. In recent years, we have seen a significant surge in deep learning-based techniques that enable the 3D reconstruction, in a non-intrusive manner, of the shape and motion of dynamic objects just from their RGB image and/or video observations. Several papers have explored their application and extension to various types of animals. This paper surveys the latest developments in this emerging and growing field of research. It categorizes and discusses the state-of-the-art methods based on their input modalities, the way the 3D geometry and motion of animals are represented, the type of reconstruction techniques they use, and the training mechanisms they adopt. It also analyzes the performance of some key methods, discusses their strengths and limitations, and identifies current challenges and directions for future research.",
        "arxiv_id": "2508.16062",
        "ARXIVID": "2508.16062",
        "COMMENT": "This is a survey paper on 3D reconstruction of animal shape and motion, summarizing recent deep learning-based techniques. It is relevant to spatial understanding and computer vision, but does not propose new methods or benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}