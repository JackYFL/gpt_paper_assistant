{
    "2507.02252": {
        "authors": [
            "Zeyu Lei",
            "Hongyuan Yu",
            "Jinlin Wu",
            "Zhen Chen"
        ],
        "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement",
        "abstract": "arXiv:2507.02252v1 Announce Type: new  Abstract: Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.",
        "arxiv_id": "2507.02252",
        "ARXIVID": "2507.02252",
        "COMMENT": "Matches criterion 2 as it introduces a new multi-modal large language model (SurgVisAgent) for surgical visual enhancement.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.02813": {
        "authors": [
            "Fangfu Liu",
            "Hao Li",
            "Jiawei Chi",
            "Hanyang Wang",
            "Minghui Yang",
            "Fudong Wang",
            "Yueqi Duan"
        ],
        "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion",
        "abstract": "arXiv:2507.02813v1 Announce Type: new  Abstract: Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.",
        "arxiv_id": "2507.02813",
        "ARXIVID": "2507.02813",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications in 3D reconstruction and language-embedded scenes.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.02825": {
        "authors": [
            "Yuxuan Zhu",
            "Tengjun Jin",
            "Yada Pruksachatkun",
            "Andy Zhang",
            "Shu Liu",
            "Sasha Cui",
            "Sayash Kapoor",
            "Shayne Longpre",
            "Kevin Meng",
            "Rebecca Weiss",
            "Fazl Barez",
            "Rahul Gupta",
            "Jwala Dhamala",
            "Jacob Merizian",
            "Mario Giulianelli",
            "Harry Coppock",
            "Cozmin Ududec",
            "Jasjeet Sekhon",
            "Jacob Steinhardt",
            "Antony Kellerman",
            "Sarah Schwettmann",
            "Matei Zaharia",
            "Ion Stoica",
            "Percy Liang",
            "Daniel Kang"
        ],
        "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
        "abstract": "arXiv:2507.02825v1 Announce Type: new  Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
        "arxiv_id": "2507.02825",
        "ARXIVID": "2507.02825",
        "COMMENT": "Matches criterion 3 as it focuses on building new benchmarks for agentic evaluation with a novel checklist for rigorous benchmarking.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.02363": {
        "authors": [
            "Jiahao Wu",
            "Rui Peng",
            "Jianbo Jiao",
            "Jiayu Yang",
            "Luyang Tang",
            "Kaiqiang Xiong",
            "Jie Liang",
            "Jinbo Yan",
            "Runling Liu",
            "Ronggang Wang"
        ],
        "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling",
        "abstract": "arXiv:2507.02363v1 Announce Type: new  Abstract: Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
        "arxiv_id": "2507.02363",
        "ARXIVID": "2507.02363",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for dynamic scene reconstruction, focusing on previously ignored aspects of large-scale motion.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.02861": {
        "authors": [
            "Zhening Huang",
            "Xiaoyang Wu",
            "Fangcheng Zhong",
            "Hengshuang Zhao",
            "Matthias Nie{\\ss}ner",
            "Joan Lasenby"
        ],
        "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
        "abstract": "arXiv:2507.02861v1 Announce Type: new  Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c",
        "arxiv_id": "2507.02861",
        "ARXIVID": "2507.02861",
        "COMMENT": "Matches criterion 1 as it focuses on spatial understanding and reconstruction of 3D scenes.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.02863": {
        "authors": [
            "Yuqi Wu",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
        "abstract": "arXiv:2507.02863v1 Announce Type: new  Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.",
        "arxiv_id": "2507.02863",
        "ARXIVID": "2507.02863",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for streaming 3D reconstruction with explicit spatial pointer memory, which is a new method in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.02705": {
        "authors": [
            "Qi Xu",
            "Dongxu Wei",
            "Lingzhe Zhao",
            "Wenpu Li",
            "Zhangchi Huang",
            "Shunping Ji",
            "Peidong Liu"
        ],
        "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment",
        "abstract": "arXiv:2507.02705v1 Announce Type: new  Abstract: Simultaneous understanding and 3D reconstruction plays an important role in developing end-to-end embodied intelligent systems. To achieve this, recent approaches resort to 2D-to-3D feature alignment paradigm, which leads to limited 3D understanding capability and potential semantic information loss. In light of this, we propose SIU3R, the first alignment-free framework for generalizable simultaneous understanding and 3D reconstruction from unposed images. Specifically, SIU3R bridges reconstruction and understanding tasks via pixel-aligned 3D representation, and unifies multiple understanding tasks into a set of unified learnable queries, enabling native 3D understanding without the need of alignment with 2D models. To encourage collaboration between the two tasks with shared representation, we further conduct in-depth analyses of their mutual benefits, and propose two lightweight modules to facilitate their interaction. Extensive experiments demonstrate that our method achieves state-of-the-art performance not only on the individual tasks of 3D reconstruction and understanding, but also on the task of simultaneous understanding and 3D reconstruction, highlighting the advantages of our alignment-free framework and the effectiveness of the mutual benefit designs.",
        "arxiv_id": "2507.02705",
        "ARXIVID": "2507.02705",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for simultaneous scene understanding and 3D reconstruction, which is relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.02859": {
        "authors": [
            "Jiaer Xia",
            "Bingkui Tong",
            "Yuhang Zang",
            "Rui Shao",
            "Kaiyang Zhou"
        ],
        "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation",
        "abstract": "arXiv:2507.02859v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.",
        "arxiv_id": "2507.02859",
        "ARXIVID": "2507.02859",
        "COMMENT": "Matches criterion 2 as it focuses on adapting multimodal large language models (MLLMs) for specialized vision tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.02860": {
        "authors": [
            "Xin Zhou",
            "Dingkang Liang",
            "Kaijin Chen",
            "Tianrui Feng",
            "Xiwu Chen",
            "Hongkai Lin",
            "Yikang Ding",
            "Feiyang Tan",
            "Hengshuang Zhao",
            "Xiang Bai"
        ],
        "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching",
        "abstract": "arXiv:2507.02860v1 Announce Type: new  Abstract: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.",
        "arxiv_id": "2507.02860",
        "ARXIVID": "2507.02860",
        "COMMENT": "Matches criterion 4 as it proposes a novel acceleration framework for video diffusion models, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02713": {
        "authors": [
            "Qin Guo",
            "Ailing Zeng",
            "Dongxu Yue",
            "Ceyuan Yang",
            "Yang Cao",
            "Hanzhong Guo",
            "Fei Shen",
            "Wei Liu",
            "Xihui Liu",
            "Dan Xu"
        ],
        "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation",
        "abstract": "arXiv:2507.02713v1 Announce Type: new  Abstract: Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.",
        "arxiv_id": "2507.02713",
        "ARXIVID": "2507.02713",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model (DiT-based framework) and its application to keypoint-guided multi-class image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02316": {
        "authors": [
            "Zecheng Zhao",
            "Selena Song",
            "Tong Chen",
            "Zhi Chen",
            "Shazia Sadiq",
            "Yadan Luo"
        ],
        "title": "Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos",
        "abstract": "arXiv:2507.02316v1 Announce Type: new  Abstract: Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation metrics primarily capture visual quality and temporal consistency, offering limited insight into how synthetic videos perform in downstream tasks such as text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset and benchmark designed to evaluate the utility of synthetic videos for building retrieval models. Based on 800 diverse user queries derived from MSRVTT training split, we generate synthetic videos using state-of-the-art T2V models and annotate each video-text pair along four key semantic alignment dimensions: Object \\& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation framework correlates general video quality assessment (VQA) metrics with these alignment scores, and examines their predictive power for downstream TVR performance. To explore pathways of scaling up, we further develop an Auto-Evaluator to estimate alignment quality from existing metrics. Beyond benchmarking, our results show that SynTVA is a valuable asset for dataset augmentation, enabling the selection of high-utility synthetic samples that measurably improve TVR outcomes. Project page and dataset can be found at https://jasoncodemaker.github.io/SynTVA/.",
        "arxiv_id": "2507.02316",
        "ARXIVID": "2507.02316",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating synthetic videos in retrieval tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02321": {
        "authors": [
            "Nina Konovalova",
            "Maxim Nikolaev",
            "Andrey Kuznetsov",
            "Aibek Alanov"
        ],
        "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback",
        "abstract": "arXiv:2507.02321v1 Announce Type: new  Abstract: Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).",
        "arxiv_id": "2507.02321",
        "ARXIVID": "2507.02321",
        "COMMENT": "Matches criterion 1 as it proposes a new method for improving spatial control in text-to-image diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02554": {
        "authors": [
            "Edan Toledo",
            "Karen Hambardzumyan",
            "Martin Josifoski",
            "Rishi Hazra",
            "Nicolas Baldwin",
            "Alexis Audran-Reiss",
            "Michael Kuchnik",
            "Despoina Magka",
            "Minqi Jiang",
            "Alisia Maria Lupidi",
            "Andrei Lupu",
            "Roberta Raileanu",
            "Kelvin Niu",
            "Tatiana Shavrina",
            "Jean-Christophe Gagnon-Audet",
            "Michael Shvartsman",
            "Shagun Sodhani",
            "Alexander H. Miller",
            "Abhishek Charnalia",
            "Derek Dunfield",
            "Carole-Jean Wu",
            "Pontus Stenetorp",
            "Nicola Cancedda",
            "Jakob Nicolaus Foerster",
            "Yoram Bachrach"
        ],
        "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
        "abstract": "arXiv:2507.02554v1 Announce Type: new  Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
        "arxiv_id": "2507.02554",
        "ARXIVID": "2507.02554",
        "COMMENT": "Matches criterion 3 as it focuses on a new benchmark (MLE-bench) and methods for improving AI research agents, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02862": {
        "authors": [
            "Xiang Fan",
            "Xiaohang Sun",
            "Kushan Thakkar",
            "Zhu Liu",
            "Vimal Bhat",
            "Ranjay Krishna",
            "Xiang Hao"
        ],
        "title": "RefTok: Reference-Based Tokenization for Video Generation",
        "abstract": "arXiv:2507.02862v1 Announce Type: new  Abstract: Effectively handling temporal redundancy remains a key challenge in learning video models. Prevailing approaches often treat each set of frames independently, failing to effectively capture the temporal dependencies and redundancies inherent in videos. To address this limitation, we introduce RefTok, a novel reference-based tokenization method capable of capturing complex temporal dynamics and contextual information. Our method encodes and decodes sets of frames conditioned on an unquantized reference frame. When decoded, RefTok preserves the continuity of motion and the appearance of objects across frames. For example, RefTok retains facial details despite head motion, reconstructs text correctly, preserves small patterns, and maintains the legibility of handwriting from the context. Across 4 video datasets (K600, UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or higher compression ratios. When a video generation model is trained using RefTok's latents on the BAIR Robot Pushing task, the generations not only outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters, across all generation metrics by an average of 27.9%.",
        "arxiv_id": "2507.02862",
        "ARXIVID": "2507.02862",
        "COMMENT": "Matches criterion 1 as it introduces a novel tokenization method for video generation, which involves spatial and temporal understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02748": {
        "authors": [
            "Alex Colagrande",
            "Paul Caillon",
            "Eva Feillet",
            "Alexandre Allauzen"
        ],
        "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics",
        "abstract": "arXiv:2507.02748v1 Announce Type: new  Abstract: Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.",
        "arxiv_id": "2507.02748",
        "ARXIVID": "2507.02748",
        "COMMENT": "Matches criterion 4 as it introduces a novel attention mechanism for vision tasks, which could be foundational.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.02393": {
        "authors": [
            "Seokyeong Lee",
            "Sithu Aung",
            "Junyong Choi",
            "Seungryong Kim",
            "Ig-Jae Kim",
            "Junghyun Cho"
        ],
        "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection",
        "abstract": "arXiv:2507.02393v1 Announce Type: new  Abstract: Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.",
        "arxiv_id": "2507.02393",
        "ARXIVID": "2507.02393",
        "COMMENT": "Matches criterion 3 as it introduces a novel pseudo-labeling framework for monocular 3D object detection, which is a new method in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.02288": {
        "authors": [
            "De Cheng",
            "Zhipeng Xu",
            "Xinyang Jiang",
            "Dongsheng Li",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization",
        "abstract": "arXiv:2507.02288v1 Announce Type: new  Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of performing effectively on unseen target domains. Notably, recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated considerable potential in enhancing the generalization capabilities of deep learning models. Despite the increasing attention toward VFM-based domain prompt tuning within DG, the effective design of prompts capable of disentangling invariant features across diverse domains remains a critical challenge. In this paper, we propose addressing this challenge by leveraging the controllable and flexible language prompt of the VFM. Noting that the text modality of VFMs is naturally easier to disentangle, we introduce a novel framework for text feature-guided visual prompt tuning. This framework first automatically disentangles the text prompt using a large language model (LLM) and then learns domain-invariant visual representation guided by the disentangled text feature. However, relying solely on language to guide visual feature disentanglement has limitations, as visual features can sometimes be too complex or nuanced to be fully captured by descriptive text. To address this, we introduce Worst Explicit Representation Alignment (WERA), which extends text-guided visual prompts by incorporating an additional set of abstract prompts. These prompts enhance source domain diversity through stylized image augmentations, while alignment constraints ensure that visual representations remain consistent across both the original and augmented distributions. Experiments conducted on major DG datasets, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method outperforms state-of-the-art DG methods.",
        "arxiv_id": "2507.02288",
        "ARXIVID": "2507.02288",
        "COMMENT": "Matches criterion 4 as it focuses on domain generalization using visual foundation models and prompt tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.02798": {
        "authors": [
            "Miguel Espinosa",
            "Chenhongyi Yang",
            "Linus Ericsson",
            "Steven McDonagh",
            "Elliot J. Crowley"
        ],
        "title": "No time to train! Training-Free Reference-Based Instance Segmentation",
        "abstract": "arXiv:2507.02798v1 Announce Type: new  Abstract: The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).",
        "arxiv_id": "2507.02798",
        "ARXIVID": "2507.02798",
        "COMMENT": "Matches criterion 4 as it leverages foundation models for segmentation tasks, focusing on semantic priors and training-free methods.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.02687": {
        "authors": [
            "JungWoo Chae",
            "Jiyoon Kim",
            "JaeWoong Choi",
            "Kyungyul Kim",
            "Sangheum Hwang"
        ],
        "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
        "abstract": "arXiv:2507.02687v1 Announce Type: new  Abstract: Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.",
        "arxiv_id": "2507.02687",
        "ARXIVID": "2507.02687",
        "COMMENT": "Matches criterion 4 as it proposes a novel framework for improving diffusion models, which are foundational in generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02857": {
        "authors": [
            "Ziye Li",
            "Hao Luo",
            "Xincheng Shuai",
            "Henghui Ding"
        ],
        "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
        "abstract": "arXiv:2507.02857v1 Announce Type: new  Abstract: Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.",
        "arxiv_id": "2507.02857",
        "ARXIVID": "2507.02857",
        "COMMENT": "Matches criterion 1 as it introduces a new method for spatial and motion-controlled video generation, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02546": {
        "authors": [
            "Ruicheng Wang",
            "Sicheng Xu",
            "Yue Dong",
            "Yu Deng",
            "Jianfeng Xiang",
            "Zelong Lv",
            "Guangzhong Sun",
            "Xin Tong",
            "Jiaolong Yang"
        ],
        "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details",
        "abstract": "arXiv:2507.02546v1 Announce Type: new  Abstract: We propose MoGe-2, an advanced open-domain geometry estimation model that recovers a metric scale 3D point map of a scene from a single image. Our method builds upon the recent monocular geometry estimation approach, MoGe, which predicts affine-invariant point maps with unknown scales. We explore effective strategies to extend MoGe for metric geometry prediction without compromising the relative geometry accuracy provided by the affine-invariant point representation. Additionally, we discover that noise and errors in real data diminish fine-grained detail in the predicted geometry. We address this by developing a unified data refinement approach that filters and completes real data from different sources using sharp synthetic labels, significantly enhancing the granularity of the reconstructed geometry while maintaining the overall accuracy. We train our model on a large corpus of mixed datasets and conducted comprehensive evaluations, demonstrating its superior performance in achieving accurate relative geometry, precise metric scale, and fine-grained detail recovery -- capabilities that no previous methods have simultaneously achieved.",
        "arxiv_id": "2507.02546",
        "ARXIVID": "2507.02546",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application to geometry estimation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02373": {
        "authors": [
            "Xizhe Xue",
            "Yang Zhou",
            "Dawei Yan",
            "Ying Li",
            "Haokui Zhang",
            "Rong Xiao"
        ],
        "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding",
        "abstract": "arXiv:2507.02373v1 Announce Type: new  Abstract: Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.",
        "arxiv_id": "2507.02373",
        "ARXIVID": "2507.02373",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for video language models in underwater environments.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02299": {
        "authors": [
            "Yunhan Yang",
            "Shuo Chen",
            "Yukun Huang",
            "Xiaoyang Wu",
            "Yuan-Chen Guo",
            "Edmund Y. Lam",
            "Hengshuang Zhao",
            "Tong He",
            "Xihui Liu"
        ],
        "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation",
        "abstract": "arXiv:2507.02299v1 Announce Type: new  Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the generation of high-quality novel views from a single in-the-wild image. However, existing works face challenges in producing controllable novel views due to the lack of information from multiple views. In this paper, we present DreamComposer++, a flexible and scalable framework designed to improve current view-aware diffusion models by incorporating multi-view conditions. Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to extract 3D representations of an object from various views. These representations are then aggregated and rendered into the latent features of target view through the multi-view feature fusion module. Finally, the obtained features of target view are integrated into pre-trained image or video diffusion models for novel view synthesis. Experimental results demonstrate that DreamComposer++ seamlessly integrates with cutting-edge view-aware diffusion models and enhances their abilities to generate controllable novel views from multi-view conditions. This advancement facilitates controllable 3D object reconstruction and enables a wide range of applications.",
        "arxiv_id": "2507.02299",
        "ARXIVID": "2507.02299",
        "COMMENT": "Matches criterion 4 as it involves diffusion models for 3D content generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02743": {
        "authors": [
            "M\\'elanie Gaillochet",
            "Mehrdad Noori",
            "Sahar Dastani",
            "Christian Desrosiers",
            "Herv\\'e Lombaert"
        ],
        "title": "Prompt learning with bounding box constraints for medical image segmentation",
        "abstract": "arXiv:2507.02743v1 Announce Type: new  Abstract: Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at https://github.com/Minimel/box-prompt-learning-VFM.git",
        "arxiv_id": "2507.02743",
        "ARXIVID": "2507.02743",
        "COMMENT": "Matches criterion 4 as it involves vision foundation models and their application to medical image segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02200": {
        "authors": [
            "Xiao Wang",
            "Jingtao Jiang",
            "Qiang Chen",
            "Lan Chen",
            "Lin Zhu",
            "Yaowei Wang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning",
        "abstract": "arXiv:2507.02200v1 Announce Type: new  Abstract: Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.",
        "arxiv_id": "2507.02200",
        "ARXIVID": "2507.02200",
        "COMMENT": "Matches criterion 2 as it involves a vision encoder and large language model integration for scene text recognition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02494": {
        "authors": [
            "Hyunsoo Son",
            "Jeonghyun Noh",
            "Suemin Jeon",
            "Chaoli Wang",
            "Won-Ki Jeong"
        ],
        "title": "MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations",
        "abstract": "arXiv:2507.02494v1 Announce Type: new  Abstract: Implicit Neural Representations (INRs) are widely used to encode data as continuous functions, enabling the visualization of large-scale multivariate scientific simulation data with reduced memory usage. However, existing INR-based methods face three main limitations: (1) inflexible representation of complex structures, (2) primarily focusing on single-variable data, and (3) dependence on structured grids. Thus, their performance degrades when applied to complex real-world datasets. To address these limitations, we propose a novel neural network-based framework, MC-INR, which handles multivariate data on unstructured grids. It combines meta-learning and clustering to enable flexible encoding of complex structures. To further improve performance, we introduce a residual-based dynamic re-clustering mechanism that adaptively partitions clusters based on local error. We also propose a branched layer to leverage multivariate data through independent branches simultaneously. Experimental results demonstrate that MC-INR outperforms existing methods on scientific data encoding tasks.",
        "arxiv_id": "2507.02494",
        "ARXIVID": "2507.02494",
        "COMMENT": "Does not match any specific criteria but is related to encoding scientific data using neural representations.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02686": {
        "authors": [
            "Charlesquin Kemajou Mbakam",
            "Jonathan Spence",
            "Marcelo Pereyra"
        ],
        "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models",
        "abstract": "arXiv:2507.02686v1 Announce Type: new  Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.",
        "arxiv_id": "2507.02686",
        "ARXIVID": "2507.02686",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and computational imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02271": {
        "authors": [
            "Feizhen Huang",
            "Yu Wu",
            "Yutian Lin",
            "Bo Du"
        ],
        "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation",
        "abstract": "arXiv:2507.02271v1 Announce Type: new  Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a crucial role in film and video post-production. However, current methods overlook the cinematic language, a critical component of artistic expression in filmmaking. As a result, their performance deteriorates in scenarios where Foley targets are only partially visible. To address this challenge, we propose a simple self-distillation approach to extend V2A models to cinematic language scenarios. By simulating the cinematic language variations, the student model learns to align the video features of training pairs with the same audio-visual correspondences, enabling it to effectively capture the associations between sounds and partial visual information. Our method not only achieves impressive improvements under partial visibility across all evaluation metrics, but also enhances performance on the large-scale V2A dataset, VGGSound.",
        "arxiv_id": "2507.02271",
        "ARXIVID": "2507.02271",
        "COMMENT": "Does not match any specific criteria but is tangentially related to multi-modal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02437": {
        "authors": [
            "Wei Li",
            "Jingyang Zhang",
            "Lihao Liu",
            "Guoan Wang",
            "Junjun He",
            "Yang Chen",
            "Lixu Gu"
        ],
        "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning",
        "abstract": "arXiv:2507.02437v1 Announce Type: new  Abstract: Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a source model to unseen medical sites using unlabeled test data, due to the high cost of data annotation. Existing TTA methods consider scenarios where data from one or multiple domains arrives in complete domain units. However, in clinical practice, data usually arrives in domain fragments of arbitrary lengths and in random arrival orders, due to resource constraints and patient variability. This paper investigates a practical Free-Form Test-Time Adaptation (F$^{2}$TTA) task, where a source model is adapted to such free-form domain fragments, with shifts occurring between fragments unpredictably. In this setting, these shifts could distort the adaptation process. To address this problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT) framework. I-DiPT employs an image-invariant prompt to explore domain-invariant representations for mitigating the unpredictable shifts, and an image-specific prompt to adapt the source model to each test image from the incoming fragments. The prompts may suffer from insufficient knowledge representation since only one image is available for training. To overcome this limitation, we first introduce Uncertainty-oriented Masking (UoM), which encourages the prompts to extract sufficient information from the incoming image via masked consistency learning driven by the uncertainty of the source model representations. Then, we further propose a Parallel Graph Distillation (PGD) method that reuses knowledge from historical image-specific and image-invariant prompts through parallel graph networks. Experiments on breast cancer and glaucoma classification demonstrate the superiority of our method over existing TTA approaches in F$^{2}$TTA. Code is available at https://github.com/mar-cry/F2TTA.",
        "arxiv_id": "2507.02437",
        "ARXIVID": "2507.02437",
        "COMMENT": "Does not match any specific criteria but is related to domain adaptation in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02576": {
        "authors": [
            "Alina F. Dima",
            "Suprosanna Shit",
            "Huaqi Qiu",
            "Robbie Holland",
            "Tamara T. Mueller",
            "Fabio Antonio Musio",
            "Kaiyuan Yang",
            "Bjoern Menze",
            "Rickmer Braren",
            "Marcus Makowski",
            "Daniel Rueckert"
        ],
        "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization",
        "abstract": "arXiv:2507.02576v1 Announce Type: new  Abstract: Vessels are complex structures in the body that have been studied extensively in multiple representations. While voxelization is the most common of them, meshes and parametric models are critical in various applications due to their desirable properties. However, these representations are typically extracted through segmentations and used disjointly from each other. We propose a framework that joins the three representations under differentiable transformations. By leveraging differentiable voxelization, we automatically extract a parametric shape model of the vessels through shape-to-segmentation fitting, where we learn shape parameters from segmentations without the explicit need for ground-truth shape parameters. The vessel is parametrized as centerlines and radii using cubic B-splines, ensuring smoothness and continuity by construction. Meshes are differentiably extracted from the learned shape parameters, resulting in high-fidelity meshes that can be manipulated post-fit. Our method can accurately capture the geometry of complex vessels, as demonstrated by the volumetric fits in experiments on aortas, aneurysms, and brain vessels.",
        "arxiv_id": "2507.02576",
        "ARXIVID": "2507.02576",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02841": {
        "authors": [
            "Kaiyi Zhang",
            "Ang Lv",
            "Jinpeng Li",
            "Yongbo Wang",
            "Feng Wang",
            "Haoyuan Hu",
            "Rui Yan"
        ],
        "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason",
        "abstract": "arXiv:2507.02841v1 Announce Type: new  Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.",
        "arxiv_id": "2507.02841",
        "ARXIVID": "2507.02841",
        "COMMENT": "Does not match any specific criterion but is tangentially related to reinforcement learning and reasoning in LLMs, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02663": {
        "authors": [
            "Yongjiang Liu",
            "Haoxi Li",
            "Xiaosong Ma",
            "Jie Zhang",
            "Song Guo"
        ],
        "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models",
        "abstract": "arXiv:2507.02663v1 Announce Type: new  Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities in handling complex reasoning tasks, but are hindered by excessive overthinking. To explore its essence, our empirical analysis reveals that LRMs are primarily limited to recognizing task properties (i.e., difficulty levels) like humans before solving the problem, leading to a one-size-fits-all reasoning process. Inspired by this, a pressing and natural question emerges: Can we bootstrap such ability to further alleviate the overthinking phenomenon in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning strategy that progressively inspires LRMs' difficulty cognition and redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes of model outputs to intervene in the internal reasoning trajectory. Combined with a heterogeneous short and long reasoning dataset, the trained model enhances its sensitivity to task difficulty, enabling native, differentiated reasoning strategies across various tasks. Second, we further extend redundancy-hypnosis to the internal reasoning process, guiding the model to identify redundant structures within the reasoning steps and generate more concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that TH2T significantly reduces inference costs (more than 70% on easy tasks and 40% on hard tasks) while maintaining performance stability. The resulting outputs exhibit clear difficulty-aware capabilities and reduced redundancy (e.g., reflection).",
        "arxiv_id": "2507.02663",
        "ARXIVID": "2507.02663",
        "COMMENT": "Does not match any specific criterion but is tangentially related to reasoning in LLMs, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02083": {
        "authors": [
            "Haonan Duan",
            "Stephen Zhewen Lu",
            "Caitlin Fiona Harrigan",
            "Nishkrit Desai",
            "Jiarui Lu",
            "Micha{\\l} Koziarski",
            "Leonardo Cotta",
            "Chris J. Maddison"
        ],
        "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab",
        "abstract": "arXiv:2507.02083v1 Announce Type: new  Abstract: Designing experiments and result interpretations are core scientific competencies, particularly in biology, where researchers perturb complex systems to uncover the underlying systems. Recent efforts to evaluate the scientific capabilities of large language models (LLMs) fail to test these competencies because wet-lab experimentation is prohibitively expensive: in expertise, time and equipment. We introduce SciGym, a first-in-class benchmark that assesses LLMs' iterative experiment design and analysis abilities in open-ended scientific discovery tasks. SciGym overcomes the challenge of wet-lab costs by running a dry lab of biological systems. These models, encoded in Systems Biology Markup Language, are efficient for generating simulated data, making them ideal testbeds for experimentation on realistically complex systems. We evaluated six frontier LLMs on 137 small systems, and released a total of 350 systems. Our evaluation shows that while more capable models demonstrated superior performance, all models' performance declined significantly as system complexity increased, suggesting substantial room for improvement in the scientific capabilities of LLM agents.",
        "arxiv_id": "2507.02083",
        "ARXIVID": "2507.02083",
        "COMMENT": "Does not match any specific criterion but is tangentially related to the general interest area of evaluating LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02477": {
        "authors": [
            "Gaochao Song",
            "Zibo Zhao",
            "Haohan Weng",
            "Jingbo Zeng",
            "Rongfei Jia",
            "Shenghua Gao"
        ],
        "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk",
        "abstract": "arXiv:2507.02477v1 Announce Type: new  Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation tailored to generate the polygon mesh in an auto-regressive manner akin to silk weaving. Existing mesh tokenization methods always produce token sequences with repeated vertex tokens, wasting the network capability. Therefore, our approach tokenizes mesh vertices by accessing each mesh vertice only once, reduces the token sequence's redundancy by 50\\%, and achieves a state-of-the-art compression rate of approximately 22\\%. Furthermore, Mesh Silksong produces polygon meshes with superior geometric properties, including manifold topology, watertight detection, and consistent face normals, which are critical for practical applications. Experimental results demonstrate the effectiveness of our approach, showcasing not only intricate mesh generation but also significantly improved geometric integrity.",
        "arxiv_id": "2507.02477",
        "ARXIVID": "2507.02477",
        "COMMENT": "Does not match any specific criteria but is related to mesh generation and compression.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02714": {
        "authors": [
            "Yuxuan Wang",
            "Tianwei Cao",
            "Huayu Zhang",
            "Zhongjiang He",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models",
        "abstract": "arXiv:2507.02714v1 Announce Type: new  Abstract: Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.",
        "arxiv_id": "2507.02714",
        "ARXIVID": "2507.02714",
        "COMMENT": "Does not match any specific criteria but is related to improving human image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02197": {
        "authors": [
            "Amogh Mannekote",
            "Adam Davies",
            "Guohao Li",
            "Kristy Elizabeth Boyer",
            "ChengXiang Zhai",
            "Bonnie J Dorr",
            "Francesco Pinto"
        ],
        "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust",
        "abstract": "arXiv:2507.02197v1 Announce Type: new  Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic data for human behavioral research, ensuring that their outputs remain coherent with their assigned roles has become a critical concern. In this paper, we investigate how consistently LLM-based role-playing agents' stated beliefs about the behavior of the people they are asked to role-play (\"what they say\") correspond to their actual behavior during role-play (\"how they act\"). Specifically, we establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance. Using an augmented version of the GenAgents persona bank and the Trust Game (a standard economic game used to quantify players' trust and reciprocity), we introduce a belief-behavior consistency metric to systematically investigate how it is affected by factors such as: (1) the types of beliefs we elicit from LLMs, like expected outcomes of simulations versus task-relevant attributes of individual characters LLMs are asked to simulate; (2) when and how we present LLMs with relevant information about Trust Game; and (3) how far into the future we ask the model to forecast its actions. We also explore how feasible it is to impose a researcher's own theoretical priors in the event that the originally elicited beliefs are misaligned with research objectives. Our results reveal systematic inconsistencies between LLMs' stated (or imposed) beliefs and the outcomes of their role-playing simulation, at both an individual- and population-level. Specifically, we find that, even when models appear to encode plausible beliefs, they may fail to apply them in a consistent way. These findings highlight the need to identify how and when LLMs' stated beliefs align with their simulated behavior, allowing researchers to use LLM-based agents appropriately in behavioral studies.",
        "arxiv_id": "2507.02197",
        "ARXIVID": "2507.02197",
        "COMMENT": "Does not match any specific criterion but is tangentially relevant to LLM-based simulations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02405": {
        "authors": [
            "Ayantika Das",
            "Moitreya Chaudhuri",
            "Koushik Bhat",
            "Keerthi Ram",
            "Mihail Bota",
            "Mohanasankar Sivaprakasam"
        ],
        "title": "PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration",
        "abstract": "arXiv:2507.02405v1 Announce Type: new  Abstract: Denoising diffusion models produce high-fidelity image samples by capturing the image distribution in a progressive manner while initializing with a simple distribution and compounding the distribution complexity. Although these models have unlocked new applicabilities, the sampling mechanism of diffusion does not offer means to extract image-specific semantic representation, which is inherently provided by auto-encoders. The encoding component of auto-encoders enables mapping between a specific image and its latent space, thereby offering explicit means of enforcing structures in the latent space. By integrating an encoder with the diffusion model, we establish an auto-encoding formulation, which learns image-specific representations and offers means to organize the latent space. In this work, First, we devise a mechanism to structure the latent space of a diffusion auto-encoding model, towards recognizing region-specific cellular patterns in brain images. We enforce the representations to regress positional information of the patches from high-resolution images. This creates a conducive latent space for differentiating tissue types of the brain. Second, we devise an unsupervised tear artifact restoration technique based on neighborhood awareness, utilizing latent representations and the constrained generation capability of diffusion models during inference. Third, through representational guidance and leveraging the inference time steerable noising and denoising capability of diffusion, we devise an unsupervised JPEG artifact restoration technique.",
        "arxiv_id": "2507.02405",
        "ARXIVID": "2507.02405",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02349": {
        "authors": [
            "Rafic Nader",
            "Vincent L'Allinec",
            "Romain Bourcier",
            "Florent Autrusseau"
        ],
        "title": "Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection",
        "abstract": "arXiv:2507.02349v1 Announce Type: new  Abstract: Intracranial aneurysms (ICA) commonly occur in specific segments of the Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations. An accurate detection of these critical landmarks is necessary for a prompt and efficient diagnosis. We introduce a fully automated landmark detection approach for CoW bifurcations using a two-step neural networks process. Initially, an object detection network identifies regions of interest (ROIs) proximal to the landmark locations. Subsequently, a modified U-Net with deep supervision is exploited to accurately locate the bifurcations. This two-step method reduces various problems, such as the missed detections caused by two landmarks being close to each other and having similar visual characteristics, especially when processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for the anatomical variability of the CoW, which affects the number of detectable landmarks per scan. We assessed the effectiveness of our approach using two cerebral MRA datasets: our In-House dataset which had varying numbers of landmarks, and a public dataset with standardized landmark configuration. Our experimental results demonstrate that our method achieves the highest level of performance on a bifurcation detection task.",
        "arxiv_id": "2507.02349",
        "ARXIVID": "2507.02349",
        "COMMENT": "Does not match any specific criteria but is related to computer vision applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02076": {
        "authors": [
            "Mohammad Ali Alomrani",
            "Yingxue Zhang",
            "Derek Li",
            "Qianyi Sun",
            "Soumyasundar Pal",
            "Zhanguang Zhang",
            "Yaochen Hu",
            "Rohan Deepak Ajwani",
            "Antonios Valkanas",
            "Raika Karimi",
            "Peng Cheng",
            "Yunzhou Wang",
            "Pengyi Liao",
            "Hanrui Huang",
            "Bin Wang",
            "Jianye Hao",
            "Mark Coates"
        ],
        "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs",
        "abstract": "arXiv:2507.02076v1 Announce Type: new  Abstract: Large language models (LLMs) have rapidly progressed into general-purpose agents capable of solving a broad spectrum of tasks. However, current models remain inefficient at reasoning: they apply fixed inference-time compute regardless of task complexity, often overthinking simple problems while underthinking hard ones. This survey presents a comprehensive review of efficient test-time compute (TTC) strategies, which aim to improve the computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy that distinguishes between L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. We benchmark leading proprietary LLMs across diverse datasets, highlighting critical trade-offs between reasoning performance and token usage. Compared to prior surveys on efficient reasoning, our review emphasizes the practical control, adaptability, and scalability of TTC methods. Finally, we discuss emerging trends such as hybrid thinking models and identify key challenges for future work towards making LLMs more computationally efficient, robust, and responsive to user constraints.",
        "arxiv_id": "2507.02076",
        "ARXIVID": "2507.02076",
        "COMMENT": "Does not match any specific criterion but is tangentially related to efficient reasoning in LLMs, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02660": {
        "authors": [
            "Deepak Narayan Gadde",
            "Keerthan Kopparam Radhakrishna",
            "Vaisakh Naduvodi Viswambharan",
            "Aman Kumar",
            "Djones Lettnin",
            "Wolfgang Kunz",
            "Sebastian Simon"
        ],
        "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification",
        "abstract": "arXiv:2507.02660v1 Announce Type: new  Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.",
        "arxiv_id": "2507.02660",
        "ARXIVID": "2507.02660",
        "COMMENT": "Does not match any specific criteria but is related to AI-based hardware design and verification.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02493": {
        "authors": [
            "Luca Parolari",
            "Andrea Cherubini",
            "Lamberto Ballan",
            "Carlo Biffi"
        ],
        "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy",
        "abstract": "arXiv:2507.02493v1 Announce Type: new  Abstract: Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive re-associations between visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at https://github.com/lparolari/temporally-aware-polyp-counting.",
        "arxiv_id": "2507.02493",
        "ARXIVID": "2507.02493",
        "COMMENT": "Does not match any specific criteria but is related to temporal awareness in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02307": {
        "authors": [
            "Haoxuan Li",
            "Chenxu Wei",
            "Haodong Wang",
            "Xiaomeng Hu",
            "Boyuan An",
            "Lingyan Ran",
            "Baosen Zhang",
            "Jin Jin",
            "Omirzhan Taukebayev",
            "Amirkhan Temirbayev",
            "Junrui Liu",
            "Xiuwei Zhang"
        ],
        "title": "Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images",
        "abstract": "arXiv:2507.02307v1 Announce Type: new  Abstract: Change detection typically involves identifying regions with changes between bitemporal images taken at the same location. Besides significant changes, slow changes in bitemporal images are also important in real-life scenarios. For instance, weak changes often serve as precursors to major hazards in scenarios like slopes, dams, and tailings ponds. Therefore, designing a change detection network that simultaneously detects slow and fast changes presents a novel challenge. In this paper, to address this challenge, we propose a change detection network named Flow-CDNet, consisting of two branches: optical flow branch and binary change detection branch. The first branch utilizes a pyramid structure to extract displacement changes at multiple scales. The second one combines a ResNet-based network with the optical flow branch's output to generate fast change outputs. Subsequently, to supervise and evaluate this new change detection framework, a self-built change detection dataset Flow-Change, a loss function combining binary tversky loss and L2 norm loss, along with a new evaluation metric called FEPE are designed. Quantitative experiments conducted on Flow-Change dataset demonstrated that our approach outperforms the existing methods. Furthermore, ablation experiments verified that the two branches can promote each other to enhance the detection performance.",
        "arxiv_id": "2507.02307",
        "ARXIVID": "2507.02307",
        "COMMENT": "Does not match any specific criteria but is related to change detection in images.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02751": {
        "authors": [
            "Mingxin Liu",
            "Peiyuan Zhang",
            "Yuan Liu",
            "Wei Zhang",
            "Yue Zhou",
            "Ning Liao",
            "Ziyang Gong",
            "Junwei Luo",
            "Zhirui Wang",
            "Yi Yu",
            "Xue Yang"
        ],
        "title": "Partial Weakly-Supervised Oriented Object Detection",
        "abstract": "arXiv:2507.02751v1 Announce Type: new  Abstract: The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.",
        "arxiv_id": "2507.02751",
        "ARXIVID": "2507.02751",
        "COMMENT": "Does not match any specific criteria but is related to object detection and weakly supervised learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}