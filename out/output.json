{
    "2510.26160": {
        "authors": [
            "Jiaqi Wang",
            "Xiao Yang",
            "Kai Sun",
            "Parth Suresh",
            "Sanat Sharma",
            "Adam Czyzewski",
            "Derek Andersen",
            "Surya Appini",
            "Arkav Banerjee",
            "Sajal Choudhary",
            "Shervin Ghasemlou",
            "Ziqiang Guan",
            "Akil Iyer",
            "Haidar Khan",
            "Lingkun Kong",
            "Roy Luo",
            "Tiffany Ma",
            "Zhen Qiao",
            "David Tran",
            "Wenfang Xu",
            "Skyler Yeatman",
            "Chen Zhou",
            "Gunveer Gujral",
            "Yinglong Xia",
            "Shane Moon",
            "Nicolas Scheffer",
            "Nirav Shah",
            "Eun Chang",
            "Yue Liu",
            "Florian Metze",
            "Tammy Stark",
            "Zhaleh Feizollahi",
            "Andrea Jessee",
            "Mangesh Pujari",
            "Ahmed Aly",
            "Babak Damavandi",
            "Rakesh Wanga",
            "Anuj Kumar",
            "Rohit Patel",
            "Wen-tau Yih",
            "Xin Luna Dong"
        ],
        "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
        "abstract": "arXiv:2510.26160v1 Announce Type: new  Abstract: Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.",
        "arxiv_id": "2510.26160",
        "ARXIVID": "2510.26160",
        "COMMENT": "Matches criteria 3 and 2. Introduces CRAG-MM, a new comprehensive benchmark for multi-modal retrieval-augmented generation (MM-RAG) in wearable/egocentric scenarios, with multi-turn conversations and visual QA. This is a new benchmark for embodied AI and multi-modal LLMs, with a focus on real-world, egocentric data.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.26800": {
        "authors": [
            "Yukun Huang",
            "Jiwen Yu",
            "Yanning Zhou",
            "Jianan Wang",
            "Xintao Wang",
            "Pengfei Wan",
            "Xihui Liu"
        ],
        "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes",
        "abstract": "arXiv:2510.26800v1 Announce Type: new  Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
        "arxiv_id": "2510.26800",
        "ARXIVID": "2510.26800",
        "COMMENT": "Matches criteria 1 and 4. Presents a unified framework (OmniX) for panoramic generation and perception, leveraging 2D generative priors for panoramic vision tasks and 3D scene generation, with applications to simulation and graphics-ready 3D scenes. Also introduces a large-scale multimodal panorama dataset. This is highly relevant to spatial understanding and vision foundation models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.26125": {
        "authors": [
            "Runsheng Xu",
            "Hubert Lin",
            "Wonseok Jeon",
            "Hao Feng",
            "Yuliang Zou",
            "Liting Sun",
            "John Gorman",
            "Kate Tolstaya",
            "Sarah Tang",
            "Brandyn White",
            "Ben Sapp",
            "Mingxing Tan",
            "Jyh-Jing Hwang",
            "Drago Anguelov"
        ],
        "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
        "abstract": "arXiv:2510.26125v1 Announce Type: new  Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
        "arxiv_id": "2510.26125",
        "ARXIVID": "2510.26125",
        "COMMENT": "Directly matches criterion 3: introduces a new benchmark (WOD-E2E) for embodied AI in end-to-end driving, focusing on challenging long-tail scenarios and proposing a novel evaluation metric. Also relevant to spatial intelligence (criterion 1) and multi-modal learning (criterion 2) due to the synergy with MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.26583": {
        "authors": [
            "Yufeng Cui",
            "Honghao Chen",
            "Haoge Deng",
            "Xu Huang",
            "Xinghang Li",
            "Jirong Liu",
            "Yang Liu",
            "Zhuoyan Luo",
            "Jinsheng Wang",
            "Wenxuan Wang",
            "Yueze Wang",
            "Chengyuan Wang",
            "Fan Zhang",
            "Yingli Zhao",
            "Ting Pan",
            "Xianduo Li",
            "Zecheng Hao",
            "Wenxuan Ma",
            "Zhuo Chen",
            "Yulong Ao",
            "Tiejun Huang",
            "Zhongyuan Wang",
            "Xinlong Wang"
        ],
        "title": "Emu3.5: Native Multimodal Models are World Learners",
        "abstract": "arXiv:2510.26583v1 Announce Type: new  Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.",
        "arxiv_id": "2510.26583",
        "ARXIVID": "2510.26583",
        "COMMENT": "This paper introduces Emu3.5, a new large-scale native multimodal world model (MLLM) with strong vision-language and world-modeling abilities, and a novel inference acceleration method. It matches criterion 2 (new VLLMs/MLLMs) and also touches on criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.26794": {
        "authors": [
            "Jing Lin",
            "Ruisi Wang",
            "Junzhe Lu",
            "Ziqi Huang",
            "Guorui Song",
            "Ailing Zeng",
            "Xian Liu",
            "Chen Wei",
            "Wanqi Yin",
            "Qingping Sun",
            "Zhongang Cai",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation",
        "abstract": "arXiv:2510.26794v1 Announce Type: new  Abstract: Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
        "arxiv_id": "2510.26794",
        "ARXIVID": "2510.26794",
        "COMMENT": "This paper presents a new framework for generalizable 3D human motion generation, including a large-scale dataset, a new diffusion transformer model, and a new benchmark. It matches criterion 3 (embodied AI, new benchmark and methods with novel angles) and is also relevant to generative modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.26796": {
        "authors": [
            "Dongyue Lu",
            "Ao Liang",
            "Tianxin Huang",
            "Xiao Fu",
            "Yuyang Zhao",
            "Baorui Ma",
            "Liang Pan",
            "Wei Yin",
            "Lingdong Kong",
            "Wei Tsang Ooi",
            "Ziwei Liu"
        ],
        "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting",
        "abstract": "arXiv:2510.26796v1 Announce Type: new  Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.",
        "arxiv_id": "2510.26796",
        "ARXIVID": "2510.26796",
        "COMMENT": "Matches criterion 4: SEE4D is a novel pose-free 4D video generation method using auto-regressive video inpainting, advancing practical 4D world modeling from casual videos. The method is relevant to vision foundation models and their applications, and also touches on spatial understanding (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.26802": {
        "authors": [
            "Ziyu Guo",
            "Xinyan Chen",
            "Renrui Zhang",
            "Ruichuan An",
            "Yu Qi",
            "Dongzhi Jiang",
            "Xiangtai Li",
            "Manyuan Zhang",
            "Hongsheng Li",
            "Pheng-Ann Heng"
        ],
        "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
        "abstract": "arXiv:2510.26802v1 Announce Type: new  Abstract: Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",
        "arxiv_id": "2510.26802",
        "ARXIVID": "2510.26802",
        "COMMENT": "Directly matches criterion 2 and 4: empirical study of video models (Veo-3) as zero-shot visual reasoners, introducing a new benchmark (MME-CoF) for chain-of-frame reasoning, with surprising empirical findings about current limitations and strengths.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.25970": {
        "authors": [
            "Sung-Hoon Yoon",
            "Minghan Li",
            "Gaspard Beaudouin",
            "Congcong Wen",
            "Muhammad Rafay Azhar",
            "Mengyu Wang"
        ],
        "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing",
        "abstract": "arXiv:2510.25970v1 Announce Type: new  Abstract: Rectified flow models have become a de facto standard in image generation due to their stable sampling trajectories and high-fidelity outputs. Despite their strong generative capabilities, they face critical limitations in image editing tasks: inaccurate inversion processes for mapping real images back into the latent space, and gradient entanglement issues during editing often result in outputs that do not faithfully reflect the target prompt. Recent efforts have attempted to directly map source and target distributions via ODE-based approaches without inversion; however,these methods still yield suboptimal editing quality. In this work, we propose a flow decomposition-and-aggregation framework built upon an inversion-free formulation to address these limitations. Specifically, we semantically decompose the target prompt into multiple sub-prompts, compute an independent flow for each, and aggregate them to form a unified editing trajectory. While we empirically observe that decomposing the original flow enhances diversity in the target space, generating semantically aligned outputs still requires consistent guidance toward the full target prompt. To this end, we design a projection and soft-aggregation mechanism for flow, inspired by gradient conflict resolution in multi-task learning. This approach adaptively weights the sub-target velocity fields, suppressing semantic redundancy while emphasizing distinct directions, thereby preserving both diversity and consistency in the final edited output. Experimental results demonstrate that our method outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The code is available at https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.",
        "arxiv_id": "2510.25970",
        "ARXIVID": "2510.25970",
        "COMMENT": "Matches criteria 4. Proposes SplitFlow, a novel flow decomposition framework for inversion-free text-to-image editing, building on rectified flow models and introducing a projection/aggregation mechanism inspired by multi-task learning. This is a methodological advance in generative modeling for vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26795": {
        "authors": [
            "Philipp Lindenberger",
            "Paul-Edouard Sarlin",
            "Jan Hosang",
            "Matteo Balice",
            "Marc Pollefeys",
            "Simon Lynen",
            "Eduard Trulls"
        ],
        "title": "Scaling Image Geo-Localization to Continent Level",
        "abstract": "arXiv:2510.26795v1 Announce Type: new  Abstract: Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (>100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.",
        "arxiv_id": "2510.26795",
        "ARXIVID": "2510.26795",
        "COMMENT": "Matches criteria 1. Proposes a hybrid approach for fine-grained image geo-localization at continent scale, combining proxy classification and cross-view retrieval with embeddings of aerial imagery. This is a methodological improvement in spatial understanding for vision systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26114": {
        "authors": [
            "Caoshuo Li",
            "Zengmao Ding",
            "Xiaobin Hu",
            "Bang Li",
            "Donghao Luo",
            "Xu Peng",
            "Taisong Jin",
            "Yongge Liu",
            "Shengwei Han",
            "Jing Yang",
            "Xiaoping He",
            "Feng Gao",
            "AndyPian Wu",
            "SevenShu",
            "Chaoyang Wang",
            "Chengjie Wang"
        ],
        "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research",
        "abstract": "arXiv:2510.26114v1 Announce Type: new  Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.",
        "arxiv_id": "2510.26114",
        "ARXIVID": "2510.26114",
        "COMMENT": "Matches criteria 2. Introduces OracleAgent, a multimodal reasoning agent for Oracle Bone Script research, leveraging LLMs and a large multimodal knowledge base. Demonstrates superior performance over mainstream MLLMs (e.g., GPT-4o) on domain-specific multimodal reasoning and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26270": {
        "authors": [
            "Jiazhen Yuan",
            "Wei Zhao",
            "Zhengbiao Bai"
        ],
        "title": "Graph-Enhanced Policy Optimization in LLM Agent Training",
        "abstract": "arXiv:2510.26270v1 Announce Type: new  Abstract: Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.",
        "arxiv_id": "2510.26270",
        "ARXIVID": "2510.26270",
        "COMMENT": "Matches criterion 3: proposes a new method (Graph-Enhanced Policy Optimization) for LLM agent training in embodied AI settings, explicitly modeling environmental structure for better exploration and credit assignment. Also relevant to spatial intelligence (criterion 1).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26098": {
        "authors": [
            "Chenrui Shi",
            "Zedong Yu",
            "Zhi Gao",
            "Ruining Feng",
            "Enqi Liu",
            "Yuwei Wu",
            "Yunde Jia",
            "Liuyu Xiang",
            "Zhaofeng He",
            "Qing Li"
        ],
        "title": "GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks",
        "abstract": "arXiv:2510.26098v1 Announce Type: new  Abstract: Large vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.",
        "arxiv_id": "2510.26098",
        "ARXIVID": "2510.26098",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): introduces a new benchmark (GUI Knowledge Bench) to analyze and improve VLMs' performance on GUI tasks, revealing knowledge gaps and providing insights for building better GUI agents. Also touches on criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26441": {
        "authors": [
            "Shihab Aaqil Ahamed",
            "Udaya S. K. P. Miriya Thanthrige",
            "Ranga Rodrigo",
            "Muhammad Haris Khan"
        ],
        "title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models",
        "abstract": "arXiv:2510.26441v1 Announce Type: new  Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.",
        "arxiv_id": "2510.26441",
        "ARXIVID": "2510.26441",
        "COMMENT": "This paper proposes a new test-time prompt tuning method for vision-language models, focusing on angular diversity for better calibration. It is a methodological improvement for VLMs (criterion 2) and relevant to vision foundation models (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.26443": {
        "authors": [
            "Rhodri Guerrier",
            "Adam W. Harley",
            "Dima Damen"
        ],
        "title": "PointSt3R: Point Tracking through 3D Grounded Correspondence",
        "abstract": "arXiv:2510.26443v1 Announce Type: new  Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and MASt3R, have shown great potential in 2D and 3D correspondence in static scenes. In this paper, we propose to adapt them for the task of point tracking through 3D grounded correspondence. We first demonstrate that these models are competitive point trackers when focusing on static points, present in current point tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose to combine the reconstruction loss with training for dynamic correspondence along with a visibility head, and fine-tuning MASt3R for point tracking using a relatively small amount of synthetic data. Importantly, we only train and evaluate on pairs of frames where one contains the query point, effectively removing any temporal context. Using a mix of dynamic and static point correspondences, we achieve competitive or superior point tracking results on four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\% occlusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs 82.8). We also present results on 3D point tracking along with several ablations on training datasets and percentage of dynamic correspondences.",
        "arxiv_id": "2510.26443",
        "ARXIVID": "2510.26443",
        "COMMENT": "This paper adapts foundational 3D reconstruction models for point tracking via 3D grounded correspondence, with new training strategies and competitive results. It is a methodological improvement in spatial intelligence for embodied agents (criterion 1), and also relates to vision foundation models (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.26614": {
        "authors": [
            "Christoffer Koo {\\O}hrstr{\\o}m",
            "Ronja G\\\"uldenring",
            "Lazaros Nalpantidis"
        ],
        "title": "Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras",
        "abstract": "arXiv:2510.26614v1 Announce Type: new  Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches, specifically designed for event cameras. Given a stream of asynchronous and spatially sparse events, our goal is to discover an event representation that preserves these properties. Prior works have represented events as frames or as voxels. However, while these representations yield high accuracy, both frames and voxels are synchronous and decrease the spatial sparsity. Spiking Patches gives the means to preserve the unique properties of event cameras and we show in our experiments that this comes without sacrificing accuracy. We evaluate our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and object detection. Tokens from Spiking Patches yield inference times that are up to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We achieve this while matching their accuracy and even surpassing in some cases with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for object detection. Thus, tokenization constitutes a novel direction in event-based vision and marks a step towards methods that preserve the properties of event cameras.",
        "arxiv_id": "2510.26614",
        "ARXIVID": "2510.26614",
        "COMMENT": "This paper introduces a new tokenization method for event cameras, preserving asynchronous and sparse properties for efficient event-based vision. It is a methodological improvement in spatial understanding (criterion 1), especially for event-based vision.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.26173": {
        "authors": [
            "Wontae Choi",
            "Jaelin Lee",
            "Hyung Sup Yun",
            "Byeungwoo Jeon",
            "Il Yong Chun"
        ],
        "title": "MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models",
        "abstract": "arXiv:2510.26173v1 Announce Type: new  Abstract: Accurate estimation of motion information is crucial in diverse computational imaging and computer vision applications. Researchers have investigated various methods to extract motion information from a single blurred image, including blur kernels and optical flow. However, existing motion representations are often of low quality, i.e., coarse-grained and inaccurate. In this paper, we propose the first high-resolution (HR) Motion Trajectory estimation framework using Diffusion models (MoTDiff). Different from existing motion representations, we aim to estimate an HR motion trajectory with high-quality from a single motion-blurred image. The proposed MoTDiff consists of two key components: 1) a new conditional diffusion framework that uses multi-scale feature maps extracted from a single blurred image as a condition, and 2) a new training method that can promote precise identification of a fine-grained motion trajectory, consistent estimation of overall shape and position of a motion path, and pixel connectivity along a motion trajectory. Our experiments demonstrate that the proposed MoTDiff can outperform state-of-the-art methods in both blind image deblurring and coded exposure photography applications.",
        "arxiv_id": "2510.26173",
        "ARXIVID": "2510.26173",
        "COMMENT": "This paper proposes a new diffusion-based method for high-resolution motion trajectory estimation from a single blurred image. It is a methodological improvement in spatial understanding for vision tasks, which aligns with criterion 1 (new methodological improvements to spatial understanding).",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.26315": {
        "authors": [
            "Junlai Qiu",
            "Yunzhu Chen",
            "Hao Zheng",
            "Yawen Huang",
            "Yuexiang Li"
        ],
        "title": "A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading",
        "abstract": "arXiv:2510.26315v1 Announce Type: new  Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged and elderly people, which significantly impacts their daily lives and mental health. To improve the efficiency of clinical screening and enable the early detection of DR, a variety of automated DR diagnosis systems have been recently established based on convolutional neural network (CNN) or vision Transformer (ViT). However, due to the own shortages of CNN / ViT, the performance of existing methods using single-type backbone has reached a bottleneck. One potential way for the further improvements is integrating different kinds of backbones, which can fully leverage the respective strengths of them (\\emph{i.e.,} the local feature extraction capability of CNN and the global feature capturing ability of ViT). To this end, we propose a novel paradigm to effectively fuse the features extracted by different backbones based on the theory of evidence. Specifically, the proposed evidential fusion paradigm transforms the features from different backbones into supporting evidences via a set of deep evidential networks. With the supporting evidences, the aggregated opinion can be accordingly formed, which can be used to adaptively tune the fusion pattern between different backbones and accordingly boost the performance of our hybrid model. We evaluated our method on two publicly available DR grading datasets. The experimental results demonstrate that our hybrid model not only improves the accuracy of DR grading, compared to the state-of-the-art frameworks, but also provides the excellent interpretability for feature fusion and decision-making.",
        "arxiv_id": "2510.26315",
        "ARXIVID": "2510.26315",
        "COMMENT": "This paper proposes a hybrid CNN/ViT model for diabetic retinopathy grading, using evidential fusion. While it is a vision foundation model application (criterion 4), the methodological novelty is more in the fusion technique than in spatial intelligence or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.26732": {
        "authors": [
            "J. de Curt\\`o",
            "I. de Zarz\\`a",
            "Pablo Garc\\'ia",
            "Jordi Cabot"
        ],
        "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
        "abstract": "arXiv:2510.26732v1 Announce Type: new  Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).   We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.   The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.",
        "arxiv_id": "2510.26732",
        "ARXIVID": "2510.26732",
        "COMMENT": "This paper introduces a new cross-platform benchmark for evaluating reasoning in foundation models, with a focus on infrastructure-agnostic evaluation and surprising empirical findings about scaling and data quality. Closely matches criterion 4 (vision foundation models and applications), though the focus is more on reasoning than vision per se.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.26049": {
        "authors": [
            "Yuyue Zhou",
            "Jessica Knight",
            "Shrimanti Ghosh",
            "Banafshe Felfeliyan",
            "Jacob L. Jaremko",
            "Abhilash R. Hareendranathan"
        ],
        "title": "FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation",
        "abstract": "arXiv:2510.26049v1 Announce Type: new  Abstract: Elbow and wrist fractures are the most common fractures in pediatric populations. Automatic segmentation of musculoskeletal structures in ultrasound (US) can improve diagnostic accuracy and treatment planning. Fractures appear as cortical defects but require expert interpretation. Deep learning (DL) can provide real-time feedback and highlight key structures, helping lightly trained users perform exams more confidently. However, pixel-wise expert annotations for training remain time-consuming and costly. To address this challenge, we propose FlexICL, a novel and flexible in-context learning (ICL) framework for segmenting bony regions in US images. We apply it to an intra-video segmentation setting, where experts annotate only a small subset of frames, and the model segments unseen frames. We systematically investigate various image concatenation techniques and training strategies for visual ICL and introduce novel concatenation methods that significantly enhance model performance with limited labeled data. By integrating multiple augmentation strategies, FlexICL achieves robust segmentation performance across four wrist and elbow US datasets while requiring only 5% of the training images. It outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and conventional segmentation models like U-Net and TransUNet by 1-27% Dice coefficient on 1,252 US sweeps. These initial results highlight the potential of FlexICL as an efficient and scalable solution for US image segmentation well suited for medical imaging use cases where labeled data is scarce.",
        "arxiv_id": "2510.26049",
        "ARXIVID": "2510.26049",
        "COMMENT": "This paper introduces a flexible in-context learning framework for ultrasound segmentation, with novel concatenation and augmentation strategies. It is a methodological improvement in spatial understanding (criterion 1), though focused on a medical imaging application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.26268": {
        "authors": [
            "Lin Guo",
            "Xiaoqing Luo",
            "Wei Xie",
            "Zhancheng Zhang",
            "Hui Li",
            "Rui Wang",
            "Zhenhua Feng",
            "Xiaoning Song"
        ],
        "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws",
        "abstract": "arXiv:2510.26268v1 Announce Type: new  Abstract: Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality.",
        "arxiv_id": "2510.26268",
        "ARXIVID": "2510.26268",
        "COMMENT": "This paper introduces a generative image fusion method for infrared and visible images, inspired by human cognitive laws and using diffusion models. It is relevant to vision foundation models and generative modeling (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.26601": {
        "authors": [
            "Anirban Ray",
            "Vera Galinova",
            "Florian Jug"
        ],
        "title": "ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching",
        "abstract": "arXiv:2510.26601v1 Announce Type: new  Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions.",
        "arxiv_id": "2510.26601",
        "ARXIVID": "2510.26601",
        "COMMENT": "Proposes a new computational super-resolution method (ResMatching) using guided conditional flow matching. This is an application of generative modeling in vision, but does not directly address spatial intelligence, embodied AI, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2510.25990": {
        "authors": [
            "Valentin Boussot",
            "C\\'edric H\\'emon",
            "Jean-Claude Nunes",
            "Jean-Louis Dillenseger"
        ],
        "title": "Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI",
        "abstract": "arXiv:2510.25990v1 Announce Type: new  Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences of the thoracic and abdominal regions under strong data scarcity constraints. Two complementary strategies were explored: (i) unsupervised registration with the IMPACT similarity metric and (ii) foundation model-based segmentation leveraging SAM 2.1 and its recent variants through prompt-based interaction. Due to the one-second runtime constraint, the SAM-based method was ultimately selected. The final configuration used SAM2.1 b+ with mask-based prompts from the first annotated slice, fine-tuned solely on the small labeled subset from TrackRAD2025. Training was configured to minimize overfitting, using 1024x1024 patches (batch size 1), standard augmentations, and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve generalization while adapting to annotator-specific styles. Training lasted 300 epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently applied across all anatomical sites and MRI field strengths. Test-time augmentation was considered but ultimately discarded due to negligible performance gains. The final model was selected based on the highest Dice Similarity Coefficient achieved on the validation set after fine-tuning. On the hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall in the TrackRAD2025 challenge. These results highlight the strong potential of foundation models for accurate and real-time tumor tracking in MRI-guided radiotherapy.",
        "arxiv_id": "2510.25990",
        "ARXIVID": "2510.25990",
        "COMMENT": "This paper fine-tunes the Segment Anything Model (SAM) for real-time tumor tracking in MRI, demonstrating the application of vision foundation models (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2510.26167": {
        "authors": [
            "Renhao Li",
            "Jianhong Tu",
            "Yang Su",
            "Hamid Alinejad-Rokny",
            "Derek F. Wong",
            "Junyang Lin",
            "Min Yang"
        ],
        "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning",
        "abstract": "arXiv:2510.26167v1 Announce Type: new  Abstract: Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.",
        "arxiv_id": "2510.26167",
        "ARXIVID": "2510.26167",
        "COMMENT": "Does not match any specific criteria. Proposes ToolRM, a reward model for agentic tool-use, and a new dataset for function-calling tasks. While relevant to agentic AI, it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.26149": {
        "authors": [
            "Wei Shang",
            "Wanying Zhang",
            "Shuhang Gu",
            "Pengfei Zhu",
            "Qinghua Hu",
            "Dongwei Ren"
        ],
        "title": "BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation",
        "abstract": "arXiv:2510.26149v1 Announce Type: new  Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution of video frames, potentially at various scaling factors, which presents several challenges regarding spatial detail reproduction, temporal consistency, and computational complexity. In this paper, we propose a strong baseline BasicAVSR for AVSR by integrating four key components: 1) adaptive multi-scale frequency priors generated from image Laplacian pyramids, 2) a flow-guided propagation unit to aggregate spatiotemporal information from adjacent frames, 3) a second-order motion compensation unit for more accurate spatial alignment of adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and content-independent upsampling kernels. To meet diverse application demands, we instantiate three propagation variants: (i) a unidirectional RNN unit for strictly online inference, (ii) a unidirectional RNN unit empowered with a limited lookahead that tolerates a small output delay, and (iii) a bidirectional RNN unit designed for offline tasks where computational resources are less constrained. Experimental results demonstrate the effectiveness and adaptability of our model across these different scenarios. Through extensive experiments, we show that BasicAVSR significantly outperforms existing methods in terms of super-resolution quality, generalization ability, and inference speed. Our work not only advances the state-of-the-art in AVSR but also extends its core components to multiple frameworks for diverse scenarios. The code is available at https://github.com/shangwei5/BasicAVSR.",
        "arxiv_id": "2510.26149",
        "ARXIVID": "2510.26149",
        "COMMENT": "Does not match any specific criteria. Focuses on video super-resolution using image priors and motion compensation, which is relevant to computer vision but not to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.26752": {
        "authors": [
            "William Overman",
            "Mohsen Bayati"
        ],
        "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy",
        "abstract": "arXiv:2510.26752v1 Announce Type: new  Abstract: As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.",
        "arxiv_id": "2510.26752",
        "ARXIVID": "2510.26752",
        "COMMENT": "This paper is about human-AI oversight and safety in agent autonomy, modeled as a Markov game. It does not match any of the specific criteria, though it is tangentially related to embodied AI safety.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.26569": {
        "authors": [
            "Wen Xie",
            "Yanjun Zhu",
            "Gijs Overgoor",
            "Yakov Bart",
            "Agata Lapedriza Garcia",
            "Sarah Ostadabbas"
        ],
        "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping",
        "abstract": "arXiv:2510.26569v1 Announce Type: new  Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.",
        "arxiv_id": "2510.26569",
        "ARXIVID": "2510.26569",
        "COMMENT": "Presents a two-stream audio-visual model for video ad summarization. While it is multi-modal, it does not directly address any of the four criteria, as it is not about spatial intelligence, embodied AI, VLLMs/MLLMs, or vision foundation models in a general sense.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}