{
    "2508.02095": {
        "authors": [
            "Shijie Zhou",
            "Alexander Vilesov",
            "Xuehai He",
            "Ziyu Wan",
            "Shuwang Zhang",
            "Aditya Nagachandra",
            "Di Chang",
            "Dongdong Chen",
            "Xin Eric Wang",
            "Achuta Kadambi"
        ],
        "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
        "abstract": "arXiv:2508.02095v1 Announce Type: new  Abstract: Vision language models (VLMs) have shown remarkable capabilities in integrating linguistic and visual reasoning but remain fundamentally limited in understanding dynamic spatiotemporal interactions. Humans effortlessly track and reason about object movements, rotations, and perspective shifts-abilities essential for robust dynamic real-world understanding yet notably lacking in current VLMs. In this paper, we introduce VLM4D, the first benchmark specifically designed to evaluate the spatiotemporal reasoning capabilities of VLMs. Our benchmark comprises diverse real-world and synthetic videos accompanied by carefully curated question-answer pairs emphasizing translational and rotational motions, perspective awareness, and motion continuity. Through comprehensive evaluations of state-of-the-art open and closed-source VLMs, we identify significant performance gaps compared to human baselines, highlighting fundamental deficiencies in existing models. Extensive analysis reveals that VLMs struggle particularly with integrating multiple visual cues and maintaining temporal coherence. We further explore promising directions, such as leveraging 4D feature field reconstruction and targeted spatiotemporal supervised fine-tuning, demonstrating their effectiveness in enhancing spatiotemporal comprehension. Our work aims to encourage deeper exploration into improving VLMs' spatial and temporal grounding, paving the way towards more capable and reliable visual intelligence for dynamic environments.",
        "arxiv_id": "2508.02095",
        "ARXIVID": "2508.02095",
        "COMMENT": "Matches criterion 3: Embodied AI benchmark for spatiotemporal reasoning in VLMs. Also matches criterion 2: Evaluates and improves VLLMs for dynamic spatial/temporal understanding. Introduces a new benchmark (VLM4D) and analyzes model deficiencies.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.02134": {
        "authors": [
            "Kuo Wang",
            "Quanlong Zheng",
            "Junlin Xie",
            "Yanhao Zhang",
            "Jinguo Luo",
            "Haonan Lu",
            "Liang Lin",
            "Fan Zhou",
            "Guanbin Li"
        ],
        "title": "Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference",
        "abstract": "arXiv:2508.02134v1 Announce Type: new  Abstract: Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable advancements in video understanding tasks. However, constrained by the context length limitation in the underlying LLMs, existing Video-MLLMs typically exhibit suboptimal performance on long video scenarios. To understand extended input frames, common solutions span token compression and streaming inference techniques, which sacrifice feature granularity or inference efficiency. Differently, to efficiently achieve comprehensive understanding of longer frame inputs, we draw ideas from MoE and propose a training-free approach \\textbf{Free-MoRef}, which instantly multiplexes the context perception capabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef reconstructs the vision tokens into several short sequences as multi-references. Subsequently, we introduce MoRef-attention, which gathers clues from the multi-reference chunks in parallel to summarize unified query activations. After the shadow layers in LLMs, a reference fusion step is derived to compose a final mixed reasoning sequence with key tokens from parallel chunks, which compensates the cross-reference vision interactions that are neglected in MoRef-attention. By splitting and fusing the long vision token sequences, Free-MoRef achieves improved performance under much lower computing costs in reasoning multiplexed context length, demonstrating strong efficiency and effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that Free-MoRef achieves full perception of 2$\\times$ to 8$\\times$ longer input frames without compression on a single A100 GPU while keeping instant responses, thereby bringing significant performance gains, even surpassing dedicatedly trained long-video-MLLMs. Codes are available at https://github.com/wkfdb/Free-MoRef",
        "arxiv_id": "2508.02134",
        "ARXIVID": "2508.02134",
        "COMMENT": "This paper proposes Free-MoRef, a training-free method to extend the context perception of Video-MLLMs for long video understanding, using a MoE-inspired approach to multiplex context within a single inference. It directly matches criterion 2 (new MLLMs/VLLMs) and is highly relevant to vision-language models and multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.01274": {
        "authors": [
            "Jui-Ming Yao",
            "Bing-Cheng Xie",
            "Sheng-Wei Peng",
            "Hao-Yuan Chen",
            "He-Rong Zheng",
            "Bing-Jia Tan",
            "Peter Shaojui Wang",
            "Shun-Feng Su"
        ],
        "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan",
        "abstract": "arXiv:2508.01274v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and textual inputs, addressing the limitations of single-modality LLMs. However, existing benchmarks often overlook tri-modal evaluation in Traditional Chinese and do not consider inference latency. To address this, we introduce Multi-TW, the first Traditional Chinese benchmark for evaluating the performance and latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice questions (image and text, audio and text pairs) sourced from official proficiency tests developed with the Steering Committee for the Test of Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and vision-language models (VLMs) with audio transcription. Our results show that closed-source models generally outperform open-source ones across modalities, although open-source models can perform well in audio tasks. End-to-end any-to-any pipelines offer clear latency advantages compared to VLMs using separate audio transcription. Multi-TW presents a comprehensive view of model capabilities and highlights the need for Traditional Chinese fine-tuning and efficient multimodal architectures.",
        "arxiv_id": "2508.01274",
        "ARXIVID": "2508.01274",
        "COMMENT": "Multi-TW introduces a new benchmark for evaluating multimodal models (MLLMs and VLMs) on Traditional Chinese question answering, including tri-modal evaluation and latency. This is a direct match to criterion 3 (new embodied AI benchmark, novel evaluation angle) and criterion 2 (MLLMs/VLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.02324": {
        "authors": [
            "Chenfei Wu",
            "Jiahao Li",
            "Jingren Zhou",
            "Junyang Lin",
            "Kaiyuan Gao",
            "Kun Yan",
            "Sheng-ming Yin",
            "Shuai Bai",
            "Xiao Xu",
            "Yilei Chen",
            "Yuxiang Chen",
            "Zecheng Tang",
            "Zekai Zhang",
            "Zhengyi Wang",
            "An Yang",
            "Bowen Yu",
            "Chen Cheng",
            "Dayiheng Liu",
            "Deqing Li",
            "Hang Zhang",
            "Hao Meng",
            "Hu Wei",
            "Jingyuan Ni",
            "Kai Chen",
            "Kuan Cao",
            "Liang Peng",
            "Lin Qu",
            "Minggang Wu",
            "Peng Wang",
            "Shuting Yu",
            "Tingkun Wen",
            "Wensen Feng",
            "Xiaoxiao Xu",
            "Yi Wang",
            "Yichang Zhang",
            "Yongqiang Zhu",
            "Yujia Wu",
            "Yuxuan Cai",
            "Zenan Liu"
        ],
        "title": "Qwen-Image Technical Report",
        "abstract": "arXiv:2508.02324v1 Announce Type: new  Abstract: We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.",
        "arxiv_id": "2508.02324",
        "ARXIVID": "2508.02324",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications). Presents Qwen-Image, a new image generation foundation model with strong text rendering and image editing capabilities, and introduces several novel training and data pipeline strategies.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.02464": {
        "authors": [
            "Yonghuang Wu",
            "Wenwen Zeng",
            "Xuan Xie",
            "Chengqian Zhao",
            "Guoqing Wu",
            "Jinhua Yu"
        ],
        "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models",
        "abstract": "arXiv:2508.02464v1 Announce Type: new  Abstract: Foundation models like Segment Anything Model (SAM) excel in promptable segmentation but suffer from an intent gap: they segment only explicitly prompted objects, failing to generalize to semantically related instances implicitly desired by users. This limitation is critical in domains with dense homogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual prompts typically yield incomplete results, rendering dense annotations impractical due to prohibitive cost. To bridge this gap, we introduce SAMPO (Segment Anything Model with Preference Optimization), a novel framework that teaches visual foundation models to infer high-level categorical intent from sparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO optimizes models to implicitly capture target-class characteristics through preference optimization. This approach, which operates without dependency on language models, enables robust multi-object segmentation even under sparse prompting and demonstrates superior data efficiency during fine-tuning. Validated on three medical segmentation tasks, SAMPO achieves state-of-the-art performance: on challenging tasks like PanNuke-T2, our method, when fine-tuned with only 10% of the training data, significantly outperforms all existing methods trained on the full 100% dataset, achieving an improvement of over 9 percentage points compared to the best baseline. Our work establishes a new paradigm for intent-aware alignment in visual foundation models, removing dependencies on auxiliary prompt generators or language-model-assisted preference learning.",
        "arxiv_id": "2508.02464",
        "ARXIVID": "2508.02464",
        "COMMENT": "Matches criterion 4: This paper introduces SAMPO, a framework for intent-aware segmentation using vision foundation models (e.g., SAM), optimizing for user intent with preference optimization. It demonstrates a new paradigm for aligning visual foundation models to user intent, especially in dense object domains.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.01150": {
        "authors": [
            "Dianyi Yang",
            "Xihan Wang",
            "Yu Gao",
            "Shiyang Liu",
            "Bohan Ren",
            "Yufeng Yue",
            "Yi Yang"
        ],
        "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding",
        "abstract": "arXiv:2508.01150v1 Announce Type: new  Abstract: Recent advancements in 3D scene understanding have made significant strides in enabling interaction with scenes using open-vocabulary queries, particularly for VR/AR and robotic applications. Nevertheless, existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries. In this paper, we present OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding. OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, we introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds, achieving an improvement 17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments demonstrate that our method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction. The code is available at https://young-bit.github.io/opengs-fusion.github.io/ .",
        "arxiv_id": "2508.01150",
        "ARXIVID": "2508.01150",
        "COMMENT": "This paper introduces OpenGS-Fusion, a new open-vocabulary dense mapping framework for 3D scene understanding, combining 3D Gaussian splatting and MLLM-assisted adaptive thresholding. It directly matches criterion 1 (spatial understanding on embodied agents), criterion 3 (novel embodied AI methods for mapping and segmentation), and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.02028": {
        "authors": [
            "Tianyuan Zhang",
            "Ting Jin",
            "Lu Wang",
            "Jiangfan Liu",
            "Siyuan Liang",
            "Mingchuan Zhang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving",
        "abstract": "arXiv:2508.02028v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions.",
        "arxiv_id": "2508.02028",
        "ARXIVID": "2508.02028",
        "COMMENT": "Strongly matches criterion 3 (embodied AI, new benchmark/simulator). Introduces Bench2ADVLM, a closed-loop benchmark for VLMs in autonomous driving, with a novel hierarchical evaluation pipeline and scenario generation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.01766": {
        "authors": [
            "Shuo Feng",
            "Zihan Wang",
            "Yuchen Li",
            "Rui Kong",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Gim Hee Lee",
            "Piji Li",
            "Shuqiang Jiang"
        ],
        "title": "VPN: Visual Prompt Navigation",
        "abstract": "arXiv:2508.01766v1 Announce Type: new  Abstract: While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.",
        "arxiv_id": "2508.01766",
        "ARXIVID": "2508.01766",
        "COMMENT": "Matches criterion 1: New methodological improvement for spatial intelligence in embodied agents (visual prompt navigation instead of language). Also matches criterion 3: Proposes new tasks and datasets for navigation with visual prompts.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.02605": {
        "authors": [
            "Zhengdao Li",
            "Siheng Wang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
        "abstract": "arXiv:2508.02605v1 Announce Type: new  Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.",
        "arxiv_id": "2508.02605",
        "ARXIVID": "2508.02605",
        "COMMENT": "ReMoMask proposes a retrieval-augmented masked motion generation framework for text-to-motion (T2M) tasks, integrating cross-modal retrieval, semantic spatio-temporal attention, and classifier-free guidance. This is a strong match to criterion 2 (new VLLMs/MLLMs, multi-modal generative modeling) and also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.01558": {
        "authors": [
            "Kun Ding",
            "Ying Wang",
            "Shiming Xiang"
        ],
        "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation",
        "abstract": "arXiv:2508.01558v1 Announce Type: new  Abstract: Pre-trained Vision-Language Models (VLMs) have been exploited in various Computer Vision tasks (e.g., few-shot recognition) via model adaptation, such as prompt tuning and adapters. However, existing adaptation methods are designed by human experts, requiring significant time cost and experience. Inspired by recent advances in Large Language Models (LLMs) based code generation, we propose an Evolutionary Vision-Language Model Adaptation (EvoVLMA) method to automatically search training-free efficient adaptation algorithms for VLMs. We recognize feature selection and logits computation as the key functions in training-free VLM adaptation, and propose a two-stage LLM-assisted evolutionary algorithm for optimizing these parts in a sequential manner, effectively addressing the challenge posed by the expansive search space through a divide-and-conquer strategy. Besides, to enhance the stability and efficiency of searching process, we propose low-precision code conversion, web based code execution and process monitoring, leading to a highly effective automatic algorithm design system. Extensive experiments demonstrate that the algorithms found by EvoVLMA can obtain promising results compared to previous manually-designed ones. More specifically, in the 8-shot image classification setting, the classical APE algorithm can be improved by 1.91 points in recognition accuracy. This research opens new possibilities for automating the optimization of adaptation algorithms of pre-trained multimodal models. Code is available at: https://github.com/kding1225/EvoVLMA",
        "arxiv_id": "2508.01558",
        "ARXIVID": "2508.01558",
        "COMMENT": "EvoVLMA proposes an automated, LLM-assisted evolutionary search for adaptation algorithms in pre-trained vision-language models (VLMs). This is a direct match to criterion 4 (vision foundation models and applications) and also relevant to criterion 2 (VLLMs/MLLMs, new methods for adaptation).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.02669": {
        "authors": [
            "Xiaoke Huang",
            "Juncheng Wu",
            "Hui Liu",
            "Xianfeng Tang",
            "Yuyin Zhou"
        ],
        "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
        "abstract": "arXiv:2508.02669v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have introduced a new paradigm in AI by enabling models to ``think before responding\" via chain-of-thought reasoning. However, the absence of open and reproducible recipes for building reasoning-centric medical LMMs hinders community-wide research, analysis, and comparison. In this paper, we present MedVLThinker, a suite of simple yet strong baselines. Our fully open recipe consists of: (1) systematic data curation for both text-only and image-text medical data, filtered according to varying levels of reasoning difficulty, and (2) two training paradigms: Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement Learning with Verifiable Rewards (RLVR) based on final answer correctness. Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six medical QA benchmarks, we find that RLVR consistently and significantly outperforms SFT. Additionally, under the RLVR framework, a key, counter-intuitive finding is that training on our curated text-only reasoning data provides a more substantial performance boost than training on multimodal image-text data. Our best open 7B model, trained using the RLVR recipe on text-only data, establishes a new state-of-the-art on existing public VQA benchmarks, surpassing all previous open-source medical LMMs. Furthermore, scaling our model to 32B achieves performance on par with the proprietary GPT-4o. We release all curated data, models, and code to provide the community with a strong, open foundation for future research in multimodal medical reasoning.",
        "arxiv_id": "2508.02669",
        "ARXIVID": "2508.02669",
        "COMMENT": "This paper presents MedVLThinker, a suite of open baselines for multimodal medical reasoning, including new training paradigms for medical VLLMs/MLLMs. It provides empirical insights and a new state-of-the-art open-source medical VLLM. Directly matches criterion 2 (new VLLMs/MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.02129": {
        "authors": [
            "Yuru Xiao",
            "Zihan Lin",
            "Chao Lu",
            "Deming Zhai",
            "Kui Jiang",
            "Wenbo Zhao",
            "Wei Zhang",
            "Junjun Jiang",
            "Huanran Wang",
            "Xianming Liu"
        ],
        "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling",
        "abstract": "arXiv:2508.02129v1 Announce Type: new  Abstract: Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.",
        "arxiv_id": "2508.02129",
        "ARXIVID": "2508.02129",
        "COMMENT": "Directly matches criterion 1 (spatial understanding) and criterion 3 (embodied AI/new benchmarks or methods for simulators). Proposes a novel video diffusion-enhanced 4D Gaussian Splatting framework for dynamic urban scene modeling, with innovations in pose alignment and uncertainty distillation, and strong empirical improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.01464": {
        "authors": [
            "Quankai Gao",
            "Iliyan Georgiev",
            "Tuanfeng Y. Wang",
            "Krishna Kumar Singh",
            "Ulrich Neumann",
            "Jae Shin Yoon"
        ],
        "title": "Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians",
        "abstract": "arXiv:2508.01464v1 Announce Type: new  Abstract: 3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.",
        "arxiv_id": "2508.01464",
        "ARXIVID": "2508.01464",
        "COMMENT": "Directly matches criterion 1 (spatial understanding) and criterion 4 (vision foundation models and applications). Introduces Can3Tok, the first 3D scene-level VAE for 3D Gaussian Splatting, enabling scene-level generative modeling and addressing scale inconsistency, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.02429": {
        "authors": [
            "Miaosen Luo",
            "Jiesen Long",
            "Zequn Li",
            "Yunying Yang",
            "Yuncheng Jiang",
            "Sijie Mai"
        ],
        "title": "Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting",
        "abstract": "arXiv:2508.02429v1 Announce Type: new  Abstract: Multimodal Affective Computing (MAC) aims to recognize and interpret human emotions by integrating information from diverse modalities such as text, video, and audio. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly reshaped the landscape of MAC by offering a unified framework for processing and aligning cross-modal information. However, practical challenges remain, including performance variability across complex MAC tasks and insufficient understanding of how architectural designs and data characteristics impact affective analysis. To address these gaps, we conduct a systematic benchmark evaluation of state-of-the-art open-source MLLMs capable of concurrently processing audio, visual, and textual modalities across multiple established MAC datasets. Our evaluation not only compares the performance of these MLLMs but also provides actionable insights into model optimization by analyzing the influence of model architectures and dataset properties. Furthermore, we propose a novel hybrid strategy that combines generative knowledge prompting with supervised fine-tuning to enhance MLLMs' affective computing capabilities. Experimental results demonstrate that this integrated approach significantly improves performance across various MAC tasks, offering a promising avenue for future research and development in this field. Our code is released on https://github.com/LuoMSen/MLLM-MAC.",
        "arxiv_id": "2508.02429",
        "ARXIVID": "2508.02429",
        "COMMENT": "Directly matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Benchmarks and improves state-of-the-art open-source MLLMs for affective computing, introduces a hybrid generative knowledge prompting strategy, and provides actionable insights into model optimization.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.02172": {
        "authors": [
            "Lei Yao",
            "Yi Wang",
            "Yi Zhang",
            "Moyun Liu",
            "Lap-Pui Chau"
        ],
        "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
        "abstract": "arXiv:2508.02172v1 Announce Type: new  Abstract: The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
        "arxiv_id": "2508.02172",
        "ARXIVID": "2508.02172",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Introduces a cross-modal self-supervised 3D representation learning method using Gaussian Splatting, with strong empirical results and generalization, and a novel tri-attribute adaptive distillation module.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.00945": {
        "authors": [
            "Yifan Wang",
            "Hongfeng Ai",
            "Quangao Liu",
            "Maowei Jiang",
            "Ruiyuan Kang",
            "Ruiqi Li",
            "Jiahua Dong",
            "Mengting Xiao",
            "Cheng Jiang",
            "Chenzhong Li"
        ],
        "title": "Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment",
        "abstract": "arXiv:2508.00945v1 Announce Type: new  Abstract: Vision Language Models (VLMs) face challenges in effectively coordinating diverse attention mechanisms for cross-modal embedding learning, leading to mismatched attention and suboptimal performance. We propose Consistent Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross Attention (LPWCA) to capture fine-grained regional-semantic correlations by jointly weighting patch and layer-wise embedding, and Progressive Attention Integration (PAI) that systematically coordinates LPWCA, layer-wise, and patch-wise attention mechanisms in sequence. This progressive design ensures consistency from semantic to regional levels while preventing attention drift and maximizing individual attention benefits. Experimental results on ten diverse vision-language benchmarks demonstrate that our CCRA-enhanced LLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all baseline methods with only 3.55M additional parameters, while providing enhanced interpretability through more regionally focused and semantically aligned attention patterns.",
        "arxiv_id": "2508.00945",
        "ARXIVID": "2508.00945",
        "COMMENT": "Strongly matches criterion 2 (new VLLMs/MLLMs). The paper proposes a new method (CCRA) for improving attention alignment in VLMs, with empirical results on multiple benchmarks and interpretability improvements.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.01740": {
        "authors": [
            "Zhaonan Wang",
            "Manyi Li",
            "Changhe Tu"
        ],
        "title": "AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing",
        "abstract": "arXiv:2508.01740v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.",
        "arxiv_id": "2508.01740",
        "ARXIVID": "2508.01740",
        "COMMENT": "Matches criterion 1 (spatial understanding) and criterion 4 (vision foundation models and applications). Proposes a new anchor-graph structure for semantic-aware 3D Gaussian Splatting, enabling instance-level 3D scene understanding and editing, which is a novel approach.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.01661": {
        "authors": [
            "Zhixuan Li",
            "Yujia Liu",
            "Chen Hui",
            "Weisi Lin"
        ],
        "title": "Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation",
        "abstract": "arXiv:2508.01661v1 Announce Type: new  Abstract: Amodal segmentation aims to recover complete object shapes, including occluded regions with no visual appearance, whereas conventional segmentation focuses solely on visible areas. Existing methods typically rely on strong prompts, such as visible masks or bounding boxes, which are costly or impractical to obtain in real-world settings. While recent approaches such as the Segment Anything Model (SAM) support point-based prompts for guidance, they often perform direct mask regression without explicitly modeling shape evolution, limiting generalization in complex occlusion scenarios. Moreover, most existing methods suffer from a black-box nature, lacking geometric interpretability and offering limited insight into how occluded shapes are inferred. To deal with these limitations, we propose VELA, an end-to-end VElocity-driven Level-set Amodal segmentation method that performs explicit contour evolution from point-based prompts. VELA first constructs an initial level set function from image features and the point input, which then progressively evolves into the final amodal mask under the guidance of a shape-specific motion field predicted by a fully differentiable network. This network learns to generate evolution dynamics at each step, enabling geometrically grounded and topologically flexible contour modeling. Extensive experiments on COCOA-cls, D2SA, and KINS benchmarks demonstrate that VELA outperforms existing strongly prompted methods while requiring only a single-point prompt, validating the effectiveness of interpretable geometric modeling under weak guidance. The code will be publicly released.",
        "arxiv_id": "2508.01661",
        "ARXIVID": "2508.01661",
        "COMMENT": "Matches criterion 4: This paper proposes VELA, a velocity-driven level-set method for amodal segmentation from point prompts, improving geometric interpretability and generalization. It leverages vision foundation models (e.g., SAM) and introduces a novel, interpretable approach to segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.01699": {
        "authors": [
            "Zuhao Yang",
            "Yingchen Yu",
            "Yunqing Zhao",
            "Shijian Lu",
            "Song Bai"
        ],
        "title": "TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding",
        "abstract": "arXiv:2508.01699v1 Announce Type: new  Abstract: Video Temporal Grounding (VTG) aims to precisely identify video event segments in response to textual queries. The outputs of VTG tasks manifest as sequences of events, each defined by precise timestamps, saliency scores, and textual descriptions. Despite recent advances, a fundamental limitation persists in existing Video Large Language Models (Video-LLMs): they process all task tokens through identical and static pathways, failing to recognize that temporal localization, saliency assessment, and textual generation represent fundamentally distinct tasks requiring specialized processing. To address this, we introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that effectively decomposes VTG tasks by dynamically routing task-specific tokens (e.g., timestamps, saliency scores) to specialized experts, with increased computational efficiency. Our design choices enable precise handling of each subtask, leading to improved event modeling across diverse VTG applications. Extensive experiments demonstrate that TimeExpert consistently achieves state-of-the-art performance on various VTG tasks such as Dense Video Captioning, Moment Retrieval, and Video Highlight Detection.",
        "arxiv_id": "2508.01699",
        "ARXIVID": "2508.01699",
        "COMMENT": "Matches criterion 2: This paper introduces TimeExpert, a Mixture-of-Experts Video-LLM for video temporal grounding, with dynamic routing for task-specific tokens. It is a new VLLM with a novel architecture for video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.01984": {
        "authors": [
            "Chen Li",
            "Chinthani Sugandhika",
            "Yeo Keat Ee",
            "Eric Peh",
            "Hao Zhang",
            "Hong Yang",
            "Deepu Rajan",
            "Basura Fernando"
        ],
        "title": "IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A",
        "abstract": "arXiv:2508.01984v1 Announce Type: new  Abstract: Existing human motion Q\\&A methods rely on explicit program execution, where the requirement for manually defined functional modules may limit the scalability and adaptability. To overcome this, we propose an implicit program-guided motion reasoning (IMoRe) framework that unifies reasoning across multiple query types without manually designed modules. Unlike existing implicit reasoning approaches that infer reasoning operations from question words, our model directly conditions on structured program functions, ensuring a more precise execution of reasoning steps. Additionally, we introduce a program-guided reading mechanism, which dynamically selects multi-level motion representations from a pretrained motion Vision Transformer (ViT), capturing both high-level semantics and fine-grained motion cues. The reasoning module iteratively refines memory representations, leveraging structured program functions to extract relevant information for different query types. Our model achieves state-of-the-art performance on Babel-QA and generalizes to a newly constructed motion Q\\&A dataset based on HuMMan, demonstrating its adaptability across different motion reasoning datasets. Code and dataset are available at: https://github.com/LUNAProject22/IMoRe.",
        "arxiv_id": "2508.01984",
        "ARXIVID": "2508.01984",
        "COMMENT": "This paper proposes IMoRe, a new framework for human motion Q&A that unifies reasoning across multiple query types using a program-guided approach and pretrained motion ViT. It is a methodological improvement in spatial reasoning and multi-modal understanding, matching criterion 1 (spatial intelligence on embodied agents) and criterion 3 (novel embodied AI methods for motion reasoning).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.01875": {
        "authors": [
            "Haolin Yang",
            "Feilong Tang",
            "Linxiao Zhao",
            "Xiang An",
            "Ming Hu",
            "Huifa Li",
            "Xinlin Zhuang",
            "Boqian Wang",
            "Yifan Lu",
            "Xiaofeng Zhang",
            "Abdalla Swikir",
            "Junjun He",
            "Zongyuan Ge",
            "Imran Razzak"
        ],
        "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding",
        "abstract": "arXiv:2508.01875v1 Announce Type: new  Abstract: Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.",
        "arxiv_id": "2508.01875",
        "ARXIVID": "2508.01875",
        "COMMENT": "Matches criterion 3: Embodied AI with a new method (StreamAgent) for anticipatory, real-time video understanding in agents. Focuses on proactive spatial/temporal planning and memory for streaming video.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01664": {
        "authors": [
            "Zhixuan Li",
            "Yujia Liu",
            "Chen Hui",
            "Jeonghaeng Lee",
            "Sanghoon Lee",
            "Weisi Lin"
        ],
        "title": "Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions",
        "abstract": "arXiv:2508.01664v1 Announce Type: new  Abstract: Amodal segmentation targets to predict complete object masks, covering both visible and occluded regions. This task poses significant challenges due to complex occlusions and extreme shape variation, from rigid furniture to highly deformable clothing. Existing one-size-fits-all approaches rely on a single model to handle all shape types, struggling to capture and reason about diverse amodal shapes due to limited representation capacity. A natural solution is to adopt a Mixture-of-Experts (MoE) framework, assigning experts to different shape patterns. However, naively applying MoE without considering the object's underlying shape distribution can lead to mismatched expert routing and insufficient expert specialization, resulting in redundant or underutilized experts. To deal with these issues, we introduce ShapeMoE, a shape-specific sparse Mixture-of-Experts framework for amodal segmentation. The key idea is to learn a latent shape distribution space and dynamically route each object to a lightweight expert tailored to its shape characteristics. Specifically, ShapeMoE encodes each object into a compact Gaussian embedding that captures key shape characteristics. A Shape-Aware Sparse Router then maps the object to the most suitable expert, enabling precise and efficient shape-aware expert routing. Each expert is designed as lightweight and specialized in predicting occluded regions for specific shape patterns. ShapeMoE offers well interpretability via clear shape-to-expert correspondence, while maintaining high capacity and efficiency. Experiments on COCOA-cls, KINS, and D2SA show that ShapeMoE consistently outperforms state-of-the-art methods, especially in occluded region segmentation. The code will be released.",
        "arxiv_id": "2508.01664",
        "ARXIVID": "2508.01664",
        "COMMENT": "Matches criterion 1: Methodological improvement for spatial understanding (amodal segmentation under occlusion) using a shape-specific Mixture-of-Experts. Also relevant to criterion 4 as it advances segmentation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01236": {
        "authors": [
            "Mingyu Fu",
            "Wei Suo",
            "Ji Ma",
            "Lin Yuanbo Wu",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "title": "Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models",
        "abstract": "arXiv:2508.01236v1 Announce Type: new  Abstract: Despite the great success of Large Vision Language Models (LVLMs), their high computational cost severely limits their broad applications. The computational cost of LVLMs mainly stems from the visual sequence of the input, which consists of hundreds or even thousands of tokens. Although existing methods have made progress by removing redundant tokens, they suffer from severe performance degradation with high pruning rates due to the loss of visual information. In this paper, we propose an Adaptive Content Compensation Method (ACCM), which can effectively mitigate the visual information loss via an image caption. Specifically, ACCM comprises two key components: a lightweight caption model and a selector. Firstly the caption model generates question-related descriptions under the guidance of the user instruction. Then the selector further identifies a contextually appropriate caption from multiple candidates. Leveraging self-supervised learning, our modules could be learned efficiently without any human or automated labeling. We conduct extensive experiments across seven benchmarks and the results show that ACCM significantly outperforms existing methods with lower FLOPs (e.g., surpassing SOTA by 20.6% with 6.5% fewer FLOPs).",
        "arxiv_id": "2508.01236",
        "ARXIVID": "2508.01236",
        "COMMENT": "This paper proposes an adaptive content compensation method to mitigate information loss in highly pruned large vision-language models (LVLMs), improving efficiency without sacrificing performance. It matches criterion 2 (VLLMs) and is relevant to vision foundation models (criterion 4).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01915": {
        "authors": [
            "Akshay Paruchuri",
            "Sinan Hersek",
            "Lavisha Aggarwal",
            "Qiao Yang",
            "Xin Liu",
            "Achin Kulshrestha",
            "Andrea Colaco",
            "Henry Fuchs",
            "Ishan Chatterjee"
        ],
        "title": "EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses",
        "abstract": "arXiv:2508.01915v1 Announce Type: new  Abstract: All-day smart glasses are likely to emerge as platforms capable of continuous contextual sensing, uniquely positioning them for unprecedented assistance in our daily lives. Integrating the multi-modal AI agents required for human memory enhancement while performing continuous sensing, however, presents a major energy efficiency challenge for all-day usage. Achieving this balance requires intelligent, context-aware sensor management. Our approach, EgoTrigger, leverages audio cues from the microphone to selectively activate power-intensive cameras, enabling efficient sensing while preserving substantial utility for human memory enhancement. EgoTrigger uses a lightweight audio model (YAMNet) and a custom classification head to trigger image capture from hand-object interaction (HOI) audio cues, such as the sound of a drawer opening or a medication bottle being opened. In addition to evaluating on the QA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement Question-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated first-person QA pairs from full-length Ego4D videos that were curated to ensure that they contained audio, focusing on HOI moments critical for contextual understanding and memory. Our results show EgoTrigger can use 54% fewer frames on average, significantly saving energy in both power-hungry sensing components (e.g., cameras) and downstream operations (e.g., wireless transmission), while achieving comparable performance on datasets for an episodic memory task. We believe this context-aware triggering strategy represents a promising direction for enabling energy-efficient, functional smart glasses capable of all-day use -- supporting applications like helping users recall where they placed their keys or information about their routine activities (e.g., taking medications).",
        "arxiv_id": "2508.01915",
        "ARXIVID": "2508.01915",
        "COMMENT": "This paper presents EgoTrigger, a context-aware, audio-driven image capture system for smart glasses, using multi-modal AI for human memory enhancement. It leverages audio cues to trigger visual sensing, and introduces a new dataset for evaluation. Closely matches criterion 3 (embodied AI, new methods for contextual understanding in embodied agents, and new benchmark/dataset).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01181": {
        "authors": [
            "Zhiyuan Han",
            "Beier Zhu",
            "Yanlong Xu",
            "Peipei Song",
            "Xun Yang"
        ],
        "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning",
        "abstract": "arXiv:2508.01181v1 Announce Type: new  Abstract: Despite their strong performance in multimodal emotion reasoning, existing Multimodal Large Language Models (MLLMs) often overlook the scenarios involving emotion conflicts, where emotional cues from different modalities are inconsistent. To fill this gap, we first introduce CA-MER, a new benchmark designed to examine MLLMs under realistic emotion conflicts. It consists of three subsets: video-aligned, audio-aligned, and consistent, where only one or all modalities reflect the true emotion. However, evaluations on our CA-MER reveal that current state-of-the-art emotion MLLMs systematically over-rely on audio signal during emotion conflicts, neglecting critical cues from visual modality. To mitigate this bias, we propose MoSEAR, a parameter-efficient framework that promotes balanced modality integration. MoSEAR consists of two modules: (1)MoSE, modality-specific experts with a regularized gating mechanism that reduces modality bias in the fine-tuning heads; and (2)AR, an attention reallocation mechanism that rebalances modality contributions in frozen backbones during inference. Our framework offers two key advantages: it mitigates emotion conflicts and improves performance on consistent samples-without incurring a trade-off between audio and visual modalities. Experiments on multiple benchmarks-including MER2023, EMER, DFEW, and our CA-MER-demonstrate that MoSEAR achieves state-of-the-art performance, particularly under modality conflict conditions.",
        "arxiv_id": "2508.01181",
        "ARXIVID": "2508.01181",
        "COMMENT": "This paper introduces a new benchmark (CA-MER) for multimodal emotion reasoning under modality conflict and proposes a new MLLM method (MoSEAR) to address modality bias. It matches criterion 2 (new MLLMs) and criterion 3 (new benchmark for embodied/multimodal AI with a novel angle: emotion conflict).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01697": {
        "authors": [
            "Shiqi Huang",
            "Tingfa Xu",
            "Wen Yan",
            "Dean Barratt",
            "Yipeng Hu"
        ],
        "title": "Register Anything: Estimating \"Corresponding Prompts\" for Segment Anything Model",
        "abstract": "arXiv:2508.01697v1 Announce Type: new  Abstract: Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly \"searching for corresponding prompts\", using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the \"corresponding prompt problem\", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an \"inverse prompt\" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.",
        "arxiv_id": "2508.01697",
        "ARXIVID": "2508.01697",
        "COMMENT": "This paper introduces a novel, training-free registration method (PromptReg) that leverages the Segment Anything Model (SAM) to establish region-level correspondences via prompt inversion. This is a methodological improvement in spatial understanding and spatial intelligence, especially in leveraging vision foundation models (SAM) for spatial correspondence. Matches criteria 1 and 4.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.02374": {
        "authors": [
            "Shuo Lu",
            "Yanyin Chen",
            "Wei Feng",
            "Jiahao Fan",
            "Fengheng Li",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law",
            "Jian Liang"
        ],
        "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation",
        "abstract": "arXiv:2508.02374v1 Announce Type: new  Abstract: Layout generation plays a crucial role in enhancing both user experience and design efficiency. However, current approaches suffer from task-specific generation capabilities and perceptually misaligned evaluation metrics, leading to limited applicability and ineffective measurement. In this paper, we propose \\textit{Uni-Layout}, a novel framework that achieves unified generation, human-mimicking evaluation and alignment between the two. For universal generation, we incorporate various layout tasks into a single taxonomy and develop a unified generator that handles background or element contents constrained tasks via natural language prompts. To introduce human feedback for the effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first large-scale human feedback dataset with 100,000 expertly annotated layouts. Based on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that integrates visual and geometric information, employing a Chain-of-Thought mechanism to conduct qualitative assessments alongside a confidence estimation module to yield quantitative measurements. For better alignment between the generator and the evaluator, we integrate them into a cohesive system by adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically adjusts margins based on preference strength to better align with human judgments. Extensive experiments show that \\textit{Uni-Layout} significantly outperforms both task-specific and general-purpose methods. Our code is publicly available at https://github.com/JD-GenX/Uni-Layout.",
        "arxiv_id": "2508.02374",
        "ARXIVID": "2508.02374",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding via unified layout generation and evaluation with human feedback) and criterion 4 (vision foundation models and applications, as it integrates visual and geometric information and uses a large-scale human feedback dataset). The use of Chain-of-Thought and dynamic preference optimization is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.02516": {
        "authors": [
            "Wei Sun",
            "Linhan Cao",
            "Yuqin Cao",
            "Weixia Zhang",
            "Wen Wen",
            "Kaiwei Zhang",
            "Zijian Chen",
            "Fangfang Lu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "Engagement Prediction of Short Videos with Large Multimodal Models",
        "abstract": "arXiv:2508.02516v1 Announce Type: new  Abstract: The rapid proliferation of user-generated content (UGC) on short-form video platforms has made video engagement prediction increasingly important for optimizing recommendation systems and guiding content creation. However, this task remains challenging due to the complex interplay of factors such as semantic content, visual quality, audio characteristics, and user background. Prior studies have leveraged various types of features from different modalities, such as visual quality, semantic content, background sound, etc., but often struggle to effectively model their cross-feature and cross-modality interactions. In this work, we empirically investigate the potential of large multimodal models (LMMs) for video engagement prediction. We adopt two representative LMMs: VideoLLaMA2, which integrates audio, visual, and language modalities, and Qwen2.5-VL, which models only visual and language modalities. Specifically, VideoLLaMA2 jointly processes key video frames, text-based metadata, and background sound, while Qwen2.5-VL utilizes only key video frames and text-based metadata. Trained on the SnapUGC dataset, both models demonstrate competitive performance against state-of-the-art baselines, showcasing the effectiveness of LMMs in engagement prediction. Notably, VideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of audio features in engagement prediction. By ensembling two types of models, our method achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on short-form video engagement prediction. The code is available at https://github.com/sunwei925/LMM-EVQA.git.",
        "arxiv_id": "2508.02516",
        "ARXIVID": "2508.02516",
        "COMMENT": "This paper empirically investigates large multimodal models (LMMs) for video engagement prediction, comparing VideoLLaMA2 and Qwen2.5-VL. It directly matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), and provides empirical insights into the importance of audio features.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2508.02630": {
        "authors": [
            "Amine Allouah",
            "Omar Besbes",
            "Josu\\'e D Figueroa",
            "Yash Kanoria",
            "Akshit Kumar"
        ],
        "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce",
        "abstract": "arXiv:2508.02630v1 Announce Type: new  Abstract: Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal \"top\" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.",
        "arxiv_id": "2508.02630",
        "ARXIVID": "2508.02630",
        "COMMENT": "This paper studies VLM agents in e-commerce, using a sandbox environment to analyze their shopping behavior and the implications for marketplaces. It directly matches criterion 2 (VLLMs/MLLMs) and provides surprising empirical results about VLM agent behavior, which is of high interest.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01561": {
        "authors": [
            "Zijian Guo",
            "\\.Ilker I\\c{s}{\\i}k",
            "H. M. Sabbir Ahmad",
            "Wenchao Li"
        ],
        "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning",
        "abstract": "arXiv:2508.01561v1 Announce Type: new  Abstract: Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of B\\\"uchi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \\textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.",
        "arxiv_id": "2508.01561",
        "ARXIVID": "2508.01561",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI). Proposes GenZ-LTL for zero-shot generalization to arbitrary LTL requirements in multi-task RL, with a novel subgoal decomposition and observation reduction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01216": {
        "authors": [
            "Bolei Chen",
            "Shengsheng Yan",
            "Yongzheng Cui",
            "Jiaxu Kang",
            "Ping Zhong",
            "Jianxin Wang"
        ],
        "title": "Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?",
        "abstract": "arXiv:2508.01216v1 Announce Type: new  Abstract: Since a building's floorplan remains consistent over time and is inherently robust to changes in visual appearance, visual Floorplan Localization (FLoc) has received increasing attention from researchers. However, as a compact and minimalist representation of the building's layout, floorplans contain many repetitive structures (e.g., hallways and corners), thus easily result in ambiguous localization. Existing methods either pin their hopes on matching 2D structural cues in floorplans or rely on 3D geometry-constrained visual pre-trainings, ignoring the richer contextual information provided by visual images. In this paper, we suggest using broader visual scene context to empower FLoc algorithms with scene layout priors to eliminate localization uncertainty. In particular, we propose an unsupervised learning technique with clustering constraints to pre-train a room discriminator on self-collected unlabeled room images. Such a discriminator can empirically extract the hidden room type of the observed image and distinguish it from other room types. By injecting the scene context information summarized by the discriminator into an FLoc algorithm, the room style knowledge is effectively exploited to guide definite visual FLoc. We conducted sufficient comparative studies on two standard visual Floc benchmarks. Our experiments show that our approach outperforms state-of-the-art methods and achieves significant improvements in robustness and accuracy.",
        "arxiv_id": "2508.01216",
        "ARXIVID": "2508.01216",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). The paper introduces a method to use room style knowledge for visual floorplan localization, improving spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.01248": {
        "authors": [
            "Jiazhen Yan",
            "Fan Wang",
            "Weiwei Jiang",
            "Ziqiang Li",
            "Zhangjie Fu"
        ],
        "title": "NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection",
        "abstract": "arXiv:2508.01248v1 Announce Type: new  Abstract: The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.",
        "arxiv_id": "2508.01248",
        "ARXIVID": "2508.01248",
        "COMMENT": "Matches criterion 4: Vision foundation models (CLIP) and its application to AI-generated image detection. Proposes a novel method (NS-Net) to decouple semantic information from CLIP features for better generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.02278": {
        "authors": [
            "Xiangzeng Liu",
            "Chi Wang",
            "Guanglu Shi",
            "Xiaodong Zhang",
            "Qiguang Miao",
            "Miao Fan"
        ],
        "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching",
        "abstract": "arXiv:2508.02278v1 Announce Type: new  Abstract: Local feature matching remains a fundamental challenge in computer vision. Recent Area to Point Matching (A2PM) methods have improved matching accuracy. However, existing research based on this framework relies on inefficient pixel-level comparisons and complex graph matching that limit scalability. In this work, we introduce the Semantic and Geometric-aware Descriptor Network (SGAD), which fundamentally rethinks area-based matching by generating highly discriminative area descriptors that enable direct matching without complex graph optimization. This approach significantly improves both accuracy and efficiency of area matching. We further improve the performance of area matching through a novel supervision strategy that decomposes the area matching task into classification and ranking subtasks. Finally, we introduce the Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping areas by analyzing containment graphs. SGAD demonstrates remarkable performance gains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive evaluations show consistent improvements across multiple point matchers: SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy (0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA delivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new state-of-the-art.",
        "arxiv_id": "2508.02278",
        "ARXIVID": "2508.02278",
        "COMMENT": "This paper introduces SGAD, a semantic and geometric-aware descriptor for local feature matching, with a novel area-based matching approach and a new supervision strategy. It is a methodological improvement in spatial understanding (criterion 1) and is relevant to computer vision and spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01253": {
        "authors": [
            "Yupeng Zhang",
            "Ruize Han",
            "Fangnan Zhou",
            "Song Wang",
            "Wei Feng",
            "Liang Wan"
        ],
        "title": "ODOV: Towards Open-Domain Open-Vocabulary Object Detection",
        "abstract": "arXiv:2508.01253v1 Announce Type: new  Abstract: In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV) object detection, which considers the detection model's adaptability to the real world including both domain and category shifts. For this problem, we first construct a new benchmark OD-LVIS, which includes 46,949 images, covers 18 complex real-world domains and 1,203 categories, and provides a comprehensive dataset for evaluating real-world object detection. Besides, we develop a novel baseline method for ODOV detection.The proposed method first leverages large language models to generate the domain-agnostic text prompts for category embedding. It further learns the domain embedding from the given image, which, during testing, can be integrated into the category embedding to form the customized domain-specific category embedding for each test image. We provide sufficient benchmark evaluations for the proposed ODOV detection task and report the results, which verify the rationale of ODOV detection, the usefulness of our benchmark, and the superiority of the proposed method.",
        "arxiv_id": "2508.01253",
        "ARXIVID": "2508.01253",
        "COMMENT": "This paper introduces a new benchmark (OD-LVIS) and a baseline for open-domain open-vocabulary object detection, leveraging LLMs for domain-agnostic text prompts and domain-specific category embeddings. It matches criterion 3 (new benchmarks and methods for vision tasks) and is relevant to vision foundation models (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01641": {
        "authors": [
            "Yujian Liu",
            "Yuechuan Lin",
            "Dongxu Shen",
            "Haoran Li",
            "Yutong Wang",
            "Xiaoli Liu",
            "Shidang Xu"
        ],
        "title": "Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction",
        "abstract": "arXiv:2508.01641v1 Announce Type: new  Abstract: Whole-slide image (WSI) analysis remains challenging due to the gigapixel scale and sparsely distributed diagnostic regions. Multiple Instance Learning (MIL) mitigates this by modeling the WSI as bags of patches for slide-level prediction. However, most MIL approaches emphasize aggregator design while overlooking the impact of the feature extractor of the feature extraction stage, which is often pretrained on natural images. This leads to domain gap and suboptimal representations. Self-supervised learning (SSL) has shown promise in bridging domain gap via pretext tasks, but it still primarily builds upon generic backbones, thus requiring WSIs to be split into small patches. This inevitably splits histological structures and generates both redundant and interdependent patches, which in turn degrades aggregator performance and drastically increases training costs. To address this challenge, we propose a Cascaded Dual-Scale Reconstruction (CDSR) framework, demonstrating that only an average of 9 high-resolution patches per WSI are sufficient for robust slide-level representation. CDSR employs a two-stage selective sampling strategy that identifies the most informative representative regions from both model-based and semantic perspectives. These patches are then fed into a Local-to-Global Network, which reconstructs spatially coherent high-resolution WSI representations by integrating fine-grained local detail with global contextual information. Unlike existing dense-sampling or SSL pipelines, CDSR is optimized for efficiency and morphological fidelity. Experiments on Camelyon16, TCGA-NSCLC, and TCGA-RCC demonstrate that CDSR achieves improvements of 6.3% in accuracy and 5.5% in area under ROC curve on downstream classification tasks with only 7,070 (4.5% of total) high-resolution patches per dataset on average, outperforming state-of-the-art methods trained on over 10,000,000 patches.",
        "arxiv_id": "2508.01641",
        "ARXIVID": "2508.01641",
        "COMMENT": "This paper proposes a new method for efficient whole-slide image representation by selecting minimal high-resolution patches and reconstructing spatially coherent representations. It introduces a novel dual-scale reconstruction framework that integrates local and global spatial information, which is a methodological improvement in spatial understanding (criterion 1).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.02127": {
        "authors": [
            "Mingjie Liu",
            "Hanqing Liu",
            "Chuang Zhu"
        ],
        "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps",
        "abstract": "arXiv:2508.02127v1 Announce Type: new  Abstract: Accurate object detection under adverse lighting conditions is critical for real-world applications such as autonomous driving. Although neuromorphic event cameras have been introduced to handle these scenarios, adverse lighting often induces distracting reflections from tunnel walls or road surfaces, which frequently lead to false obstacle detections. However, neither RGB nor event data alone is robust enough to address these complexities, and mitigating these issues without additional sensors remains underexplored. To overcome these challenges, we propose leveraging normal maps, directly predicted from monocular RGB images, as robust geometric cues to suppress false positives and enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection framework that effectively fuses three complementary modalities: monocularly predicted surface normal maps, RGB images, and event streams. To optimize the fusion process, our framework incorporates two key modules: the Adaptive Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features, and the Event-modality Aware Fusion Module (EAFM), which adapts to the high dynamic range characteristics of event data. Extensive evaluations on the DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly outperforms state-of-the-art methods. Our approach achieves mAP50 improvements of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by 7.1% on the PKU-DAVIS-SOD dataset.",
        "arxiv_id": "2508.02127",
        "ARXIVID": "2508.02127",
        "COMMENT": "This paper proposes a new multi-modal detection framework (NRE-Net) that fuses RGB, event, and monocular normal maps for robust object detection under adverse lighting. It introduces a novel method for spatial understanding in embodied agents (criterion 1) and advances multi-modal fusion (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.02427": {
        "authors": [
            "Tung-Thuy Pham",
            "Duy-Quan Luong",
            "Minh-Quan Duong",
            "Trung-Hieu Nguyen",
            "Thu-Trang Nguyen",
            "Son Nguyen",
            "Hieu Dinh Vo"
        ],
        "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models",
        "abstract": "arXiv:2508.02427v1 Announce Type: new  Abstract: Composable AI offers a scalable and effective paradigm for tackling complex AI tasks by decomposing them into sub-tasks and solving each sub-task using ready-to-use well-trained models. However, systematically evaluating methods under this setting remains largely unexplored. In this paper, we introduce CABENCH, the first public benchmark comprising 70 realistic composable AI tasks, along with a curated pool of 700 models across multiple modalities and domains. We also propose an evaluation framework to enable end-to-end assessment of composable AI solutions. To establish initial baselines, we provide human-designed reference solutions and compare their performance with two LLM-based approaches. Our results illustrate the promise of composable AI in addressing complex real-world problems while highlighting the need for methods that can fully unlock its potential by automatically generating effective execution pipelines.",
        "arxiv_id": "2508.02427",
        "ARXIVID": "2508.02427",
        "COMMENT": "CABENCH introduces a new benchmark for composable AI, focusing on solving complex tasks by composing ready-to-use models across modalities. This matches criterion 3 (new benchmark for embodied/multimodal AI with a novel angle: composability and model composition).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01237": {
        "authors": [
            "Cheng Tan",
            "Qi Chen",
            "Jingxuan Wei",
            "Gaowei Wu",
            "Zhangyang Gao",
            "Siyuan Li",
            "Bihui Yu",
            "Ruifeng Guo",
            "Stan Z. Li"
        ],
        "title": "SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches",
        "abstract": "arXiv:2508.01237v1 Announce Type: new  Abstract: Hand-drawn sketches are a natural and efficient medium for capturing and conveying ideas. Despite significant advancements in controllable natural image generation, translating freehand sketches into structured, machine-readable diagrams remains a labor-intensive and predominantly manual task. The primary challenge stems from the inherent ambiguity of sketches, which lack the structural constraints and semantic precision required for automated diagram generation. To address this challenge, we introduce SketchAgent, a multi-agent system designed to automate the transformation of hand-drawn sketches into structured diagrams. SketchAgent integrates sketch recognition, symbolic reasoning, and iterative validation to produce semantically coherent and structurally accurate diagrams, significantly reducing the need for manual effort. To evaluate the effectiveness of our approach, we propose the Sketch2Diagram Benchmark, a comprehensive dataset and evaluation framework encompassing eight diverse diagram categories, such as flowcharts, directed graphs, and model architectures. The dataset comprises over 6,000 high-quality examples with token-level annotations, standardized preprocessing, and rigorous quality control. By streamlining the diagram generation process, SketchAgent holds great promise for applications in design, education, and engineering, while offering a significant step toward bridging the gap between intuitive sketching and machine-readable diagram generation. The benchmark is released at https://huggingface.co/datasets/DiagramAgent/Sketch2Diagram-Benchmark.",
        "arxiv_id": "2508.01237",
        "ARXIVID": "2508.01237",
        "COMMENT": "Matches criterion 3 (embodied AI papers on building new benchmarks or methods) by introducing a new benchmark (Sketch2Diagram) and a multi-agent system for structured diagram generation from sketches, which is a novel angle in spatial intelligence and diagram understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01579": {
        "authors": [
            "Lingfeng He",
            "De Cheng",
            "Huaijie Wang",
            "Nannan Wang"
        ],
        "title": "Harnessing Textual Semantic Priors for Knowledge Transfer and Refinement in CLIP-Driven Continual Learning",
        "abstract": "arXiv:2508.01579v1 Announce Type: new  Abstract: Continual learning (CL) aims to equip models with the ability to learn from a stream of tasks without forgetting previous knowledge. With the progress of vision-language models like Contrastive Language-Image Pre-training (CLIP), their promise for CL has attracted increasing attention due to their strong generalizability. However, the potential of rich textual semantic priors in CLIP in addressing the stability-plasticity dilemma remains underexplored. During backbone training, most approaches transfer past knowledge without considering semantic relevance, leading to interference from unrelated tasks that disrupt the balance between stability and plasticity. Besides, while text-based classifiers provide strong generalization, they suffer from limited plasticity due to the inherent modality gap in CLIP. Visual classifiers help bridge this gap, but their prototypes lack rich and precise semantics. To address these challenges, we propose Semantic-Enriched Continual Adaptation (SECA), a unified framework that harnesses the anti-forgetting and structured nature of textual priors to guide semantic-aware knowledge transfer in the backbone and reinforce the semantic structure of the visual classifier. Specifically, a Semantic-Guided Adaptive Knowledge Transfer (SG-AKT) module is proposed to assess new images' relevance to diverse historical visual knowledge via textual cues, and aggregate relevant knowledge in an instance-adaptive manner as distillation signals. Moreover, a Semantic-Enhanced Visual Prototype Refinement (SE-VPR) module is introduced to refine visual prototypes using inter-class semantic relations captured in class-wise textual embeddings. Extensive experiments on multiple benchmarks validate the effectiveness of our approach.",
        "arxiv_id": "2508.01579",
        "ARXIVID": "2508.01579",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) by leveraging CLIP for continual learning and knowledge transfer, with a novel semantic-guided adaptation and prototype refinement.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01684": {
        "authors": [
            "Yufeng Chi",
            "Huimin Ma",
            "Kafeng Wang",
            "Jianmin Li"
        ],
        "title": "DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing",
        "abstract": "arXiv:2508.01684v1 Announce Type: new  Abstract: While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \\textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.",
        "arxiv_id": "2508.01684",
        "ARXIVID": "2508.01684",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is relevant to generative modeling in multi-modal learning. The method for distilling 3D consistency into a 2D editor for 3D scene editing is a novel approach for multi-view consistency in diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.02671": {
        "authors": [
            "Haoyang Li",
            "Liang Wang",
            "Chao Wang",
            "Siyu Zhou",
            "Jing Jiang",
            "Yan Peng",
            "Guodong Long"
        ],
        "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models",
        "abstract": "arXiv:2508.02671v1 Announce Type: new  Abstract: For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: https://github.com/JREion/AugPT .",
        "arxiv_id": "2508.02671",
        "ARXIVID": "2508.02671",
        "COMMENT": "Matches criterion 4: This paper proposes AugPT, a new prompt tuning method for CLIP-based vision-language models using internal augmentation, improving performance and generalization without external data. It is directly about vision foundation models and their application.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.01345": {
        "authors": [
            "Rongzhen Zhao",
            "Jian Li",
            "Juho Kannala",
            "Joni Pajarinen"
        ],
        "title": "Predicting Video Slot Attention Queries from Random Slot-Feature Pairs",
        "abstract": "arXiv:2508.01345v1 Announce Type: new  Abstract: Unsupervised video Object-Centric Learning (OCL) is promising as it enables object-level scene representation and dynamics modeling as we humans do. Mainstream video OCL methods adopt a recurrent architecture: An aggregator aggregates current video frame into object features, termed slots, under some queries; A transitioner transits current slots to queries for the next frame. This is an effective architecture but all existing implementations both (\\textit{i1}) neglect to incorporate next frame features, the most informative source for query prediction, and (\\textit{i2}) fail to learn transition dynamics, the knowledge essential for query prediction. To address these issues, we propose Random Slot-Feature pair for learning Query prediction (RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both slots and features, which provides more information for query prediction; (\\textit{t2}) We train the transitioner to predict queries from slot-feature pairs randomly sampled from available recurrences, which drives it to learn transition dynamics. Experiments on scene representation demonstrate that our method surpass existing video OCL methods significantly, e.g., up to 10 points on object discovery, setting new state-of-the-art. Such superiority also benefits downstream tasks like dynamics modeling. Our core source code and training logs are available as the supplement.",
        "arxiv_id": "2508.01345",
        "ARXIVID": "2508.01345",
        "COMMENT": "This paper proposes a new method for unsupervised video object-centric learning, improving query prediction for slot attention models. It is a methodological improvement in object-centric video representation, which is relevant to spatial understanding and intelligence (criterion 1), and may be of interest for embodied AI (criterion 3) due to its focus on scene representation and dynamics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.01698": {
        "authors": [
            "Zuhao Yang",
            "Jiahui Zhang",
            "Yingchen Yu",
            "Shijian Lu",
            "Song Bai"
        ],
        "title": "Versatile Transition Generation with Image-to-Video Diffusion",
        "abstract": "arXiv:2508.01698v1 Announce Type: new  Abstract: Leveraging text, images, structure maps, or motion trajectories as conditional guidance, diffusion models have achieved great success in automated and high-quality video generation. However, generating smooth and rational transition videos given the first and last video frames as well as descriptive text prompts is far underexplored. We present VTG, a Versatile Transition video Generation framework that can generate smooth, high-fidelity, and semantically coherent video transitions. VTG introduces interpolation-based initialization that helps preserve object identity and handle abrupt content changes effectively. In addition, it incorporates dual-directional motion fine-tuning and representation alignment regularization to mitigate the limitations of pre-trained image-to-video diffusion models in motion smoothness and generation fidelity, respectively. To evaluate VTG and facilitate future studies on unified transition generation, we collected TransitBench, a comprehensive benchmark for transition generation covering two representative transition tasks: concept blending and scene transition. Extensive experiments show that VTG achieves superior transition performance consistently across all four tasks.",
        "arxiv_id": "2508.01698",
        "ARXIVID": "2508.01698",
        "COMMENT": "This paper presents a new diffusion-based framework for generating smooth video transitions from initial and final frames and text prompts. It introduces a new benchmark (TransitBench) for transition generation, which matches criterion 3 (new benchmarks for embodied AI/video generation). The method is also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.01158": {
        "authors": [
            "Yunlong Lin",
            "Zirui Li",
            "Guodong Du",
            "Xiaocong Zhao",
            "Cheng Gong",
            "Xinwei Wang",
            "Chao Lu",
            "Jianwei Gong"
        ],
        "title": "H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving",
        "abstract": "arXiv:2508.01158v1 Announce Type: new  Abstract: Deep learning (DL) has shown state-of-the-art performance in trajectory prediction, which is critical to safe navigation in autonomous driving (AD). However, most DL-based methods suffer from catastrophic forgetting, where adapting to a new distribution may cause significant performance degradation in previously learned ones. Such inability to retain learned knowledge limits their applicability in the real world, where AD systems need to operate across varying scenarios with dynamic distributions. As revealed by neuroscience, the hippocampal circuit plays a crucial role in memory replay, effectively reconstructing learned knowledge based on limited resources. Inspired by this, we propose a hippocampal circuit-inspired continual learning method (H2C) for trajectory prediction across varying scenarios. H2C retains prior knowledge by selectively recalling a small subset of learned samples. First, two complementary strategies are developed to select the subset to represent learned knowledge. Specifically, one strategy maximizes inter-sample diversity to represent the distinctive knowledge, and the other estimates the overall knowledge by equiprobable sampling. Then, H2C updates via a memory replay loss function calculated by these selected samples to retain knowledge while learning new data. Experiments based on various scenarios from the INTERACTION dataset are designed to evaluate H2C. Experimental results show that H2C reduces catastrophic forgetting of DL baselines by 22.71% on average in a task-free manner, without relying on manually informed distributional shifts. The implementation is available at https://github.com/BIT-Jack/H2C-lifelong.",
        "arxiv_id": "2508.01158",
        "ARXIVID": "2508.01158",
        "COMMENT": "This paper proposes H2C, a hippocampal circuit-inspired continual learning method for lifelong trajectory prediction in autonomous driving. It is a new method for spatial intelligence in embodied agents (criterion 1), with a neuroscience-inspired approach.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.02507": {
        "authors": [
            "Yaofeng Cheng",
            "Xinkai Gao",
            "Sen Zhang",
            "Chao Zeng",
            "Fusheng Zha",
            "Lining Sun",
            "Chenguang Yang"
        ],
        "title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular Depth Estimation and Instance Mask",
        "abstract": "arXiv:2508.02507v1 Announce Type: new  Abstract: Due to the optical properties, transparent objects often lead depth cameras to generate incomplete or invalid depth data, which in turn reduces the accuracy and reliability of robotic grasping. Existing approaches typically input the RGB-D image directly into the network to output the complete depth, expecting the model to implicitly infer the reliability of depth values. However, while effective in training datasets, such methods often fail to generalize to real-world scenarios, where complex light interactions lead to highly variable distributions of valid and invalid depth data. To address this, we propose ReMake, a novel depth completion framework guided by an instance mask and monocular depth estimation. By explicitly distinguishing transparent regions from non-transparent ones, the mask enables the model to concentrate on learning accurate depth estimation in these areas from RGB-D input during training. This targeted supervision reduces reliance on implicit reasoning and improves generalization to real-world scenarios. Additionally, monocular depth estimation provides depth context between the transparent object and its surroundings, enhancing depth prediction accuracy. Extensive experiments show that our method outperforms existing approaches on both benchmark datasets and real-world scenarios, demonstrating superior accuracy and generalization capability. Code and videos are available at https://chengyaofeng.github.io/ReMake.github.io/.",
        "arxiv_id": "2508.02507",
        "ARXIVID": "2508.02507",
        "COMMENT": "This paper introduces ReMake, a depth completion framework for transparent object grasping, using instance masks and monocular depth estimation. It is a methodological improvement for spatial understanding in robotic grasping, matching criterion 1 (spatial intelligence on embodied agents).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.01254": {
        "authors": [
            "Zihan Li",
            "Wei Sun",
            "Jing Hu",
            "Jianhua Yin",
            "Jianlong Wu",
            "Liqiang Nie"
        ],
        "title": "Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency",
        "abstract": "arXiv:2508.01254v1 Announce Type: new  Abstract: While large language-image pre-trained models like CLIP offer powerful generic features for image clustering, existing methods typically freeze the encoder. This creates a fundamental mismatch between the model's task-agnostic representations and the demands of a specific clustering task, imposing a ceiling on performance. To break this ceiling, we propose a self-enhanced framework based on cross-modal semantic consistency for efficient image clustering. Our framework first builds a strong foundation via Cross-Modal Semantic Consistency and then specializes the encoder through Self-Enhancement. In the first stage, we focus on Cross-Modal Semantic Consistency. By mining consistency between generated image-text pairs at the instance, cluster assignment, and cluster center levels, we train lightweight clustering heads to align with the rich semantics of the pre-trained model. This alignment process is bolstered by a novel method for generating higher-quality cluster centers and a dynamic balancing regularizer to ensure well-distributed assignments. In the second stage, we introduce a Self-Enhanced fine-tuning strategy. The well-aligned model from the first stage acts as a reliable pseudo-label generator. These self-generated supervisory signals are then used to feed back the efficient, joint optimization of the vision encoder and clustering heads, unlocking their full potential. Extensive experiments on six mainstream datasets show that our method outperforms existing deep clustering methods by significant margins. Notably, our ViT-B/32 model already matches or even surpasses the accuracy of state-of-the-art methods built upon the far larger ViT-L/14.",
        "arxiv_id": "2508.01254",
        "ARXIVID": "2508.01254",
        "COMMENT": "This paper proposes a self-enhanced image clustering framework leveraging cross-modal semantic consistency, using large language-image pre-trained models (like CLIP). It is relevant to criterion 4 (vision foundation models and applications), as it adapts and fine-tunes vision foundation models for clustering.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.01711": {
        "authors": [
            "Bowen Yang",
            "Yun Cao",
            "Chen He",
            "Xiaosu Su"
        ],
        "title": "GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval",
        "abstract": "arXiv:2508.01711v1 Announce Type: new  Abstract: Text-to-video retrieval requires precise alignment between language and temporally rich video signals. Existing methods predominantly exploit visual cues and often overlook complementary audio semantics or adopt coarse fusion strategies, leading to suboptimal multimodal representations. We present GAID, a framework that jointly address this gap via two key components: (i) a Frame-level Gated Fusion (FGF) that adaptively integrates audio and visual features under textual guidance, enabling fine-grained temporal alignment; and (ii) a Directional Adaptive Semantic Perturbation (DASP) that injects structure-aware perturbations into text embeddings, enhancing robustness and discrimination without incurring multi-pass inference. These modules complement each other -- fusion reduces modality gaps while perturbation regularizes cross-modal matching -- yielding more stable and expressive representations. Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent state-of-the-art results across all retrieval metrics with notable efficiency gains. Our code is available at https://github.com/YangBowenn/GAID.",
        "arxiv_id": "2508.01711",
        "ARXIVID": "2508.01711",
        "COMMENT": "GAID introduces a new framework for text-video retrieval with frame-level gated audio-visual fusion and semantic perturbation. It is relevant to criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications in retrieval).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.02258": {
        "authors": [
            "Wenchuan Zhang",
            "Jingru Guo",
            "Hengzhe Zhang",
            "Penghao Zhang",
            "Jie Chen",
            "Shuwan Zhang",
            "Zhang Zhang",
            "Yuhao Yi",
            "Hong Bu"
        ],
        "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning",
        "abstract": "arXiv:2508.02258v1 Announce Type: new  Abstract: Although Vision Language Models (VLMs) have shown strong generalization in medical imaging, pathology presents unique challenges due to ultra-high resolution, complex tissue structures, and nuanced clinical semantics. These factors make pathology VLMs prone to hallucinations, i.e., generating outputs inconsistent with visual evidence, which undermines clinical trust. Existing RAG approaches in this domain largely depend on text-based knowledge bases, limiting their ability to leverage diagnostic visual cues. To address this, we propose Patho-AgenticRAG, a multimodal RAG framework with a database built on page-level embeddings from authoritative pathology textbooks. Unlike traditional text-only retrieval systems, it supports joint text-image search, enabling direct retrieval of textbook pages that contain both the queried text and relevant visual cues, thus avoiding the loss of critical image-based information. Patho-AgenticRAG also supports reasoning, task decomposition, and multi-turn search interactions, improving accuracy in complex diagnostic scenarios. Experiments show that Patho-AgenticRAG significantly outperforms existing multimodal models in complex pathology tasks like multiple-choice diagnosis and visual question answering. Our project is available at the Patho-AgenticRAG repository: https://github.com/Wenchuan-Zhang/Patho-AgenticRAG.",
        "arxiv_id": "2508.02258",
        "ARXIVID": "2508.02258",
        "COMMENT": "Patho-AgenticRAG presents a multimodal retrieval-augmented generation framework for pathology VLMs, supporting joint text-image search and agentic reasoning. This matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications in medical imaging).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.01303": {
        "authors": [
            "Shuangli Du",
            "Jing Wang",
            "Minghua Zhao",
            "Zhenyu Xu",
            "Jie Li"
        ],
        "title": "Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation",
        "abstract": "arXiv:2508.01303v1 Announce Type: new  Abstract: State-of-the-art stereo matching (SM) models trained on synthetic data often fail to generalize to real data domains due to domain differences, such as color, illumination, contrast, and texture. To address this challenge, we leverage data augmentation to expand the training domain, encouraging the model to acquire robust cross-domain feature representations instead of domain-dependent shortcuts. This paper proposes an uncertainty-guided data augmentation (UgDA) method, which argues that the image statistics in RGB space (mean and standard deviation) carry the domain characteristics. Thus, samples in unseen domains can be generated by properly perturbing these statistics. Furthermore, to simulate more potential domains, Gaussian distributions founded on batch-level statistics are poposed to model the unceratinty of perturbation direction and intensity. Additionally, we further enforce feature consistency between original and augmented data for the same scene, encouraging the model to learn structure aware, shortcuts-invariant feature representations. Our approach is simple, architecture-agnostic, and can be integrated into any SM networks. Extensive experiments on several challenging benchmarks have demonstrated that our method can significantly improve the generalization performance of existing SM networks.",
        "arxiv_id": "2508.01303",
        "ARXIVID": "2508.01303",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding) by proposing uncertainty-guided data augmentation for stereo matching, improving domain generalization in spatial perception tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.01780": {
        "authors": [
            "Guozhao Mo",
            "Wenliang Zhong",
            "Jiawei Chen",
            "Xuanang Chen",
            "Yaojie Lu",
            "Hongyu Lin",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
        "abstract": "arXiv:2508.01780v1 Announce Type: new  Abstract: With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
        "arxiv_id": "2508.01780",
        "ARXIVID": "2508.01780",
        "COMMENT": "Partially matches criterion 3 (embodied AI, new benchmark) as it introduces LiveMCPBench, a large-scale benchmark for LLM agents in tool-rich environments, but the focus is not on vision or spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.02621": {
        "authors": [
            "Yinghao Zhu",
            "Yifan Qi",
            "Zixiang Wang",
            "Lei Gu",
            "Dehao Sui",
            "Haoran Hu",
            "Xichen Zhang",
            "Ziyi He",
            "Liantao Ma",
            "Lequan Yu"
        ],
        "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research",
        "abstract": "arXiv:2508.02621v1 Announce Type: new  Abstract: The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.",
        "arxiv_id": "2508.02621",
        "ARXIVID": "2508.02621",
        "COMMENT": "Partially matches criterion 3 (embodied AI, new benchmark/methods) as it introduces a new benchmark (EHRFlowBench) and a self-evolving agent for healthcare research, but the focus is not on spatial or vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.01380": {
        "authors": [
            "Jie Wei",
            "Erika Ardiles-Cruz",
            "Aleksey Panasyuk",
            "Erik Blasch"
        ],
        "title": "Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models",
        "abstract": "arXiv:2508.01380v1 Announce Type: new  Abstract: It is of crucial importance to assess damages promptly and accurately in humanitarian assistance and disaster response (HADR). Current deep learning approaches struggle to generalize effectively due to the imbalance of data classes, scarcity of moderate damage examples, and human inaccuracy in pixel labeling during HADR situations. To accommodate for these limitations and exploit state-of-the-art techniques in vision-language models (VLMs) to fuse imagery with human knowledge understanding, there is an opportunity to generate a diversified set of image-based damage data effectively. Our initial experimental results suggest encouraging data generation quality, which demonstrates an improvement in classifying scenes with different levels of structural damage to buildings, roads, and infrastructures.",
        "arxiv_id": "2508.01380",
        "ARXIVID": "2508.01380",
        "COMMENT": "Matches criterion 2 (new VLLMs) and criterion 4 (vision foundation models and applications). The paper uses vision-language models to fuse imagery with human knowledge for damage data generation, which is a novel application of VLMs in humanitarian assistance.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.02149": {
        "authors": [
            "Ziyang Luo",
            "Nian Liu",
            "Fahad Shahbaz Khan",
            "Junwei Han"
        ],
        "title": "AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation",
        "abstract": "arXiv:2508.02149v1 Announce Type: new  Abstract: Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to precisely locate sounding objects by integrating visual, auditory, and textual cues. Existing methods often lack genuine semantic understanding, tending to memorize fixed reasoning patterns. Furthermore, jointly training for reasoning and segmentation can compromise pixel-level precision. To address these issues, we introduce AURORA, a novel framework designed to enhance genuine reasoning and language comprehension in reference audio-visual segmentation. We employ a structured Chain-of-Thought (CoT) prompting mechanism to guide the model through a step-by-step reasoning process and introduce a novel segmentation feature distillation loss to effectively integrate these reasoning abilities without sacrificing segmentation performance. To further cultivate the model's genuine reasoning capabilities, we devise a further two-stage training strategy: first, a ``corrective reflective-style training\" stage utilizes self-correction to enhance the quality of reasoning paths, followed by reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster robustness in challenging scenarios. Experiments demonstrate that AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes effectively to unreferenced segmentation.",
        "arxiv_id": "2508.02149",
        "ARXIVID": "2508.02149",
        "COMMENT": "Related to multi-modal segmentation and reasoning, but not directly a VLLM/MLLM or vision foundation model paper. Some relevance to multi-modal learning and clever training strategies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.02490": {
        "authors": [
            "Puyu Yang",
            "Laifa Tao",
            "Zijian Huang",
            "Haifei Liu",
            "Wenyan Cao",
            "Hao Ji",
            "Jianan Qiu",
            "Qixuan Huang",
            "Xuanyuan Su",
            "Yuhang Xie",
            "Jun Zhang",
            "Shangyu Li",
            "Chen Lu",
            "Zhixuan Lian"
        ],
        "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic Evaluation of Large Models in Prognostics and Health Management",
        "abstract": "arXiv:2508.02490v1 Announce Type: new  Abstract: With the rapid advancement of generative artificial intelligence, large language models (LLMs) are increasingly adopted in industrial domains, offering new opportunities for Prognostics and Health Management (PHM). These models help address challenges such as high development costs, long deployment cycles, and limited generalizability. However, despite the growing synergy between PHM and LLMs, existing evaluation methodologies often fall short in structural completeness, dimensional comprehensiveness, and evaluation granularity. This hampers the in-depth integration of LLMs into the PHM domain. To address these limitations, this study proposes PHM-Bench, a novel three-dimensional evaluation framework for PHM-oriented large models. Grounded in the triadic structure of fundamental capability, core task, and entire lifecycle, PHM-Bench is tailored to the unique demands of PHM system engineering. It defines multi-level evaluation metrics spanning knowledge comprehension, algorithmic generation, and task optimization. These metrics align with typical PHM tasks, including condition monitoring, fault diagnosis, RUL prediction, and maintenance decision-making. Utilizing both curated case sets and publicly available industrial datasets, our study enables multi-dimensional evaluation of general-purpose and domain-specific models across diverse PHM tasks. PHM-Bench establishes a methodological foundation for large-scale assessment of LLMs in PHM and offers a critical benchmark to guide the transition from general-purpose to PHM-specialized models.",
        "arxiv_id": "2508.02490",
        "ARXIVID": "2508.02490",
        "COMMENT": "This paper introduces PHM-Bench, a new domain-specific benchmark for evaluating large models in prognostics and health management. It matches criterion 3 (new benchmarks for embodied AI or related domains), though the focus is more on industrial PHM than embodied agents per se.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.02362": {
        "authors": [
            "Xu Wang",
            "Shengeng Tang",
            "Fei Wang",
            "Lechao Cheng",
            "Dan Guo",
            "Feng Xue",
            "Richang Hong"
        ],
        "title": "Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering",
        "abstract": "arXiv:2508.02362v1 Announce Type: new  Abstract: Generating semantically coherent and visually accurate talking faces requires bridging the gap between linguistic meaning and facial articulation. Although audio-driven methods remain prevalent, their reliance on high-quality paired audio visual data and the inherent ambiguity in mapping acoustics to lip motion pose significant challenges in terms of scalability and robustness. To address these issues, we propose Text2Lip, a viseme-centric framework that constructs an interpretable phonetic-visual bridge by embedding textual input into structured viseme sequences. These mid-level units serve as a linguistically grounded prior for lip motion prediction. Furthermore, we design a progressive viseme-audio replacement strategy based on curriculum learning, enabling the model to gradually transition from real audio to pseudo-audio reconstructed from enhanced viseme features via cross-modal attention. This allows for robust generation in both audio-present and audio-free scenarios. Finally, a landmark-guided renderer synthesizes photorealistic facial videos with accurate lip synchronization. Extensive evaluations show that Text2Lip outperforms existing approaches in semantic fidelity, visual realism, and modality robustness, establishing a new paradigm for controllable and flexible talking face generation. Our project homepage is https://plyon1.github.io/Text2Lip/.",
        "arxiv_id": "2508.02362",
        "ARXIVID": "2508.02362",
        "COMMENT": "This paper introduces a new viseme-guided framework for text-to-lip talking face generation, which is a novel method in multi-modal generative modeling. It is relevant to criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), as it bridges text and visual modalities for controllable face generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.01730": {
        "authors": [
            "Jianbo Ma",
            "Hui Luo",
            "Qi Chen",
            "Yuankai Qi",
            "Yumei Sun",
            "Amin Beheshti",
            "Jianlin Zhang",
            "Ming-Hsuan Yang"
        ],
        "title": "Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos",
        "abstract": "arXiv:2508.01730v1 Announce Type: new  Abstract: Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.",
        "arxiv_id": "2508.01730",
        "ARXIVID": "2508.01730",
        "COMMENT": "This paper introduces AMOT, a method for robust multi-object tracking in UAV videos by jointly modeling appearance and motion cues. It is a novel method for spatial understanding in vision, relevant to criterion 1, though not directly in embodied agents. Also relevant to vision foundation model applications (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.01331": {
        "authors": [
            "Jiaxing Yang",
            "Lihe Zhang",
            "Huchuan Lu"
        ],
        "title": "Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network",
        "abstract": "arXiv:2508.01331v1 Announce Type: new  Abstract: Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused wide attention. To handle drastic scale variation of remote targets, existing methods only use the full image as input and nest the saliency-preferring techniques of cross-scale information interaction into traditional single-view structure. Although effective for visually salient targets, they still struggle in handling tiny, ambiguous ones in lots of real scenarios. In this work, we instead propose a paralleled yet unified segmentation framework Cross-view Semantics Interaction Network (CSINet) to solve the limitations. Motivated by human behavior in observing targets of interest, the network orchestrates visual cues from remote and close distances to conduct synergistic prediction. In its every encoding stage, a Cross-View Window-attention module (CVWin) is utilized to supplement global and local semantics into close-view and remote-view branch features, finally promoting the unified representation of feature in every encoding stage. In addition, we develop a Collaboratively Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of target and meanwhile integrate cross-view multiscale features. The proposed network seamlessly enhances the exploitation of global and local semantics, achieving significant improvements over others while maintaining satisfactory speed.",
        "arxiv_id": "2508.01331",
        "ARXIVID": "2508.01331",
        "COMMENT": "This paper proposes a new segmentation framework (CSINet) for referring remote sensing image segmentation, focusing on cross-view semantic interaction. It introduces a novel method for spatial understanding in vision tasks, which is relevant to criterion 1 (spatial intelligence), though not in embodied agents. Also relevant to vision foundation model applications (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.01980": {
        "authors": [
            "Xiaoyu Zhang",
            "Ziwei Wang",
            "Hai Dong",
            "Zhifeng Bao",
            "Jiajun Liu"
        ],
        "title": "On-the-Fly Object-aware Representative Point Selection in Point Cloud",
        "abstract": "arXiv:2508.01980v1 Announce Type: new  Abstract: Point clouds are essential for object modeling and play a critical role in assisting driving tasks for autonomous vehicles (AVs). However, the significant volume of data generated by AVs creates challenges for storage, bandwidth, and processing cost. To tackle these challenges, we propose a representative point selection framework for point cloud downsampling, which preserves critical object-related information while effectively filtering out irrelevant background points. Our method involves two steps: (1) Object Presence Detection, where we introduce an unsupervised density peak-based classifier and a supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2) Sampling Budget Allocation, where we propose a strategy that selects object-relevant points while maintaining a high retention rate of object information. Extensive experiments on the KITTI and nuScenes datasets demonstrate that our method consistently outperforms state-of-the-art baselines in both efficiency and effectiveness across varying sampling rates. As a model-agnostic solution, our approach integrates seamlessly with diverse downstream models, making it a valuable and scalable addition to the 3D point cloud downsampling toolkit for AV applications.",
        "arxiv_id": "2508.01980",
        "ARXIVID": "2508.01980",
        "COMMENT": "Somewhat related to criterion 1 (spatial understanding) as it proposes a new method for object-aware point selection in point clouds, which is important for spatial intelligence in AVs, but does not focus on embodied agents or multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.01785": {
        "authors": [
            "Xiaotong Zhang",
            "Alexander Broersen",
            "Gonnie CM van Erp",
            "Silvia L. Pintea",
            "Jouke Dijkstra"
        ],
        "title": "Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation",
        "abstract": "arXiv:2508.01785v1 Announce Type: new  Abstract: The preoperative planning of liver surgery relies on Couinaud segmentation from computed tomography (CT) images, to reduce the risk of bleeding and guide the resection procedure. Using 3D point-based representations, rather than voxelizing the CT volume, has the benefit of preserving the physical resolution of the CT. However, point-based representations need prior knowledge of the liver vessel structure, which is time consuming to acquire. Here, we propose a point-based method for Couinaud segmentation, without explicitly providing the prior liver vessel structure. To allow the model to learn this anatomical liver vessel structure, we add a graph reasoning module on top of the point features. This adds implicit anatomical information to the model, by learning affinities across point neighborhoods. Our method is competitive on the MSD and LiTS public datasets in Dice coefficient and average surface distance scores compared to four pioneering point-based methods. Our code is available at https://github.com/ZhangXiaotong015/GrPn.",
        "arxiv_id": "2508.01785",
        "ARXIVID": "2508.01785",
        "COMMENT": "Relevant to spatial understanding (criterion 1) by introducing graph-based anatomical information for point-based segmentation, but focused on medical imaging. The use of implicit anatomical priors via graph reasoning is a novel angle.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.02106": {
        "authors": [
            "Kaiyang Ji",
            "Ye Shi",
            "Zichen Jin",
            "Kangyi Chen",
            "Lan Xu",
            "Yuexin Ma",
            "Jingyi Yu",
            "Jingya Wang"
        ],
        "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis",
        "abstract": "arXiv:2508.02106v1 Announce Type: new  Abstract: Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.",
        "arxiv_id": "2508.02106",
        "ARXIVID": "2508.02106",
        "COMMENT": "Related to motion synthesis and human-robot interaction, but does not directly match any of the four criteria. Some relevance to embodied AI, but not focused on spatial intelligence or benchmarks.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.01112": {
        "authors": [
            "Yizhou Zhao",
            "Haoyu Chen",
            "Chunjiang Liu",
            "Zhenyang Li",
            "Charles Herrmann",
            "Junhwa Hur",
            "Yinxiao Li",
            "Ming-Hsuan Yang",
            "Bhiksha Raj",
            "Min Xu"
        ],
        "title": "MASIV: Toward Material-Agnostic System Identification from Videos",
        "abstract": "arXiv:2508.01112v1 Announce Type: new  Abstract: System identification from videos aims to recover object geometry and governing physical laws. Existing methods integrate differentiable rendering with simulation but rely on predefined material priors, limiting their ability to handle unknown ones. We introduce MASIV, the first vision-based framework for material-agnostic system identification. Unlike existing approaches that depend on hand-crafted constitutive laws, MASIV employs learnable neural constitutive models, inferring object dynamics without assuming a scene-specific material prior. However, the absence of full particle state information imposes unique challenges, leading to unstable optimization and physically implausible behaviors. To address this, we introduce dense geometric guidance by reconstructing continuum particle trajectories, providing temporally rich motion constraints beyond sparse visual cues. Comprehensive experiments show that MASIV achieves state-of-the-art performance in geometric accuracy, rendering quality, and generalization ability.",
        "arxiv_id": "2508.01112",
        "ARXIVID": "2508.01112",
        "COMMENT": "Related to system identification and physical reasoning from video, but does not directly match any of the four criteria. Some relevance to embodied AI, but not focused on spatial intelligence or benchmarks.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.01210": {
        "authors": [
            "Tianze Wang",
            "Zhang Zhang",
            "Chao Yue",
            "Nuoran Li",
            "Chao Sun"
        ],
        "title": "RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification",
        "abstract": "arXiv:2508.01210v1 Announce Type: new  Abstract: Acquiring the road surface conditions in advance based on visual technologies provides effective information for the planning and control system of autonomous vehicles, thus improving the safety and driving comfort of the vehicles. Recently, the Mamba architecture based on state-space models has shown remarkable performance in visual processing tasks, benefiting from the efficient global receptive field. However, existing Mamba architectures struggle to achieve state-of-the-art visual road surface classification due to their lack of effective extraction of the local texture of the road surface. In this paper, we explore for the first time the potential of visual Mamba architectures for road surface classification task and propose a method that effectively combines local and global perception, called RoadMamba. Specifically, we utilize the Dual State Space Model (DualSSM) to effectively extract the global semantics and local texture of the road surface and decode and fuse the dual features through the Dual Attention Fusion (DAF). In addition, we propose a dual auxiliary loss to explicitly constrain dual branches, preventing the network from relying only on global semantic information from the deep large receptive field and ignoring the local texture. The proposed RoadMamba achieves the state-of-the-art performance in experiments on a large-scale road surface classification dataset containing 1 million samples.",
        "arxiv_id": "2508.01210",
        "ARXIVID": "2508.01210",
        "COMMENT": "RoadMamba introduces a dual-branch visual state space model for road surface classification, combining global and local features. While it is a novel vision model, it is not about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to criterion 4, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.02409": {
        "authors": [
            "Yimeng Liu",
            "Maolin Gan",
            "Huaili Zeng",
            "Li Liu",
            "Younsuk Dong",
            "Zhichao Cao"
        ],
        "title": "Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion",
        "abstract": "arXiv:2508.02409v1 Announce Type: new  Abstract: Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is crucial in the development of plant diseases. Existing LWD detection lacks standardized measurement techniques, and variations across different plant characteristics limit its effectiveness. Prior research proposes diverse approaches, but they fail to measure real natural leaves directly and lack resilience in various environmental conditions. This reduces the precision and robustness, revealing a notable practical application and effectiveness gap in real-world agricultural settings. This paper presents Hydra, an innovative approach that integrates millimeter-wave (mm-Wave) radar with camera technology to detect leaf wetness by determining if there is water on the leaf. We can measure the time to determine the LWD based on this detection. Firstly, we design a Convolutional Neural Network (CNN) to selectively fuse multiple mm-Wave depth images with an RGB image to generate multiple feature images. Then, we develop a transformer-based encoder to capture the inherent connection among the multiple feature images to generate a feature map, which is further fed to a classifier for detection. Moreover, we augment the dataset during training to generalize our model. Implemented using a frequency-modulated continuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance is meticulously evaluated on plants, demonstrating the potential to classify leaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra in the farm, including rainy, dawn, or poorly light nights, it still achieves an accuracy rate of around 90%.",
        "arxiv_id": "2508.02409",
        "ARXIVID": "2508.02409",
        "COMMENT": "Hydra proposes a multi-modal (mm-Wave + camera) fusion method for leaf wetness detection, using CNNs and transformers. This is a novel multi-modal sensing method, but not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to criterion 4 (vision foundation models and applications), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.02043": {
        "authors": [
            "Hui Xie",
            "Haiqin Hu",
            "Lijuan Ding",
            "Qing Li",
            "Yue Sun",
            "Tao Tan"
        ],
        "title": "Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction",
        "abstract": "arXiv:2508.02043v1 Announce Type: new  Abstract: Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency.",
        "arxiv_id": "2508.02043",
        "ARXIVID": "2508.02043",
        "COMMENT": "Applies conditional diffusion models to radiotherapy dose prediction, which is a novel application of generative modeling, but not directly related to spatial intelligence on embodied agents or vision foundation models. Some relevance to generative modeling.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.02197": {
        "authors": [
            "Wouter W. L. Nuijten",
            "Mykola Lukashchuk",
            "Thijs van de Laar",
            "Bert de Vries"
        ],
        "title": "A Message Passing Realization of Expected Free Energy Minimization",
        "abstract": "arXiv:2508.02197v1 Announce Type: new  Abstract: We present a message passing approach to Expected Free Energy (EFE) minimization on factor graphs, based on the theory introduced in arXiv:2504.14898. By reformulating EFE minimization as Variational Free Energy minimization with epistemic priors, we transform a combinatorial search problem into a tractable inference problem solvable through standard variational techniques. Applying our message passing method to factorized state-space models enables efficient policy inference. We evaluate our method on environments with epistemic uncertainty: a stochastic gridworld and a partially observable Minigrid task. Agents using our approach consistently outperform conventional KL-control agents on these tasks, showing more robust planning and efficient exploration under uncertainty. In the stochastic gridworld environment, EFE-minimizing agents avoid risky paths, while in the partially observable minigrid setting, they conduct more systematic information-seeking. This approach bridges active inference theory with practical implementations, providing empirical evidence for the efficiency of epistemic priors in artificial agents.",
        "arxiv_id": "2508.02197",
        "ARXIVID": "2508.02197",
        "COMMENT": "This paper presents a message passing approach to Expected Free Energy minimization for artificial agents in uncertain environments. It is relevant to embodied AI and planning, but does not introduce a new benchmark or simulator, nor does it focus on spatial intelligence or vision-language models. It is more about statistical inference in agent planning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.01031": {
        "authors": [
            "Jingzhe Ni",
            "Xiaolong Yin",
            "Xintong Li",
            "Xingyu Lu",
            "Ji Wei",
            "Ruofeng Tong",
            "Min Tang",
            "Peng Du"
        ],
        "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent",
        "abstract": "arXiv:2508.01031v1 Announce Type: new  Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.",
        "arxiv_id": "2508.01031",
        "ARXIVID": "2508.01031",
        "COMMENT": "This paper introduces an LLM-powered agent for conceptual CAD design, accepting both text and sketches, and generating CAD code with iterative visual feedback. It is a new method for multi-modal interaction and agent-based design, but does not focus on spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Closest to criterion 2, but not a new VLLM/MLLM.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.01749": {
        "authors": [
            "Runkai Zheng",
            "Vishnu Asutosh Dasu",
            "Yinong Oliver Wang",
            "Haohan Wang",
            "Fernando De la Torre"
        ],
        "title": "Improving Noise Efficiency in Privacy-preserving Dataset Distillation",
        "abstract": "arXiv:2508.01749v1 Announce Type: new  Abstract: Modern machine learning models heavily rely on large datasets that often include sensitive and private information, raising serious privacy concerns. Differentially private (DP) data generation offers a solution by creating synthetic datasets that limit the leakage of private information within a predefined privacy budget; however, it requires a substantial amount of data to achieve performance comparable to models trained on the original data. To mitigate the significant expense incurred with synthetic data generation, Dataset Distillation (DD) stands out for its remarkable training and storage efficiency. This efficiency is particularly advantageous when integrated with DP mechanisms, curating compact yet informative synthetic datasets without compromising privacy. However, current state-of-the-art private DD methods suffer from a synchronized sampling-optimization process and the dependency on noisy training signals from randomly initialized networks. This results in the inefficient utilization of private information due to the addition of excessive noise. To address these issues, we introduce a novel framework that decouples sampling from optimization for better convergence and improves signal quality by mitigating the impact of DP noise through matching in an informative subspace. On CIFAR-10, our method achieves a \\textbf{10.0\\%} improvement with 50 images per class and \\textbf{8.3\\%} increase with just \\textbf{one-fifth} the distilled set size of previous state-of-the-art methods, demonstrating significant potential to advance privacy-preserving DD.",
        "arxiv_id": "2508.01749",
        "ARXIVID": "2508.01749",
        "COMMENT": "This paper proposes a new framework for privacy-preserving dataset distillation, improving noise efficiency and performance under differential privacy. While it is a clever statistical method, it does not match the four criteria directly.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02085": {
        "authors": [
            "Jiaye Lin",
            "Yifu Guo",
            "Yuzhen Han",
            "Sen Hu",
            "Ziyi Ni",
            "Licheng Wang",
            "Mingguang Chen",
            "Daxin Jiang",
            "Binxing Jiao",
            "Chen Hu",
            "Huacan Wang"
        ],
        "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
        "abstract": "arXiv:2508.02085v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents' interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at https://github.com/wanghuacan/SE-Agent.",
        "arxiv_id": "2508.02085",
        "ARXIVID": "2508.02085",
        "COMMENT": "This paper introduces SE-Agent, a self-evolution framework for LLM-based agents to optimize multi-step reasoning trajectories. While it is about agent reasoning, it does not focus on spatial intelligence, embodied AI, or vision-language models, so it does not match the criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02034": {
        "authors": [
            "Ziling Wang",
            "Shuya Yang",
            "Jialin Lu",
            "Ka-Ho Chow"
        ],
        "title": "Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure",
        "abstract": "arXiv:2508.02034v1 Announce Type: new  Abstract: Face recognition (FR) technologies are increasingly used to power large-scale image retrieval systems, raising serious privacy concerns. Services like Clearview AI and PimEyes allow anyone to upload a facial photo and retrieve a large amount of online content associated with that person. This not only enables identity inference but also exposes their digital footprint, such as social media activity, private photos, and news reports, often without their consent. In response to this emerging threat, we propose Protego, a user-centric privacy protection method that safeguards facial images from such retrieval-based privacy intrusions. Protego encapsulates a user's 3D facial signatures into a pose-invariant 2D representation, which is dynamically deformed into a natural-looking 3D mask tailored to the pose and expression of any facial image of the user, and applied prior to online sharing. Motivated by a critical limitation of existing methods, Protego amplifies the sensitivity of FR models so that protected images cannot be matched even among themselves. Experiments show that Protego significantly reduces retrieval accuracy across a wide range of black-box FR models and performs at least 2x better than existing methods. It also offers unprecedented visual coherence, particularly in video settings where consistency and natural appearance are essential. Overall, Protego contributes to the fight against the misuse of FR for mass surveillance and unsolicited identity tracing.",
        "arxiv_id": "2508.02034",
        "ARXIVID": "2508.02034",
        "COMMENT": "Protego is a user-centric privacy protection method against face recognition, using pose-invariant 2D/3D facial representations. While it is a vision application, it does not match the specific criteria of spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02480": {
        "authors": [
            "Wenwen Zeng",
            "Yonghuang Wu",
            "Yifan Chen",
            "Xuan Xie",
            "Chengqian Zhao",
            "Feiyu Yin",
            "Guoqing Wu",
            "Jinhua Yu"
        ],
        "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
        "abstract": "arXiv:2508.02480v1 Announce Type: new  Abstract: Reconstructing dynamic videos from fMRI is important for understanding visual cognition and enabling vivid brain-computer interfaces. However, current methods are critically limited to single-shot clips, failing to address the multi-shot nature of real-world experiences. Multi-shot reconstruction faces fundamental challenges: fMRI signal mixing across shots, the temporal resolution mismatch between fMRI and video obscuring rapid scene changes, and the lack of dedicated multi-shot fMRI-video datasets. To overcome these limitations, we propose a novel divide-and-decode framework for multi-shot fMRI video reconstruction. Our core innovations are: (1) A shot boundary predictor module explicitly decomposing mixed fMRI signals into shot-specific segments. (2) Generative keyframe captioning using LLMs, which decodes robust textual descriptions from each segment, overcoming temporal blur by leveraging high-level semantics. (3) Novel large-scale data synthesis (20k samples) from existing datasets. Experimental results demonstrate our framework outperforms state-of-the-art methods in multi-shot reconstruction fidelity. Ablation studies confirm the critical role of fMRI decomposition and semantic captioning, with decomposition significantly improving decoded caption CLIP similarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI reconstruction, enabling accurate recovery of complex visual narratives through explicit decomposition and semantic prompting.",
        "arxiv_id": "2508.02480",
        "ARXIVID": "2508.02480",
        "COMMENT": "MindShot proposes a new framework for multi-shot video reconstruction from fMRI using LLM-based decoding and explicit shot decomposition. While it is a novel multi-modal generative modeling approach, it is more neuroscience-focused and does not directly match the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02348": {
        "authors": [
            "Byeonggyu Park",
            "Hee-Yeun Kim",
            "Byonghyok Choi",
            "Hansang Cho",
            "Byungkwan Kim",
            "Soomok Lee",
            "Mingu Jeon",
            "Seong-Woo Kim"
        ],
        "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera",
        "abstract": "arXiv:2508.02348v1 Announce Type: new  Abstract: Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban environments poses a significant challenge for autonomous driving systems. While mmWave radar has demonstrated potential for detecting objects in such scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions caused by multipath reflections, making accurate spatial inference difficult. Additionally, although camera images provide high-resolution visual information, they lack depth perception and cannot directly observe objects in NLoS regions. In this paper, we propose a novel framework that interprets radar PCD through road layout inferred from camera for localization of NLoS pedestrians. The proposed method leverages visual information from the camera to interpret 2D radar PCD, enabling spatial scene reconstruction. The effectiveness of the proposed approach is validated through experiments conducted using a radar-camera system mounted on a real vehicle. The localization performance is evaluated using a dataset collected in outdoor NLoS driving environments, demonstrating the practical applicability of the method.",
        "arxiv_id": "2508.02348",
        "ARXIVID": "2508.02348",
        "COMMENT": "Related to criterion 1 (spatial understanding) and criterion 4 (vision foundation models and applications) as it fuses mmWave radar and camera for NLoS pedestrian localization, but the main novelty is in sensor fusion for AVs rather than embodied agents or foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.01218": {
        "authors": [
            "Yujian Liu",
            "Linlang Cao",
            "Chuang Chen",
            "Fanyu Geng",
            "Dongxu Shen",
            "Peng Cao",
            "Shidang Xu",
            "Xiaoli Liu"
        ],
        "title": "MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry",
        "abstract": "arXiv:2508.01218v1 Announce Type: new  Abstract: Existing 3D head avatar reconstruction methods adopt a two-stage process, relying on tracked FLAME meshes derived from facial landmarks, followed by Gaussian-based rendering. However, misalignment between the estimated mesh and target images often leads to suboptimal rendering quality and loss of fine visual details. In this paper, we present MoGaFace, a novel 3D head avatar modeling framework that continuously refines facial geometry and texture attributes throughout the Gaussian rendering process. To address the misalignment between estimated FLAME meshes and target images, we introduce the Momentum-Guided Consistent Geometry module, which incorporates a momentum-updated expression bank and an expression-aware correction mechanism to ensure temporal and multi-view consistency. Additionally, we propose Latent Texture Attention, which encodes compact multi-view features into head-aware representations, enabling geometry-aware texture refinement via integration into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity head avatar reconstruction and significantly improves novel-view synthesis quality, even under inaccurate mesh initialization and unconstrained real-world settings.",
        "arxiv_id": "2508.01218",
        "ARXIVID": "2508.01218",
        "COMMENT": "This paper proposes MoGaFace, a new method for 3D head avatar modeling with momentum-guided geometry refinement and texture-aware Gaussian rendering. While it is a novel vision method, it does not match the criteria of spatial intelligence for embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.01650": {
        "authors": [
            "Na Zhang",
            "Moran Li",
            "Chengming Xu",
            "Han Feng",
            "Xiaobin Hu",
            "Jiangning Zhang",
            "Weijian Cao",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "title": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance",
        "abstract": "arXiv:2508.01650v1 Announce Type: new  Abstract: Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).",
        "arxiv_id": "2508.01650",
        "ARXIVID": "2508.01650",
        "COMMENT": "Does not match any specific criterion. Focuses on sketch-based hair strand generation, which is a generative vision task but not related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02493": {
        "authors": [
            "Jianchao Wang",
            "Peng Zhou",
            "Cen Li",
            "Rong Quan",
            "Jie Qin"
        ],
        "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting",
        "abstract": "arXiv:2508.02493v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. Our implementation will be released on GitHub.",
        "arxiv_id": "2508.02493",
        "ARXIVID": "2508.02493",
        "COMMENT": "Does not match any specific criterion. Focuses on 3D Gaussian Splatting for 3D reconstruction, not directly related to spatial intelligence in embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02165": {
        "authors": [
            "Jia-Chen Zhang",
            "Yu-Jie Xiong"
        ],
        "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs",
        "abstract": "arXiv:2508.02165v1 Announce Type: new  Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable performance in subject-driven or style-driven generation tasks. Studies have explored combinations of different LoRAs to jointly generate learned styles and content. However, current methods struggle to balance the original subject and style, and often require additional training. Recently, K-LoRA proposed a training-free LoRA fusion method. But it involves multiple hyperparameters, making it difficult to adapt to all styles and subjects. In this paper, we propose EST-LoRA, a training-free adaptive LoRA fusion method. It comprehensively considers three critical factors: \\underline{E}nergy of matrix, \\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to the Mixture of Experts (MoE) architecture, the model adaptively selects between subject LoRA and style LoRA within each attention layer. This integrated selection mechanism ensures balanced contributions from both components during the generation process. Experimental results show that EST-LoRA outperforms state-of-the-art methods in both qualitative and quantitative evaluations and achieves faster generation speed compared to other efficient fusion approaches. Our code is publicly available at: https://anonymous.4open.science/r/EST-LoRA-F318.",
        "arxiv_id": "2508.02165",
        "ARXIVID": "2508.02165",
        "COMMENT": "Related to generative modeling and LoRA fusion, but does not directly match any of the four criteria. Closest to general interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02511": {
        "authors": [
            "Chenxu Yang",
            "Qingyi Si",
            "Mz Dai",
            "Dingyu Yao",
            "Mingyu Zheng",
            "Minghui Chen",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "title": "Test-time Prompt Intervention",
        "abstract": "arXiv:2508.02511v1 Announce Type: new  Abstract: Test-time compute has led to remarkable success in the large language model (LLM) community, particularly for complex tasks, where longer chains of thought (CoTs) are generated to enhance reasoning capabilities. However, growing evidence reveals that such reasoning models often produce CoTs plagued by excessive redundancy, including unnecessary verification steps and repetitive reasoning shifts. The root cause lies in post-training of them that overly rely on outcome reward paradigms, as the data of process reward paradigms, which regulate intermediate reasoning steps, is difficult to construct at scale. To address this, we propose PI, a novel framework for Test-time Prompt Intervention. PI provides an interface to dynamically guide and regulate reasoning paths during inference through timely (When module) and proper (How module) interventions and post-intervention sampling (Which module). This allows human problem-solving expertise and cognitive science principles to be seamlessly integrated into LLMs' reasoning processes, enhancing controllability and interpretability. Extensive experiments across multiple models and datasets demonstrate that PI significantly shortens CoTs while reducing hallucination, yielding more concise and reliable reasoning.",
        "arxiv_id": "2508.02511",
        "ARXIVID": "2508.02511",
        "COMMENT": "This paper introduces a framework for test-time prompt intervention in LLMs to improve reasoning efficiency and reduce hallucination. While it is an interesting LLM paper, it does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01873": {
        "authors": [
            "Siran Peng",
            "Haoyuan Zhang",
            "Li Gao",
            "Tianshuo Zhang",
            "Bao Li",
            "Zhen Lei"
        ],
        "title": "DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization",
        "abstract": "arXiv:2508.01873v1 Announce Type: new  Abstract: The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness.",
        "arxiv_id": "2508.01873",
        "ARXIVID": "2508.01873",
        "COMMENT": "DiffusionFF introduces a diffusion-based method for face forgery detection and artifact localization. While it is a novel application of diffusion models in vision, it does not directly match the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01594": {
        "authors": [
            "Kai Han",
            "Chongwen Lyu",
            "Lele Ma",
            "Chengxuan Qian",
            "Siqi Ma",
            "Zheng Pang",
            "Jun Chen",
            "Zhe Liu"
        ],
        "title": "CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis",
        "abstract": "arXiv:2508.01594v1 Announce Type: new  Abstract: Clinicians usually combine information from multiple sources to achieve the most accurate diagnosis, and this has sparked increasing interest in leveraging multimodal deep learning for diagnosis. However, in real clinical scenarios, due to differences in incidence rates, multimodal medical data commonly face the issue of class imbalance, which makes it difficult to adequately learn the features of minority classes. Most existing methods tackle this issue with resampling or loss reweighting, but they are prone to overfitting or underfitting and fail to capture cross-modal interactions. Therefore, we propose a Curriculum Learning framework for Imbalanced Multimodal Diagnosis (CLIMD). Specifically, we first design multimodal curriculum measurer that combines two indicators, intra-modal confidence and inter-modal complementarity, to enable the model to focus on key samples and gradually adapt to complex category distributions. Additionally, a class distribution-guided training scheduler is introduced, which enables the model to progressively adapt to the imbalanced class distribution during training. Extensive experiments on multiple multimodal medical datasets demonstrate that the proposed method outperforms state-of-the-art approaches across various metrics and excels in handling imbalanced multimodal medical data. Furthermore, as a plug-and-play CL framework, CLIMD can be easily integrated into other models, offering a promising path for improving multimodal disease diagnosis accuracy. Code is publicly available at https://github.com/KHan-UJS/CLIMD.",
        "arxiv_id": "2508.01594",
        "ARXIVID": "2508.01594",
        "COMMENT": "This paper proposes a curriculum learning framework for imbalanced multimodal diagnosis, focusing on medical data. While it introduces a new method for multimodal learning, it is more of an application in medical AI and does not directly address the core criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02340": {
        "authors": [
            "Fan Hu",
            "Zijie Xin",
            "Xirong Li"
        ],
        "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search",
        "abstract": "arXiv:2508.02340v1 Announce Type: new  Abstract: Ad-hoc Video Search (AVS) involves using a textual query to search for multiple relevant videos in a large collection of unlabeled short videos. The main challenge of AVS is the visual diversity of relevant videos. A simple query such as \"Find shots of a man and a woman dancing together indoors\" can span a multitude of environments, from brightly lit halls and shadowy bars to dance scenes in black-and-white animations. It is therefore essential to retrieve relevant videos as comprehensively as possible. Current solutions for the AVS task primarily fuse multiple features into one or more common spaces, yet overlook the need for diverse spaces. To fully exploit the expressive capability of individual features, we propose LPD, short for Learning Partially Decorrelated common spaces. LPD incorporates two key innovations: feature-specific common space construction and the de-correlation loss. Specifically, LPD learns a separate common space for each video and text feature, and employs de-correlation loss to diversify the ordering of negative samples across different spaces. To enhance the consistency of multi-space convergence, we designed an entropy-based fair multi-space triplet ranking loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces highlight its ability to enhance result diversity.",
        "arxiv_id": "2508.02340",
        "ARXIVID": "2508.02340",
        "COMMENT": "This paper proposes a new method (LPD) for ad-hoc video search by learning partially decorrelated common spaces for video-text retrieval. It is relevant to multi-modal learning and retrieval, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. Closest to criterion 4, but not a direct match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01432": {
        "authors": [
            "Yuanzhe Shen",
            "Kaimin Wang",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "title": "TripTailor: A Real-World Benchmark for Personalized Travel Planning",
        "abstract": "arXiv:2508.01432v1 Announce Type: new  Abstract: The continuous evolution and enhanced reasoning capabilities of large language models (LLMs) have elevated their role in complex tasks, notably in travel planning, where demand for personalized, high-quality itineraries is rising. However, current benchmarks often rely on unrealistic simulated data, failing to reflect the differences between LLM-generated and real-world itineraries. Existing evaluation metrics, which primarily emphasize constraints, fall short of providing a comprehensive assessment of the overall quality of travel plans. To address these limitations, we introduce TripTailor, a benchmark designed specifically for personalized travel planning in real-world scenarios. This dataset features an extensive collection of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries, complete with detailed information, providing a more authentic evaluation framework. Experiments show that fewer than 10\\% of the itineraries generated by the latest state-of-the-art LLMs achieve human-level performance. Moreover, we identify several critical challenges in travel planning, including the feasibility, rationality, and personalized customization of the proposed solutions. We hope that TripTailor will drive the development of travel planning agents capable of understanding and meeting user needs while generating practical itineraries. Our code and dataset are available at https://github.com/swxkfm/TripTailor",
        "arxiv_id": "2508.01432",
        "ARXIVID": "2508.01432",
        "COMMENT": "This paper introduces a new benchmark for personalized travel planning using LLMs, but it is not focused on spatial intelligence for embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models. It is more about LLMs in planning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01852": {
        "authors": [
            "Junlong Tong",
            "Wei Zhang",
            "Yaohui Jin",
            "Xiaoyu Shen"
        ],
        "title": "Context Guided Transformer Entropy Modeling for Video Compression",
        "abstract": "arXiv:2508.01852v1 Announce Type: new  Abstract: Conditional entropy models effectively leverage spatio-temporal contexts to reduce video redundancy. However, incorporating temporal context often introduces additional model complexity and increases computational cost. In parallel, many existing spatial context models lack explicit modeling the ordering of spatial dependencies, which may limit the availability of relevant context during decoding. To address these issues, we propose the Context Guided Transformer (CGT) entropy model, which estimates probability mass functions of the current frame conditioned on resampled temporal context and dependency-weighted spatial context. A temporal context resampler learns predefined latent queries to extract critical temporal information using transformer encoders, reducing downstream computational overhead. Meanwhile, a teacher-student network is designed as dependency-weighted spatial context assigner to explicitly model the dependency of spatial context order. The teacher generates an attention map to represent token importance and an entropy map to reflect prediction certainty from randomly masked inputs, guiding the student to select the weighted top-k tokens with the highest spatial dependency. During inference, only the student is used to predict undecoded tokens based on high-dependency context. Experimental results demonstrate that our CGT model reduces entropy modeling time by approximately 65% and achieves an 11% BD-Rate reduction compared to the previous state-of-the-art conditional entropy model.",
        "arxiv_id": "2508.01852",
        "ARXIVID": "2508.01852",
        "COMMENT": "This paper proposes a new transformer-based entropy model for video compression, leveraging spatio-temporal context and explicit spatial dependency modeling. While it involves spatial context modeling, it is focused on video compression rather than spatial understanding for embodied agents or vision-language models. It does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01259": {
        "authors": [
            "Zhengxue Wang",
            "Yuan Wu",
            "Xiang Li",
            "Zhiqiang Yan",
            "Jian Yang"
        ],
        "title": "SpatioTemporal Difference Network for Video Depth Super-Resolution",
        "abstract": "arXiv:2508.01259v1 Announce Type: new  Abstract: Depth super-resolution has achieved impressive performance, and the incorporation of multi-frame information further enhances reconstruction quality. Nevertheless, statistical analyses reveal that video depth super-resolution remains affected by pronounced long-tailed distributions, with the long-tailed effects primarily manifesting in spatial non-smooth regions and temporal variation zones. To address these challenges, we propose a novel SpatioTemporal Difference Network (STDNet) comprising two core branches: a spatial difference branch and a temporal difference branch. In the spatial difference branch, we introduce a spatial difference mechanism to mitigate the long-tailed issues in spatial non-smooth regions. This mechanism dynamically aligns RGB features with learned spatial difference representations, enabling intra-frame RGB-D aggregation for depth calibration. In the temporal difference branch, we further design a temporal difference strategy that preferentially propagates temporal variation information from adjacent RGB and depth frames to the current depth frame, leveraging temporal difference representations to achieve precise motion compensation in temporal long-tailed areas. Extensive experimental results across multiple datasets demonstrate the effectiveness of our STDNet, outperforming existing approaches.",
        "arxiv_id": "2508.01259",
        "ARXIVID": "2508.01259",
        "COMMENT": "This paper introduces a SpatioTemporal Difference Network for video depth super-resolution, addressing long-tailed distributions in spatial and temporal domains. It is a methodological improvement in video depth estimation, but does not directly address the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02180": {
        "authors": [
            "Zeshuai Deng",
            "Guohao Chen",
            "Shuaicheng Niu",
            "Hui Luo",
            "Shuhai Zhang",
            "Yifan Yang",
            "Renjie Chen",
            "Wei Luo",
            "Mingkui Tan"
        ],
        "title": "Test-Time Model Adaptation for Quantized Neural Networks",
        "abstract": "arXiv:2508.02180v1 Announce Type: new  Abstract: Quantizing deep models prior to deployment is a widely adopted technique to speed up inference for various real-time applications, such as autonomous driving. However, quantized models often suffer from severe performance degradation in dynamic environments with potential domain shifts and this degradation is significantly more pronounced compared with their full-precision counterparts, as shown by our theoretical and empirical illustrations. To address the domain shift problem, test-time adaptation (TTA) has emerged as an effective solution by enabling models to learn adaptively from test data. Unfortunately, existing TTA methods are often impractical for quantized models as they typically rely on gradient backpropagation--an operation that is unsupported on quantized models due to vanishing gradients, as well as memory and latency constraints. In this paper, we focus on TTA for quantized models to improve their robustness and generalization ability efficiently. We propose a continual zeroth-order adaptation (ZOA) framework that enables efficient model adaptation using only two forward passes, eliminating the computational burden of existing methods. Moreover, we propose a domain knowledge management scheme to store and reuse different domain knowledge with negligible memory consumption, reducing the interference of different domain knowledge and fostering the knowledge accumulation during long-term adaptation. Experimental results on three classical architectures, including quantized transformer-based and CNN-based models, demonstrate the superiority of our methods for quantized model adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve a 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The source code is available at https://github.com/DengZeshuai/ZOA.",
        "arxiv_id": "2508.02180",
        "ARXIVID": "2508.02180",
        "COMMENT": "This paper presents a new test-time adaptation method for quantized neural networks, focusing on robustness and generalization. While it is a methodological improvement in model adaptation, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01087": {
        "authors": [
            "Ryan Rabinowitz",
            "Steve Cruz",
            "Walter Scheirer",
            "Terrance E. Boult"
        ],
        "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition",
        "abstract": "arXiv:2508.01087v1 Announce Type: new  Abstract: Handling novelty remains a key challenge in visual recognition systems. Existing open-set recognition (OSR) methods rely on the familiarity hypothesis, detecting novelty by the absence of familiar features. We propose a novel attenuation hypothesis: small weights learned during training attenuate features and serve a dual role-differentiating known classes while discarding information useful for distinguishing known from unknown classes. To leverage this overlooked information, we present COSTARR, a novel approach that combines both the requirement of familiar features and the lack of unfamiliar ones. We provide a probabilistic interpretation of the COSTARR score, linking it to the likelihood of correct classification and belonging in a known class. To determine the individual contributions of the pre- and post-attenuated features to COSTARR's performance, we conduct ablation studies that show both pre-attenuated deep features and the underutilized post-attenuated Hadamard product features are essential for improving OSR. Also, we evaluate COSTARR in a large-scale setting using ImageNet2012-1K as known data and NINCO, iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments demonstrate that COSTARR generalizes effectively across various architectures and significantly outperforms prior state-of-the-art methods by incorporating previously discarded attenuation information, advancing open-set recognition capabilities.",
        "arxiv_id": "2508.01087",
        "ARXIVID": "2508.01087",
        "COMMENT": "This paper proposes a new method (COSTARR) for open-set recognition in visual recognition systems, leveraging previously discarded attenuation information. While it is a methodological improvement in visual recognition, it does not directly address spatial understanding, embodied agents, VLLMs/MLLMs, or vision foundation models. It is most relevant to general computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01773": {
        "authors": [
            "Jiuzhou Han",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning",
        "abstract": "arXiv:2508.01773v1 Announce Type: new  Abstract: Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.",
        "arxiv_id": "2508.01773",
        "ARXIVID": "2508.01773",
        "COMMENT": "Does not match any specific criterion. Focuses on mathematical reasoning and process reward models, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.01382": {
        "authors": [
            "Qiang Guo",
            "Rubo Zhang",
            "Bingbing Zhang",
            "Junjie Liu",
            "Jianqing Liu"
        ],
        "title": "A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods",
        "abstract": "arXiv:2508.01382v1 Announce Type: new  Abstract: False positives in pedestrian detection remain a challenge that has yet to be effectively resolved. To address this issue, this paper proposes a Full-stage Refined Proposal (FRP) algorithm aimed at eliminating these false positives within a two-stage CNN-based pedestrian detection framework. The main innovation of this work lies in employing various pedestrian feature re-evaluation strategies to filter out low-quality pedestrian proposals during both the training and testing stages. Specifically, in the training phase, the Training mode FRP algorithm (TFRP) introduces a novel approach for validating pedestrian proposals to effectively guide the model training process, thereby constructing a model with strong capabilities for false positive suppression. During the inference phase, two innovative strategies are implemented: the Classifier-guided FRP (CFRP) algorithm integrates a pedestrian classifier into the proposal generation pipeline to yield high-quality proposals through pedestrian feature evaluation, and the Split-proposal FRP (SFRP) algorithm vertically divides all proposals, sending both the original and the sub-region proposals to the subsequent subnetwork to evaluate their confidence scores, filtering out those with lower sub-region pedestrian confidence scores. As a result, the proposed algorithm enhances the model's ability to suppress pedestrian false positives across all stages. Various experiments conducted on multiple benchmarks and the SY-Metro datasets demonstrate that the model, supported by different combinations of the FRP algorithm, can effectively eliminate false positives to varying extents. Furthermore, experiments conducted on embedded platforms underscore the algorithm's effectiveness in enhancing the comprehensive pedestrian detection capabilities of the small pedestrian detector in resource-constrained edge devices.",
        "arxiv_id": "2508.01382",
        "ARXIVID": "2508.01382",
        "COMMENT": "This paper proposes a refined proposal algorithm for suppressing false positives in pedestrian detection. While it is a methodological improvement in computer vision, it does not directly address the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.01633": {
        "authors": [
            "Wanhao Ma",
            "Wei Zhang",
            "Shuai Wan",
            "Fuzheng Yang"
        ],
        "title": "Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression",
        "abstract": "arXiv:2508.01633v1 Announce Type: new  Abstract: Geometry-based point cloud compression (G-PCC), an international standard designed by MPEG, provides a generic framework for compressing diverse types of point clouds while ensuring interoperability across applications and devices. However, G-PCC underperforms compared to recent deep learning-based PCC methods despite its lower computational power consumption. To enhance the efficiency of G-PCC without sacrificing its interoperability or computational flexibility, we propose a novel preprocessing framework that integrates a compression-oriented voxelization network with a differentiable G-PCC surrogate model, jointly optimized in the training phase. The surrogate model mimics the rate-distortion behaviour of the non-differentiable G-PCC codec, enabling end-to-end gradient propagation. The versatile voxelization network adaptively transforms input point clouds using learning-based voxelization and effectively manipulates point clouds via global scaling, fine-grained pruning, and point-level editing for rate-distortion trade-offs. During inference, only the lightweight voxelization network is appended to the G-PCC encoder, requiring no modifications to the decoder, thus introducing no computational overhead for end users. Extensive experiments demonstrate a 38.84% average BD-rate reduction over G-PCC. By bridging classical codecs with deep learning, this work offers a practical pathway to enhance legacy compression standards while preserving their backward compatibility, making it ideal for real-world deployment.",
        "arxiv_id": "2508.01633",
        "ARXIVID": "2508.01633",
        "COMMENT": "This paper presents a deep learning-based preprocessing framework to improve geometry-based point cloud compression. While it is relevant to computer vision and machine learning, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.00898": {
        "authors": [
            "Jose M. S\\'anchez Vel\\'azquez",
            "Mingbo Cai",
            "Andrew Coney",
            "\\'Alvaro J. Garc\\'ia- Tejedor",
            "Alberto Nogales"
        ],
        "title": "Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models",
        "abstract": "arXiv:2508.00898v1 Announce Type: new  Abstract: In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.",
        "arxiv_id": "2508.00898",
        "ARXIVID": "2508.00898",
        "COMMENT": "This paper evaluates hybrid deep learning models for video frame prediction, combining autoencoders and temporal sequence models. While it is relevant to computer vision and temporal modeling, it does not match any of the four criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.00974": {
        "authors": [
            "Daniel Andr\\'es L\\'opez",
            "Vincent Weber",
            "Severin Zentgraf",
            "Barlo Hillen",
            "Perikles Simon",
            "Elmar Sch\\\"omer"
        ],
        "title": "ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling",
        "abstract": "arXiv:2508.00974v1 Announce Type: new  Abstract: Infrared thermography is emerging as a powerful tool in sports medicine, allowing assessment of thermal radiation during exercise and analysis of anatomical regions of interest, such as the well-exposed calves. Building on our previous advanced automatic annotation method, we aimed to transfer the stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling. Therefore, the training of the semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images has been examined and compared in different data set combinations. The results indicate that fine-tuning with a small fraction of manual data is sufficient to improve the overall performance of the deep neural network. Finally, combining automatically generated labels with small manually annotated data sets accelerates the adaptation of deep neural networks to new use cases, such as the transition from treadmill to bicycle.",
        "arxiv_id": "2508.00974",
        "ARXIVID": "2508.00974",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models and applications) as it adapts a deep neural network for thermogram labeling in sports medicine, but is more of an application paper with limited methodological novelty.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}