{
    "2512.14696": {
        "authors": [
            "Zihan Wang",
            "Jiashun Wang",
            "Jeff Tan",
            "Yiwen Zhao",
            "Jessica Hodgins",
            "Shubham Tulsiani",
            "Deva Ramanan"
        ],
        "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
        "abstract": "arXiv:2512.14696v1 Announce Type: new  Abstract: We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
        "arxiv_id": "2512.14696",
        "ARXIVID": "2512.14696",
        "COMMENT": "CRISP introduces a method for recovering simulatable human motion and scene geometry from monocular video, with a focus on generating physically-valid human motion and interaction environments for real-to-sim applications. This directly matches criterion 3 (embodied AI, new methods for simulation and benchmarking) and also addresses spatial understanding in embodied agents (criterion 1). The use of contact-guided reconstruction and RL-based validation is a novel angle.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.14014": {
        "authors": [
            "Shufan Li",
            "Konstantinos Kallidromitis",
            "Akash Gokul",
            "Yusuke Kato",
            "Kazuki Kozuka",
            "Aditya Grover"
        ],
        "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
        "abstract": "arXiv:2512.14014v1 Announce Type: new  Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
        "arxiv_id": "2512.14014",
        "ARXIVID": "2512.14014",
        "COMMENT": "This paper matches criterion 3 very closely: it introduces MobileWorldBench, a new benchmark for evaluating vision-language models as world models for mobile GUI agents, and proposes a novel framework for integrating VLM world models into agent planning. It also releases a large-scale dataset. The focus on semantic world modeling for embodied agents and benchmarking is novel and directly relevant.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.13874": {
        "authors": [
            "Jitesh Jain",
            "Jialuo Li",
            "Zixian Ma",
            "Jieyu Zhang",
            "Chris Dongjoo Kim",
            "Sangho Lee",
            "Rohun Tripathi",
            "Tanmay Gupta",
            "Christopher Clark",
            "Humphrey Shi"
        ],
        "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
        "abstract": "arXiv:2512.13874v1 Announce Type: new  Abstract: As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
        "arxiv_id": "2512.13874",
        "ARXIVID": "2512.13874",
        "COMMENT": "Matches criterion 3: proposes SAGE, a new agent system for any-horizon video reasoning, introduces a new benchmark (SAGE-Bench) for long video reasoning, and a novel RL-based training recipe. Also touches on spatial intelligence in embodied agents (criterion 1).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.14008": {
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
        "abstract": "arXiv:2512.14008v1 Announce Type: new  Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.",
        "arxiv_id": "2512.14008",
        "ARXIVID": "2512.14008",
        "COMMENT": "Matches criterion 2 (Shows new VLLMs or MLLMs). Proposes Sparse-LaViDa, a new multimodal discrete diffusion language model with efficient inference for text-to-image generation, editing, and reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.14225": {
        "authors": [
            "Tao Tang",
            "Enhui Ma",
            "xia zhou",
            "Letian Wang",
            "Tianyi Yan",
            "Xueyang Zhang",
            "Kun Zhan",
            "Peng Jia",
            "XianPeng Lang",
            "Jia-Wang Bian",
            "Kaicheng Yu",
            "Xiaodan Liang"
        ],
        "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
        "abstract": "arXiv:2512.14225v1 Announce Type: new  Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
        "arxiv_id": "2512.14225",
        "ARXIVID": "2512.14225",
        "COMMENT": "OmniGen presents a unified generative model for multimodal sensor data (LiDAR and camera) for autonomous driving, using a shared BEV space and a Diffusion Transformer. This matches criterion 4 (vision foundation models and applications) and criterion 2 (new MLLMs), as it is a multi-modal generative model with a novel unified approach for sensor data.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14654": {
        "authors": [
            "Lihong Wang",
            "Liangqi Li",
            "Weiwei Feng",
            "Jiamin Wu",
            "Changtao Miao",
            "Tieru Wu",
            "Rui Ma",
            "Bo Zhang",
            "Zhe Li"
        ],
        "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking",
        "abstract": "arXiv:2512.14654v1 Announce Type: new  Abstract: CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.",
        "arxiv_id": "2512.14654",
        "ARXIVID": "2512.14654",
        "COMMENT": "This paper proposes ViRC, a framework for visual interleaved mathematical chain-of-thought (CoT) reasoning with a novel Reason Chunking mechanism, and introduces the CRUX dataset for multimodal mathematical reasoning. It directly matches criterion 2 (new MLLMs) and also provides a new dataset and training strategy, which is a novel angle for vision-language reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14126": {
        "authors": [
            "Junyi Wu",
            "Van Nguyen Nguyen",
            "Benjamin Planche",
            "Jiachen Tao",
            "Changchang Sun",
            "Zhongpai Gao",
            "Zhenghao Zhao",
            "Anwesa Choudhuri",
            "Gengyu Zhang",
            "Meng Zheng",
            "Feiran Wang",
            "Terrence Chen",
            "Yan Yan",
            "Ziyan Wu"
        ],
        "title": "Consistent Instance Field for Dynamic Scene Understanding",
        "abstract": "arXiv:2512.14126v1 Announce Type: new  Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
        "arxiv_id": "2512.14126",
        "ARXIVID": "2512.14126",
        "COMMENT": "This paper introduces a new spatio-temporal representation for dynamic scene understanding, using deformable 3D Gaussians and continuous probabilistic modeling. It is highly relevant to criterion 1 (new methodological improvements to spatial understanding for embodied agents), as it focuses on consistent instance representation in space and time, and also relates to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14614": {
        "authors": [
            "Wenqiang Sun",
            "Haiyu Zhang",
            "Haoyuan Wang",
            "Junta Wu",
            "Zehan Wang",
            "Zhenwei Wang",
            "Yunhong Wang",
            "Jun Zhang",
            "Tengfei Wang",
            "Chunchao Guo"
        ],
        "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "abstract": "arXiv:2512.14614v1 Announce Type: new  Abstract: This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
        "arxiv_id": "2512.14614",
        "ARXIVID": "2512.14614",
        "COMMENT": "This paper introduces WorldPlay, a real-time interactive world modeling system with long-term geometric consistency, using a streaming video diffusion model. It proposes several novel mechanisms for memory and consistency. This is highly relevant to criterion 3 (embodied AI, new methods for world modeling/simulators) and also touches on criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14017": {
        "authors": [
            "Zongyao Li",
            "Kengo Ishida",
            "Satoshi Yamazaki",
            "Xiaotong Ji",
            "Jianquan Liu"
        ],
        "title": "KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding",
        "abstract": "arXiv:2512.14017v1 Announce Type: new  Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.",
        "arxiv_id": "2512.14017",
        "ARXIVID": "2512.14017",
        "COMMENT": "Matches criterion 3: introduces KFS-Bench, a new benchmark for key frame sampling in long video QA, with novel multi-scene annotations and direct evaluation of sampling strategies. Also relevant to criterion 2 as it discusses MLLMs in the context of video QA.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14068": {
        "authors": [
            "Shuang Cheng",
            "Yuhua Jiang",
            "Zineng Zhou",
            "Dawei Liu",
            "Wang Tao",
            "Linfeng Zhang",
            "Biqing Qi",
            "Bowen Zhou"
        ],
        "title": "SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding",
        "abstract": "arXiv:2512.14068v1 Announce Type: new  Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \\textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \\emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \\textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \\textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \\textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \\emph{training efficiency}, \\emph{convergence stability}, and \\emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.",
        "arxiv_id": "2512.14068",
        "ARXIVID": "2512.14068",
        "COMMENT": "Matches criterion 2 (new VLLM/MLLM method: block-wise diffusion for vision-language understanding) and criterion 4 (vision foundation models and their applications). Also introduces new training techniques for diffusion-based VLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14137": {
        "authors": [
            "Ashish Mishra",
            "Tarun Kumar",
            "Gyanaranjan Nayak",
            "Arpit Shah",
            "Suparna Bhattacharya",
            "Martin Foltin"
        ],
        "title": "Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models",
        "abstract": "arXiv:2512.14137v1 Announce Type: new  Abstract: We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.",
        "arxiv_id": "2512.14137",
        "ARXIVID": "2512.14137",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes a novel, data-free, closed-form method for selective unlearning in CLIP (a vision-language foundation model), with strong privacy and model decontamination implications.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.14320": {
        "authors": [
            "Shuai Dong",
            "Jie Zhang",
            "Guoying Zhao",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
        "abstract": "arXiv:2512.14320v1 Announce Type: new  Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
        "arxiv_id": "2512.14320",
        "ARXIVID": "2512.14320",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a new method (SIFM) and metric (ISR) for image editing immunity using MLLMs for evaluation, with a novel perspective on semantic alignment disruption.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.14480": {
        "authors": [
            "Weiheng Zhao",
            "Zilong Huang",
            "Jiashi Feng",
            "Xinggang Wang"
        ],
        "title": "SuperCLIP: CLIP with Simple Classification Supervision",
        "abstract": "arXiv:2512.14480v1 Announce Type: new  Abstract: Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.",
        "arxiv_id": "2512.14480",
        "ARXIVID": "2512.14480",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes SuperCLIP, a methodological improvement to CLIP for better visual-textual alignment, with empirical results on zero-shot and retrieval tasks. Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.14336": {
        "authors": [
            "Jooyeol Yun",
            "Jaegul Choo"
        ],
        "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
        "abstract": "arXiv:2512.14336v1 Announce Type: new  Abstract: Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.",
        "arxiv_id": "2512.14336",
        "ARXIVID": "2512.14336",
        "COMMENT": "This paper presents a framework for animating vector graphics by recovering semantic structure, enabling vision-language models to produce more coherent SVG animations. It directly addresses a missing layer in current VLM systems and proposes a statistical aggregation method. This is a strong match for criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications), as it improves VLMs' handling of vector graphics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13855": {
        "authors": [
            "Ujjwal Mishra",
            "Vinita Shukla",
            "Praful Hambarde",
            "Amit Shukla"
        ],
        "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging",
        "abstract": "arXiv:2512.13855v1 Announce Type: new  Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.",
        "arxiv_id": "2512.13855",
        "ARXIVID": "2512.13855",
        "COMMENT": "Matches criterion 4: proposes Telescopic Adapters for efficient fine-tuning of vision-language segmentation models (VLSMs) in medical imaging, with a novel depth-aware scaling approach. Application to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13869": {
        "authors": [
            "Wenda Li",
            "Meng Wu",
            "Sungmin Eum",
            "Heesung Kwon",
            "Qing Qu"
        ],
        "title": "Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models",
        "abstract": "arXiv:2512.13869v1 Announce Type: new  Abstract: Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \\href{https://github.com/liwd190019/CFHA}{this url}.",
        "arxiv_id": "2512.13869",
        "ARXIVID": "2512.13869",
        "COMMENT": "Matches criterion 3: proposes a new diffusion-based method for domain adaptation in UAV-based human detection using synthetic simulators, addressing domain gap with a hierarchical alignment framework. Simulator and Sim2Real focus is relevant.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13876": {
        "authors": [
            "Ye Zhang",
            "Qi Chen",
            "Wenyou Huang",
            "Rui Liu",
            "Zhengjian Kang"
        ],
        "title": "Route-DETR: Pairwise Query Routing in Transformers for Object Detection",
        "abstract": "arXiv:2512.13876v1 Announce Type: new  Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.",
        "arxiv_id": "2512.13876",
        "ARXIVID": "2512.13876",
        "COMMENT": "Matches criterion 4 (Vision foundation models related and its applications). Proposes Route-DETR, a new method for object detection transformers with novel query routing mechanisms, improving state-of-the-art baselines.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14113": {
        "authors": [
            "Ashish Mishra",
            "Gyanaranjan Nayak",
            "Tarun Kumar",
            "Arpit Shah",
            "Suparna Bhattacharya",
            "Martin Foltin"
        ],
        "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach",
        "abstract": "arXiv:2512.14113v1 Announce Type: new  Abstract: Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.",
        "arxiv_id": "2512.14113",
        "ARXIVID": "2512.14113",
        "COMMENT": "Matches criterion 4 (Vision foundation models related and its applications). Proposes a novel, training- and data-free unlearning framework for CLIP, a vision-language foundation model, enabling selective and domain-agnostic forgetting.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14536": {
        "authors": [
            "Yiheng Huang",
            "Junhong Chen",
            "Anqi Ning",
            "Zhanhong Liang",
            "Nick Michiels",
            "Luc Claesen",
            "Wenyin Liu"
        ],
        "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
        "abstract": "arXiv:2512.14536v1 Announce Type: new  Abstract: Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
        "arxiv_id": "2512.14536",
        "ARXIVID": "2512.14536",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding on embodied agents): Proposes a new self-supervised framework for nighttime monocular depth estimation using spatiotemporal priors, with domain adaptation and novel loss functions. Also relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13796": {
        "authors": [
            "Victor Rong",
            "Jan Held",
            "Victor Chu",
            "Daniel Rebain",
            "Marc Van Droogenbroeck",
            "Kiriakos N. Kutulakos",
            "Andrea Tagliasacchi",
            "David B. Lindell"
        ],
        "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries",
        "abstract": "arXiv:2512.13796v1 Announce Type: new  Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.",
        "arxiv_id": "2512.13796",
        "ARXIVID": "2512.13796",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications: novel representation for real-time novel view synthesis). Also relevant for generative modeling in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2512.14052": {
        "authors": [
            "HyperAI Team",
            "Yuchen Liu",
            "Kaiyang Han",
            "Zhiqiang Xia",
            "Yuhang Dong",
            "Chen Song",
            "Kangyu Tang",
            "Jiaming Xu",
            "Xiushi Feng",
            "WenXuan Yu",
            "Li Peng",
            "Mingyang Wang",
            "Kai Wang",
            "Changpeng Yang",
            "Yang Li",
            "Haoyu Lu",
            "Hao Wang",
            "Bingna Xu",
            "Guangyao Liu",
            "Long Huang",
            "Kaibin Guo",
            "Jinyang Wu",
            "Dan Wu",
            "Hongzhen Wang",
            "Peng Zhou",
            "Shuai Nie",
            "Shande Wang",
            "Runyu Shi",
            "Ying Huang"
        ],
        "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
        "abstract": "arXiv:2512.14052v1 Announce Type: new  Abstract: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
        "arxiv_id": "2512.14052",
        "ARXIVID": "2512.14052",
        "COMMENT": "Matches criterion 2 (new MLLM: HyperVL, efficient and dynamic for edge devices) and criterion 4 (vision foundation models and applications, especially on-device).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14177": {
        "authors": [
            "Joseph Hoche",
            "Andrei Bursuc",
            "David Brellmann",
            "Gilles Louppe",
            "Pavel Izmailov",
            "Angela Yao",
            "Gianni Franchi"
        ],
        "title": "Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes",
        "abstract": "arXiv:2512.14177v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.",
        "arxiv_id": "2512.14177",
        "ARXIVID": "2512.14177",
        "COMMENT": "Matches criterion 2 (new methodological improvements for uncertainty quantification in LVLMs, a type of VLLM/MLLM). Also relevant for clever statistical tricks (Gaussian Processes, spectral representation).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14056": {
        "authors": [
            "Kim Sung-Bin",
            "Joohyun Chang",
            "David Harwath",
            "Tae-Hyun Oh"
        ],
        "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling",
        "abstract": "arXiv:2512.14056v1 Announce Type: new  Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.",
        "arxiv_id": "2512.14056",
        "ARXIVID": "2512.14056",
        "COMMENT": "FacEDiT proposes a unified approach to talking face editing and generation via speech-conditional facial motion infilling using a Diffusion Transformer. This is a novel generative modeling approach in multi-modal learning (speech and vision), which aligns with criterion 2 (new MLLMs) and is relevant for generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.13840": {
        "authors": [
            "Yannan He",
            "Garvita Tiwari",
            "Xiaohan Zhang",
            "Pankaj Bora",
            "Tolga Birdal",
            "Jan Eric Lenssen",
            "Gerard Pons-Moll"
        ],
        "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation",
        "abstract": "arXiv:2512.13840v1 Announce Type: new  Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.",
        "arxiv_id": "2512.13840",
        "ARXIVID": "2512.13840",
        "COMMENT": "This paper introduces MoLingo, a text-to-motion generation model with semantic-aligned latent space and cross-attention text conditioning. It is a new method for text-to-motion (T2M) generation, which is a multi-modal learning problem. The focus on semantic alignment and cross-attention is a methodological improvement in multi-modal generative modeling, matching criterion 2 (new MLLMs) and is of interest for generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.13996": {
        "authors": [
            "Can Jin",
            "Hongwu Peng",
            "Mingcan Xiang",
            "Qixin Zhang",
            "Xiangchi Yuan",
            "Amit Hasan",
            "Ohiremen Dibua",
            "Yifan Gong",
            "Yan Kang",
            "Dimitris N. Metaxas"
        ],
        "title": "Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training",
        "abstract": "arXiv:2512.13996v1 Announce Type: new  Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.",
        "arxiv_id": "2512.13996",
        "ARXIVID": "2512.13996",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a new sparsity-controllable dynamic Top-p MoE routing for large foundation model pre-training, with empirical results on LLMs and diffusion transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.14352": {
        "authors": [
            "Kaizhe Zhang",
            "Yijie Zhou",
            "Weizhan Zhang",
            "Caixia Yan",
            "Haipeng Du",
            "yugui xie",
            "Yu-Hui Wen",
            "Yong-Jin Liu"
        ],
        "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
        "abstract": "arXiv:2512.14352v1 Announce Type: new  Abstract: Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
        "arxiv_id": "2512.14352",
        "ARXIVID": "2512.14352",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a new, efficient method for dynamic view synthesis using hybrid Gaussian splatting, with significant improvements in model size and speed. Also relevant to generative modeling in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.13747": {
        "authors": [
            "Siyuan Dai",
            "Lunxiao Li",
            "Kun Zhao",
            "Eardi Lila",
            "Paul K. Crane",
            "Heng Huang",
            "Dongkuan Xu",
            "Haoteng Tang",
            "Liang Zhan"
        ],
        "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making",
        "abstract": "arXiv:2512.13747v1 Announce Type: new  Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.",
        "arxiv_id": "2512.13747",
        "ARXIVID": "2512.13747",
        "COMMENT": "Matches criterion 2 (empirical study of MLLMs in medical decision making, with surprising results about text vs. vision). Also relevant for insightful empirical results in VLLMs/MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.14499": {
        "authors": [
            "Jia Guo",
            "Jiawei Du",
            "Shengzhu Yang",
            "Shuai Lu",
            "Wenquan Cheng",
            "Kaiwen Zhang",
            "Yihua Sun",
            "Chuhong Yang",
            "Weihang Zhang",
            "Fang Chen",
            "Yilan Wu",
            "Lie Ju",
            "Guochen Ning",
            "Longfei Ma",
            "Huiping Yao",
            "Jinyuan Wang",
            "Peilun Shi",
            "Yukun Zhou",
            "Jie Xu",
            "Pearse A. Keane",
            "Hanruo Liu",
            "Hongen Liao",
            "Ningli Wang",
            "Huiqi Li"
        ],
        "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency",
        "abstract": "arXiv:2512.14499v1 Announce Type: new  Abstract: Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.",
        "arxiv_id": "2512.14499",
        "ARXIVID": "2512.14499",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications: large-scale retinal foundation model from real clinical data).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.13742": {
        "authors": [
            "Md. Najib Hasan (Wichita State University",
            "USA)",
            "Imran Ahmad (Wichita State University",
            "USA)",
            "Sourav Basak Shuvo (Khulna University of Engineering and Technology",
            "Bangladesh)",
            "Md. Mahadi Hasan Ankon (Khulna University of Engineering and Technology",
            "Bangladesh)",
            "Sunanda Das (University of Arkansas",
            "USA)",
            "Nazmul Siddique (Ulster University",
            "UK)",
            "Hui Wang (Queen's University Belfast",
            "UK)"
        ],
        "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
        "abstract": "arXiv:2512.13742v1 Announce Type: new  Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
        "arxiv_id": "2512.13742",
        "ARXIVID": "2512.13742",
        "COMMENT": "Matches criterion 2 (MLLMs: vision-to-language framework for medical reasoning, combining image classification and LLMs). Also relevant for vision-language modeling in a high-stakes domain.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.14677": {
        "authors": [
            "Sicheng Xu",
            "Guojun Chen",
            "Jiaolong Yang",
            "Yizhong Zhang",
            "Yu Deng",
            "Steve Lin",
            "Baining Guo"
        ],
        "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
        "abstract": "arXiv:2512.14677v1 Announce Type: new  Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.",
        "arxiv_id": "2512.14677",
        "ARXIVID": "2512.14677",
        "COMMENT": "This paper introduces VASA-3D, a method for generating lifelike 3D head avatars from a single image, driven by audio. It leverages motion latents from a previous 2D talking head model and translates them to 3D. While it is a strong vision+generative modeling paper, it does not introduce a new VLLM/MLLM or focus on spatial intelligence for embodied agents. It is most closely related to criterion 4 (vision foundation models and applications), as it applies generative modeling to 3D avatars.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.14051": {
        "authors": [
            "Mengzhang Cai",
            "Xin Gao",
            "Yu Li",
            "Honglin Lin",
            "Zheng Liu",
            "Zhuoshi Pan",
            "Qizhi Pei",
            "Xiaoran Shang",
            "Mengyuan Sun",
            "Zinan Tang",
            "Xiaoyang Wang",
            "Zhanping Zhong",
            "Yun Zhu",
            "Dahua Lin",
            "Conghui He",
            "Lijun Wu"
        ],
        "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
        "abstract": "arXiv:2512.14051v1 Announce Type: new  Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.",
        "arxiv_id": "2512.14051",
        "ARXIVID": "2512.14051",
        "COMMENT": "Related to criterion 4: introduces OpenDataArena, a new open platform for benchmarking post-training dataset value for LLMs, with insights into data-centric AI. Not directly about vision foundation models, but relevant to foundation model data.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.14692": {
        "authors": [
            "Jianfeng Xiang",
            "Xiaoxue Chen",
            "Sicheng Xu",
            "Ruicheng Wang",
            "Zelong Lv",
            "Yu Deng",
            "Hongyuan Zhu",
            "Yue Dong",
            "Hao Zhao",
            "Nicholas Jing Yuan",
            "Jiaolong Yang"
        ],
        "title": "Native and Compact Structured Latents for 3D Generation",
        "abstract": "arXiv:2512.14692v1 Announce Type: new  Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
        "arxiv_id": "2512.14692",
        "ARXIVID": "2512.14692",
        "COMMENT": "Relevant to 3D generative modeling and vision foundation models, but does not directly match any specific criterion. Proposes a new structured latent representation and large-scale 3D generation models.",
        "RELEVANCE": 4,
        "NOVELTY": 8
    },
    "2512.14309": {
        "authors": [
            "Abdullah Al Mamun",
            "Miaohua Zhang",
            "David Ahmedt-Aristizabal",
            "Zeeshan Hayder",
            "Mohammad Awrangjeb"
        ],
        "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
        "abstract": "arXiv:2512.14309v1 Announce Type: new  Abstract: Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.",
        "arxiv_id": "2512.14309",
        "ARXIVID": "2512.14309",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications: Vision Mamba for plant disease recognition, with novel self-supervised learning).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.14228": {
        "authors": [
            "Aneesha Fernando",
            "Surangika Ranathunga",
            "Kristin Stock",
            "Raj Prasanna",
            "Christopher B. Jones"
        ],
        "title": "Georeferencing complex relative locality descriptions with large language models",
        "abstract": "arXiv:2512.14228v1 Announce Type: new  Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.",
        "arxiv_id": "2512.14228",
        "ARXIVID": "2512.14228",
        "COMMENT": "Matches criterion 1 (spatial understanding: georeferencing complex spatial descriptions with LLMs). Also relevant for spatial intelligence in embodied agents, though not directly embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.14341": {
        "authors": [
            "Jie Zhang",
            "Shuai Dong",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Towards Transferable Defense Against Malicious Image Edits",
        "abstract": "arXiv:2512.14341v1 Announce Type: new  Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.",
        "arxiv_id": "2512.14341",
        "ARXIVID": "2512.14341",
        "COMMENT": "This paper proposes a new bimodal (image-text) defense framework for diffusion-based image editing, introducing a novel gradient regularization and adversarial prompt optimization. While it is multi-modal and involves vision-language, it is not a new VLLM/MLLM, nor is it about spatial intelligence or embodied agents. It is most closely related to criterion 4 (vision foundation models and applications), but the focus is on adversarial robustness.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14333": {
        "authors": [
            "Jie Zhang",
            "Shuai Dong",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Dual Attention Guided Defense Against Malicious Edits",
        "abstract": "arXiv:2512.14333v1 Announce Type: new  Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.",
        "arxiv_id": "2512.14333",
        "ARXIVID": "2512.14333",
        "COMMENT": "Somewhat related to criterion 4: proposes a new defense mechanism for text-to-image diffusion models, focusing on robustness against malicious edits. Involves vision foundation models in generative settings.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14236": {
        "authors": [
            "Nando Metzger",
            "Prune Truong",
            "Goutam Bhat",
            "Konrad Schindler",
            "Federico Tombari"
        ],
        "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
        "abstract": "arXiv:2512.14236v1 Announce Type: new  Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
        "arxiv_id": "2512.14236",
        "ARXIVID": "2512.14236",
        "COMMENT": "This paper introduces Elastic3D, a controllable stereo video conversion method using conditional latent diffusion and a novel guided VAE decoder. While it is a generative vision paper, it does not match any of the four criteria closely, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in a broad sense.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.13739": {
        "authors": [
            "Yajie Yang",
            "Yuqing Zhao",
            "Xiaochao Xi",
            "Yinan Zhu"
        ],
        "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage",
        "abstract": "arXiv:2512.13739v1 Announce Type: new  Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).",
        "arxiv_id": "2512.13739",
        "ARXIVID": "2512.13739",
        "COMMENT": "Somewhat related to criterion 4: discusses human-AI collaboration in AIGC-assisted image production, with a modular pipeline using vision foundation models (SAM, GroundingDINO, CLIP). Focus is on journalism and editorial control.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2512.14440": {
        "authors": [
            "Leon Sick",
            "Lukas Hoyer",
            "Dominik Engel",
            "Pedro Hermosilla",
            "Timo Ropinski"
        ],
        "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
        "abstract": "arXiv:2512.14440v1 Announce Type: new  Abstract: In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
        "arxiv_id": "2512.14440",
        "ARXIVID": "2512.14440",
        "COMMENT": "Somewhat related to criterion 1: proposes a new method for unsupervised video instance segmentation using real video data and temporal coherence, but not directly about spatial intelligence in embodied agents. No new benchmark or VLLM/MLLM.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.14550": {
        "authors": [
            "Zhiwen Yang",
            "Jiaju Zhang",
            "Yang Yi",
            "Jian Liang",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
        "abstract": "arXiv:2512.14550v1 Announce Type: new  Abstract: Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.",
        "arxiv_id": "2512.14550",
        "ARXIVID": "2512.14550",
        "COMMENT": "Relevant to vision foundation models and multi-task learning, but does not directly match any specific criterion. Proposes a task-adaptive transformer for all-in-one medical image restoration.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.14366": {
        "authors": [
            "Julian McGinnis",
            "Florian A. H\\\"olzl",
            "Suprosanna Shit",
            "Florentin Bieder",
            "Paul Friedrich",
            "Mark M\\\"uhlau",
            "Bj\\\"orn Menze",
            "Daniel Rueckert",
            "Benedikt Wiestler"
        ],
        "title": "Optimizing Rank for High-Fidelity Implicit Neural Representations",
        "abstract": "arXiv:2512.14366v1 Announce Type: new  Abstract: Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).",
        "arxiv_id": "2512.14366",
        "ARXIVID": "2512.14366",
        "COMMENT": "Relevant to implicit neural representations and generative modeling, but does not directly match any specific criterion. Proposes a novel approach to improve high-frequency content learning in INRs.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2512.14121": {
        "authors": [
            "Wenbo Tian",
            "Ruting Lin",
            "Hongxian Zheng",
            "Yaodong Yang",
            "Geng Wu",
            "Zihao Zhang",
            "Zhang Zhang"
        ],
        "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance",
        "abstract": "arXiv:2512.14121v1 Announce Type: new  Abstract: Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.",
        "arxiv_id": "2512.14121",
        "ARXIVID": "2512.14121",
        "COMMENT": "Somewhat relevant to embodied AI and multi-modal learning, but does not directly match any specific criterion. Proposes SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.14435": {
        "authors": [
            "Chang Cai",
            "Hao Jiang",
            "Xiaojun Yuan",
            "Ying-Jun Angela Zhang"
        ],
        "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging",
        "abstract": "arXiv:2512.14435v1 Announce Type: new  Abstract: Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.",
        "arxiv_id": "2512.14435",
        "ARXIVID": "2512.14435",
        "COMMENT": "Relevant to generative modeling and computer vision, but does not directly match any specific criterion. Proposes a message-passing framework using score-based generative models for compressive imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2512.14465": {
        "authors": [
            "Siyuan Zhu",
            "Chengdong Xu",
            "Kaiqiang Ke",
            "Chao Yu"
        ],
        "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning",
        "abstract": "arXiv:2512.14465v1 Announce Type: new  Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.",
        "arxiv_id": "2512.14465",
        "ARXIVID": "2512.14465",
        "COMMENT": "This paper proposes Context-Picker, a multi-stage reinforcement learning framework for dynamic context selection in long-context question answering. While it is a clever statistical method, it is not about spatial intelligence, VLLMs/MLLMs (in the sense of new models), embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14693": {
        "authors": [
            "Zitian Gao",
            "Lynx Chen",
            "Yihao Xiao",
            "He Xing",
            "Ran Tao",
            "Haoming Luo",
            "Joey Zhou",
            "Bryan Dai"
        ],
        "title": "Universal Reasoning Model",
        "abstract": "arXiv:2512.14693v1 Announce Type: new  Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
        "arxiv_id": "2512.14693",
        "ARXIVID": "2512.14693",
        "COMMENT": "Not directly related to any criterion: proposes a new universal reasoning model for ARC-AGI and Sudoku, focused on transformer architectures and reasoning, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14423": {
        "authors": [
            "Zhuo Chen",
            "Fanyue Wei",
            "Runze Xu",
            "Jingjing Li",
            "Lixin Duan",
            "Angela Yao",
            "Wen Li"
        ],
        "title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy",
        "abstract": "arXiv:2512.14423v1 Announce Type: new  Abstract: Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.",
        "arxiv_id": "2512.14423",
        "ARXIVID": "2512.14423",
        "COMMENT": "Relevant to computer vision and generative modeling, but does not directly match any specific criterion. Focuses on improving non-rigid image editing with diffusion models via attention synergy.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14360": {
        "authors": [
            "Ankita Raj",
            "Kaashika Prajaapat",
            "Tapan Kumar Gandhi",
            "Chetan Arora"
        ],
        "title": "Mimicking Human Visual Development for Learning Robust Image Representations",
        "abstract": "arXiv:2512.14360v1 Announce Type: new  Abstract: The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.",
        "arxiv_id": "2512.14360",
        "ARXIVID": "2512.14360",
        "COMMENT": "Does not directly match any specific criterion, but is a clever curriculum learning method for vision models inspired by human development. Relevant to general computer vision and robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14639": {
        "authors": [
            "Fei Wu",
            "Marcel Dreier",
            "Nora Gourmelon",
            "Sebastian Wind",
            "Jianlin Zhang",
            "Thorsten Seehaus",
            "Matthias Braun",
            "Andreas Maier",
            "Vincent Christlein"
        ],
        "title": "AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation",
        "abstract": "arXiv:2512.14639v1 Announce Type: new  Abstract: The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.",
        "arxiv_id": "2512.14639",
        "ARXIVID": "2512.14639",
        "COMMENT": "This paper presents AMD-HookNet++, a hybrid CNN-Transformer model for glacier calving front segmentation. While it introduces a hybrid architecture and spatial-channel attention, it is focused on remote sensing and segmentation, not on spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14648": {
        "authors": [
            "Daniel Capell\\'an-Mart\\'in",
            "Abhijeet Parida",
            "Zhifan Jiang",
            "Nishad Kulkarni",
            "Krithika Iyer",
            "Austin Tapp",
            "Syed Muhammad Anwar",
            "Mar\\'ia J. Ledesma-Carbayo",
            "Marius George Linguraru"
        ],
        "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble",
        "abstract": "arXiv:2512.14648v1 Announce Type: new  Abstract: Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.",
        "arxiv_id": "2512.14648",
        "ARXIVID": "2512.14648",
        "COMMENT": "This paper presents a modular segmentation pipeline for brain tumors using radiomic-guided subtyping and lesion-wise model ensemble. While it introduces methodological improvements in segmentation, it is focused on medical imaging and does not address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14540": {
        "authors": [
            "Andreas Lolos",
            "Theofilos Christodoulou",
            "Aris L. Moustakas",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou"
        ],
        "title": "CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning",
        "abstract": "arXiv:2512.14540v1 Announce Type: new  Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL",
        "arxiv_id": "2512.14540",
        "ARXIVID": "2512.14540",
        "COMMENT": "This paper proposes a new method for multiple instance learning in computational pathology, focusing on context-aware patch representations. While it is a novel method in computer vision, it does not match any of the four criteria closely, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14489": {
        "authors": [
            "Alessia Micieli",
            "Giovanni Maria Farinella",
            "Francesco Ragusa"
        ],
        "title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition",
        "abstract": "arXiv:2512.14489v1 Announce Type: new  Abstract: In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.",
        "arxiv_id": "2512.14489",
        "ARXIVID": "2512.14489",
        "COMMENT": "Presents a new dataset and benchmark for Italian Sign Language recognition, which is relevant to vision and multi-modal learning, but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14114": {
        "authors": [
            "Rui-Yang Ju",
            "KokSheik Wong",
            "Yanlin Jin",
            "Jen-Shiun Chiang"
        ],
        "title": "MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction",
        "abstract": "arXiv:2512.14114v1 Announce Type: new  Abstract: Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.",
        "arxiv_id": "2512.14114",
        "ARXIVID": "2512.14114",
        "COMMENT": "This paper presents a new GAN-based framework for document image enhancement and binarization. It is a computer vision paper, but does not match any of the four criteria closely, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in a broad sense.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.14477": {
        "authors": [
            "Andreas Sj\\\"olander",
            "Valeria Belloni",
            "Robel Fekadu",
            "Andrea Nascetti"
        ],
        "title": "TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels",
        "abstract": "arXiv:2512.14477v1 Announce Type: new  Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.",
        "arxiv_id": "2512.14477",
        "ARXIVID": "2512.14477",
        "COMMENT": "Not directly related to any criterion: introduces a new dataset for tunnel defect detection, relevant to computer vision but not to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}