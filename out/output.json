{
    "2512.01342": {
        "authors": [
            "Chenting Wang",
            "Yuhan Zhu",
            "Yicheng Xu",
            "Jiange Yang",
            "Ziang Yan",
            "Yali Wang",
            "Yi Wang",
            "Limin Wang"
        ],
        "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
        "abstract": "arXiv:2512.01342v1 Announce Type: new  Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
        "arxiv_id": "2512.01342",
        "ARXIVID": "2512.01342",
        "COMMENT": "Strongly matches criterion 4 (vision foundation models and applications) and is highly relevant to your friend's interest in generative modeling and multi-modal learning. InternVideo-Next proposes a new general video foundation model trained without video-text supervision, using a novel Encoder-Predictor-Decoder framework and a two-stage pretraining scheme. The approach addresses limitations of previous masked video modeling and video-text pretraining.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.01204": {
        "authors": [
            "Ziqian Wang",
            "Yonghao He",
            "Licheng Yang",
            "Wei Zou",
            "Hongxuan Ma",
            "Liu Liu",
            "Wei Sui",
            "Yuxin Guo",
            "Hu Su"
        ],
        "title": "TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image",
        "abstract": "arXiv:2512.01204v1 Announce Type: new  Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.",
        "arxiv_id": "2512.01204",
        "ARXIVID": "2512.01204",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark/method): TabletopGen is a training-free, automatic framework for generating interactive 3D tabletop scenes for embodied AI, with a novel pose and scale alignment approach for spatial reasoning. Directly relevant to spatial intelligence and simulation benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.02014": {
        "authors": [
            "Zhiheng Liu",
            "Weiming Ren",
            "Haozhe Liu",
            "Zijian Zhou",
            "Shoufa Chen",
            "Haonan Qiu",
            "Xiaoke Huang",
            "Zhaochong An",
            "Fanny Yang",
            "Aditya Patel",
            "Viktar Atliha",
            "Tony Ng",
            "Xiao Han",
            "Chuyan Zhu",
            "Chenyang Zhang",
            "Ding Liu",
            "Juan-Manuel Perez-Rua",
            "Sen He",
            "J\\\"urgen Schmidhuber",
            "Wenhu Chen",
            "Ping Luo",
            "Wei Liu",
            "Tao Xiang",
            "Jonas Schult",
            "Yuren Cong"
        ],
        "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "abstract": "arXiv:2512.02014v1 Announce Type: new  Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
        "arxiv_id": "2512.02014",
        "ARXIVID": "2512.02014",
        "COMMENT": "Directly matches criteria 2 (proposes TUNA, a new unified multimodal model for understanding and generation, with a novel unified visual representation). Also relevant to criteria 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.00975": {
        "authors": [
            "Haotian Liang",
            "Xinyi Chen",
            "Bin Wang",
            "Mingkang Chen",
            "Yitian Liu",
            "Yuhao Zhang",
            "Zanxin Chen",
            "Tianshuo Yang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dong Liu",
            "Xiaokang Yang",
            "Yao Mu",
            "Wenqi Shao",
            "Ping Luo"
        ],
        "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
        "abstract": "arXiv:2512.00975v1 Announce Type: new  Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.",
        "arxiv_id": "2512.00975",
        "ARXIVID": "2512.00975",
        "COMMENT": "Directly matches criteria 1 (new method for spatial intelligence and action in embodied agents via unified vision-language-action model) and criteria 2 (multi-modal large language model for robotics).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.01989": {
        "authors": [
            "Fengzhe Zhou",
            "Jiannan Huang",
            "Jialuo Li",
            "Deva Ramanan",
            "Humphrey Shi"
        ],
        "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
        "abstract": "arXiv:2512.01989v1 Announce Type: new  Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
        "arxiv_id": "2512.01989",
        "ARXIVID": "2512.01989",
        "COMMENT": "Directly matches criteria 3 (introduces a new comprehensive benchmark for Physical AI, focusing on perception and prediction in multi-modal LLMs and video generative models). Also relevant to criteria 2 (evaluation of MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.01300": {
        "authors": [
            "Dacheng Liao",
            "Mengshi Qi",
            "Peng Shu",
            "Zhining Zhang",
            "Yuxin Lin",
            "Liang Liu",
            "Huadong Ma"
        ],
        "title": "RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving",
        "abstract": "arXiv:2512.01300v1 Announce Type: new  Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.",
        "arxiv_id": "2512.01300",
        "ARXIVID": "2512.01300",
        "COMMENT": "Matches criteria 3 (new embodied AI benchmark for VLM-based autonomous driving, with novel focus on robustness to sensor and prompt corruption) and criteria 2 (proposes a new VLM-based method for autonomous driving). Also relevant to criteria 1 (spatial intelligence in embodied agents).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.00903": {
        "authors": [
            "Chaojun Ni",
            "Cheng Chen",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenzhao Zheng",
            "Boyuan Wang",
            "Tianrun Chen",
            "Guosheng Zhao",
            "Haoyun Li",
            "Zhehao Dong",
            "Qiang Zhang",
            "Yun Ye",
            "Yang Wang",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
        "abstract": "arXiv:2512.00903v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
        "arxiv_id": "2512.00903",
        "ARXIVID": "2512.00903",
        "COMMENT": "Matches criterion 1 (spatial understanding/spatial intelligence on embodied agents) and criterion 3 (new methods for embodied AI, focusing on lightweight VLA models with spatiotemporal reasoning). Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.01988": {
        "authors": [
            "Wei Tang",
            "Yanpeng Sun",
            "Shan Zhang",
            "Xiaofan Li",
            "Piotr Koniusz",
            "Wei Li",
            "Na Zhao",
            "Zechao Li"
        ],
        "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
        "abstract": "arXiv:2512.01988v1 Announce Type: new  Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
        "arxiv_id": "2512.01988",
        "ARXIVID": "2512.01988",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 3 (novel method for perception-policy learning in embodied AI, focusing on structured visual reasoning rather than language-based reasoning). Also relevant to criterion 2 (MLLMs) and 4 (vision foundation models) as it builds on Qwen2.5-VL-3B and evaluates on MLLM benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.00818": {
        "authors": [
            "Haozhen Gong",
            "Xiaozhong Ji",
            "Yuansen Liu",
            "Wenbin Wu",
            "Xiaoxiao Yan",
            "Jingjing Liu",
            "Kai Wu",
            "Jiazhen Pan",
            "Bailiang Jian",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Hongwei Bran Li"
        ],
        "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning",
        "abstract": "arXiv:2512.00818v1 Announce Type: new  Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.",
        "arxiv_id": "2512.00818",
        "ARXIVID": "2512.00818",
        "COMMENT": "This paper introduces Med-CMR, a new fine-grained benchmark for medical complex multimodal reasoning, evaluating MLLMs on visual and clinical logic tasks. This matches criterion 2 (new MLLMs) and criterion 3 (new embodied AI benchmark with novel angles, especially spatial understanding in medical images).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.01481": {
        "authors": [
            "Qisen Wang",
            "Yifan Zhao",
            "Peisen Shen",
            "Jialu Li",
            "Jia Li"
        ],
        "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
        "abstract": "arXiv:2512.01481v1 Announce Type: new  Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
        "arxiv_id": "2512.01481",
        "ARXIVID": "2512.01481",
        "COMMENT": "This paper introduces ChronosObserver, a training-free method for generating 3D-consistent, time-synchronized multi-view videos using hyperspace diffusion sampling. This is highly relevant to criterion 4 (vision foundation models and applications), and also touches on generative modeling in multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.01707": {
        "authors": [
            "Daeun Lee",
            "Subhojyoti Mukherjee",
            "Branislav Kveton",
            "Ryan A. Rossi",
            "Viet Dac Lai",
            "Seunghyun Yoon",
            "Trung Bui",
            "Franck Dernoncourt",
            "Mohit Bansal"
        ],
        "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
        "abstract": "arXiv:2512.01707v1 Announce Type: new  Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
        "arxiv_id": "2512.01707",
        "ARXIVID": "2512.01707",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark): Introduces StreamGaze, a new benchmark for evaluating MLLMs' ability to use gaze for temporal and proactive reasoning in streaming videos. Focuses on a novel angle (gaze-guided temporal reasoning) that previous work ignored.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.00885": {
        "authors": [
            "Masatoshi Tateno",
            "Gido Kato",
            "Hirokatsu Kataoka",
            "Yoichi Sato",
            "Takuma Yagi"
        ],
        "title": "HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics",
        "abstract": "arXiv:2512.00885v1 Announce Type: new  Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.",
        "arxiv_id": "2512.00885",
        "ARXIVID": "2512.00885",
        "COMMENT": "Directly matches criteria 3 (introduces a new fine-grained video QA benchmark for hand-object interaction dynamics, focusing on spatial and temporal reasoning in vision-language models).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.01827": {
        "authors": [
            "Yize Zhang",
            "Meiqi Chen",
            "Sirui Chen",
            "Bo Peng",
            "Yanxi Zhang",
            "Tianyu Li",
            "Chaochao Lu"
        ],
        "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
        "abstract": "arXiv:2512.01827v1 Announce Type: new  Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
        "arxiv_id": "2512.01827",
        "ARXIVID": "2512.01827",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new vision-language model for visual causal discovery and a large-scale dataset, with a novel causal reasoning approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.00532": {
        "authors": [
            "Ye Pang"
        ],
        "title": "Image Generation as a Visual Planner for Robotic Manipulation",
        "abstract": "arXiv:2512.00532v1 Announce Type: new  Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.   We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.   Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.",
        "arxiv_id": "2512.00532",
        "ARXIVID": "2512.00532",
        "COMMENT": "Matches criterion 1 and 3: Proposes using image generation models as visual planners for robotic manipulation, unifying perception, planning, and action in embodied agents. Also explores a novel method for using pretrained image generators for temporal planning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02012": {
        "authors": [
            "Zhengyang Geng",
            "Yiyang Lu",
            "Zongze Wu",
            "Eli Shechtman",
            "J. Zico Kolter",
            "Kaiming He"
        ],
        "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "abstract": "arXiv:2512.02012v1 Announce Type: new  Abstract: MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
        "arxiv_id": "2512.02012",
        "ARXIVID": "2512.02012",
        "COMMENT": "This paper presents improved MeanFlow, a one-step generative modeling framework for images, with new training and guidance mechanisms. It is relevant to criterion 4 (vision foundation models and generative modeling), especially for fastforward generative models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.00891": {
        "authors": [
            "Yiyu Wang",
            "Xuyang Liu",
            "Xiyan Gui",
            "Xinying Lin",
            "Boxue Yang",
            "Chenfei Liao",
            "Tailai Chen",
            "Linfeng Zhang"
        ],
        "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
        "abstract": "arXiv:2512.00891v1 Announce Type: new  Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
        "arxiv_id": "2512.00891",
        "ARXIVID": "2512.00891",
        "COMMENT": "Directly matches criteria 2 (new method for accelerating VideoLLMs, a type of VLLM, with hierarchical token compression).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.01540": {
        "authors": [
            "Zipeng Wang",
            "Dan Xu"
        ],
        "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
        "abstract": "arXiv:2512.01540v1 Announce Type: new  Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
        "arxiv_id": "2512.01540",
        "ARXIVID": "2512.01540",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications). FlashVGGT introduces a new, efficient attention mechanism for 3D reconstruction from multi-view images, which is a core spatial understanding task. The descriptor-based attention and chunk-recursive mechanism are novel for scaling to long sequences.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01103": {
        "authors": [
            "Roy Velich",
            "Arkadi Piven",
            "David Bensa\\\"id",
            "Daniel Cremers",
            "Thomas Dag\\`es",
            "Ron Kimmel"
        ],
        "title": "Learning Eigenstructures of Unstructured Data Manifolds",
        "abstract": "arXiv:2512.01103v1 Announce Type: new  Abstract: We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
        "arxiv_id": "2512.01103",
        "ARXIVID": "2512.01103",
        "COMMENT": "This paper proposes a framework for learning spectral bases for shape and manifold analysis from unstructured data, which is relevant to spatial understanding and geometry processing. It is a methodological improvement for spatial understanding (criterion 1), especially in unstructured and high-dimensional data.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2512.00872": {
        "authors": [
            "Tim Veenboer",
            "George Yiasemis",
            "Eric Marcus",
            "Vivien Van Veldhuizen",
            "Cees G. M. Snoek",
            "Jonas Teuwen",
            "Kevin B. W. Groot Lipman"
        ],
        "title": "TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models",
        "abstract": "arXiv:2512.00872v1 Announce Type: new  Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.",
        "arxiv_id": "2512.00872",
        "ARXIVID": "2512.00872",
        "COMMENT": "This paper introduces a new 3D vision foundation model for CT data, adapting ViTs and DINOv2 for volumetric data. This matches criterion 4 (vision foundation models and applications) and is also relevant to criterion 2 (if considering medical vision models as a form of VLM).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01636": {
        "authors": [
            "Xin Wang",
            "Haipeng Zhang",
            "Mang Li",
            "Zhaohui Xia",
            "Yueguo Chen",
            "Yu Zhang",
            "Chunyu Wei"
        ],
        "title": "Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval",
        "abstract": "arXiv:2512.01636v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.",
        "arxiv_id": "2512.01636",
        "ARXIVID": "2512.01636",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes a novel generative editing framework in the joint vision-language space for zero-shot composed image retrieval, with a multimodal fusion feature editing strategy and a lightweight Control-Adapter. Also relevant to criterion 4 (vision foundation models and applications) due to the focus on joint vision-language space and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01312": {
        "authors": [
            "Ning Han",
            "Yawen Zeng",
            "Shaohua Long",
            "Chengqing Li",
            "Sijie Yang",
            "Dun Tan",
            "Jianfeng Dong",
            "Jingjing Chen"
        ],
        "title": "IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval",
        "abstract": "arXiv:2512.01312v1 Announce Type: new  Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful \"interaction\" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.",
        "arxiv_id": "2512.01312",
        "ARXIVID": "2512.01312",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Introduces a new multi-turn, conversational video retrieval benchmark and a comprehensive MLLM-based framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00305": {
        "authors": [
            "Zhengzhuo Xu",
            "SiNan Du",
            "Yiyan Qi",
            "SiwenLu",
            "Chengjin Xu",
            "Chun Yuan",
            "Jian Guo"
        ],
        "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
        "abstract": "arXiv:2512.00305v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\\% on ChartBench.",
        "arxiv_id": "2512.00305",
        "ARXIVID": "2512.00305",
        "COMMENT": "Matches criterion 2: Proposes a new method for chart reasoning in MLLMs, introducing a dataset and models that improve visual grounding and reasoning in multi-modal LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00756": {
        "authors": [
            "Ruihan Chen",
            "Qiming Li",
            "Xiaocheng Feng",
            "Xiaoliang Yang",
            "Weihong Zhong",
            "Yuxuan Gu",
            "Zekun Zhou",
            "Bing Qin"
        ],
        "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents",
        "abstract": "arXiv:2512.00756v1 Announce Type: new  Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.",
        "arxiv_id": "2512.00756",
        "ARXIVID": "2512.00756",
        "COMMENT": "Matches criterion 2 and 3: Introduces a new multilingual benchmark for LVLMs on GUI tasks, with fine-grained spatial relationship analysis and a novel cross-lingual intervention method. Focuses on spatial reasoning and benchmarking for VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01478": {
        "authors": [
            "Omer Sela (Amazon",
            "Tel Aviv University)",
            "Michael Chertok (Amazon)",
            "Lior Wolf (Tel Aviv University)"
        ],
        "title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball",
        "abstract": "arXiv:2512.01478v1 Announce Type: new  Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.",
        "arxiv_id": "2512.01478",
        "ARXIVID": "2512.01478",
        "COMMENT": "Matches criterion 1: Presents a new spatiotemporal modeling framework for understanding spatial motion and event prediction in basketball using skeletal data, with GNNs and Transformers for spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00995": {
        "authors": [
            "Han Su",
            "Tianyu Huang",
            "Zichen Wan",
            "Xiaohe Wu",
            "Wangmeng Zuo"
        ],
        "title": "S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud",
        "abstract": "arXiv:2512.00995v1 Announce Type: new  Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.",
        "arxiv_id": "2512.00995",
        "ARXIVID": "2512.00995",
        "COMMENT": "This paper introduces S2AM3D, a method for scale-controllable part segmentation of 3D point clouds, combining 2D segmentation priors with 3D supervision. It is relevant to criterion 1 (spatial understanding in embodied agents) and criterion 4 (vision foundation models and applications), as it addresses spatial reasoning in 3D data.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.01975": {
        "authors": [
            "Xu Zhang",
            "Jin Yuan",
            "Hanwang Zhang",
            "Guojin Zhong",
            "Yongsheng Zang",
            "Jiacheng Lin",
            "Zhiyong Li"
        ],
        "title": "SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning",
        "abstract": "arXiv:2512.01975v1 Announce Type: new  Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.",
        "arxiv_id": "2512.01975",
        "ARXIVID": "2512.01975",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Introduces a new task (SegCaptioning) and a scene graph guided diffusion model for joint segmentation and captioning, leveraging structured scene graph features and a bimodal transformer. The use of diffusion models and scene graphs for multi-modal understanding is a novel application.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.00752": {
        "authors": [
            "Shawn Li",
            "Ryan Rossi",
            "Sungchul Kim",
            "Sunav Choudhary",
            "Franck Dernoncourt",
            "Puneet Mathur",
            "Zhengzhong Tu",
            "Yue Zhao"
        ],
        "title": "Charts Are Not Images: On the Challenges of Scientific Chart Editing",
        "abstract": "arXiv:2512.00752v1 Announce Type: new  Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \\textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \\textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.",
        "arxiv_id": "2512.00752",
        "ARXIVID": "2512.00752",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces a new benchmark for structure-aware editing of scientific charts, highlighting the limitations of current generative models and evaluation metrics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.01373": {
        "authors": [
            "Sheng Liu",
            "Tianyu Luan",
            "Phani Nuney",
            "Xuelu Feng",
            "Junsong Yuan"
        ],
        "title": "SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation",
        "abstract": "arXiv:2512.01373v1 Announce Type: new  Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.",
        "arxiv_id": "2512.01373",
        "ARXIVID": "2512.01373",
        "COMMENT": "Matches criterion 4: Proposes a new no-reference 3D shape realism metric leveraging LLMs and introduces a new dataset. Applies vision foundation models to 3D evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.01960": {
        "authors": [
            "Zisu Li",
            "Hengye Lyu",
            "Jiaxin Shi",
            "Yufeng Zeng",
            "Mingming Fan",
            "Hanwang Zhang",
            "Chen Liang"
        ],
        "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
        "abstract": "arXiv:2512.01960v1 Announce Type: new  Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
        "arxiv_id": "2512.01960",
        "ARXIVID": "2512.01960",
        "COMMENT": "Relevant to criterion 4 (vision foundation models and applications): SpriteHand is an autoregressive video generation framework for real-time hand-object interaction, using causal inference and hybrid post-training for visual realism. Focuses on generative modeling and interaction fidelity.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.01677": {
        "authors": [
            "Haodong Yan",
            "Hang Yu",
            "Zhide Zhong",
            "Weilin Yuan",
            "Xin Gong",
            "Zehang Luo",
            "Chengxi Heyu",
            "Junfeng Li",
            "Wenxuan Song",
            "Shunbo Zhou",
            "Haoang Li"
        ],
        "title": "Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation",
        "abstract": "arXiv:2512.01677v1 Announce Type: new  Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.",
        "arxiv_id": "2512.01677",
        "ARXIVID": "2512.01677",
        "COMMENT": "Relevant to criterion 4 (vision foundation models and applications): Proposes a structure and contact-aware representation for hand-object interaction video generation, enabling generalization to open-world scenarios. Focuses on spatial reasoning and generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.00911": {
        "authors": [
            "Yuhao Shan",
            "Qianyi Yuan",
            "Jingguo Liu",
            "Shigang Li",
            "Jianfeng Li",
            "Tong Chen"
        ],
        "title": "Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision",
        "abstract": "arXiv:2512.00911v1 Announce Type: new  Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360{\\deg} continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.",
        "arxiv_id": "2512.00911",
        "ARXIVID": "2512.00911",
        "COMMENT": "Matches criterion 1: Proposes a dual-projection fusion network for spatially accurate upright panorama generation in robotic vision, improving spatial understanding for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.00369": {
        "authors": [
            "Wenshuo Chen",
            "Haosen Li",
            "Shaofeng Liang",
            "Lei Wang",
            "Haozhe Jia",
            "Kaishen Yuan",
            "Jieming Wu",
            "Bowen Tian",
            "Yutao Yue"
        ],
        "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
        "abstract": "arXiv:2512.00369v1 Announce Type: new  Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale {\\omega} as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.",
        "arxiv_id": "2512.00369",
        "ARXIVID": "2512.00369",
        "COMMENT": "Relevant to criterion 4 (vision foundation models and applications): Proposes POLARIS, a new method for robust and adaptive inversion in diffusion models, addressing a previously overlooked error source. This is a methodological improvement in generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.01236": {
        "authors": [
            "Shulei Wang",
            "Longhui Wei",
            "Xin He",
            "Jianbo Ouyang",
            "Hui Lu",
            "Zhou Zhao",
            "Qi Tian"
        ],
        "title": "PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards",
        "abstract": "arXiv:2512.01236v1 Announce Type: new  Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR",
        "arxiv_id": "2512.01236",
        "ARXIVID": "2512.01236",
        "COMMENT": "Proposes a new method and benchmark for multi-subject personalized image generation. Relevant to generative modeling and vision foundation models (criteria 4), but not directly to spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.01390": {
        "authors": [
            "Seungho Choi",
            "Jeahun Sung",
            "Jihyong Oh"
        ],
        "title": "FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution",
        "abstract": "arXiv:2512.01390v1 Announce Type: new  Abstract: Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise \"low-first, high-later\" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.",
        "arxiv_id": "2512.01390",
        "ARXIVID": "2512.01390",
        "COMMENT": "Presents a new training scheme for diffusion models in image super-resolution. Relevant to generative modeling and vision foundation models (criteria 4), but not a direct match to spatial intelligence, VLLMs, or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.01382": {
        "authors": [
            "Yuke Li",
            "Lianli Gao",
            "Ji Zhang",
            "Pengpeng Zeng",
            "Lichuan Xiang",
            "Hongkai Wen",
            "Heng Tao Shen",
            "Jingkuan Song"
        ],
        "title": "Reversible Inversion for Training-Free Exemplar-guided Image Editing",
        "abstract": "arXiv:2512.01382v1 Announce Type: new  Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \\textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.",
        "arxiv_id": "2512.01382",
        "ARXIVID": "2512.01382",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications), as it introduces a new training-free method for exemplar-guided image editing, which is a generative vision task. The reversible inversion and mask-guided denoising are novel techniques.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.01380": {
        "authors": [
            "Tianyu Luan",
            "Xuelu Feng",
            "Zixin Zhu",
            "Phani Nuney",
            "Sheng Liu",
            "Xuan Gong",
            "David Doermann",
            "Chunming Qiao",
            "Junsong Yuan"
        ],
        "title": "Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network",
        "abstract": "arXiv:2512.01380v1 Announce Type: new  Abstract: Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.",
        "arxiv_id": "2512.01380",
        "ARXIVID": "2512.01380",
        "COMMENT": "Related to criterion 4 (vision foundation models and applications), as it proposes a new metric for evaluating 3D textured shapes, which is important for generative modeling and 3D vision. The method is novel in that it directly evaluates 3D meshes with texture, without rendering.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.01763": {
        "authors": [
            "Xurui Zhou",
            "Gongwei Chen",
            "Yuquan Xie",
            "Zaijing Li",
            "Kaiwen Zhou",
            "Shuai Wang",
            "Shuo Yang",
            "Zhuotao Tian",
            "Rui Shao"
        ],
        "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
        "abstract": "arXiv:2512.01763v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
        "arxiv_id": "2512.01763",
        "ARXIVID": "2512.01763",
        "COMMENT": "Somewhat relevant to criterion 3 (embodied AI, new method): HiconAgent introduces history context-aware policy optimization for GUI agents, with dynamic context sampling and history compression. Focuses on sequential decision making, but less on spatial intelligence or simulation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00308": {
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Wengang Zhou",
            "Hongsheng Li",
            "Houqiang Li"
        ],
        "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation",
        "abstract": "arXiv:2512.00308v1 Announce Type: new  Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.",
        "arxiv_id": "2512.00308",
        "ARXIVID": "2512.00308",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications): Proposes a new method for dataset distillation using optimal transport and diffusion models, which is a clever statistical trick for generative modeling, but not directly about spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00794": {
        "authors": [
            "Bo Guo",
            "Sijia Wen",
            "Yifan Zhao",
            "Jia Li",
            "Zhiming Zheng"
        ],
        "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery",
        "abstract": "arXiv:2512.00794v1 Announce Type: new  Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.",
        "arxiv_id": "2512.00794",
        "ARXIVID": "2512.00794",
        "COMMENT": "Presents a new method for 3D Gaussian Splatting using polarization cues for improved geometry recovery. This is a novel vision method, but not directly about embodied agents, VLLMs, or benchmarks. Some relevance to vision foundation models (criteria 4).",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2512.00534": {
        "authors": [
            "Zeyuan An",
            "Yanghang Xiao",
            "Zhiying Leng",
            "Frederick W. B. Li",
            "Xiaohui Liang"
        ],
        "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
        "abstract": "arXiv:2512.00534v1 Announce Type: new  Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
        "arxiv_id": "2512.00534",
        "ARXIVID": "2512.00534",
        "COMMENT": "Related to spatial understanding and 3D scene updating, which is tangential to criteria 1 (spatial intelligence), but not directly about embodied agents or new benchmarks. Interesting for vision foundation models, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2512.00904": {
        "authors": [
            "Xingyu Zhu",
            "Beier Zhu",
            "Yunfan Li",
            "Junfeng Fang",
            "Shuo Wang",
            "Kesen Zhao",
            "Hanwang Zhang"
        ],
        "title": "Hierarchical Semantic Alignment for Image Clustering",
        "abstract": "arXiv:2512.00904v1 Announce Type: new  Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.",
        "arxiv_id": "2512.00904",
        "ARXIVID": "2512.00904",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications), as it proposes a new method for image clustering using hierarchical semantic alignment, leveraging external semantic knowledge. However, it is not directly about foundation models or multi-modal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.00729": {
        "authors": [
            "Yuxiang Chen",
            "Zuohan Wu",
            "Ziwei Wang",
            "Xiangning Yu",
            "Xujia Li",
            "Linyi Yang",
            "Mengyue Yang",
            "Jun Wang",
            "Lei Chen"
        ],
        "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens",
        "abstract": "arXiv:2512.00729v1 Announce Type: new  Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.",
        "arxiv_id": "2512.00729",
        "ARXIVID": "2512.00729",
        "COMMENT": "This paper analyzes large reasoning models (LRMs) from a human cognitive perspective, but it does not introduce new VLLMs/MLLMs, nor does it focus on spatial intelligence, embodied AI, or vision foundation models. It is more about interpretability and taxonomy.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00475": {
        "authors": [
            "Xin Gu",
            "Congcong Li",
            "Xinyao Wang",
            "Dexiang Hong",
            "Libo Zhang",
            "Tiejian Luo",
            "Longyin Wen",
            "Heng Fan"
        ],
        "title": "Structured Context Learning for Generic Event Boundary Detection",
        "abstract": "arXiv:2512.00475v1 Announce Type: new  Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.",
        "arxiv_id": "2512.00475",
        "ARXIVID": "2512.00475",
        "COMMENT": "This paper proposes a new method for event boundary detection in videos using structured context learning. While it is a novel computer vision method, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00125": {
        "authors": [
            "Ruo-Syuan Mei",
            "Sixian Jia",
            "Guangze Li",
            "Soo Yeon Lee",
            "Brian Musser",
            "William Keller",
            "Sreten Zakula",
            "Jorge Arinez",
            "Chenhui Shao"
        ],
        "title": "Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance",
        "abstract": "arXiv:2512.00125v1 Announce Type: new  Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.",
        "arxiv_id": "2512.00125",
        "ARXIVID": "2512.00125",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications): Proposes a hybrid synthetic data generation framework for zero-shot vision-based part inspection, using domain randomization and simulation. Focuses on industrial application rather than foundational vision models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.01771": {
        "authors": [
            "Ahsan Raza Siyal",
            "Markus Haltmeier",
            "Ruth Steiger",
            "Malik Galijasevic",
            "Elke Ruth Gizewski",
            "Astrid Ellen Grams"
        ],
        "title": "Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels",
        "abstract": "arXiv:2512.01771v1 Announce Type: new  Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.",
        "arxiv_id": "2512.01771",
        "ARXIVID": "2512.01771",
        "COMMENT": "Relevant to computer vision, but does not match any specific criterion. Focuses on medical image registration using learnable edge kernels, not on embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00087": {
        "authors": [
            "Ivo Bueno",
            "Ruikun Hou",
            "Babette B\\\"uhler",
            "Tim F\\\"utterer",
            "James Drimalla",
            "Jonathan Kyle Foster",
            "Peter Youngs",
            "Peter Gerjets",
            "Ulrich Trautwein",
            "Enkelejda Kasneci"
        ],
        "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data",
        "abstract": "arXiv:2512.00087v1 Announce Type: new  Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.",
        "arxiv_id": "2512.00087",
        "ARXIVID": "2512.00087",
        "COMMENT": "Somewhat relevant to multi-modal learning, but does not match any specific criterion. Focuses on automated recognition of instructional activity from classroom data using vision-language models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.01340": {
        "authors": [
            "Yingjie Zhou",
            "Xilei Zhu",
            "Siyu Ren",
            "Ziyi Zhao",
            "Ziwen Wang",
            "Farong Wen",
            "Yu Zhou",
            "Jiezhang Cao",
            "Xiongkuo Min",
            "Fengjiao Chen",
            "Xiaoyu Li",
            "Xuezhi Cao",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans",
        "abstract": "arXiv:2512.01340v1 Announce Type: new  Abstract: Speech-driven Talking Human (TH) generation, commonly known as \"Talker,\" currently faces limitations in multi-subject driving capabilities. Extending this paradigm to \"Multi-Talker,\" capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.",
        "arxiv_id": "2512.01340",
        "ARXIVID": "2512.01340",
        "COMMENT": "Relevant to multi-modal learning and evaluation, but does not directly match any specific criterion. Focuses on quality assessment for multi-subject talking human generation.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2512.01970": {
        "authors": [
            "Sitao Cheng",
            "Xunjian Yin",
            "Ruiwen Zhou",
            "Yuxuan Li",
            "Xinyi Wang",
            "Liangming Pan",
            "William Yang Wang",
            "Victor Zhong"
        ],
        "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
        "abstract": "arXiv:2512.01970v1 Announce Type: new  Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
        "arxiv_id": "2512.01970",
        "ARXIVID": "2512.01970",
        "COMMENT": "Does not match any specific criterion. Focuses on RL for reasoning in LLMs, not vision, multi-modal, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.01128": {
        "authors": [
            "Haotian Liu",
            "Haoyu Chen",
            "Chenhui Pan",
            "You Hu",
            "Guoying Zhao",
            "Xiaobai Li"
        ],
        "title": "OmniFD: A Unified Model for Versatile Face Forgery Detection",
        "abstract": "arXiv:2512.01128v1 Announce Type: new  Abstract: Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.",
        "arxiv_id": "2512.01128",
        "ARXIVID": "2512.01128",
        "COMMENT": "Does not match any specific criterion. Focuses on unified face forgery detection, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.01485": {
        "authors": [
            "Jindi Lv",
            "Yuhao Zhou",
            "Zheng Zhu",
            "Xiaofeng Wang",
            "Guan Huang",
            "Jiancheng Lv"
        ],
        "title": "Multi-Path Collaborative Reasoning via Reinforcement Learning",
        "abstract": "arXiv:2512.01485v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.",
        "arxiv_id": "2512.01485",
        "ARXIVID": "2512.01485",
        "COMMENT": "Does not match any specific criterion. Focuses on multi-path collaborative reasoning in LLMs, not vision or multi-modal models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.01099": {
        "authors": [
            "Daria Smirnova",
            "Hamid Nasiri",
            "Marta Adamska",
            "Zhengxin Yu",
            "Peter Garraghan"
        ],
        "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems",
        "abstract": "arXiv:2512.01099v1 Announce Type: new  Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.",
        "arxiv_id": "2512.01099",
        "ARXIVID": "2512.01099",
        "COMMENT": "Does not directly match any specific criterion. Focuses on energy-aware model selection in LLM-orchestrated systems, which is more about system optimization and orchestration than spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.00281": {
        "authors": [
            "Sylvain Bodard",
            "Pierre Baudot",
            "Benjamin Renoust",
            "Charles Voyton",
            "Gwendoline De Bie",
            "Ezequiel Geremia",
            "Van-Khoa Le",
            "Danny Francis",
            "Pierre-Henri Siot",
            "Yousra Haddou",
            "Vincent Bobin",
            "Jean-Christophe Brisset",
            "Carey C. Thomson",
            "Valerie Bourdes",
            "Benoit Huet"
        ],
        "title": "Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth",
        "abstract": "arXiv:2512.00281v1 Announce Type: new  Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.",
        "arxiv_id": "2512.00281",
        "ARXIVID": "2512.00281",
        "COMMENT": "This paper presents a new AI system for lung nodule detection and diagnosis, but it is focused on medical imaging and not on spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02017": {
        "authors": [
            "Shaowei Liu",
            "David Yifan Yao",
            "Saurabh Gupta",
            "Shenlong Wang"
        ],
        "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
        "abstract": "arXiv:2512.02017v1 Announce Type: new  Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
        "arxiv_id": "2512.02017",
        "ARXIVID": "2512.02017",
        "COMMENT": "Relevant to computer vision, but does not match any specific criterion. Focuses on multi-camera synchronization via visual cues, not spatial intelligence in embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}