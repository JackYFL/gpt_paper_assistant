{
    "2601.16208": {
        "authors": [
            "Shengbang Tong",
            "Boyang Zheng",
            "Ziteng Wang",
            "Bingda Tang",
            "Nanye Ma",
            "Ellis Brown",
            "Jihan Yang",
            "Rob Fergus",
            "Yann LeCun",
            "Saining Xie"
        ],
        "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "abstract": "arXiv:2601.16208v1 Announce Type: new  Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
        "arxiv_id": "2601.16208",
        "ARXIVID": "2601.16208",
        "COMMENT": "This paper matches criterion 4 very closely: it presents a new methodological improvement for vision foundation models (Representation Autoencoders) in large-scale text-to-image diffusion transformers, with strong empirical results and analysis. The work is highly relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.15876": {
        "authors": [
            "Taofeng Xue",
            "Chong Peng",
            "Mianqiu Huang",
            "Linsen Guo",
            "Tiancheng Han",
            "Haozhe Wang",
            "Jianing Wang",
            "Xiaocheng Zhang",
            "Xin Yang",
            "Dengchang Zhao",
            "Jinrui Ding",
            "Xiandi Ma",
            "Yuchen Xie",
            "Peng Pei",
            "Xunliang Cai",
            "Xipeng Qiu"
        ],
        "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
        "abstract": "arXiv:2601.15876v1 Announce Type: new  Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
        "arxiv_id": "2601.15876",
        "ARXIVID": "2601.15876",
        "COMMENT": "Matches criterion 3: Introduces EvoCUA, a new agentic model for computer use agents with a self-sustaining evolutionary learning paradigm, scalable synthetic experience, and a new benchmark (OSWorld). Focuses on novel methods and benchmarking for embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16007": {
        "authors": [
            "Chak-Wing Mak",
            "Guanyu Zhu",
            "Boyi Zhang",
            "Hongji Li",
            "Xiaowei Chi",
            "Kevin Zhang",
            "Yichen Wu",
            "Yangfan He",
            "Chun-Kai Fan",
            "Wentao Lu",
            "Kuangzhi Ge",
            "Xinyu Fang",
            "Hongyang He",
            "Kuan Lu",
            "Tianxiang Xu",
            "Li Zhang",
            "Yongxin Ni",
            "Youhua Li",
            "Shanghang Zhang"
        ],
        "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
        "abstract": "arXiv:2601.16007v1 Announce Type: new  Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
        "arxiv_id": "2601.16007",
        "ARXIVID": "2601.16007",
        "COMMENT": "Strong match for criterion 2 (new MLLMs and VLMs), criterion 3 (new benchmark for embodied AI and physical reasoning), and criterion 4 (vision foundation models and their evaluation). PhysicsMind is a new benchmark for evaluating physical reasoning in foundational VLMs and world models, with both simulation and real-world tasks, and provides surprising empirical results about current model limitations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.16163": {
        "authors": [
            "Moo Jin Kim",
            "Yihuai Gao",
            "Tsung-Yi Lin",
            "Yen-Chen Lin",
            "Yunhao Ge",
            "Grace Lam",
            "Percy Liang",
            "Shuran Song",
            "Ming-Yu Liu",
            "Chelsea Finn",
            "Jinwei Gu"
        ],
        "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
        "abstract": "arXiv:2601.16163v1 Announce Type: new  Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
        "arxiv_id": "2601.16163",
        "ARXIVID": "2601.16163",
        "COMMENT": "Strong match for criterion 1 (spatial intelligence on embodied agents), criterion 3 (new methods for embodied AI, with a novel approach to adapting video foundation models for visuomotor control and planning), and criterion 4 (application of vision foundation models in robotics). The Cosmos Policy method is a new, simple adaptation of a large pretrained video model for robot policy learning, with strong empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.15780": {
        "authors": [
            "Pascal Benschop",
            "Justin Dauwels",
            "Jan van Gemert"
        ],
        "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video",
        "abstract": "arXiv:2601.15780v1 Announce Type: new  Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.",
        "arxiv_id": "2601.15780",
        "ARXIVID": "2601.15780",
        "COMMENT": "Matches criterion 1: Introduces a synthetic benchmark to assess situational and spatial awareness in vision-language models (VLMs), focusing on spatial reasoning and awareness. Provides new empirical insights and a diagnostic tool for spatial intelligence in VLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.15698": {
        "authors": [
            "Mingyu Yu",
            "Lana Liu",
            "Zhehao Zhao",
            "Wei Wang",
            "Sujuan Qin"
        ],
        "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
        "abstract": "arXiv:2601.15698v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
        "arxiv_id": "2601.15698",
        "ARXIVID": "2601.15698",
        "COMMENT": "Matches criterion 2: Focuses on jailbreaking Multimodal Large Language Models (MLLMs) for harmful image generation, introducing a new framework (BVS) to probe visual safety boundaries. Directly about MLLMs and their vulnerabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.15897": {
        "authors": [
            "Zhaoqi Su",
            "Shihai Chen",
            "Xinyan Lin",
            "Liqin Huang",
            "Zhipeng Su",
            "Xiaoqiang Lu"
        ],
        "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
        "abstract": "arXiv:2601.15897v1 Announce Type: new  Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
        "arxiv_id": "2601.15897",
        "ARXIVID": "2601.15897",
        "COMMENT": "This paper matches criterion 1 and 4: it introduces a novel cross-modal 3D scene reconstruction method (ThermoSplat) that fuses RGB and thermal data using new feature modulation and geometry decoupling techniques, advancing spatial understanding in embodied agents and vision foundation model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15703": {
        "authors": [
            "Jiaxin Zhang",
            "Prafulla Kumar Choubey",
            "Kung-Hsiang Huang",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "title": "Agentic Uncertainty Quantification",
        "abstract": "arXiv:2601.15703v1 Announce Type: new  Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
        "arxiv_id": "2601.15703",
        "ARXIVID": "2601.15703",
        "COMMENT": "Matches criterion 3: Proposes a dual-process agentic uncertainty quantification framework (AUQ) for reliable AI agents, addressing the spiral of hallucination in long-horizon reasoning. Focuses on new methods for agent reliability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15808": {
        "authors": [
            "Yuxuan Wan",
            "Tianqing Fang",
            "Zaitang Li",
            "Yintong Huo",
            "Wenxuan Wang",
            "Haitao Mi",
            "Dong Yu",
            "Michael R. Lyu"
        ],
        "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
        "abstract": "arXiv:2601.15808v1 Announce Type: new  Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
        "arxiv_id": "2601.15808",
        "ARXIVID": "2601.15808",
        "COMMENT": "Matches criterion 3: Proposes a new paradigm for self-evolving deep research agents via inference-time, rubric-guided verification, and releases a new dataset for agent verification. Focuses on novel methods for agent reliability and benchmarking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15778": {
        "authors": [
            "Jiaxin Zhang",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "title": "Agentic Confidence Calibration",
        "abstract": "arXiv:2601.15778v1 Announce Type: new  Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
        "arxiv_id": "2601.15778",
        "ARXIVID": "2601.15778",
        "COMMENT": "Matches criterion 3: Proposes a novel process-centric paradigm for confidence calibration in agentic systems, introducing Holistic Trajectory Calibration (HTC) and a General Agent Calibrator (GAC) for diagnosing and enhancing reliability of AI agents. Focuses on agentic systems and their unique challenges, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15644": {
        "authors": [
            "Zichen Yu",
            "Quanli Liu",
            "Wei Wang",
            "Liyong Zhang",
            "Xiaoguang Zhao"
        ],
        "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction",
        "abstract": "arXiv:2601.15644v1 Announce Type: new  Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.",
        "arxiv_id": "2601.15644",
        "ARXIVID": "2601.15644",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel methods for 3D occupancy prediction in autonomous driving, with a focus on temporal modeling and efficient representations). The use of superquadric-based representations and cohesive temporal modeling is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15507": {
        "authors": [
            "Jinrui Yang",
            "Qing Liu",
            "Yijun Li",
            "Mengwei Ren",
            "Letian Zhang",
            "Zhe Lin",
            "Cihang Xie",
            "Yuyin Zhou"
        ],
        "title": "Controllable Layered Image Generation for Real-World Editing",
        "abstract": "arXiv:2601.15507v1 Announce Type: new  Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.",
        "arxiv_id": "2601.15507",
        "ARXIVID": "2601.15507",
        "COMMENT": "Matches criterion 4: Proposes LASAGNA, a new framework for controllable layered image generation with a new dataset and benchmark for layer editing, relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.15888": {
        "authors": [
            "Shiqi Huang",
            "Yipei Wang",
            "Natasha Thorley",
            "Alexander Ng",
            "Shaheer Saeed",
            "Mark Emberton",
            "Shonit Punwani",
            "Veeru Kasivisvanathan",
            "Dean Barratt",
            "Daniel Alexander",
            "Yipeng Hu"
        ],
        "title": "Understanding the Transfer Limits of Vision Foundation Models",
        "abstract": "arXiv:2601.15888v1 Announce Type: new  Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
        "arxiv_id": "2601.15888",
        "ARXIVID": "2601.15888",
        "COMMENT": "Matches criterion 4: Investigates transfer limits of Vision Foundation Models (VFMs) and analyzes how pretraining objectives align with downstream tasks, providing empirical insights into VFM transferability.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.15734": {
        "authors": [
            "Shadi Alijani",
            "Fereshteh Aghaee Meibodi",
            "Homayoun Najjaran"
        ],
        "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
        "abstract": "arXiv:2601.15734v1 Announce Type: new  Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.",
        "arxiv_id": "2601.15734",
        "ARXIVID": "2601.15734",
        "COMMENT": "This paper matches criterion 4 as it proposes new methods for adapting vision foundation models to multi-modal medical imaging, with innovations in modality fusion and adaptive prompting. The focus on sub-region-aware attention and prompting is a novel methodological improvement for foundation model adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.15931": {
        "authors": [
            "Xiangyu Wang",
            "Zhixin Lv",
            "Yongjiao Sun",
            "Anrui Han",
            "Ye Yuan",
            "Hangxu Ji"
        ],
        "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search",
        "abstract": "arXiv:2601.15931v1 Announce Type: new  Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.",
        "arxiv_id": "2601.15931",
        "ARXIVID": "2601.15931",
        "COMMENT": "Partially matches criterion 1: Proposes ICON, a framework for text-based person search with spatial intervention and neuro-symbolic priors, addressing spatial semantic misalignment and robustness. Focuses on spatial understanding in vision-language tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.16073": {
        "authors": [
            "Hanwen Zhang",
            "Qiaojin Shen",
            "Yuxi Liu",
            "Yuesheng Zhu",
            "Guibo Luo"
        ],
        "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
        "abstract": "arXiv:2601.16073v1 Announce Type: new  Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
        "arxiv_id": "2601.16073",
        "ARXIVID": "2601.16073",
        "COMMENT": "This paper proposes a federated learning framework for medical image segmentation that leverages foundation models and lightweight models. It matches criterion 4 (vision foundation models and applications), with a focus on efficient deployment and mutual distillation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.15772": {
        "authors": [
            "Yuhan Chen",
            "Wenxuan Yu",
            "Guofa Li",
            "Yijun Xu",
            "Ying Fang",
            "Yicui Shi",
            "Long Cao",
            "Wenbo Chu",
            "Keqiang Li"
        ],
        "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
        "abstract": "arXiv:2601.15772v1 Announce Type: new  Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
        "arxiv_id": "2601.15772",
        "ARXIVID": "2601.15772",
        "COMMENT": "This paper proposes a zero-shot low-light enhancement method that operates directly in the 2D Gaussian Splatting compressed domain. It is relevant to criterion 4 (vision foundation models and applications), as it explores explicit scene representations for image enhancement.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.15766": {
        "authors": [
            "Yuhan Chen",
            "Ying Fang",
            "Guofa Li",
            "Wenxuan Yu",
            "Yicui Shi",
            "Jingrui Zhang",
            "Kefei Qian",
            "Wenbo Chu",
            "Keqiang Li"
        ],
        "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
        "abstract": "arXiv:2601.15766v1 Announce Type: new  Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
        "arxiv_id": "2601.15766",
        "ARXIVID": "2601.15766",
        "COMMENT": "This paper introduces a new method for low-light image enhancement using 2D Gaussian Splatting, which is an explicit scene representation. It is relevant to criterion 4 (vision foundation models and applications), as it explores explicit representations for image enhancement, but does not directly address VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.15706": {
        "authors": [
            "Akriti Vij",
            "Benjamin Chua",
            "Darshini Ramiah",
            "En Qi Ng",
            "Mahran Morsidi",
            "Naga Nikshith Gangarapu",
            "Sharmini Johnson",
            "Vanessa Wilfred",
            "Vikneswaran Kumaran",
            "Wan Sie Lee",
            "Wenzhuo Yang",
            "Yongsen Zheng",
            "Bill Black",
            "Boming Xia",
            "Frank Sun",
            "Hao Zhang",
            "Qinghua Lu",
            "Suyu Ma",
            "Yue Liu",
            "Chi-kiu Lo",
            "Fatemeh Azadi",
            "Isar Nejadgholi",
            "Sowmya Vajjala",
            "Agnes Delaborde",
            "Nicolas Rolin",
            "Tom Seimandi",
            "Akiko Murakami",
            "Haruto Ishi",
            "Satoshi Sekine",
            "Takayuki Semitsu",
            "Tasuku Sasaki",
            "Angela Kinuthia",
            "Jean Wangari",
            "Michael Michie",
            "Stephanie Kasaon",
            "Hankyul Baek",
            "Jaewon Noh",
            "Kihyuk Nam",
            "Sang Seo",
            "Sungpil Shin",
            "Taewhi Lee",
            "Yongsu Kim",
            "Daisy Newbold-Harrop",
            "Jessica Wang",
            "Mahmoud Ghanem",
            "Vy Hong"
        ],
        "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
        "abstract": "arXiv:2601.15706v1 Announce Type: new  Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
        "arxiv_id": "2601.15706",
        "ARXIVID": "2601.15706",
        "COMMENT": "This paper focuses on multilingual safety evaluation methodologies for LLMs, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about evaluation frameworks and safety in language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.16027": {
        "authors": [
            "Yiran Qiao",
            "Xiang Ao",
            "Jing Chen",
            "Yang Liu",
            "Qiwei Zhong",
            "Qing He"
        ],
        "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment",
        "abstract": "arXiv:2601.16027v1 Announce Type: new  Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.",
        "arxiv_id": "2601.16027",
        "ARXIVID": "2601.16027",
        "COMMENT": "Does not match any specific criterion. Focuses on risk assessment in live streaming using retrieval-augmented LLMs, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.15798": {
        "authors": [
            "Zhikai Xue",
            "Tianqianjin Lin",
            "Pengwei Yan",
            "Ruichun Wang",
            "Yuxin Liu",
            "Zhuoren Jiang",
            "Xiaozhong Liu"
        ],
        "title": "VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management",
        "abstract": "arXiv:2601.15798v1 Announce Type: new  Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.",
        "arxiv_id": "2601.15798",
        "ARXIVID": "2601.15798",
        "COMMENT": "This paper presents an LLM-driven system for chronic disease management, integrating wearable data and LLM reasoning. While it uses LLMs, it does not focus on vision, spatial intelligence, or embodied AI, and is not a VLLM/MLLM paper.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.15690": {
        "authors": [
            "Jiaxin Zhang",
            "Wendi Cui",
            "Zhuohang Li",
            "Lifu Huang",
            "Bradley Malin",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
        "abstract": "arXiv:2601.15690v1 Announce Type: new  Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
        "arxiv_id": "2601.15690",
        "ARXIVID": "2601.15690",
        "COMMENT": "This is a survey on uncertainty quantification in LLMs, with some discussion of autonomous agents and reinforcement learning. It does not focus on vision, spatial intelligence, or embodied AI, nor does it introduce new VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}