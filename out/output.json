{
    "2602.15918": {
        "authors": [
            "Zelin Xu",
            "Yupu Zhang",
            "Saugat Adhikari",
            "Saiful Islam",
            "Tingsong Xiao",
            "Zibo Liu",
            "Shigang Chen",
            "Da Yan",
            "Zhe Jiang"
        ],
        "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
        "abstract": "arXiv:2602.15918v1 Announce Type: new  Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \\textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.",
        "arxiv_id": "2602.15918",
        "ARXIVID": "2602.15918",
        "COMMENT": "Matches criterion 3 (new benchmark for spatial reasoning in MLLMs on Earth imagery, with a focus on spatial intelligence and embodied AI relevance). Also relevant to criterion 1 (spatial understanding in embodied agents) and criterion 2 (MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.16682": {
        "authors": [
            "Chuhan Li",
            "Ruilin Han",
            "Joy Hsu",
            "Yongyuan Liang",
            "Rajiv Dhawan",
            "Jiajun Wu",
            "Ming-Hsuan Yang",
            "Xin Eric Wang"
        ],
        "title": "Learning Situated Awareness in the Real World",
        "abstract": "arXiv:2602.16682v1 Announce Type: new  Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
        "arxiv_id": "2602.16682",
        "ARXIVID": "2602.16682",
        "COMMENT": "Matches criterion 1 and 3: New methodological improvements to spatial understanding/spatial intelligence on embodied agents, and new benchmarks for embodied AI. Introduces SAW-Bench, a novel benchmark for egocentric situated awareness, focusing on observer-centric spatial reasoning, which is a novel angle.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.16455": {
        "authors": [
            "Jinsong Li",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Dahua Lin"
        ],
        "title": "Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing",
        "abstract": "arXiv:2602.16455v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.",
        "arxiv_id": "2602.16455",
        "ARXIVID": "2602.16455",
        "COMMENT": "Matches criterion 2 and 4: Shows new VLLMs (LVLMs) and vision foundation models and their applications. Proposes Visual Self-Refine, a pixel-guided paradigm for chart parsing, and introduces a new benchmark. The feedback mechanism is a novel angle for vision-centric tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.16412": {
        "authors": [
            "Daichi Yashima",
            "Shuhei Kurita",
            "Yusuke Oda",
            "Komei Sugiura"
        ],
        "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
        "abstract": "arXiv:2602.16412v1 Announce Type: new  Abstract: While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.",
        "arxiv_id": "2602.16412",
        "ARXIVID": "2602.16412",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes ReMoRa, a new multimodal large language model for long-video understanding, with a novel motion representation and efficient processing. Strong empirical results on long-video benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.16110": {
        "authors": [
            "Tianwei Lin",
            "Zhongwei Qiu",
            "Wenqiao Zhang",
            "Jiang Liu",
            "Yihan Xie",
            "Mingjian Gao",
            "Zhenxuan Fan",
            "Zhaocheng Li",
            "Sijing Li",
            "Zhongle Xie",
            "Peng LU",
            "Yueting Zhuang",
            "Yingda Xia",
            "Ling Zhang",
            "Beng Chin Ooi"
        ],
        "title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis",
        "abstract": "arXiv:2602.16110v1 Announce Type: new  Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.",
        "arxiv_id": "2602.16110",
        "ARXIVID": "2602.16110",
        "COMMENT": "Proposes OmniCT, a unified slice-volume LVLM for CT analysis, with new methods for spatial consistency and organ-level semantic enhancement. Matches criterion 2 (new LVLM for medical imaging) and criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.15989": {
        "authors": [
            "Xitong Yang",
            "Devansh Kukreja",
            "Don Pinkus",
            "Anushka Sagar",
            "Taosha Fan",
            "Jinhyung Park",
            "Soyong Shin",
            "Jinkun Cao",
            "Jiawei Liu",
            "Nicolas Ugrinovic",
            "Matt Feiszli",
            "Jitendra Malik",
            "Piotr Dollar",
            "Kris Kitani"
        ],
        "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
        "abstract": "arXiv:2602.15989v1 Announce Type: new  Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
        "arxiv_id": "2602.15989",
        "ARXIVID": "2602.15989",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. Introduces SAM 3D Body, a promptable model for 3D human mesh recovery, with a new parametric mesh representation and open-source release.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16238": {
        "authors": [
            "Hiroki Nakamura",
            "Hiroto Iino",
            "Masashi Okada",
            "Tadahiro Taniguchi"
        ],
        "title": "EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection",
        "abstract": "arXiv:2602.16238v1 Announce Type: new  Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.",
        "arxiv_id": "2602.16238",
        "ARXIVID": "2602.16238",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. The paper adapts an image-generation foundation model for edge detection, introducing new objectives and inference guidance for crisp, data-efficient edge maps.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16689": {
        "authors": [
            "Ferdinand Kapl",
            "Amir Mohammad Karimi Mamaghan",
            "Maximilian Seitzer",
            "Karl Henrik Johansson",
            "Carsten Marr",
            "Stefan Bauer",
            "Andrea Dittadi"
        ],
        "title": "Are Object-Centric Representations Better At Compositional Generalization?",
        "abstract": "arXiv:2602.16689v1 Announce Type: new  Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.",
        "arxiv_id": "2602.16689",
        "ARXIVID": "2602.16689",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Systematic study of object-centric representations for compositional generalization in vision-language tasks, using foundation models (DINOv2, SigLIP2) and new VQA benchmarks. Provides empirical insights into object-centric vs. dense representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16664": {
        "authors": [
            "Jiaming Liu",
            "Felix Petersen",
            "Yunhe Gao",
            "Yabin Zhang",
            "Hyojin Kim",
            "Akshay S. Chaudhari",
            "Yu Sun",
            "Stefano Ermon",
            "Sergios Gatidis"
        ],
        "title": "Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge",
        "abstract": "arXiv:2602.16664v1 Announce Type: new  Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.",
        "arxiv_id": "2602.16664",
        "ARXIVID": "2602.16664",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a self-supervised semantic bridge for unpaired image-to-image translation, leveraging self-supervised visual encoders for spatially faithful translation. Strong results in medical image synthesis and text-guided editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16105": {
        "authors": [
            "Thinh Hung Truong",
            "Jey Han Lau",
            "Jianzhong Qi"
        ],
        "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
        "abstract": "arXiv:2602.16105v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench",
        "arxiv_id": "2602.16105",
        "ARXIVID": "2602.16105",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI, simulator-related). Introduces GPSBench, a large dataset for evaluating geospatial reasoning in LLMs, with a focus on GPS coordinates and real-world geography. Provides new insights into LLMs' spatial/geospatial reasoning abilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16669": {
        "authors": [
            "Bo Lang",
            "Nirav Savaliya",
            "Zhihao Zheng",
            "Jinglun Feng",
            "Zheng-Hang Yeh",
            "Mooi Choo Chuah"
        ],
        "title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction",
        "abstract": "arXiv:2602.16669v1 Announce Type: new  Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.",
        "arxiv_id": "2602.16669",
        "ARXIVID": "2602.16669",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI, with explicit temporal consistency and map memory for HD map construction). Novel approach to online vectorized map construction with spatially aligned queries and history-guided prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16666": {
        "authors": [
            "Stephan Rabanser",
            "Sayash Kapoor",
            "Peter Kirgis",
            "Kangheng Liu",
            "Saiteja Utpala",
            "Arvind Narayanan"
        ],
        "title": "Towards a Science of AI Agent Reliability",
        "abstract": "arXiv:2602.16666v1 Announce Type: new  Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
        "arxiv_id": "2602.16666",
        "ARXIVID": "2602.16666",
        "COMMENT": "This paper proposes a new holistic evaluation framework for AI agent reliability, introducing twelve concrete metrics across four dimensions (consistency, robustness, predictability, safety). This aligns with criterion 3 (Embodied AI papers on building new benchmarks or methods, focusing on novel angles), as it addresses overlooked aspects of agent evaluation beyond standard success metrics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.15915": {
        "authors": [
            "Xianwei Mao",
            "Kai Ye",
            "Sheng Zhou",
            "Nan Zhang",
            "Haikuan Huang",
            "Bin Li",
            "Jiajun Bu"
        ],
        "title": "MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering",
        "abstract": "arXiv:2602.15915v1 Announce Type: new  Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.",
        "arxiv_id": "2602.15915",
        "ARXIVID": "2602.15915",
        "COMMENT": "Proposes a new framework for knowledge-based VQA with explicit/implicit knowledge filtering, evaluated on MLLMs (criterion 2: MLLMs, and criterion 4: vision foundation models).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.16019": {
        "authors": [
            "Ahmad Elallaf",
            "Yu Zhang",
            "Yuktha Priya Masupalli",
            "Jeong Yang",
            "Young Lee",
            "Zechun Cao",
            "Gongbo Liang"
        ],
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "abstract": "arXiv:2602.16019v1 Announce Type: new  Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
        "arxiv_id": "2602.16019",
        "ARXIVID": "2602.16019",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Introduces MedProbCLIP, a probabilistic adaptation of vision-language foundation models for radiograph-report retrieval, with improved calibration and robustness. Focused on medical domain but methodologically relevant.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.16681": {
        "authors": [
            "Yingyuan Yang",
            "Tian Lan",
            "Yifei Gao",
            "Yimeng Lu",
            "Wenjun He",
            "Meng Wang",
            "Chenghao Liu",
            "Chen Zhang"
        ],
        "title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection",
        "abstract": "arXiv:2602.16681v1 Announce Type: new  Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.",
        "arxiv_id": "2602.16681",
        "ARXIVID": "2602.16681",
        "COMMENT": "Presents a new framework unifying temporal and visual modalities for time-series anomaly detection, leveraging vision foundation models (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.16502": {
        "authors": [
            "Zeng Tao",
            "Ying Jiang",
            "Yunuo Chen",
            "Tianyi Xie",
            "Huamin Wang",
            "Yingnian Wu",
            "Yin Yang",
            "Abishek Sampath Kumar",
            "Kenji Tashiro",
            "Chenfanfu Jiang"
        ],
        "title": "DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images",
        "abstract": "arXiv:2602.16502v1 Announce Type: new  Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.",
        "arxiv_id": "2602.16502",
        "ARXIVID": "2602.16502",
        "COMMENT": "Uses vision-language models for pose normalization in garment pattern generation, which is an application of vision foundation models (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15903": {
        "authors": [
            "Jingwei Li",
            "Jiaxin Tong",
            "Pengfei Wu"
        ],
        "title": "Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment",
        "abstract": "arXiv:2602.15903v1 Announce Type: new  Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\\% and 4.02\\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.",
        "arxiv_id": "2602.15903",
        "ARXIVID": "2602.15903",
        "COMMENT": "Uses CLIP-based image-text alignment (vision-language model) for deepfake detection, which is related to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.16173": {
        "authors": [
            "Kaiqu Liang",
            "Julia Kruk",
            "Shengyi Qian",
            "Xianjun Yang",
            "Shengjie Bi",
            "Yuanshun Yao",
            "Shaoliang Nie",
            "Mingyang Zhang",
            "Lijuan Liu",
            "Jaime Fern\\'andez Fisac",
            "Shuyan Zhou",
            "Saghar Hosseini"
        ],
        "title": "Learning Personalized Agents from Human Feedback",
        "abstract": "arXiv:2602.16173v1 Announce Type: new  Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
        "arxiv_id": "2602.16173",
        "ARXIVID": "2602.16173",
        "COMMENT": "This paper is about continual personalization of agents from human feedback, with benchmarks in embodied manipulation and online shopping. It is somewhat related to embodied AI (criterion 3), but the focus is on preference adaptation, not spatial intelligence or new simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.16569": {
        "authors": [
            "Nicol\\`o Di Domenico",
            "Annalisa Franco",
            "Matteo Ferrara",
            "Davide Maltoni"
        ],
        "title": "Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face",
        "abstract": "arXiv:2602.16569v1 Announce Type: new  Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.",
        "arxiv_id": "2602.16569",
        "ARXIVID": "2602.16569",
        "COMMENT": "Applies a face foundation model (Arc2Face) for facial morphing, which is an application of vision foundation models (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2602.16132": {
        "authors": [
            "Joel Mathew Cherian",
            "Ashutosh Muralidhara Bharadwaj",
            "Vima Gupta",
            "Anand Padmanabha Iyer"
        ],
        "title": "CHAI: CacHe Attention Inference for text2video",
        "abstract": "arXiv:2602.16132v1 Announce Type: new  Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.",
        "arxiv_id": "2602.16132",
        "ARXIVID": "2602.16132",
        "COMMENT": "Related to generative modeling (text-to-video diffusion), but does not directly match any of the four criteria (not spatial intelligence, not VLLM/MLLM, not embodied AI, not vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.16322": {
        "authors": [
            "Santiago C. Vilabella",
            "Pablo P\\'erez-N\\'u\\~nez",
            "Beatriz Remeseiro"
        ],
        "title": "A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks",
        "abstract": "arXiv:2602.16322v1 Announce Type: new  Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.",
        "arxiv_id": "2602.16322",
        "ARXIVID": "2602.16322",
        "COMMENT": "Does not match any specific criterion. Presents a self-supervised approach for feature representation in object detection, but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.16160": {
        "authors": [
            "Patrick Poggi",
            "Divake Kumar",
            "Theja Tulabandhula",
            "Amit Ranjan Trivedi"
        ],
        "title": "Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking",
        "abstract": "arXiv:2602.16160v1 Announce Type: new  Abstract: Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\\% GFLOPs reduction, 8.9\\% latency reduction, and 10.8\\% energy savings while maintaining tracking accuracy within 0.2\\% of the full-depth baseline across both short-term and long-term sequences.",
        "arxiv_id": "2602.16160",
        "ARXIVID": "2602.16160",
        "COMMENT": "Does not match any specific criterion. Focuses on transformer-based visual tracking with dynamic depth adaptation, not directly on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.16337": {
        "authors": [
            "Tiou Wang",
            "Zhuoqian Yang",
            "Markus Flierl",
            "Mathieu Salzmann",
            "Sabine S\\\"usstrunk"
        ],
        "title": "Subtractive Modulative Network with Learnable Periodic Activations",
        "abstract": "arXiv:2602.16337v1 Announce Type: new  Abstract: We propose the Subtractive Modulative Network (SMN), a novel, parameter-efficient Implicit Neural Representation (INR) architecture inspired by classical subtractive synthesis. The SMN is designed as a principled signal processing pipeline, featuring a learnable periodic activation layer (Oscillator) that generates a multi-frequency basis, and a series of modulative mask modules (Filters) that actively generate high-order harmonics. We provide both theoretical analysis and empirical validation for our design. Our SMN achieves a PSNR of $40+$ dB on two image datasets, comparing favorably against state-of-the-art methods in terms of both reconstruction accuracy and parameter efficiency. Furthermore, consistent advantage is observed on the challenging 3D NeRF novel view synthesis task. Supplementary materials are available at https://inrainbws.github.io/smn/.",
        "arxiv_id": "2602.16337",
        "ARXIVID": "2602.16337",
        "COMMENT": "Does not match any specific criterion. Presents a new implicit neural representation architecture, but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.16245": {
        "authors": [
            "J. Dhar",
            "M. K. Pandey",
            "D. Chakladar",
            "M. Haghighat",
            "A. Alavi",
            "S. Mistry",
            "N. Zaidi"
        ],
        "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
        "abstract": "arXiv:2602.16245v1 Announce Type: new  Abstract: Multimodal fusion frameworks, which integrate diverse medical imaging modalities (e.g., MRI, CT), have shown great potential in applications such as skin cancer detection, dementia diagnosis, and brain tumor prediction. However, existing multimodal fusion methods face significant challenges. First, they often rely on computationally expensive models, limiting their applicability in low-resource environments. Second, they often employ cascaded attention modules, which potentially increase risk of information loss during inter-module transitions and hinder their capacity to effectively capture robust shared representations across modalities. This restricts their generalization in multi-disease analysis tasks. To address these limitations, we propose a Hybrid Parallel-Fusion Cascaded Attention Network (HyPCA-Net), composed of two core novel blocks: (a) a computationally efficient residual adaptive learning attention block for capturing refined modality-specific representations, and (b) a dual-view cascaded attention block aimed at learning robust shared representations across diverse modalities. Extensive experiments on ten publicly available datasets exhibit that HyPCA-Net significantly outperforms existing leading methods, with improvements of up to 5.2% in performance and reductions of up to 73.1% in computational cost. Code: https://github.com/misti1203/HyPCA-Net.",
        "arxiv_id": "2602.16245",
        "ARXIVID": "2602.16245",
        "COMMENT": "This paper proposes a new multimodal fusion network for medical image analysis. While it is related to multimodal learning, it does not focus on vision foundation models, VLLMs/MLLMs, spatial intelligence, or embodied AI benchmarks.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2602.16066": {
        "authors": [
            "Martin Klissarov",
            "Jonathan Cook",
            "Diego Antognini",
            "Hao Sun",
            "Jingling Li",
            "Natasha Jaques",
            "Claudiu Musat",
            "Edward Grefenstette"
        ],
        "title": "Improving Interactive In-Context Learning from Natural Language Feedback",
        "abstract": "arXiv:2602.16066v1 Announce Type: new  Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.",
        "arxiv_id": "2602.16066",
        "ARXIVID": "2602.16066",
        "COMMENT": "This paper proposes a new framework for interactive in-context learning from natural language feedback, but it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about language model adaptation and feedback.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.16149": {
        "authors": [
            "Huichan Seo",
            "Minki Hong",
            "Sieun Choi",
            "Jihie Kim",
            "Jean Oh"
        ],
        "title": "Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing",
        "abstract": "arXiv:2602.16149v1 Announce Type: new  Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias",
        "arxiv_id": "2602.16149",
        "ARXIVID": "2602.16149",
        "COMMENT": "Analyzes demographic bias in image-to-image editing using VLM scoring, but does not introduce new methods or benchmarks for spatial intelligence, embodied AI, or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.16012": {
        "authors": [
            "Jieyi Bi",
            "Zhiguang Cao",
            "Jianan Zhou",
            "Wen Song",
            "Yaoxin Wu",
            "Jie Zhang",
            "Yining Ma",
            "Cathy Wu"
        ],
        "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems",
        "abstract": "arXiv:2602.16012v1 Announce Type: new  Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.",
        "arxiv_id": "2602.16012",
        "ARXIVID": "2602.16012",
        "COMMENT": "Does not match any specific criterion; focuses on neural solvers for routing problems, not spatial intelligence in embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.16512": {
        "authors": [
            "Felix Fricke",
            "Simon Malberg",
            "Georg Groh"
        ],
        "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
        "abstract": "arXiv:2602.16512v1 Announce Type: new  Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
        "arxiv_id": "2602.16512",
        "ARXIVID": "2602.16512",
        "COMMENT": "Does not match any specific criterion. Focuses on reasoning frameworks for language models, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.16037": {
        "authors": [
            "Cameron Cagan",
            "Pedram Fard",
            "Jiazi Tian",
            "Jingya Cheng",
            "Shawn N. Murphy",
            "Hossein Estiri"
        ],
        "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection",
        "abstract": "arXiv:2602.16037v1 Announce Type: new  Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.",
        "arxiv_id": "2602.16037",
        "ARXIVID": "2602.16037",
        "COMMENT": "This paper investigates optimization instability in autonomous agentic workflows for clinical symptom detection. While it discusses autonomous agents, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}