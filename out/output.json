{
    "2508.06113": {
        "authors": [
            "Jian Wang",
            "Chaokang Jiang",
            "Haitao Xu"
        ],
        "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving",
        "abstract": "arXiv:2508.06113v1 Announce Type: new  Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end autonomous driving, yet their performance is increasingly hampered by a reliance on transformer-based fusion. These architectures face fundamental limitations: quadratic computational complexity restricts the use of high-resolution features, and a lack of spatial priors prevents them from effectively modeling the inherent structure of Bird's Eye View (BEV) representations. This paper introduces GMF-Drive (Gated Mamba Fusion for Driving), an end-to-end framework that overcomes these challenges through two principled innovations. First, we supersede the information-limited histogram-based LiDAR representation with a geometrically-augmented pillar format encoding shape descriptors and statistical features, preserving critical 3D geometric details. Second, we propose a novel hierarchical gated mamba fusion (GM-Fusion) architecture that substitutes an expensive transformer with a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM leverages directional sequencing and adaptive fusion mechanisms to capture long-range dependencies with linear complexity, while explicitly respecting the unique spatial properties of the driving scene. Extensive experiments on the challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new state-of-the-art performance, significantly outperforming DiffusionDrive. Comprehensive ablation studies validate the efficacy of each component, demonstrating that task-specific SSMs can surpass a general-purpose transformer in both performance and efficiency for autonomous driving.",
        "arxiv_id": "2508.06113",
        "ARXIVID": "2508.06113",
        "COMMENT": "This paper introduces GMF-Drive, an end-to-end autonomous driving framework with a spatially-aware BEV representation and a novel state-space model for fusion, replacing transformers. This directly matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI, especially with spatial priors and efficiency).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.06226": {
        "authors": [
            "Yumeng Fu",
            "Jiayin Zhu",
            "Lingling Zhang",
            "Bo Zhao",
            "Shaoxuan Ma",
            "Yushun Zhang",
            "Yanrui Wu",
            "Wenjun Wu"
        ],
        "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines",
        "abstract": "arXiv:2508.06226v1 Announce Type: new  Abstract: Geometry problem solving (GPS) requires models to master diagram comprehension, logical reasoning, knowledge application, numerical computation, and auxiliary line construction. This presents a significant challenge for Multimodal Large Language Models (MLLMs). However, existing benchmarks for evaluating MLLM geometry skills overlook auxiliary line construction and lack fine-grained process evaluation, making them insufficient for assessing MLLMs' long-step reasoning abilities. To bridge these gaps, we present the GeoLaux benchmark, comprising 2,186 geometry problems, incorporating both calculation and proving questions. Notably, the problems require an average of 6.51 reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary line construction. Building on the dataset, we design a novel five-dimensional evaluation strategy assessing answer correctness, process correctness, process quality, auxiliary line impact, and error causes. Extensive experiments on 13 leading MLLMs (including thinking models and non-thinking models) yield three pivotal findings: First, models exhibit substantial performance degradation in extended reasoning steps (nine models demonstrate over 50% performance drop). Second, compared to calculation problems, MLLMs tend to take shortcuts when solving proving problems. Third, models lack auxiliary line awareness, and enhancing this capability proves particularly beneficial for overall geometry reasoning improvement. These findings establish GeoLaux as both a benchmark for evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a guide for capability advancement. Our dataset and code are included in supplementary materials and will be released.",
        "arxiv_id": "2508.06226",
        "ARXIVID": "2508.06226",
        "COMMENT": "This paper introduces GeoLaux, a new benchmark for evaluating MLLMs' geometry performance, specifically focusing on long-step problems requiring auxiliary lines. This directly matches criterion 3 (new embodied AI benchmarks with novel angles) and criterion 2 (MLLMs). The benchmark addresses previously ignored aspects (auxiliary line construction, process evaluation) and provides new insights into MLLM weaknesses.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.05954": {
        "authors": [
            "Han Lin",
            "Jaemin Cho",
            "Amir Zadeh",
            "Chuan Li",
            "Mohit Bansal"
        ],
        "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
        "abstract": "arXiv:2508.05954v1 Announce Type: new  Abstract: There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.",
        "arxiv_id": "2508.05954",
        "ARXIVID": "2508.05954",
        "COMMENT": "Matches criteria 2 (new MLLM framework integrating multimodal LLMs and diffusion models), and 4 (vision foundation models and their integration with language models for controllable image generation).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.05899": {
        "authors": [
            "Zixuan Bian",
            "Ruohan Ren",
            "Yue Yang",
            "Chris Callison-Burch"
        ],
        "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing",
        "abstract": "arXiv:2508.05899v1 Announce Type: new  Abstract: 3D scene generation plays a crucial role in gaming, artistic creation, virtual reality and many other domains. However, current 3D scene design still relies heavily on extensive manual effort from creators, and existing automated methods struggle to generate open-domain scenes or support flexible editing. As a result, generating 3D worlds directly from text has garnered increasing attention. In this paper, we introduce HOLODECK 2.0, an advanced vision-language-guided framework for 3D world generation with support for interactive scene editing based on human feedback. HOLODECK 2.0 can generate diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and cyberpunk styles) that exhibit high semantic fidelity to fine-grained input descriptions, suitable for both indoor and open-domain environments. HOLODECK 2.0 leverages vision-language models (VLMs) to identify and parse the objects required in a scene and generates corresponding high-quality assets via state-of-the-art 3D generative models. It then iteratively applies spatial constraints derived from the VLMs to achieve semantically coherent and physically plausible layouts. Human evaluations and CLIP-based assessments demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely aligned with detailed textual descriptions, consistently outperforming baselines across indoor and open-domain scenarios. Additionally, we provide editing capabilities that flexibly adapt to human feedback, supporting layout refinement and style-consistent object edits. Finally, we present a practical application of HOLODECK 2.0 in procedural game modeling, generating visually rich and immersive environments, potentially boosting efficiency.",
        "arxiv_id": "2508.05899",
        "ARXIVID": "2508.05899",
        "COMMENT": "Matches criteria 1 (spatial understanding in embodied agents via VLM-guided 3D world generation and editing), 2 (uses vision-language models for scene parsing and generation), and 4 (vision foundation models applied to 3D scene generation). Also relevant to 3 (novel application in procedural game modeling, with editing and feedback).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.06146": {
        "authors": [
            "Yuchen Guan",
            "Chong Sun",
            "Canmiao Fu",
            "Zhipeng Huang",
            "Chun Yuan",
            "Chen Li"
        ],
        "title": "Text-guided Visual Prompt DINO for Generic Segmentation",
        "abstract": "arXiv:2508.06146v1 Announce Type: new  Abstract: Recent advancements in multimodal vision models have highlighted limitations in late-stage feature fusion and suboptimal query selection for hybrid prompts open-world segmentation, alongside constraints from caption-derived vocabularies. To address these challenges, we propose Prompt-DINO, a text-guided visual Prompt DINO framework featuring three key innovations. First, we introduce an early fusion mechanism that unifies text/visual prompts and backbone features at the initial encoding stage, enabling deeper cross-modal interactions to resolve semantic ambiguities. Second, we design order-aligned query selection for DETR-based architectures, explicitly optimizing the structural alignment between text and visual queries during decoding to enhance semantic-spatial consistency. Third, we develop a generative data engine powered by the Recognize Anything via Prompting (RAP) model, which synthesizes 0.5B diverse training instances through a dual-path cross-verification pipeline, reducing label noise by 80.5% compared to conventional approaches. Extensive experiments demonstrate that Prompt-DINO achieves state-of-the-art performance on open-world detection benchmarks while significantly expanding semantic coverage beyond fixed-vocabulary constraints. Our work establishes a new paradigm for scalable multimodal detection and data generation in open-world scenarios. Data&Code are available at https://github.com/WeChatCV/WeVisionOne.",
        "arxiv_id": "2508.06146",
        "ARXIVID": "2508.06146",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications). Proposes Prompt-DINO, a text-guided visual prompt framework for open-world segmentation, with early fusion and generative data engine. Also relevant for scalable multimodal detection.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.05976": {
        "authors": [
            "Zhihao Zhu",
            "Yifan Zheng",
            "Siyu Pan",
            "Yaohui Jin",
            "Yao Mu"
        ],
        "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation",
        "abstract": "arXiv:2508.05976v1 Announce Type: new  Abstract: The fragmentation between high-level task semantics and low-level geometric features remains a persistent challenge in robotic manipulation. While vision-language models (VLMs) have shown promise in generating affordance-aware visual representations, the lack of semantic grounding in canonical spaces and reliance on manual annotations severely limit their ability to capture dynamic semantic-affordance relationships. To address these, we propose Primitive-Aware Semantic Grounding (PASG), a closed-loop framework that introduces: (1) Automatic primitive extraction through geometric feature aggregation, enabling cross-category detection of keypoints and axes; (2) VLM-driven semantic anchoring that dynamically couples geometric primitives with functional affordances and task-relevant description; (3) A spatial-semantic reasoning benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's effectiveness in practical robotic manipulation tasks across diverse scenarios, achieving performance comparable to manual annotations. PASG achieves a finer-grained semantic-affordance understanding of objects, establishing a unified paradigm for bridging geometric primitives with task semantics in robotic manipulation.",
        "arxiv_id": "2508.05976",
        "ARXIVID": "2508.05976",
        "COMMENT": "Matches criterion 1 and 3: This paper proposes a new framework (PASG) for bridging geometric primitives and task semantics in robotic manipulation, with a new spatial-semantic reasoning benchmark and a fine-tuned VLM. It addresses spatial understanding in embodied agents and introduces a new benchmark/method for embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.06038": {
        "authors": [
            "Huanyu Wang",
            "Jushi Kai",
            "Haoli Bai",
            "Lu Hou",
            "Bo Jiang",
            "Ziwei He",
            "Zhouhan Lin"
        ],
        "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models",
        "abstract": "arXiv:2508.06038v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) typically replace the predefined image placeholder token () in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of vision tokens significantly increases the context length, leading to high computational overhead and inference latency. While previous efforts mitigate this by selecting only important visual features or leveraging learnable queries to reduce token count, they often compromise performance or introduce substantial extra costs. In response, we propose Fourier-VLM, a simple yet efficient method that compresses visual representations in the frequency domain. Our approach is motivated by the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components. Leveraging this, we apply a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier Transform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$, minimizing the extra computational cost while introducing no additional parameters. Extensive experiments across various image-based benchmarks demonstrate that Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.",
        "arxiv_id": "2508.06038",
        "ARXIVID": "2508.06038",
        "COMMENT": "Matches criterion 2 and 4: proposes Fourier-VLM, a method for compressing vision tokens in VLMs using frequency domain techniques, significantly improving efficiency with minimal performance loss, and is applicable to vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.05731": {
        "authors": [
            "Yuhang Liu",
            "Zeyu Liu",
            "Shuanghe Zhu",
            "Pengxiang Li",
            "Congkai Xie",
            "Jiasheng Wang",
            "Xueyu Hu",
            "Xiaotian Han",
            "Jianbo Yuan",
            "Xinyao Wang",
            "Shengyu Zhang",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization",
        "abstract": "arXiv:2508.05731v1 Announce Type: new  Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.",
        "arxiv_id": "2508.05731",
        "ARXIVID": "2508.05731",
        "COMMENT": "Matches criterion 1 and 3: proposes a new method (AEPO) for GUI grounding in embodied agents, focusing on spatial and semantic alignment, and establishes new state-of-the-art results on GUI grounding benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.06072": {
        "authors": [
            "Zijian Chen",
            "Lirong Deng",
            "Zhengyu Chen",
            "Kaiwei Zhang",
            "Qi Jia",
            "Yuan Tian",
            "Yucheng Zhu",
            "Guangtao Zhai"
        ],
        "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation",
        "abstract": "arXiv:2508.06072v1 Announce Type: new  Abstract: Evaluating the abilities of large models and manifesting their gaps are challenging. Current benchmarks adopt either ground-truth-based score-form evaluation on static datasets or indistinct textual chatbot-style human preferences collection, which may not provide users with immediate, intuitive, and perceptible feedback on performance differences. In this paper, we introduce BioMotion Arena, a novel framework for evaluating large language models (LLMs) and multimodal large language models (MLLMs) via visual animation. Our methodology draws inspiration from the inherent visual perception of motion patterns characteristic of living organisms that utilizes point-light source imaging to amplify the performance discrepancies between models. Specifically, we employ a pairwise comparison evaluation and collect more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion variants. Data analyses show that the crowd-sourced human votes are in good agreement with those of expert raters, demonstrating the superiority of our BioMotion Arena in offering discriminative feedback. We also find that over 90\\% of evaluated models, including the cutting-edge open-source InternVL3 and proprietary Claude-4 series, fail to produce fundamental humanoid point-light groups, much less smooth and biologically plausible motions. This enables BioMotion Arena to serve as a challenging benchmark for performance visualization and a flexible evaluation framework without restrictions on ground-truth.",
        "arxiv_id": "2508.06072",
        "ARXIVID": "2508.06072",
        "COMMENT": "Matches criterion 3 (embodied AI benchmarks) and criterion 2 (MLLMs) as it introduces BioMotion Arena, a new benchmark for evaluating LLMs and MLLMs via biological animation, revealing surprising empirical results about model limitations in motion understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.06317": {
        "authors": [
            "Jian Hu",
            "Zixu Cheng",
            "Shaogang Gong",
            "Isabel Guan",
            "Jianye Hao",
            "Jun Wang",
            "Kun Shao"
        ],
        "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding",
        "abstract": "arXiv:2508.06317v1 Announce Type: new  Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic matching, they often struggle with fine-grained temporal localisation. Recently, Group Relative Policy Optimisation (GRPO) reformulates the inference process as a reinforcement learning task, enabling fine-grained grounding and achieving strong in-domain performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To overcome both problems, we introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce. Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using GRPO rollouts, averages them to form a pseudo label, and estimates confidence from the variance across these rollouts. This confidence then weights the training rewards, guiding the model to focus on reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. Codes will be released once published.",
        "arxiv_id": "2508.06317",
        "ARXIVID": "2508.06317",
        "COMMENT": "Matches criterion 3: proposes a new method (URPA) for cross-domain video temporal grounding in unlabelled domains, focusing on efficient adaptation and uncertainty quantification, which is a novel angle for embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.06382": {
        "authors": [
            "Xiangyu Wu",
            "Feng Yu",
            "Yang Yang",
            "Jianfeng Lu"
        ],
        "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning",
        "abstract": "arXiv:2508.06382v1 Announce Type: new  Abstract: The integration of prompt tuning with multimodal learning has shown significant generalization abilities for various downstream tasks. Despite advancements, existing methods heavily depend on massive modality-specific labeled data (e.g., video, audio, and image), or are customized for a single modality. In this study, we present Text as Any-Modality by Consistent Prompt Tuning (TaAM-CPT), a scalable approach for constructing a general representation model toward unlimited modalities using solely text data. TaAM-CPT comprises modality prompt pools, text construction, and modality-aligned text encoders from pre-trained models, which allows for extending new modalities by simply adding prompt pools and modality-aligned text encoders. To harmonize the learning across different modalities, TaAM-CPT designs intra- and inter-modal learning objectives, which can capture category details within modalities while maintaining semantic consistency across different modalities. Benefiting from its scalable architecture and pre-trained models, TaAM-CPT can be seamlessly extended to accommodate unlimited modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT achieves leading results on diverse datasets spanning various modalities, including video classification, image classification, and audio classification. The code is available at https://github.com/Jinx630/TaAM-CPT.",
        "arxiv_id": "2508.06382",
        "ARXIVID": "2508.06382",
        "COMMENT": "Matches criterion 2: proposes TaAM-CPT, a scalable approach for zero-shot classification across unlimited modalities using only text data, and demonstrates strong results on multi-modal tasks without modality-specific labeled data.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.06169": {
        "authors": [
            "Wenpeng Xing",
            "Jie Chen",
            "Zaifeng Yang",
            "Changting Lin",
            "Jianfeng Dong",
            "Chaochao Chen",
            "Xun Zhou",
            "Meng Han"
        ],
        "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting",
        "abstract": "arXiv:2508.06169v1 Announce Type: new  Abstract: Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.",
        "arxiv_id": "2508.06169",
        "ARXIVID": "2508.06169",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it adapts 3D Gaussian Splatting for underwater 3D reconstruction with physics-aware modules. The method is novel in addressing underwater challenges and improving artifact removal.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.06202": {
        "authors": [
            "Chang Che",
            "Ziqi Wang",
            "Pengwan Yang",
            "Qi Wang",
            "Hui Ma",
            "Zenglin Shi"
        ],
        "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning",
        "abstract": "arXiv:2508.06202v1 Announce Type: new  Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.",
        "arxiv_id": "2508.06202",
        "ARXIVID": "2508.06202",
        "COMMENT": "Matches criterion 2: This paper proposes a new parameter-efficient architecture expansion method for continual visual instruction tuning in MLLMs, directly addressing improvements in multi-modal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.06092": {
        "authors": [
            "Yachun Mi",
            "Yu Li",
            "Yanting Li",
            "Shixin Sun",
            "Chen Hui",
            "Tong Zhang",
            "Yuanyuan Liu",
            "Chenyue Song",
            "Shaohui Liu"
        ],
        "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation",
        "abstract": "arXiv:2508.06092v1 Announce Type: new  Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key research challenge. Current mainstream VQA methods typically improve performance by pretraining on large-scale classification datasets (e.g., ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this strategy presents two significant challenges: (1) merely transferring semantic knowledge learned from pretraining is insufficient for VQA, as video quality depends on multiple factors (e.g., semantics, distortion, motion, aesthetics); (2) pretraining on large-scale datasets demands enormous computational resources, often dozens or even hundreds of times greater than training directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown remarkable generalization capabilities across a wide range of visual tasks, and have begun to demonstrate promising potential in quality assessment. In this work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP enhances both visual and textual representations through a Shared Cross-Modal Adapter (SCMA), which contains only a minimal number of trainable parameters and is the only component that requires training. This design significantly reduces computational cost. In addition, we introduce a set of five learnable quality-level prompts to guide the VLMs in perceiving subtle quality variations, thereby further enhancing the model's sensitivity to video quality. Furthermore, we investigate the impact of different frame sampling strategies on VQA performance, and find that frame-difference-based sampling leads to better generalization performance across datasets. Extensive experiments demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.",
        "arxiv_id": "2508.06092",
        "ARXIVID": "2508.06092",
        "COMMENT": "This paper proposes Q-CLIP, a new framework leveraging vision-language models (VLMs) for video quality assessment, with a unified cross-modal adapter and prompt-based quality guidance. It matches criterion 2 (Shows new VLLMs or MLLMs) and criterion 4 (Vision foundation models related and its applications), as it adapts VLMs for a novel application in video quality assessment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06080": {
        "authors": [
            "Bin Xia",
            "Jiyang Liu",
            "Yuechen Zhang",
            "Bohao Peng",
            "Ruihang Chu",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "title": "DreamVE: Unified Instruction-based Image and Video Editing",
        "abstract": "arXiv:2508.06080v1 Announce Type: new  Abstract: Instruction-based editing holds vast potential due to its simple and efficient interactive editing format. However, instruction-based editing, particularly for video, has been constrained by limited training data, hindering its practical application. To this end, we introduce DreamVE, a unified model for instruction-based image and video editing. Specifically, We propose a two-stage training strategy: first image editing, then video editing. This offers two main benefits: (1) Image data scales more easily, and models are more efficient to train, providing useful priors for faster and better video editing training. (2) Unifying image and video generation is natural and aligns with current trends. Moreover, we present comprehensive training data synthesis pipelines, including collage-based and generative model-based data synthesis. The collage-based data synthesis combines foreground objects and backgrounds to generate diverse editing data, such as object manipulation, background changes, and text modifications. It can easily generate billions of accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE on extensive collage-based data to achieve strong performance in key editing types and enhance generalization and transfer capabilities. However, collage-based data lacks some attribute editing cases, leading to a relative drop in performance. In contrast, the generative model-based pipeline, despite being hard to scale up, offers flexibility in handling attribute editing cases. Therefore, we use generative model-based data to further fine-tune DreamVE. Besides, we design an efficient and powerful editing framework for DreamVE. We build on the SOTA T2V model and use a token concatenation with early drop approach to inject source image guidance, ensuring strong consistency and editability. The codes and models will be released.",
        "arxiv_id": "2508.06080",
        "ARXIVID": "2508.06080",
        "COMMENT": "Matches criteria 2 (unified instruction-based image and video editing model, leveraging multi-modal learning), and 4 (vision foundation models applied to editing tasks).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06205": {
        "authors": [
            "Ruiyan Wang",
            "Lin Zuo",
            "Zonghao Lin",
            "Qiang Wang",
            "Zhengxue Cheng",
            "Rong Xie",
            "Jun Ling",
            "Li Song"
        ],
        "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset",
        "abstract": "arXiv:2508.06205v1 Announce Type: new  Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction. However, existing HOI data sets focus on details of affordance, often neglecting the influence of physical properties of objects on human long-term motion. To bridge this gap, we introduce the PA-HOI Motion Capture dataset, which highlights the impact of objects' physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics. The dataset comprises 562 motion sequences of human-object interactions, with each sequence performed by subjects of different genders interacting with 35 3D objects that vary in size, shape, and weight. This dataset stands out by significantly extending the scope of existing ones for understanding how the physical attributes of different objects influence human posture, speed, motion scale, and interacting strategies. We further demonstrate the applicability of the PA-HOI dataset by integrating it with existing motion generation methods, validating its capacity to transfer realistic physical awareness.",
        "arxiv_id": "2508.06205",
        "ARXIVID": "2508.06205",
        "COMMENT": "Matches criteria 3 (new embodied AI benchmark/dataset focusing on physical properties in human-object interaction, a novel angle not covered by previous HOI datasets).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06434": {
        "authors": [
            "Shengzhu Yang",
            "Jiawei Du",
            "Shuai Lu",
            "Weihang Zhang",
            "Ningli Wang",
            "Huiqi Li"
        ],
        "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment",
        "abstract": "arXiv:2508.06434v1 Announce Type: new  Abstract: Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the model's ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at https://github.com/T6Yang/CLIPin.",
        "arxiv_id": "2508.06434",
        "ARXIVID": "2508.06434",
        "COMMENT": "Matches criterion 4: introduces CLIPin, a non-contrastive plug-in for CLIP-style vision foundation models to improve multimodal semantic alignment, with demonstrated effectiveness across tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06009": {
        "authors": [
            "Jun Feng",
            "Zixin Wang",
            "Zhentao Zhang",
            "Yue Guo",
            "Zhihan Zhou",
            "Xiuyi Chen",
            "Zhenyang Li",
            "Dawei Yin"
        ],
        "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models",
        "abstract": "arXiv:2508.06009v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal.",
        "arxiv_id": "2508.06009",
        "ARXIVID": "2508.06009",
        "COMMENT": "Matches criterion 3: introduces MathReal, a new real-world benchmark for evaluating mathematical reasoning in MLLMs, focusing on realistic, noisy educational images and analyzing MLLM performance and error patterns in this context.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05829": {
        "authors": [
            "Guoping Xu",
            "Hua-Chieh Shao",
            "You Zhang"
        ],
        "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios",
        "abstract": "arXiv:2508.05829v1 Announce Type: new  Abstract: Promptable video object segmentation and tracking (VOST) has seen significant advances with the emergence of foundation models like Segment Anything Model 2 (SAM2); however, their application in surgical video analysis remains challenging due to complex motion dynamics and the redundancy of memory that impedes effective learning. In this work, we propose TSMS-SAM2, a novel framework that enhances promptable VOST in surgical videos by addressing challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2 introduces two key strategies: multi-temporal-scale video sampling augmentation to improve robustness against motion variability, and a memory splitting and pruning mechanism that organizes and filters past frame features for more efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018 datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73, respectively, outperforming prior SAM-based and task-specific methods. Extensive ablation studies confirm the effectiveness of multiscale temporal augmentation and memory splitting, highlighting the framework's potential for robust, efficient segmentation in complex surgical scenarios. Our source code will be available at https://github.com/apple1986/TSMS-SAM2.",
        "arxiv_id": "2508.05829",
        "ARXIVID": "2508.05829",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it builds on SAM2 for video object segmentation and tracking, and introduces new methods for temporal augmentation and memory pruning. Also relevant for spatial understanding in embodied agents (criterion 1), especially in surgical scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06142": {
        "authors": [
            "Hanqing Wang",
            "Yuan Tian",
            "Mingyu Liu",
            "Zhenhao Zhang",
            "Xiangyang Zhu"
        ],
        "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models",
        "abstract": "arXiv:2508.06142v1 Announce Type: new  Abstract: In the rapidly evolving landscape of Multimodal Large Language Models (MLLMs), the safety concerns of their outputs have earned significant attention. Although numerous datasets have been proposed, they may become outdated with MLLM advancements and are susceptible to data contamination issues. To address these problems, we propose \\textbf{SDEval}, the \\textit{first} safety dynamic evaluation framework to controllably adjust the distribution and complexity of safety benchmarks. Specifically, SDEval mainly adopts three dynamic strategies: text, image, and text-image dynamics to generate new samples from original benchmarks. We first explore the individual effects of text and image dynamics on model safety. Then, we find that injecting text dynamics into images can further impact safety, and conversely, injecting image dynamics into text also leads to safety risks. SDEval is general enough to be applied to various existing safety and even capability benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and capability benchmarks, MMBench and MMVet, show that SDEval significantly influences safety evaluation, mitigates data contamination, and exposes safety limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval",
        "arxiv_id": "2508.06142",
        "ARXIVID": "2508.06142",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) as it introduces SDEval, a dynamic safety evaluation framework for MLLMs, and is directly relevant to vision-language models' evaluation. Also of interest for empirical insights into MLLM safety.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06453": {
        "authors": [
            "Ruida Cheng",
            "Tejas Sudharshan Mathai",
            "Pritam Mukherjee",
            "Benjamin Hou",
            "Qingqing Zhu",
            "Zhiyong Lu",
            "Matthew McAuliffe",
            "Ronald M. Summers"
        ],
        "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
        "abstract": "arXiv:2508.06453v1 Announce Type: new  Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba",
        "arxiv_id": "2508.06453",
        "ARXIVID": "2508.06453",
        "COMMENT": "Matches criteria 2 (multi-modal LLMs, integrating text and image for segmentation), and 4 (application of vision foundation models in medical imaging).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06494": {
        "authors": [
            "Yehonathan Litman",
            "Fernando De la Torre",
            "Shubham Tulsiani"
        ],
        "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
        "abstract": "arXiv:2508.06494v1 Announce Type: new  Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.",
        "arxiv_id": "2508.06494",
        "ARXIVID": "2508.06494",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications) by proposing a material-guided diffusion framework for multi-view relighting, a novel application of diffusion models in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06101": {
        "authors": [
            "Yachun Mi",
            "Xingyang He",
            "Shixin Sun",
            "Yu Li",
            "Yanting Li",
            "Zhixuan Li",
            "Jian Jin",
            "Chen Hui",
            "Shaohui Liu"
        ],
        "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization",
        "abstract": "arXiv:2508.06101v1 Announce Type: new  Abstract: In the digital age, advanced image editing tools pose a serious threat to the integrity of visual content, making image forgery detection and localization a key research focus. Most existing Image Manipulation Localization (IML) methods rely on discriminative learning and require large, high-quality annotated datasets. However, current datasets lack sufficient scale and diversity, limiting model performance in real-world scenarios. To overcome this, recent studies have explored Constrained IML (CIML), which generates pixel-level annotations through algorithmic supervision. However, existing CIML approaches often depend on complex multi-stage pipelines, making the annotation process inefficient. In this work, we propose a novel generative framework based on diffusion models, named UGD-IML, which for the first time unifies both IML and CIML tasks within a single framework. By learning the underlying data distribution, generative diffusion models inherently reduce the reliance on large-scale labeled datasets, allowing our approach to perform effectively even under limited data conditions. In addition, by leveraging a class embedding mechanism and a parameter-sharing design, our model seamlessly switches between IML and CIML modes without extra components or training overhead. Furthermore, the end-to-end design enables our model to avoid cumbersome steps in the data annotation process. Extensive experimental results on multiple datasets demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and 4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the proposed method also excels in uncertainty estimation, visualization and robustness.",
        "arxiv_id": "2508.06101",
        "ARXIVID": "2508.06101",
        "COMMENT": "Presents a unified generative diffusion-based framework for image manipulation localization, relevant to criterion 4 (vision foundation models and applications) and generative modeling. The method unifies constrained and unconstrained tasks and improves annotation efficiency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06139": {
        "authors": [
            "Shaohua Pan",
            "Xinyu Yi",
            "Yan Zhou",
            "Weihua Jian",
            "Yuan Zhang",
            "Pengfei Wan",
            "Feng Xu"
        ],
        "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera",
        "abstract": "arXiv:2508.06139v1 Announce Type: new  Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.",
        "arxiv_id": "2508.06139",
        "ARXIVID": "2508.06139",
        "COMMENT": "Presents a diffusion-based framework for real-time human motion capture using sparse IMUs and a monocular camera. This is relevant to spatial understanding (criterion 1) and generative modeling in multi-modal learning, with a clever fusion of modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06127": {
        "authors": [
            "Yi Qin",
            "Rui Wang",
            "Tao Huang",
            "Tong Xiao",
            "Liping Jing"
        ],
        "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures",
        "abstract": "arXiv:2508.06127v1 Announce Type: new  Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation with zero-shot abilities, its inherent vulnerabilities present a single-point risk, potentially leading to the failure of numerous downstream applications. Proactively evaluating these transferable vulnerabilities is thus imperative. Prior adversarial attacks on SAM often present limited transferability due to insufficient exploration of common weakness across domains. To address this, we propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that leverages only the encoder of SAM for generating transferable adversarial examples. Specifically, it achieves this by explicitly characterizing the shared vulnerable regions between SAM and downstream models through a parametric simplicial complex. Our goal is to identify such complexes within adversarially potent regions by iterative vertex-wise refinement. A lightweight domain re-adaptation strategy is introduced to bridge domain divergence using minimal reference data during the initialization of simplicial complex. Ultimately, VeSCA generates consistently transferable adversarial examples through random simplicial complex sampling. Extensive experiments demonstrate that VeSCA achieves performance improved by 12.7% compared to state-of-the-art methods across three downstream model categories across five domain-specific datasets. Our findings further highlight the downstream model risks posed by SAM's vulnerabilities and emphasize the urgency of developing more robust foundation models.",
        "arxiv_id": "2508.06127",
        "ARXIVID": "2508.06127",
        "COMMENT": "This paper analyzes vulnerabilities in the Segment Anything Model (SAM), a vision foundation model, and proposes a new adversarial attack method. It provides insights into risks for downstream applications. Matches criterion 4.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06248": {
        "authors": [
            "Andrii Yermakov",
            "Jan Cech",
            "Jiri Matas",
            "Mario Fritz"
        ],
        "title": "Deepfake Detection that Generalizes Across Benchmarks",
        "abstract": "arXiv:2508.06248v1 Announce Type: new  Abstract: The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of a pre-trained CLIP vision encoder. The proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and latent space augmentations.   We conducted an extensive evaluation on 13 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities.   This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained CLIP model. The code will be made publicly available upon acceptance.",
        "arxiv_id": "2508.06248",
        "ARXIVID": "2508.06248",
        "COMMENT": "This paper proposes a parameter-efficient adaptation of CLIP for deepfake detection, leveraging vision foundation models and providing empirical insights into generalization. Matches criterion 4.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.05755": {
        "authors": [
            "Agnieszka Polowczyk",
            "Alicja Polowczyk",
            "Dawid Malarz",
            "Artur Kasymov",
            "Marcin Mazur",
            "Jacek Tabor",
            "Przemys{\\l}aw Spurek"
        ],
        "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models",
        "abstract": "arXiv:2508.05755v1 Announce Type: new  Abstract: Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.",
        "arxiv_id": "2508.05755",
        "ARXIVID": "2508.05755",
        "COMMENT": "This paper introduces UnGuide, a new method for selective unlearning in diffusion models using LoRA and classifier-free guidance. It is relevant to criterion 4 (Vision foundation models related and its applications), as it proposes a novel application and control mechanism for diffusion-based vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06032": {
        "authors": [
            "Kiran Chhatre",
            "Christopher Peters",
            "Srikrishna Karanam"
        ],
        "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts",
        "abstract": "arXiv:2508.06032v1 Announce Type: new  Abstract: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model -- obtained by fine-tuning a T2I model on 3D human texture maps -- for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments -- separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks -- and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.",
        "arxiv_id": "2508.06032",
        "ARXIVID": "2508.06032",
        "COMMENT": "Matches criteria 4 (vision foundation models and applications) by leveraging diffusion models and image-to-texture generators for fine-grained human parsing. Also relevant to multi-modal learning and segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06203": {
        "authors": [
            "Zhaopeng Gu",
            "Bingke Zhu",
            "Guibo Zhu",
            "Yingying Chen",
            "Wei Ge",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection",
        "abstract": "arXiv:2508.06203v1 Announce Type: new  Abstract: Anomaly detection is a critical task across numerous domains and modalities, yet existing methods are often highly specialized, limiting their generalizability. These specialized models, tailored for specific anomaly types like textural defects or logical errors, typically exhibit limited performance when deployed outside their designated contexts. To overcome this limitation, we propose AnomalyMoE, a novel and universal anomaly detection framework based on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the complex anomaly detection problem into three distinct semantic hierarchies: local structural anomalies, component-level semantic anomalies, and global logical anomalies. AnomalyMoE correspondingly employs three dedicated expert networks at the patch, component, and global levels, and is specialized in reconstructing features and identifying deviations at its designated semantic level. This hierarchical design allows a single model to concurrently understand and detect a wide spectrum of anomalies. Furthermore, we introduce an Expert Information Repulsion (EIR) module to promote expert diversity and an Expert Selection Balancing (ESB) module to ensure the comprehensive utilization of all experts. Experiments on 8 challenging datasets spanning industrial imaging, 3D point clouds, medical imaging, video surveillance, and logical anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art performance, significantly outperforming specialized methods in their respective domains.",
        "arxiv_id": "2508.06203",
        "ARXIVID": "2508.06203",
        "COMMENT": "Somewhat related to criterion 4: proposes AnomalyMoE, a generalist visual anomaly detection model using a Mixture-of-Experts architecture, but not directly about vision foundation models or their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.06224": {
        "authors": [
            "Guoyu Zhou",
            "Jing Zhang",
            "Yi Yan",
            "Hui Zhang",
            "Li Zhuo"
        ],
        "title": "TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images",
        "abstract": "arXiv:2508.06224v1 Announce Type: new  Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for applications such as urban planning and environmental monitoring. However, geospatial objects often exhibit subtle texture differences and similar spatial structures, which can easily lead to semantic ambiguity and misclassification. Moreover, challenges such as irregular object shapes, blurred boundaries, and overlapping spatial distributions of semantic objects contribute to complex and diverse edge morphologies, further complicating accurate segmentation. To tackle these issues, we propose a texture-aware and edge-guided Transformer (TEFormer) that integrates texture awareness and edge-guidance mechanisms for semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is designed to capture fine-grained texture differences between visually similar categories to enhance semantic discrimination. Then, an edge-guided tri-branch decoder (Eg3Head) is constructed to preserve local edges and details for multiscale context-awareness. Finally, an edge-guided feature fusion module (EgFFM) is to fuse contextual and detail information with edge information to realize refined semantic segmentation. Extensive experiments show that TEFormer achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and LoveDA datasets, respectively, shows the effectiveness in URSI semantic segmentation.",
        "arxiv_id": "2508.06224",
        "ARXIVID": "2508.06224",
        "COMMENT": "This paper proposes TEFormer, a texture-aware and edge-guided transformer for semantic segmentation of urban remote sensing images. It is relevant to vision foundation models (criterion 4) due to the transformer-based architecture and its application to geospatial data, but does not address embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05922": {
        "authors": [
            "Sri Ramana Saketh Vasanthawada",
            "Pengkun Liu",
            "Pingbo Tang"
        ],
        "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation",
        "abstract": "arXiv:2508.05922v1 Announce Type: new  Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting the exploration of computer-vision-based methodologies for enhanced efficiency and scalability. Traditional data acquisition methods, primarily focusing on indoor environments, falter in construction site's complex, cluttered, and dynamically changing conditions. This paper critically evaluates the application of two advanced 3D segmentation methods, Segment Anything Model (SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained initially on indoor datasets, both models' adaptability and performance are assessed in real-world construction settings, highlighting the gap in current segmentation approaches due to the absence of benchmarks for outdoor scenarios. Through a comparative analysis, this study not only showcases the relative effectiveness of SAM and Mask3D but also addresses the critical need for tailored segmentation workflows capable of extracting actionable insights from construction site data, thereby advancing the field towards more automated and precise monitoring techniques.",
        "arxiv_id": "2508.05922",
        "ARXIVID": "2508.05922",
        "COMMENT": "This paper evaluates 3D segmentation models (SAM and Mask3D) on construction site data and highlights the lack of outdoor benchmarks. It partially matches criterion 3 (embodied AI benchmarks, simulator-related, or new methods), as it addresses a gap in segmentation benchmarks for real-world, complex environments, which is a novel angle. Also relevant to vision foundation models (criterion 4) due to use of SAM.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05819": {
        "authors": [
            "Jong-Ik Park",
            "Carlee Joe-Wong",
            "Gary K. Fedder"
        ],
        "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses",
        "abstract": "arXiv:2508.05819v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from multiple 2D images, even those taken with unknown camera poses. However, they still miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution is fixed and compute budgets are tight, so the only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF (MZEN), the first NeRF framework that natively handles multi-zoom image sets. MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement. Across eight forward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of micro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$. MZEN, therefore, extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection.",
        "arxiv_id": "2508.05819",
        "ARXIVID": "2508.05819",
        "COMMENT": "This paper introduces a new NeRF framework for 3D reconstruction with unknown camera poses, handling multi-zoom images. While it is relevant to spatial understanding, it does not focus on embodied agents or benchmarks, nor VLLMs/MLLMs or vision foundation models. Closest to criterion 1, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2508.05772": {
        "authors": [
            "Can Zhao",
            "Pengfei Guo",
            "Dong Yang",
            "Yucheng Tang",
            "Yufan He",
            "Benjamin Simon",
            "Mason Belue",
            "Stephanie Harmon",
            "Baris Turkbey",
            "Daguang Xu"
        ],
        "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss",
        "abstract": "arXiv:2508.05772v1 Announce Type: new  Abstract: Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \\times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community.",
        "arxiv_id": "2508.05772",
        "ARXIVID": "2508.05772",
        "COMMENT": "This paper introduces MAISI-v2, an accelerated 3D medical image synthesis framework using rectified flow and a novel region-specific contrastive loss. While it is a generative model in vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or new benchmarks. It is relevant to vision foundation models and generative modeling.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.06082": {
        "authors": [
            "Yanxiao Sun",
            "Jiafu Wu",
            "Yun Cao",
            "Chengming Xu",
            "Yabiao Wang",
            "Weijian Cao",
            "Donghao Luo",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment",
        "abstract": "arXiv:2508.06082v1 Announce Type: new  Abstract: Diffusion-based or flow-based models have achieved significant progress in video synthesis but require multiple iterative sampling steps, which incurs substantial computational overhead. While many distillation methods that are solely based on trajectory-preserving or distribution-matching have been developed to accelerate video generation models, these approaches often suffer from performance breakdown or increased artifacts under few-step settings. To address these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and stable distillation framework that combines the advantages of trajectory-preserving and distribution-matching strategies. Our approach introduces continuous-time consistency distillation to ensure precise preservation of ODE trajectories. Subsequently, we propose a dual-perspective alignment that includes distribution alignment between synthetic and real data along with trajectory alignment across different inference steps. Our method maintains high-quality video generation while substantially reducing the number of inference steps. Quantitative evaluations on the OpenVid-1M benchmark demonstrate that our method significantly outperforms existing approaches in few-step video generation.",
        "arxiv_id": "2508.06082",
        "ARXIVID": "2508.06082",
        "COMMENT": "Relevant to generative modeling in vision (video generation), but does not directly match any of the four criteria. Closest to criterion 4 (vision foundation models), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.06485": {
        "authors": [
            "Sofiane Bouaziz",
            "Adel Hafiane",
            "Raphael Canals",
            "Rachid Nedjai"
        ],
        "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion",
        "abstract": "arXiv:2508.06485v1 Announce Type: new  Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.",
        "arxiv_id": "2508.06485",
        "ARXIVID": "2508.06485",
        "COMMENT": "Somewhat related to criterion 1: proposes a generative network for spatio-temporal fusion in remote sensing, with weak supervision and spatial reasoning, but not focused on embodied agents or spatial intelligence in agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.06157": {
        "authors": [
            "Xiaoxiao Yang",
            "Meiliang Liu",
            "Yunfang Xu",
            "Zijin Li",
            "Zhengye Si",
            "Xinyue Yang",
            "Zhiwen Zhao"
        ],
        "title": "An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis",
        "abstract": "arXiv:2508.06157v1 Announce Type: new  Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that severely impairs cognitive function and quality of life. Timely intervention in AD relies heavily on early and precise diagnosis, which remains challenging due to the complex and subtle structural changes in the brain. Most existing deep learning methods focus only on a single plane of structural magnetic resonance imaging (sMRI) and struggle to accurately capture the complex and nonlinear relationships among pathological regions of the brain, thus limiting their ability to precisely identify atrophic features. To overcome these limitations, we propose an innovative framework, MPF-KANSC, which integrates multi-plane fusion (MPF) for combining features from the coronal, sagittal, and axial planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention mechanism (KANSC) to more effectively learn and represent sMRI atrophy features. Specifically, the proposed model enables parallel feature extraction from multiple anatomical planes, thus capturing more comprehensive structural information. The KANSC attention mechanism further leverages a more flexible and accurate nonlinear function approximation technique, facilitating precise identification and localization of disease-related abnormalities. Experiments on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior performance in AD diagnosis. Moreover, our findings provide new evidence of right-lateralized asymmetry in subcortical structural changes during AD progression, highlighting the model's promising interpretability.",
        "arxiv_id": "2508.06157",
        "ARXIVID": "2508.06157",
        "COMMENT": "Somewhat relevant to vision foundation models (criterion 4) via novel attention mechanisms for medical imaging, but not a direct match to any main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.06227": {
        "authors": [
            "Md Sazidur Rahman",
            "David Cabecinhas",
            "Ricard Marxer"
        ],
        "title": "Depth Jitter: Seeing through the Depth",
        "abstract": "arXiv:2508.06227v1 Announce Type: new  Abstract: Depth information is essential in computer vision, particularly in underwater imaging, robotics, and autonomous navigation. However, conventional augmentation techniques overlook depth aware transformations, limiting model robustness in real world depth variations. In this paper, we introduce Depth-Jitter, a novel depth-based augmentation technique that simulates natural depth variations to improve generalization. Our approach applies adaptive depth offsetting, guided by depth variance thresholds, to generate synthetic depth perturbations while preserving structural integrity. We evaluate Depth-Jitter on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on model stability under diverse depth conditions. Extensive experiments compare Depth-Jitter against traditional augmentation strategies such as ColorJitter, analyzing performance across varying learning rates, encoders, and loss functions. While Depth-Jitter does not always outperform conventional methods in absolute performance, it consistently enhances model stability and generalization in depth-sensitive environments. These findings highlight the potential of depth-aware augmentation for real-world applications and provide a foundation for further research into depth-based learning strategies. The proposed technique is publicly available to support advancements in depth-aware augmentation. The code is publicly available on \\href{https://github.com/mim-team/Depth-Jitter}{github}.",
        "arxiv_id": "2508.06227",
        "ARXIVID": "2508.06227",
        "COMMENT": "Somewhat relevant to criteria 4 (vision foundation models and applications) as it proposes a new augmentation technique for depth-aware learning, which could be used in vision models, but does not directly address the main criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.06160": {
        "authors": [
            "Zhenbang Du (Celine)",
            "Yonggan Fu (Celine)",
            "Lifu Wang (Celine)",
            "Jiayi Qian (Celine)",
            "Xiao Luo (Celine)",
            "Yingyan (Celine)",
            "Lin"
        ],
        "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment",
        "abstract": "arXiv:2508.06160v1 Announce Type: new  Abstract: Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff.",
        "arxiv_id": "2508.06160",
        "ARXIVID": "2508.06160",
        "COMMENT": "Related to general interest in generative modeling, but does not directly match any specific criterion. Focuses on compute-optimal deployment of diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06036": {
        "authors": [
            "Jun Xie",
            "Yingjian Zhu",
            "Feng Chen",
            "Zhenghao Zhang",
            "Xiaohui Fan",
            "Hongzhu Yi",
            "Xinming Wang",
            "Chen Yu",
            "Yue Bi",
            "Zhaoran Zhao",
            "Xiongjun Guan",
            "Zhepeng Wang"
        ],
        "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment",
        "abstract": "arXiv:2508.06036v1 Announce Type: new  Abstract: In this paper, we present our solution for the semi-supervised learning track (MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the principle that \"more is better,\" to construct a robust Mixture of Experts (MoE) emotion recognition system. Our approach integrates a diverse range of input modalities as independent experts, including novel signals such as knowledge from large Vision-Language Models (VLMs) and temporal Action Unit (AU) information. To effectively utilize unlabeled data, we introduce a consensus-based pseudo-labeling strategy, generating high-quality labels from the agreement between a baseline model and Gemini, which are then used in a two-stage training paradigm. Finally, we employ a multi-expert voting ensemble combined with a rule-based re-ranking process to correct prediction bias and better align the outputs with human preferences. Evaluated on the MER2025-SEMI challenge dataset, our method achieves an F1-score of 0.8772 on the test set, ranking 2nd in the track. Our code is available at https://github.com/zhuyjan/MER2025-MRAC25.",
        "arxiv_id": "2508.06036",
        "ARXIVID": "2508.06036",
        "COMMENT": "Somewhat related to criterion 2: uses knowledge from large Vision-Language Models (VLMs) as part of a Mixture of Experts for emotion recognition, but the main focus is on emotion recognition rather than VLLM/MLLM development.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.06122": {
        "authors": [
            "Ting-Shuo Yo",
            "Shih-Hao Su",
            "Chien-Ming Wu",
            "Wei-Ting Chen",
            "Jung-Lien Chu",
            "Chiao-Wei Chang",
            "Hung-Chi Kuo"
        ],
        "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events",
        "abstract": "arXiv:2508.06122v1 Announce Type: new  Abstract: This study applied representation learning algorithms to satellite images and evaluated the learned latent spaces with classifications of various weather events. The algorithms investigated include the classical linear transformation, i.e., principal component analysis (PCA), state-of-the-art deep learning method, i.e., convolutional autoencoder (CAE), and a residual network pre-trained with large image datasets (PT). The experiment results indicated that the latent space learned by CAE consistently showed higher threat scores for all classification tasks. The classifications with PCA yielded high hit rates but also high false-alarm rates. In addition, the PT performed exceptionally well at recognizing tropical cyclones but was inferior in other tasks. Further experiments suggested that representations learned from higher-resolution datasets are superior in all classification tasks for deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent space sizes had minor impact on the classification task's hit rate. Still, a latent space dimension smaller than 128 caused a significantly higher false alarm rate. Though the CAE can learn latent spaces effectively and efficiently, the interpretation of the learned representation lacks direct connections to physical attributions. Therefore, developing a physics-informed version of CAE can be a promising outlook for the current work.",
        "arxiv_id": "2508.06122",
        "ARXIVID": "2508.06122",
        "COMMENT": "Applies representation learning to satellite images for weather event classification. While it uses deep learning and discusses representation learning, it does not introduce new methods for spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.05991": {
        "authors": [
            "Juewen Hu",
            "Yexin Li",
            "Jiulin Li",
            "Shuo Chen",
            "Pring Wong"
        ],
        "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge",
        "abstract": "arXiv:2508.05991v1 Announce Type: new  Abstract: Emotion recognition plays a vital role in enhancing human-computer interaction. In this study, we tackle the MER-SEMI challenge of the MER2025 competition by proposing a novel multimodal emotion recognition framework. To address the issue of data scarcity, we leverage large-scale pre-trained models to extract informative features from visual, audio, and textual modalities. Specifically, for the visual modality, we design a dual-branch visual encoder that captures both global frame-level features and localized facial representations. For the textual modality, we introduce a context-enriched method that employs large language models to enrich emotional cues within the input text. To effectively integrate these multimodal features, we propose a fusion strategy comprising two key components, i.e., self-attention mechanisms for dynamic modality weighting, and residual connections to preserve original representations. Beyond architectural design, we further refine noisy labels in the training set by a multi-source labeling strategy. Our approach achieves a substantial performance improvement over the official baseline on the MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to 78.63%, thereby validating the effectiveness of the proposed framework.",
        "arxiv_id": "2508.05991",
        "ARXIVID": "2508.05991",
        "COMMENT": "Presents a new multimodal emotion recognition framework using large pre-trained models and fusion strategies. While it uses MLLMs, it is focused on emotion recognition and does not introduce a new VLLM/MLLM or vision foundation model, nor does it address spatial intelligence or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.05851": {
        "authors": [
            "Ka-Wai Yung",
            "Felix J. S. Bragman",
            "Jialang Xu",
            "Imanol Luengo",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos"
        ],
        "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation",
        "abstract": "arXiv:2508.05851v1 Announce Type: new  Abstract: Vision Transformers have substantially advanced the capabilities of segmentation models across both image and video domains. Among them, the Swin Transformer stands out for its ability to capture hierarchical, multi-scale representations, making it a popular backbone for segmentation in videos. However, despite its window-attention scheme, it still incurs a high computational cost, especially in larger variants commonly used for dense prediction in videos. This remains a major bottleneck for real-time, resource-constrained applications. Whilst token reduction methods have been proposed to alleviate this, the window-based attention mechanism of Swin requires a fixed number of tokens per window, limiting the applicability of conventional pruning techniques. Meanwhile, training-free token clustering approaches have shown promise in image segmentation while maintaining window consistency. Nevertheless, they fail to exploit temporal redundancy, missing a key opportunity to further optimize video segmentation performance. We introduce Temporal Cluster Assignment (TCA), a lightweight and effective, fine-tuning-free strategy that enhances token clustering by leveraging temporal coherence across frames. Instead of indiscriminately dropping redundant tokens, TCA refines token clusters using temporal correlations, thereby retaining fine-grained details while significantly reducing computation. Extensive evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical video dataset show that TCA consistently boosts the accuracy-speed trade-off of existing clustering-based methods. Our results demonstrate that TCA generalizes competently across both natural and domain-specific videos.",
        "arxiv_id": "2508.05851",
        "ARXIVID": "2508.05851",
        "COMMENT": "This paper introduces a new method for efficient real-time video segmentation by leveraging temporal coherence in token clustering. While it is an interesting vision paper, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05732": {
        "authors": [
            "Pinxuan Li",
            "Bing Cao",
            "Changqing Zhang",
            "Qinghua Hu"
        ],
        "title": "Generalized Few-Shot Out-of-Distribution Detection",
        "abstract": "arXiv:2508.05732v1 Announce Type: new  Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical research direction in machine learning for practical deployment. Most existing Few-shot OOD detection methods suffer from insufficient generalization capability for the open world. Due to the few-shot learning paradigm, the OOD detection ability is often overfit to the limited training data itself, thus degrading the performance on generalized data and performing inconsistently across different scenarios. To address this challenge, we proposed a Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general knowledge of the OOD detection model with an auxiliary General Knowledge Model (GKM), instead of directly learning from few-shot data. We proceed to reveal the few-shot OOD detection from a generalization perspective and theoretically derive the Generality-Specificity balance (GS-balance) for OOD detection, which provably reduces the upper bound of generalization error with a general knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE) mechanism to adaptively modulate the guidance of general knowledge. KDE dynamically aligns the output distributions of the OOD detection model to the general knowledge model based on the Generalized Belief (G-Belief) of GKM, thereby boosting the GS-balance. Experiments on real-world OOD benchmarks demonstrate our superiority. Codes will be available.",
        "arxiv_id": "2508.05732",
        "ARXIVID": "2508.05732",
        "COMMENT": "This paper proposes a generalized few-shot out-of-distribution detection framework with a knowledge dynamic embedding mechanism. While it is relevant to machine learning and generalization, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06429": {
        "authors": [
            "Guido Manni",
            "Clemente Lauretti",
            "Loredana Zollo",
            "Paolo Soda"
        ],
        "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation",
        "abstract": "arXiv:2508.06429v1 Announce Type: new  Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.",
        "arxiv_id": "2508.06429",
        "ARXIVID": "2508.06429",
        "COMMENT": "This paper introduces a GAN-based semi-supervised learning framework for few-shot medical image classification, using class-conditioned image translation and ensemble-based pseudo-labeling. It is relevant to generative modeling and vision, but does not directly address the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06218": {
        "authors": [
            "Zhiyan Bo",
            "Laura C. Coates",
            "Bartlomiej W. Papiez"
        ],
        "title": "Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning",
        "abstract": "arXiv:2508.06218v1 Announce Type: new  Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials to quantify radiographic damage in Rheumatoid Arthritis (RA), but its complexity has limited its adoption in routine clinical practice. To address the inefficiency of manual scoring, this work proposes a two-stage pipeline for interpretable image-level SvdH score prediction using dual-hand radiographs. Our approach extracts disease-relevant image regions and integrates them using attention-based multiple instance learning to generate image-level features for prediction. We propose two region extraction schemes: 1) sampling image tiles most likely to contain abnormalities, and 2) cropping patches containing disease-relevant joints. With Scheme 2, our best individual score prediction model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root mean squared error (RMSE) of 15.73. Ensemble learning further boosted prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving state-of-the-art performance that is comparable to that of experienced radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively identified and made decisions based on anatomical structures which clinicians consider relevant to RA progression.",
        "arxiv_id": "2508.06218",
        "ARXIVID": "2508.06218",
        "COMMENT": "This paper presents an interpretable, anatomy-aware multiple instance learning approach for rheumatoid arthritis scoring from radiographs. It is relevant to vision and medical imaging, but does not match the specific criteria of spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05990": {
        "authors": [
            "Haichao Wang",
            "Xinyue Xi",
            "Jiangtao Wen",
            "Yuxing Han"
        ],
        "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision",
        "abstract": "arXiv:2508.05990v1 Announce Type: new  Abstract: The efficiency of video computer vision system remains a challenging task due to the high temporal redundancy inside a video. Existing works have been proposed for efficient vision computer vision. However, they do not fully reduce the temporal redundancy and neglect the front end computation overhead. In this paper, we propose an efficient video computer vision system. First, image signal processor is removed and Bayer-format data is directly fed into video computer vision models, thus saving the front end computation. Second, instead of optical flow models and video codecs, a fast block matching-based motion estimation algorithm is proposed specifically for efficient video computer vision, with a MV refinement module. To correct the error, context-aware block refinement network is introduced to refine regions with large error. To further balance the accuracy and efficiency, a frame selection strategy is employed. Experiments on multiple video computer vision tasks demonstrate that our method achieves significant acceleration with slight performance loss.",
        "arxiv_id": "2508.05990",
        "ARXIVID": "2508.05990",
        "COMMENT": "This paper proposes an efficient video computer vision system that processes Bayer-format data directly and introduces a new motion estimation and refinement method. While it is relevant to computer vision and efficiency, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, new embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06058": {
        "authors": [
            "Shiyang Zhou",
            "Haijin Zeng",
            "Yunfan Lu",
            "Yongyong Chen",
            "Jie Liu",
            "Jingyong Su"
        ],
        "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention",
        "abstract": "arXiv:2508.06058v1 Announce Type: new  Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera capture brightness changes as asynchronous \"events\" instead of frames, offering advanced application on mobile photography. However, challenges arise from combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information, resulting in aliasing and artifacts on the demosaicing process before downstream application. Current methods struggle to address these issues, especially on resource-limited mobile devices. In response, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage network via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can handle event pixels inpainting and demosaicing separately, leveraging the benefits of dividing complex tasks into manageable subtasks. Furthermore, we introduce a lightweight Cross-Swin State Block that uniquely utilizes positional prior for demosaicing and enhances global dependencies through the state space model with linear complexity. In summary, TSANet demonstrates excellent demosaicing performance on both simulated and real data of HybridEVS while maintaining a lightweight model, averaging better results than the previous state-of-the-art method DemosaicFormer across seven diverse datasets in both PSNR and SSIM, while respectively reducing parameter and computation costs by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities for efficient image demosaicing on mobile devices. Code is available in the supplementary materials.",
        "arxiv_id": "2508.06058",
        "ARXIVID": "2508.06058",
        "COMMENT": "This paper introduces a lightweight demosaicing method for event-based vision sensors, which is relevant to computer vision but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05689": {
        "authors": [
            "Jinjia Peng",
            "Zeze Tao",
            "Huibing Wang",
            "Meng Wang",
            "Yang Wang"
        ],
        "title": "Boosting Adversarial Transferability via Residual Perturbation Attack",
        "abstract": "arXiv:2508.05689v1 Announce Type: new  Abstract: Deep neural networks are susceptible to adversarial examples while suffering from incorrect predictions via imperceptible perturbations. Transfer-based attacks create adversarial examples for surrogate models and transfer these examples to target models under black-box scenarios. Recent studies reveal that adversarial examples in flat loss landscapes exhibit superior transferability to alleviate overfitting on surrogate models. However, the prior arts overlook the influence of perturbation directions, resulting in limited transferability. In this paper, we propose a novel attack method, named Residual Perturbation Attack (ResPA), relying on the residual gradient as the perturbation direction to guide the adversarial examples toward the flat regions of the loss function. Specifically, ResPA conducts an exponential moving average on the input gradients to obtain the first moment as the reference gradient, which encompasses the direction of historical gradients. Instead of heavily relying on the local flatness that stems from the current gradients as the perturbation direction, ResPA further considers the residual between the current gradient and the reference gradient to capture the changes in the global perturbation direction. The experimental results demonstrate the better transferability of ResPA than the existing typical transfer-based attack methods, while the transferability can be further improved by combining ResPA with the current input transformation methods. The code is available at https://github.com/ZezeTao/ResPA.",
        "arxiv_id": "2508.05689",
        "ARXIVID": "2508.05689",
        "COMMENT": "This paper proposes a new adversarial attack method for transferability, but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is not a direct match to any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06263": {
        "authors": [
            "Andrew Cropper",
            "David M. Cerna",
            "Matti J\\\"arvisalo"
        ],
        "title": "Symmetry breaking for inductive logic programming",
        "abstract": "arXiv:2508.06263v1 Announce Type: new  Abstract: The goal of inductive logic programming is to search for a hypothesis that generalises training data and background knowledge. The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist. To address this challenge, we introduce a method to break symmetries in the hypothesis space. We implement our idea in answer set programming. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce solving times from over an hour to just 17 seconds.",
        "arxiv_id": "2508.06263",
        "ARXIVID": "2508.06263",
        "COMMENT": "This paper proposes a symmetry breaking method for inductive logic programming, with experiments including visual reasoning. However, it does not directly address spatial understanding in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about logic programming efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.05776": {
        "authors": [
            "Thomas L. Griffiths",
            "Brenden M. Lake",
            "R. Thomas McCoy",
            "Ellie Pavlick",
            "Taylor W. Webb"
        ],
        "title": "Whither symbols in the era of advanced neural networks?",
        "abstract": "arXiv:2508.05776v1 Announce Type: new  Abstract: Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.",
        "arxiv_id": "2508.05776",
        "ARXIVID": "2508.05776",
        "COMMENT": "This paper discusses the symbolic basis of human thought in the context of neural networks, but does not present new methods for spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more of a theoretical perspective.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.06420": {
        "authors": [
            "Ch Muhammad Awais",
            "Marco Reggiannini",
            "Davide Moroni",
            "Oktay Karakus"
        ],
        "title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification",
        "abstract": "arXiv:2508.06420v1 Announce Type: new  Abstract: SAR ship classification faces the challenge of long-tailed datasets, which complicates the classification of underrepresented classes. Oversampling methods have proven effective in addressing class imbalance in optical data. In this paper, we evaluated the effect of oversampling in the feature space for SAR ship classification. We propose two novel algorithms inspired by the Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50. Additionally, we also analyzed the impact of oversampling methods on different class sizes. The results demonstrated the effectiveness of our novel methods over the original M2m and baselines, with an average F1-score increase of 8.82% for FuSARShip and 4.44% for OpenSARShip.",
        "arxiv_id": "2508.06420",
        "ARXIVID": "2508.06420",
        "COMMENT": "This paper proposes new oversampling algorithms for class imbalance in SAR ship classification. It is relevant to computer vision but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}