{
    "2508.08821": {
        "authors": [
            "Noor Ahmed",
            "Cameron Braunstein",
            "Steffen Eger",
            "Eddy Ilg"
        ],
        "title": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs",
        "abstract": "arXiv:2508.08821v1 Announce Type: new  Abstract: Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong capabilities in learning joint representations from text and images. However, their spatial reasoning remains limited. We introduce 3DFroMLLM, a novel framework that enables the generation of 3D object prototypes directly from MLLMs, including geometry and part labels. Our pipeline is agentic, comprising a designer, coder, and visual inspector operating in a refinement loop. Notably, our approach requires no additional training data or detailed user instructions. Building on prior work in 2D generation, we demonstrate that rendered images produced by our framework can be effectively used for image classification pretraining tasks and outperforms previous methods by 15%. As a compelling real-world use case, we show that the generated prototypes can be leveraged to improve fine-grained vision-language models by using the rendered, part-labeled prototypes to fine-tune CLIP for part segmentation and achieving a 55% accuracy improvement without relying on any additional human-labeled data.",
        "arxiv_id": "2508.08821",
        "ARXIVID": "2508.08821",
        "COMMENT": "Strong match to criterion 1 (spatial understanding on embodied agents) and criterion 2 (new MLLMs). 3DFroMLLM enables 3D object prototype generation from MLLMs, focusing on spatial reasoning and agentic pipelines. Also relevant to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.09123": {
        "authors": [
            "Xinyuan Wang",
            "Bowen Wang",
            "Dunjie Lu",
            "Junlin Yang",
            "Tianbao Xie",
            "Junli Wang",
            "Jiaqi Deng",
            "Xiaole Guo",
            "Yiheng Xu",
            "Chen Henry Wu",
            "Zhennan Shen",
            "Zhuokai Li",
            "Ryan Li",
            "Xiaochuan Li",
            "Junda Chen",
            "Boyuan Zheng",
            "Peihang Li",
            "Fangyu Lei",
            "Ruisheng Cao",
            "Yeqiao Fu",
            "Dongchan Shin",
            "Martin Shin",
            "Jiarui Hu",
            "Yuyan Wang",
            "Jixuan Chen",
            "Yuxiao Ye",
            "Danyang Zhang",
            "Dikang Du",
            "Hao Hu",
            "Huarong Chen",
            "Zaida Zhou",
            "Yipu Wang",
            "Heng Wang",
            "Diyi Yang",
            "Victor Zhong",
            "Flood Sung",
            "Y. Charles",
            "Zhilin Yang",
            "Tao Yu"
        ],
        "title": "OpenCUA: Open Foundations for Computer-Use Agents",
        "abstract": "arXiv:2508.09123v1 Announce Type: new  Abstract: Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.",
        "arxiv_id": "2508.09123",
        "ARXIVID": "2508.09123",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). OpenCUA is an open-source framework for computer-use agents, including a large-scale dataset and foundation models for vision-language agents. It also provides new benchmarks and methods for embodied agents (criterion 3).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.08501": {
        "authors": [
            "Yuchen Li",
            "Cong Lin",
            "Muhammad Umair Nasir",
            "Philip Bontrager",
            "Jialin Liu",
            "Julian Togelius"
        ],
        "title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games",
        "abstract": "arXiv:2508.08501v1 Announce Type: new  Abstract: We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). Built on the General Video Game AI framework, it features a diverse collection of arcade-style games designed to test a model's ability to handle tasks that differ from most existing LLM benchmarks. The benchmark leverages a game description language that enables rapid creation of new games and levels, helping to prevent overfitting over time. Each game scene is represented by a compact set of ASCII characters, allowing for efficient processing by language models. GVGAI-LLM defines interpretable metrics, including the meaningful step ratio, step efficiency, and overall score, to assess model behavior. Through zero-shot evaluations across a broad set of games and levels with diverse challenges and skill depth, we reveal persistent limitations of LLMs in spatial reasoning and basic planning. Current models consistently exhibit spatial and logical errors, motivating structured prompting and spatial grounding techniques. While these interventions lead to partial improvements, the benchmark remains very far from solved. GVGAI-LLM provides a reproducible testbed for advancing research on language model capabilities, with a particular emphasis on agentic behavior and contextual reasoning.",
        "arxiv_id": "2508.08501",
        "ARXIVID": "2508.08501",
        "COMMENT": "Directly matches criterion 3: introduces GVGAI-LLM, a new benchmark for evaluating LLM agents in video games, focusing on spatial reasoning and agentic behavior. The benchmark is simulator-based and reveals new empirical limitations of LLMs in spatial reasoning, which is highly relevant to spatial intelligence and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.08812": {
        "authors": [
            "Yuqi Peng",
            "Lingtao Zheng",
            "Yufeng Yang",
            "Yi Huang",
            "Mingfu Yan",
            "Jianzhuang Liu",
            "Shifeng Chen"
        ],
        "title": "TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models",
        "abstract": "arXiv:2508.08812v1 Announce Type: new  Abstract: Personalized text-to-image generation aims to synthesize novel images of a specific subject or style using only a few reference images. Recent methods based on Low-Rank Adaptation (LoRA) enable efficient single-concept customization by injecting lightweight, concept-specific adapters into pre-trained diffusion models. However, combining multiple LoRA modules for multi-concept generation often leads to identity missing and visual feature leakage. In this work, we identify two key issues behind these failures: (1) token-wise interference among different LoRA modules, and (2) spatial misalignment between the attention map of a rare token and its corresponding concept-specific region. To address these issues, we propose Token-Aware LoRA (TARA), which introduces a token mask to explicitly constrain each module to focus on its associated rare token to avoid interference, and a training objective that encourages the spatial attention of a rare token to align with its concept region. Our method enables training-free multi-concept composition by directly injecting multiple independently trained TARA modules at inference time. Experimental results demonstrate that TARA enables efficient multi-concept inference and effectively preserving the visual identity of each concept by avoiding mutual interference between LoRA modules. The code and models are available at https://github.com/YuqiPeng77/TARA.",
        "arxiv_id": "2508.08812",
        "ARXIVID": "2508.08812",
        "COMMENT": "This paper proposes TARA, a token-aware LoRA method for composable personalization in diffusion models, enabling multi-concept text-to-image generation. This is highly relevant to criterion 4 (vision foundation models and applications) and also touches on generative modeling in multi-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.08498": {
        "authors": [
            "Aneel Damaraju",
            "Dean Hazineh",
            "Todd Zickler"
        ],
        "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting",
        "abstract": "arXiv:2508.08498v1 Announce Type: new  Abstract: Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of \"object layers,\" each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in.",
        "arxiv_id": "2508.08498",
        "ARXIVID": "2508.08498",
        "COMMENT": "Matches criterion 1: The paper introduces a new diffusion-based architecture (CObL) for inferring occlusion-ordered object layers from images, enabling zero-shot spatial understanding and amodal object completion. This is a methodological improvement to spatial understanding and intelligence, especially relevant for embodied agents and object-centric scene understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.09105": {
        "authors": [
            "Shixuan Sun",
            "Siyuan Liang",
            "Ruoyu Chen",
            "Jianjie Huang",
            "Jingzhi Li",
            "Xiaochun Cao"
        ],
        "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling",
        "abstract": "arXiv:2508.09105v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented Generation (MRAG) significantly improve the knowledge coverage and contextual understanding of Large Language Models (LLMs) by introducing external knowledge sources. However, retrieval and multimodal fusion obscure content provenance, rendering existing membership inference methods unable to reliably attribute generated outputs to pre-training, external retrieval, or user input, thus undermining privacy leakage accountability   To address these challenges, we propose the first Source-aware Membership Audit (SMA) that enables fine-grained source attribution of generated content in a semi-black-box setting with retrieval control capabilities.To address the environmental constraints of semi-black-box auditing, we further design an attribution estimation mechanism based on zero-order optimization, which robustly approximates the true influence of input tokens on the output through large-scale perturbation sampling and ridge regression modeling. In addition, SMA introduces a cross-modal attribution technique that projects image inputs into textual descriptions via MLLMs, enabling token-level attribution in the text modality, which for the first time facilitates membership inference on image retrieval traces in MRAG systems. This work shifts the focus of membership inference from 'whether the data has been memorized' to 'where the content is sourced from', offering a novel perspective for auditing data provenance in complex generative systems.",
        "arxiv_id": "2508.09105",
        "ARXIVID": "2508.09105",
        "COMMENT": "Presents a novel auditing method (SMA) for membership leakage in retrieval-augmented generation, including cross-modal attribution for MLLMs. This directly matches criterion 2 (MLLMs) and is relevant to vision-language model provenance and privacy, with a new empirical angle.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.08991": {
        "authors": [
            "Zan Wang",
            "Jingze Zhang",
            "Yixin Chen",
            "Baoxiong Jia",
            "Wei Liang",
            "Siyuan Huang"
        ],
        "title": "Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation",
        "abstract": "arXiv:2508.08991v1 Announce Type: new  Abstract: Despite significant advancements in human motion generation, current motion representations, typically formulated as discrete frame sequences, still face two critical limitations: (i) they fail to capture motion from a multi-scale perspective, limiting the capability in complex patterns modeling; (ii) they lack compositional flexibility, which is crucial for model's generalization in diverse generation tasks. To address these challenges, we introduce MSQ, a novel quantization method that compresses the motion sequence into multi-scale discrete tokens across spatial and temporal dimensions. MSQ employs distinct encoders to capture body parts at varying spatial granularities and temporally interpolates the encoded features into multiple scales before quantizing them into discrete tokens. Building on this representation, we establish a generative mask modeling model to effectively support motion editing, motion control, and conditional motion generation. Through quantitative and qualitative analysis, we show that our quantization method enables the seamless composition of motion tokens without requiring specialized design or re-training. Furthermore, extensive evaluations demonstrate that our approach outperforms existing baseline methods on various benchmarks.",
        "arxiv_id": "2508.08991",
        "ARXIVID": "2508.08991",
        "COMMENT": "Matches criterion 1: introduces a new multi-scale quantization method for spatial-temporal motion representation, improving spatial understanding and compositional flexibility in motion generation for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08910": {
        "authors": [
            "Bin Ren",
            "Xiaoshui Huang",
            "Mengyuan Liu",
            "Hong Liu",
            "Fabio Poiesi",
            "Nicu Sebe",
            "Guofeng Mei"
        ],
        "title": "Masked Clustering Prediction for Unsupervised Point Cloud Pre-training",
        "abstract": "arXiv:2508.08910v1 Announce Type: new  Abstract: Vision transformers (ViTs) have recently been widely applied to 3D point cloud understanding, with masked autoencoding as the predominant pre-training paradigm. However, the challenge of learning dense and informative semantic features from point clouds via standard ViTs remains underexplored. We propose MaskClu, a novel unsupervised pre-training method for ViTs on 3D point clouds that integrates masked point modeling with clustering-based learning. MaskClu is designed to reconstruct both cluster assignments and cluster centers from masked point clouds, thus encouraging the model to capture dense semantic information. Additionally, we introduce a global contrastive learning mechanism that enhances instance-level feature learning by contrasting different masked views of the same point cloud. By jointly optimizing these complementary objectives, i.e., dense semantic reconstruction, and instance-level contrastive learning. MaskClu enables ViTs to learn richer and more semantically meaningful representations from 3D point clouds. We validate the effectiveness of our method via multiple 3D tasks, including part segmentation, semantic segmentation, object detection, and classification, where MaskClu sets new competitive results. The code and models will be released at:https://github.com/Amazingren/maskclu.",
        "arxiv_id": "2508.08910",
        "ARXIVID": "2508.08910",
        "COMMENT": "This paper proposes a new unsupervised pre-training method for 3D point cloud understanding using ViTs. It is relevant to spatial understanding (criterion 1) and vision foundation models (criterion 4), especially for 3D data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08487": {
        "authors": [
            "Qian Wang",
            "Ziqi Huang",
            "Ruoxi Jia",
            "Paul Debevec",
            "Ning Yu"
        ],
        "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling",
        "abstract": "arXiv:2508.08487v1 Announce Type: new  Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.",
        "arxiv_id": "2508.08487",
        "ARXIVID": "2508.08487",
        "COMMENT": "This paper introduces MAViS, a multi-agent framework for long-sequence video storytelling, orchestrating multiple generative models and agents. It is relevant to vision foundation models and their applications (criterion 4), especially in multi-modal generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08917": {
        "authors": [
            "Jintao Cheng",
            "Jiehao Luo",
            "Xieyuanli Chen",
            "Jin Wu",
            "Rui Fan",
            "Xiaoyu Tang",
            "Wei Zhang"
        ],
        "title": "A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition",
        "abstract": "arXiv:2508.08917v1 Announce Type: new  Abstract: LiDAR-based Place Recognition (LPR) remains a critical task in Embodied Artificial Intelligence (AI) and Autonomous Driving, primarily addressing localization challenges in GPS-denied environments and supporting loop closure detection. Existing approaches reduce place recognition to a Euclidean distance-based metric learning task, neglecting the feature space's intrinsic structures and intra-class variances. Such Euclidean-centric formulation inherently limits the model's capacity to capture nonlinear data distributions, leading to suboptimal performance in complex environments and temporal-varying scenarios. To address these challenges, we propose a novel cross-view network based on an innovative fusion paradigm. Our framework introduces a pseudo-global information guidance mechanism that coordinates multi-modal branches to perform feature learning within a unified semantic space. Concurrently, we propose a Manifold Adaptation and Pairwise Variance-Locality Learning Metric that constructs a Symmetric Positive Definite (SPD) matrix to compute Mahalanobis distance, superseding traditional Euclidean distance metrics. This geometric formulation enables the model to accurately characterize intrinsic data distributions and capture complex inter-class dependencies within the feature space. Experimental results demonstrate that the proposed algorithm achieves competitive performance, particularly excelling in complex environmental conditions.",
        "arxiv_id": "2508.08917",
        "ARXIVID": "2508.08917",
        "COMMENT": "This paper proposes a novel cross-view network for LiDAR-based place recognition in embodied AI, introducing a new fusion paradigm and a geometric metric for feature learning. It matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (new methods for embodied AI with a novel angle on feature space geometry).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08926": {
        "authors": [
            "Wei Cai",
            "Jian Zhao",
            "Yuchu Jiang",
            "Tianle Zhang",
            "Xuelong Li"
        ],
        "title": "Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models",
        "abstract": "arXiv:2508.08926v1 Announce Type: new  Abstract: Large Vision-Language Models face growing safety challenges with multimodal inputs. This paper introduces the concept of Implicit Reasoning Safety, a vulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due to flawed or hidden reasoning. To showcase this, we developed Safe Semantics, Unsafe Interpretations, the first dataset for this critical issue. Our demonstrations show that even simple In-Context Learning with SSUI significantly mitigates these implicit multimodal threats, underscoring the urgent need to improve cross-modal implicit reasoning.",
        "arxiv_id": "2508.08926",
        "ARXIVID": "2508.08926",
        "COMMENT": "Matches criterion 2: The paper investigates safety in large vision-language models (LVLMs), introduces a new dataset, and proposes mitigation strategies for implicit reasoning safety. This is directly about VLLMs and their empirical behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08989": {
        "authors": [
            "Ming Nie",
            "Chunwei Wang",
            "Hang Xu",
            "Li Zhang"
        ],
        "title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding",
        "abstract": "arXiv:2508.08989v1 Announce Type: new  Abstract: Recently, with the emergence of large language models, multimodal LLMs have demonstrated exceptional capabilities in image and video modalities. Despite advancements in video comprehension, the substantial computational demands of long video sequences lead current video LLMs (Vid-LLMs) to employ compression strategies at both the inter-frame level (e.g., uniform sampling of video frames) and intra-frame level (e.g., condensing all visual tokens of each frame into a limited number). However, this approach often neglects the uneven temporal distribution of critical information across frames, risking the omission of keyframes that contain essential temporal and semantic details. To tackle these challenges, we propose KFFocus, a method designed to efficiently compress video tokens and emphasize the informative context present within video frames. We substitute uniform sampling with a refined approach inspired by classic video compression principles to identify and capture keyframes based on their temporal redundancy. By assigning varying condensation ratios to frames based on their contextual relevance, KFFocus efficiently reduces token redundancy while preserving informative content details. Additionally, we introduce a spatiotemporal modeling module that encodes both the temporal relationships between video frames and the spatial structure within each frame, thus providing Vid-LLMs with a nuanced understanding of spatial-temporal dynamics. Extensive experiments on widely recognized video understanding benchmarks, especially long video scenarios, demonstrate that KFFocus significantly outperforms existing methods, achieving substantial computational efficiency and enhanced accuracy.",
        "arxiv_id": "2508.08989",
        "ARXIVID": "2508.08989",
        "COMMENT": "Matches criterion 2: The paper proposes KFFocus, a method for efficient video token compression and spatiotemporal modeling for video LLMs (Vid-LLMs), improving video understanding. This is a new method for VLLMs/MLLMs, with a focus on clever token selection and spatiotemporal modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.09000": {
        "authors": [
            "Yuhao Wang",
            "Wei Xi"
        ],
        "title": "UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale",
        "abstract": "arXiv:2508.09000v1 Announce Type: new  Abstract: Convolutional neural networks (ConvNets) with large effective receptive field (ERF), still in their early stages, have demonstrated promising effectiveness while constrained by high parameters and FLOPs costs and disrupted asymptotically Gaussian distribution (AGD) of ERF. This paper proposes an alternative paradigm: rather than merely employing extremely large ERF, it is more effective and efficient to expand the ERF while maintaining AGD of ERF by proper combination of smaller kernels, such as $7\\times{7}$, $9\\times{9}$, $11\\times{11}$. This paper introduces a Three-layer Receptive Field Aggregator and designs a Layer Operator as the fundamental operator from the perspective of receptive field. The ERF can be expanded to the level of existing large-kernel ConvNets through the stack of proposed modules while maintaining AGD of ERF. Using these designs, we propose a universal model for ConvNet of any scale, termed UniConvNet. Extensive experiments on ImageNet-1K, COCO2017, and ADE20K demonstrate that UniConvNet outperforms state-of-the-art CNNs and ViTs across various vision recognition tasks for both lightweight and large-scale models with comparable throughput. Surprisingly, UniConvNet-T achieves $84.2\\%$ ImageNet top-1 accuracy with $30M$ parameters and $5.1G$ FLOPs. UniConvNet-XL also shows competitive scalability to big data and large models, acquiring $88.4\\%$ top-1 accuracy on ImageNet. Code and models are publicly available at https://github.com/ai-paperwithcode/UniConvNet.",
        "arxiv_id": "2508.09000",
        "ARXIVID": "2508.09000",
        "COMMENT": "Matches criterion 4: proposes a new universal ConvNet architecture (UniConvNet) with improved receptive field aggregation, outperforming state-of-the-art CNNs and ViTs, relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.08830": {
        "authors": [
            "Mustafa Akben",
            "Vinayaka Gude",
            "Haya Ajjan"
        ],
        "title": "Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition",
        "abstract": "arXiv:2508.08830v1 Announce Type: new  Abstract: The ability to discern subtle emotional cues is fundamental to human social intelligence. As artificial intelligence (AI) becomes increasingly common, AI's ability to recognize and respond to human emotions is crucial for effective human-AI interactions. In particular, whether such systems can match or surpass human experts remains to be seen. However, the emotional intelligence of AI, particularly multimodal large language models (MLLMs), remains largely unexplored. This study evaluates the emotion recognition abilities of MLLMs using the Reading the Mind in the Eyes Test (RMET) and its multiracial counterpart (MRMET), and compares their performance against human participants. Results show that, on average, MLLMs outperform humans in accurately identifying emotions across both tests. This trend persists even when comparing performance across low, medium, and expert-level performing groups. Yet when we aggregate independent human decisions to simulate collective intelligence, human groups significantly surpass the performance of aggregated MLLM predictions, highlighting the wisdom of the crowd. Moreover, a collaborative approach (augmented intelligence) that combines human and MLLM predictions achieves greater accuracy than either humans or MLLMs alone. These results suggest that while MLLMs exhibit strong emotion recognition at the individual level, the collective intelligence of humans and the synergistic potential of human-AI collaboration offer the most promising path toward effective emotional AI. We discuss the implications of these findings for the development of emotionally intelligent AI systems and future research directions.",
        "arxiv_id": "2508.08830",
        "ARXIVID": "2508.08830",
        "COMMENT": "This paper evaluates emotion recognition in MLLMs and compares them to humans. It is relevant to criterion 2 (MLLMs) and provides surprising empirical results about MLLM performance versus human collective intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.09087": {
        "authors": [
            "Ahsan Habib Akash",
            "Greg Murray",
            "Annahita Amireskandari",
            "Joel Palko",
            "Carol Laxson",
            "Binod Bhattarai",
            "Prashnna Gyawali"
        ],
        "title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision",
        "abstract": "arXiv:2508.09087v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved remarkable success on multimodal tasks such as image-text retrieval and zero-shot classification, yet they can exhibit demographic biases even when explicit protected attributes are absent during training. In this work, we focus on automated glaucoma screening from retinal fundus images, a critical application given that glaucoma is a leading cause of irreversible blindness and disproportionately affects underserved populations. Building on a reweighting-based contrastive learning framework, we introduce an attribute-agnostic debiasing method that (i) infers proxy subgroups via unsupervised clustering of image-image embeddings, (ii) computes gradient-similarity weights between the CLIP-style multimodal loss and a SimCLR-style image-pair contrastive loss, and (iii) applies these weights in a joint, top-$k$ weighted objective to upweight underperforming clusters. This label-free approach adaptively targets the hardest examples, thereby reducing subgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma subset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES AUC), and Groupwise AUC to demonstrate equitable performance across inferred demographic subgroups.",
        "arxiv_id": "2508.09087",
        "ARXIVID": "2508.09087",
        "COMMENT": "Addresses bias in vision-language models (VLMs) for glaucoma detection using a novel attribute-agnostic debiasing method. This is a direct application of VLMs (criterion 4) and introduces a new statistical approach for fairness in medical imaging.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.08612": {
        "authors": [
            "Jiahua Dong",
            "Hui Yin",
            "Wenqi Liang",
            "Hanbin Zhao",
            "Henghui Ding",
            "Nicu Sebe",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "title": "Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation",
        "abstract": "arXiv:2508.08612v1 Announce Type: new  Abstract: Video instance segmentation (VIS) has gained significant attention for its capability in tracking and segmenting object instances across video frames. However, most of the existing VIS approaches unrealistically assume that the categories of object instances remain fixed over time. Moreover, they experience catastrophic forgetting of old classes when required to continuously learn object instances belonging to new categories. To resolve these challenges, we develop a novel Hierarchical Visual Prompt Learning (HVPL) model that overcomes catastrophic forgetting of previous categories from both frame-level and video-level perspectives. Specifically, to mitigate forgetting at the frame level, we devise a task-specific frame prompt and an orthogonal gradient correction (OGC) module. The OGC module helps the frame prompt encode task-specific global instance information for new classes in each individual frame by projecting its gradients onto the orthogonal feature space of old classes. Furthermore, to address forgetting at the video level, we design a task-specific video prompt and a video context decoder. This decoder first embeds structural inter-class relationships across frames into the frame prompt features, and then propagates task-specific global video contexts from the frame prompt features to the video prompt. Through rigorous comparisons, our HVPL model proves to be more effective than baseline approaches. The code is available at https://github.com/JiahuaDong/HVPL.",
        "arxiv_id": "2508.08612",
        "ARXIVID": "2508.08612",
        "COMMENT": "Proposes Hierarchical Visual Prompt Learning (HVPL) for continual video instance segmentation, addressing catastrophic forgetting. This is a new method for video instance segmentation, using prompt learning and orthogonal gradient correction, which is a clever statistical trick. It is relevant to vision foundation models and continual learning (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.08488": {
        "authors": [
            "Ankan Deria",
            "Dwarikanath Mahapatra",
            "Behzad Bozorgtabar",
            "Mohna Chakraborty",
            "Snehashis Chakraborty",
            "Sudipta Roy"
        ],
        "title": "MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization",
        "abstract": "arXiv:2508.08488v1 Announce Type: new  Abstract: Virtual try-on seeks to generate photorealistic images of individuals in desired garments, a task that must simultaneously preserve personal identity and garment fidelity for practical use in fashion retail and personalization. However, existing methods typically handle upper and lower garments separately, rely on heavy preprocessing, and often fail to preserve person-specific cues such as tattoos, accessories, and body shape-resulting in limited realism and flexibility. To this end, we introduce MuGa-VTON, a unified multi-garment diffusion framework that jointly models upper and lower garments together with person identity in a shared latent space. Specifically, we proposed three key modules: the Garment Representation Module (GRM) for capturing both garment semantics, the Person Representation Module (PRM) for encoding identity and pose cues, and the A-DiT fusion module, which integrates garment, person, and text-prompt features through a diffusion transformer. This architecture supports prompt-based customization, allowing fine-grained garment modifications with minimal user input. Extensive experiments on the VITON-HD and DressCode benchmarks demonstrate that MuGa-VTON outperforms existing methods in both qualitative and quantitative evaluations, producing high-fidelity, identity-preserving results suitable for real-world virtual try-on applications.",
        "arxiv_id": "2508.08488",
        "ARXIVID": "2508.08488",
        "COMMENT": "Proposes MuGa-VTON, a multi-garment virtual try-on system using diffusion transformers and prompt customization. This is a vision foundation model application (criterion 4) and uses generative modeling in a multi-modal context (text and image prompts).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.08660": {
        "authors": [
            "Xin Wang",
            "Yin Guo",
            "Jiamin Xia",
            "Kaiyu Zhang",
            "Niranjan Balu",
            "Mahmud Mossa-Basha",
            "Linda Shapiro",
            "Chun Yuan"
        ],
        "title": "Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation",
        "abstract": "arXiv:2508.08660v1 Announce Type: new  Abstract: Most prior unsupervised domain adaptation approaches for medical image segmentation are narrowly tailored to either the source-accessible setting, where adaptation is guided by source-target alignment, or the source-free setting, which typically resorts to implicit supervision mechanisms such as pseudo-labeling and model distillation. This substantial divergence in methodological designs between the two settings reveals an inherent flaw: the lack of an explicit, structured construction of anatomical knowledge that naturally generalizes across domains and settings. To bridge this longstanding divide, we introduce a unified, semantically grounded framework that supports both source-accessible and source-free adaptation. Fundamentally distinct from all prior works, our framework's adaptability emerges naturally as a direct consequence of the model architecture, without the need for any handcrafted adaptation strategies. Specifically, our model learns a domain-agnostic probabilistic manifold as a global space of anatomical regularities, mirroring how humans establish visual understanding. Thus, the structural content in each image can be interpreted as a canonical anatomy retrieved from the manifold and a spatial transformation capturing individual-specific geometry. This disentangled, interpretable formulation enables semantically meaningful prediction with intrinsic adaptability. Extensive experiments on challenging cardiac and abdominal datasets show that our framework achieves state-of-the-art results in both settings, with source-free performance closely approaching its source-accessible counterpart, a level of consistency rarely observed in prior works. Beyond quantitative improvement, we demonstrate strong interpretability of the proposed framework via manifold traversal for smooth shape manipulation.",
        "arxiv_id": "2508.08660",
        "ARXIVID": "2508.08660",
        "COMMENT": "This paper proposes a unified, semantically grounded domain adaptation framework for medical image segmentation, learning a domain-agnostic probabilistic manifold. It is a methodological improvement in spatial understanding (criterion 1) and introduces a novel angle on domain adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.08939": {
        "authors": [
            "Eduarda Caldeira",
            "Fadi Boutros",
            "Naser Damer"
        ],
        "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation",
        "abstract": "arXiv:2508.08939v1 Announce Type: new  Abstract: Face Morphing Attack Detection (MAD) is a critical challenge in face recognition security, where attackers can fool systems by interpolating the identity information of two or more individuals into a single face image, resulting in samples that can be verified as belonging to multiple identities by face recognition systems. While multimodal foundation models (FMs) like CLIP offer strong zero-shot capabilities by jointly modeling images and text, most prior works on FMs for biometric recognition have relied on fine-tuning for specific downstream tasks, neglecting their potential for direct, generalizable deployment. This work explores a pure zero-shot approach to MAD by leveraging CLIP without any additional training or fine-tuning, focusing instead on the design and aggregation of multiple textual prompts per class. By aggregating the embeddings of diverse prompts, we better align the model's internal representations with the MAD task, capturing richer and more varied cues indicative of bona-fide or attack samples. Our results show that prompt aggregation substantially improves zero-shot detection performance, demonstrating the effectiveness of exploiting foundation models' built-in multimodal knowledge through efficient prompt engineering.",
        "arxiv_id": "2508.08939",
        "ARXIVID": "2508.08939",
        "COMMENT": "Matches criterion 4: applies vision foundation models (CLIP) for zero-shot morphing attack detection using prompt aggregation, showing a novel application of VLMs in biometrics.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.08944": {
        "authors": [
            "Wenhan Wu",
            "Zhishuai Guo",
            "Chen Chen",
            "Aidong Lu"
        ],
        "title": "UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition",
        "abstract": "arXiv:2508.08944v1 Announce Type: new  Abstract: Skeleton-based action recognition (SAR) has achieved impressive progress with transformer architectures. However, existing methods often rely on complex module compositions and heavy designs, leading to increased parameter counts, high computational costs, and limited scalability. In this paper, we propose a unified spatio-temporal lightweight transformer framework that integrates spatial and temporal modeling within a single attention module, eliminating the need for separate temporal modeling blocks. This approach reduces redundant computations while preserving temporal awareness within the spatial modeling process. Furthermore, we introduce a simplified multi-scale pooling fusion module that combines local and global pooling pathways to enhance the model's ability to capture fine-grained local movements and overarching global motion patterns. Extensive experiments on benchmark datasets demonstrate that our lightweight model achieves a superior balance between accuracy and efficiency, reducing parameter complexity by over 58% and lowering computational cost by over 60% compared to state-of-the-art transformer-based baselines, while maintaining competitive recognition performance.",
        "arxiv_id": "2508.08944",
        "ARXIVID": "2508.08944",
        "COMMENT": "This paper proposes a lightweight transformer for skeleton-based action recognition, focusing on spatio-temporal modeling. It is relevant to spatial understanding (criterion 1) and vision foundation models (criterion 4) for action recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.09022": {
        "authors": [
            "Zhiqiang Yang",
            "Renshuai Tao",
            "Xiaolong Zheng",
            "Guodong Yang",
            "Chunjie Zhang"
        ],
        "title": "When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges",
        "abstract": "arXiv:2508.09022v1 Announce Type: new  Abstract: Existing deepfake detection methods heavily depend on labeled training data. However, as AI-generated content becomes increasingly realistic, even \\textbf{human annotators struggle to distinguish} between deepfakes and authentic images. This makes the labeling process both time-consuming and less reliable. Specifically, there is a growing demand for approaches that can effectively utilize large-scale unlabeled data from online social networks. Unlike typical unsupervised learning tasks, where categories are distinct, AI-generated faces closely mimic real image distributions and share strong similarities, causing performance drop in conventional strategies. In this paper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key challenges: (1) bridging the domain gap between faces from different generation models, and (2) utilizing unlabeled image samples. The method features two core modules: text-guided cross-domain alignment, which uses learnable prompts to unify visual and textual embeddings into a domain-invariant feature space, and curriculum-driven pseudo label generation, which dynamically exploit more informative unlabeled samples. To prevent catastrophic forgetting, we also facilitate bridging between domains via cross-domain knowledge distillation. Extensive experiments on \\textbf{11 popular datasets}, show that DPGNet outperforms SoTA approaches by \\textbf{6.3\\%}, highlighting its effectiveness in leveraging unlabeled data to address the annotation challenges posed by the increasing realism of deepfakes.",
        "arxiv_id": "2508.09022",
        "ARXIVID": "2508.09022",
        "COMMENT": "This paper proposes a new method for deepfake detection using unlabeled data and cross-domain alignment. It is relevant to vision foundation models (criterion 4) as it leverages vision-language techniques for a challenging vision task.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.08811": {
        "authors": [
            "Shi-Chen Zhang",
            "Yunheng Li",
            "Yu-Huan Wu",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "title": "Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment",
        "abstract": "arXiv:2508.08811v1 Announce Type: new  Abstract: Semantic segmentation is fundamental to vision systems requiring pixel-level scene understanding, yet deploying it on resource-constrained devices demands efficient architectures. Although existing methods achieve real-time inference through lightweight designs, we reveal their inherent limitation: misalignment between class representations and image features caused by a per-pixel classification paradigm. With experimental analysis, we find that this paradigm results in a highly challenging assumption for efficient scenarios: Image pixel features should not vary for the same category in different images. To address this dilemma, we propose a coupled dual-branch offset learning paradigm that explicitly learns feature and class offsets to dynamically refine both class representations and spatial image features. Based on the proposed paradigm, we construct an efficient semantic segmentation network, OffSeg. Notably, the offset learning paradigm can be adopted to existing methods with no additional architectural changes. Extensive experiments on four datasets, including ADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistent improvements with negligible parameters. For instance, on the ADE20K dataset, our proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, and Mask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2M additional parameters required.",
        "arxiv_id": "2508.08811",
        "ARXIVID": "2508.08811",
        "COMMENT": "This paper introduces a new offset learning paradigm for efficient semantic segmentation, improving spatial and class feature alignment. It is a methodological improvement in spatial understanding (criterion 1), especially for resource-constrained vision systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.09137": {
        "authors": [
            "Timo Teufel",
            "Pulkit Gera",
            "Xilong Zhou",
            "Umar Iqbal",
            "Pramod Rao",
            "Jan Kautz",
            "Vladislav Golyanik",
            "Christian Theobalt"
        ],
        "title": "HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis",
        "abstract": "arXiv:2508.09137v1 Announce Type: new  Abstract: Simultaneous relighting and novel-view rendering of digital human representations is an important yet challenging task with numerous applications. Progress in this area has been significantly limited due to the lack of publicly available, high-quality datasets, especially for full-body human captures. To address this critical gap, we introduce the HumanOLAT dataset, the first publicly accessible large-scale dataset of multi-view One-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes HDR RGB frames under various illuminations, such as white light, environment maps, color gradients and fine-grained OLAT illuminations. Our evaluations of state-of-the-art relighting and novel-view synthesis methods underscore both the dataset's value and the significant challenges still present in modeling complex human-centric appearance and lighting interactions. We believe HumanOLAT will significantly facilitate future research, enabling rigorous benchmarking and advancements in both general and human-specific relighting and rendering techniques.",
        "arxiv_id": "2508.09137",
        "ARXIVID": "2508.09137",
        "COMMENT": "This paper introduces a new large-scale dataset for full-body human relighting and novel-view synthesis, which is relevant to vision foundation models and their applications (criterion 4). It also provides a new benchmark for relighting and rendering techniques, which partially matches criterion 3.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.08556": {
        "authors": [
            "Yunqi Miao",
            "Zhiyu Qu",
            "Mingqi Gao",
            "Changrui Chen",
            "Jifei Song",
            "Jungong Han",
            "Jiankang Deng"
        ],
        "title": "Unlocking the Potential of Diffusion Priors in Blind Face Restoration",
        "abstract": "arXiv:2508.08556v1 Announce Type: new  Abstract: Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images. The vanilla diffusion model is trained on images with no or less degradations, whereas BFR handles moderately to severely degraded images. Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate complex and unknown degradations in real-world scenarios. In this work, we use a unified network FLIPNET that switches between two modes to resolve specific gaps. In Restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration. In Degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets. Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations.",
        "arxiv_id": "2508.08556",
        "ARXIVID": "2508.08556",
        "COMMENT": "Related to vision foundation models and generative modeling (criterion 4), as it leverages diffusion priors for blind face restoration and proposes a unified network for real-world degradations.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.08697": {
        "authors": [
            "Tong Sun",
            "Hongliang Ye",
            "Jilin Mei",
            "Liang Chen",
            "Fangzhou Zhao",
            "Leiqiang Zong",
            "Yu Hu"
        ],
        "title": "ROD: RGB-Only Fast and Efficient Off-road Freespace Detection",
        "abstract": "arXiv:2508.08697v1 Announce Type: new  Abstract: Off-road freespace detection is more challenging than on-road scenarios because of the blurred boundaries of traversable areas. Previous state-of-the-art (SOTA) methods employ multi-modal fusion of RGB images and LiDAR data. However, due to the significant increase in inference time when calculating surface normal maps from LiDAR data, multi-modal methods are not suitable for real-time applications, particularly in real-world scenarios where higher FPS is required compared to slow navigation. This paper presents a novel RGB-only approach for off-road freespace detection, named ROD, eliminating the reliance on LiDAR data and its computational demands. Specifically, we utilize a pre-trained Vision Transformer (ViT) to extract rich features from RGB images. Additionally, we design a lightweight yet efficient decoder, which together improve both precision and inference speed. ROD establishes a new SOTA on ORFD and RELLIS-3D datasets, as well as an inference speed of 50 FPS, significantly outperforming prior models.",
        "arxiv_id": "2508.08697",
        "ARXIVID": "2508.08697",
        "COMMENT": "Introduces ROD, an RGB-only off-road freespace detection method using a pre-trained Vision Transformer. This is an application of vision foundation models (criterion 4) and proposes a new, efficient method for a challenging vision task.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.08997": {
        "authors": [
            "Sizhe Yuen",
            "Francisco Gomez Medina",
            "Ting Su",
            "Yali Du",
            "Adam J. Sobey"
        ],
        "title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory",
        "abstract": "arXiv:2508.08997v1 Announce Type: new  Abstract: Multi-agent systems built on Large Language Models (LLMs) show exceptional promise for complex collaborative problem-solving, yet they face fundamental challenges stemming from context window limitations that impair memory consistency, role adherence, and procedural integrity. This paper introduces Intrinsic Memory Agents, a novel framework that addresses these limitations through structured agent-specific memories that evolve intrinsically with agent outputs. Specifically, our method maintains role-aligned memory templates that preserve specialized perspectives while focusing on task-relevant information. We benchmark our approach on the PDDL dataset, comparing its performance to existing state-of-the-art multi-agentic memory approaches and showing an improvement of 38.6\\% with the highest token efficiency. An additional evaluation is performed on a complex data pipeline design task, we demonstrate that our approach produces higher quality designs when comparing 5 metrics: scalability, reliability, usability, cost-effectiveness and documentation with additional qualitative evidence of the improvements. Our findings suggest that addressing memory limitations through structured, intrinsic approaches can improve the capabilities of multi-agent LLM systems on structured planning tasks.",
        "arxiv_id": "2508.08997",
        "ARXIVID": "2508.08997",
        "COMMENT": "Proposes a new memory framework for multi-agent LLM systems, improving structured planning tasks. This is relevant to criterion 3 (new methods for multi-agent systems), but not specifically about spatial intelligence, embodied agents, or vision/multi-modal models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.08549": {
        "authors": [
            "Wei Li",
            "Pengcheng Zhou",
            "Linye Ma",
            "Wenyi Zhao",
            "Huihua Yang"
        ],
        "title": "Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation",
        "abstract": "arXiv:2508.08549v1 Announce Type: new  Abstract: Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios.",
        "arxiv_id": "2508.08549",
        "ARXIVID": "2508.08549",
        "COMMENT": "This paper proposes a generic framework for semi-supervised medical image segmentation using diverse teaching and label propagation. It is a methodological improvement in spatial understanding (criterion 1), but does not introduce a new benchmark or foundation model.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.08293": {
        "authors": [
            "Sridhar Mahadevan"
        ],
        "title": "Topos Theory for Generative AI and LLMs",
        "abstract": "arXiv:2508.08293v1 Announce Type: new  Abstract: We propose the design of novel categorical generative AI architectures (GAIAs) using topos theory, a type of category that is ``set-like\": a topos has all (co)limits, is Cartesian closed, and has a subobject classifier. Previous theoretical results on the Transformer model have shown that it is a universal sequence-to-sequence function approximator, and dense in the space of all continuous functions with compact support on the Euclidean space of embeddings of tokens. Building on this theoretical result, we explore novel architectures for LLMs that exploit the property that the category of LLMs, viewed as functions, forms a topos. Previous studies of large language models (LLMs) have focused on daisy-chained linear architectures or mixture-of-experts. In this paper, we use universal constructions in category theory to construct novel LLM architectures based on new types of compositional structures. In particular, these new compositional structures are derived from universal properties of LLM categories, and include pullback, pushout, (co) equalizers, exponential objects, and subobject classifiers. We theoretically validate these new compositional structures by showing that the category of LLMs is (co)complete, meaning that all diagrams have solutions in the form of (co)limits. Building on this completeness result, we then show that the category of LLMs forms a topos, a ``set-like\" category, which requires showing the existence of exponential objects as well as subobject classifiers. We use a functorial characterization of backpropagation to define a potential implementation of an LLM topos architecture.",
        "arxiv_id": "2508.08293",
        "ARXIVID": "2508.08293",
        "COMMENT": "This paper proposes a theoretical framework for generative AI and LLMs using topos theory. While it is highly novel in theory, it does not match the four criteria directly, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 9
    },
    "2508.08987": {
        "authors": [
            "Ding Xia",
            "Naoto Inoue",
            "Qianru Qiu",
            "Kotaro Kikuchi"
        ],
        "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation",
        "abstract": "arXiv:2508.08987v1 Announce Type: new  Abstract: Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating communication, improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models (LLMs) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained LLMs serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our LLM-based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.",
        "arxiv_id": "2508.08987",
        "ARXIVID": "2508.08987",
        "COMMENT": "This paper uses LLMs for color recommendation in design, which is a multi-modal application but not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about generative design applications.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.09129": {
        "authors": [
            "Xianghe Pang",
            "Shuo Tang",
            "Rui Ye",
            "Yuwen Du",
            "Yaxin Du",
            "Siheng Chen"
        ],
        "title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair",
        "abstract": "arXiv:2508.09129v1 Announce Type: new  Abstract: Effective information seeking in the vast and ever-growing digital landscape requires balancing expansive search with strategic reasoning. Current large language model (LLM)-based agents struggle to achieve this balance due to limitations in search breadth and reasoning depth, where slow, serial querying restricts coverage of relevant sources and noisy raw inputs disrupt the continuity of multi-step reasoning. To address these challenges, we propose BrowseMaster, a scalable framework built around a programmatically augmented planner-executor agent pair. The planner formulates and adapts search strategies based on task constraints, while the executor conducts efficient, targeted retrieval to supply the planner with concise, relevant evidence. This division of labor preserves coherent, long-horizon reasoning while sustaining broad and systematic exploration, overcoming the trade-off that limits existing agents. Extensive experiments on challenging English and Chinese benchmarks show that BrowseMaster consistently outperforms open-source and proprietary baselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh, which demonstrates its strong capability in complex, reasoning-heavy information-seeking tasks at scale.",
        "arxiv_id": "2508.09129",
        "ARXIVID": "2508.09129",
        "COMMENT": "This paper presents a scalable web browsing agent using a planner-executor pair, but it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about LLM-based agents for information seeking.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.08700": {
        "authors": [
            "Qi Zheng",
            "Li-Heng Chen",
            "Chenlong He",
            "Neil Berkbeck",
            "Yilin Wang",
            "Balu Adsumilli",
            "Alan C. Bovik",
            "Yibo Fan",
            "Zhengzhong Tu"
        ],
        "title": "Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos",
        "abstract": "arXiv:2508.08700v1 Announce Type: new  Abstract: Although there have been notable advancements in video compression technologies in recent years, banding artifacts remain a serious issue affecting the quality of compressed videos, particularly on smooth regions of high-definition videos. Noticeable banding artifacts can severely impact the perceptual quality of videos viewed on a high-end HDTV or high-resolution screen. Hence, there is a pressing need for a systematic investigation of the banding video quality assessment problem for advanced video codecs. Given that the existing publicly available datasets for studying banding artifacts are limited to still picture data only, which cannot account for temporal banding dynamics, we have created a first-of-a-kind open video dataset, dubbed LIVE-YT-Banding, which consists of 160 videos generated by four different compression parameters using the AV1 video codec. A total of 7,200 subjective opinions are collected from a cohort of 45 human subjects. To demonstrate the value of this new resources, we tested and compared a variety of models that detect banding occurrences, and measure their impact on perceived quality. Among these, we introduce an effective and efficient new no-reference (NR) video quality evaluator which we call CBAND. CBAND leverages the properties of the learned statistics of natural images expressed in the embeddings of deep neural networks. Our experimental results show that the perceptual banding prediction performance of CBAND significantly exceeds that of previous state-of-the-art models, and is also orders of magnitude faster. Moreover, CBAND can be employed as a differentiable loss function to optimize video debanding models. The LIVE-YT-Banding database, code, and pre-trained model are all publically available at https://github.com/uniqzheng/CBAND.",
        "arxiv_id": "2508.08700",
        "ARXIVID": "2508.08700",
        "COMMENT": "This paper introduces a new open video dataset (LIVE-YT-Banding) and a new no-reference video quality evaluator (CBAND) for banding artifact assessment. It is a new benchmark and method for video quality, but not directly about spatial intelligence, embodied agents, VLLMs, or vision foundation models. Closest to criterion 3 (benchmark), but not in embodied AI or simulators.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.08344": {
        "authors": [
            "Dongzhuoran Zhou",
            "Yuqicheng Zhu",
            "Xiaxia Wang",
            "Hongkuan Zhou",
            "Yuan He",
            "Jiaoyan Chen",
            "Evgeny Kharlamov",
            "Steffen Staab"
        ],
        "title": "What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge",
        "abstract": "arXiv:2508.08344v1 Announce Type: new  Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an increasingly explored approach for combining the reasoning capabilities of large language models with the structured evidence of knowledge graphs. However, current evaluation practices fall short: existing benchmarks often include questions that can be directly answered using existing triples in KG, making it unclear whether models perform reasoning or simply retrieve answers directly. Moreover, inconsistent evaluation metrics and lenient answer matching criteria further obscure meaningful comparisons. In this work, we introduce a general method for constructing benchmarks, together with an evaluation protocol, to systematically assess KG-RAG methods under knowledge incompleteness. Our empirical results show that current KG-RAG methods have limited reasoning ability under missing knowledge, often rely on internal memorization, and exhibit varying degrees of generalization depending on their design.",
        "arxiv_id": "2508.08344",
        "ARXIVID": "2508.08344",
        "COMMENT": "This paper introduces a new benchmark construction method and evaluation protocol for knowledge graph-based retrieval-augmented generation (KG-RAG) under incomplete knowledge. It partially matches criterion 3 (new benchmark for reasoning in multi-modal/embodied AI), but is more NLP/knowledge-graph focused than vision/embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.08446": {
        "authors": [
            "Woojeong Kim",
            "Junxiong Wang",
            "Jing Nathan Yan",
            "Mohamed Abdelfattah",
            "Alexander M. Rush"
        ],
        "title": "OverFill: Two-Stage Models for Efficient Language Model Decoding",
        "abstract": "arXiv:2508.08446v1 Announce Type: new  Abstract: Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.",
        "arxiv_id": "2508.08446",
        "ARXIVID": "2508.08446",
        "COMMENT": "This paper proposes OverFill, a two-stage model for efficient LLM decoding. While it is a clever statistical trick for LLMs, it does not match the four criteria directly, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.08798": {
        "authors": [
            "Yao Lu",
            "Jiawei Li",
            "Ming Jiang"
        ],
        "title": "MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields",
        "abstract": "arXiv:2508.08798v1 Announce Type: new  Abstract: In recent years, Neural Radiance Fields (NeRF) have achieved remarkable progress in dynamic human reconstruction and rendering. Part-based rendering paradigms, guided by human segmentation, allow for flexible parameter allocation based on structural complexity, thereby enhancing representational efficiency. However, existing methods still struggle with complex pose variations, often producing unnatural transitions at part boundaries and failing to reconstruct occluded regions accurately in monocular settings. We propose MonoPartNeRF, a novel framework for monocular dynamic human rendering that ensures smooth transitions and robust occlusion recovery. First, we build a bidirectional deformation model that combines rigid and non-rigid transformations to establish a continuous, reversible mapping between observation and canonical spaces. Sampling points are projected into a parameterized surface-time space (u, v, t) to better capture non-rigid motion. A consistency loss further suppresses deformation-induced artifacts and discontinuities. We introduce a part-based pose embedding mechanism that decomposes global pose vectors into local joint embeddings based on body regions. This is combined with keyframe pose retrieval and interpolation, along three orthogonal directions, to guide pose-aware feature sampling. A learnable appearance code is integrated via attention to model dynamic texture changes effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that our method significantly outperforms prior approaches under complex pose and occlusion conditions, achieving superior joint alignment, texture fidelity, and structural continuity.",
        "arxiv_id": "2508.08798",
        "ARXIVID": "2508.08798",
        "COMMENT": "This paper presents MonoPartNeRF, a part-based NeRF for monocular human reconstruction. While it involves spatial modeling and novel neural rendering, it does not directly address spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is closest to general computer vision interests.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.08632": {
        "authors": [
            "Bo Yang",
            "Yu Zhang",
            "Lanfei Feng",
            "Yunkui Chen",
            "Jianyu Zhang",
            "Xiao Xu",
            "Nueraili Aierken",
            "Yurui Li",
            "Yuxuan Chen",
            "Guijun Yang",
            "Yong He",
            "Runhe Huang",
            "Shijian Li"
        ],
        "title": "AgriGPT: a Large Language Model Ecosystem for Agriculture",
        "abstract": "arXiv:2508.08632v1 Announce Type: new  Abstract: Despite the rapid progress of Large Language Models (LLMs), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized LLM ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning, thereby improving the LLM's reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose LLMs on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible LLM ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized LLMs. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research.",
        "arxiv_id": "2508.08632",
        "ARXIVID": "2508.08632",
        "COMMENT": "Presents AgriGPT, a domain-specific LLM ecosystem for agriculture, with a new dataset, retrieval-augmented generation, and evaluation benchmark. While it is a new LLM and benchmark, it is not visual or multi-modal, nor is it about spatial intelligence or embodied AI. Closest to criterion 2, but not a VLLM/MLLM.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.08726": {
        "authors": [
            "Yuwei Yan",
            "Jinghua Piao",
            "Xiaochong Lan",
            "Chenyang Shao",
            "Pan Hui",
            "Yong Li"
        ],
        "title": "Simulating Generative Social Agents via Theory-Informed Workflow Design",
        "abstract": "arXiv:2508.08726v1 Announce Type: new  Abstract: Recent advances in large language models have demonstrated strong reasoning and role-playing capabilities, opening new opportunities for agent-based social simulations. However, most existing agents' implementations are scenario-tailored, without a unified framework to guide the design. This lack of a general social agent limits their ability to generalize across different social contexts and to produce consistent, realistic behaviors. To address this challenge, we propose a theory-informed framework that provides a systematic design process for LLM-based social agents. Our framework is grounded in principles from Social Cognition Theory and introduces three key modules: motivation, action planning, and learning. These modules jointly enable agents to reason about their goals, plan coherent actions, and adapt their behavior over time, leading to more flexible and contextually appropriate responses. Comprehensive experiments demonstrate that our theory-driven agents reproduce realistic human behavior patterns under complex conditions, achieving up to 75% lower deviation from real-world behavioral data across multiple fidelity metrics compared to classical generative baselines. Ablation studies further show that removing motivation, planning, or learning modules increases errors by 1.5 to 3.2 times, confirming their distinct and essential contributions to generating realistic and coherent social behaviors.",
        "arxiv_id": "2508.08726",
        "ARXIVID": "2508.08726",
        "COMMENT": "This paper proposes a theory-informed workflow for LLM-based social agents, focusing on social cognition and agent-based simulation. It does not match the four criteria directly, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.08765": {
        "authors": [
            "Andrea Montibeller",
            "Dasara Shullani",
            "Daniele Baracchi",
            "Alessandro Piva",
            "Giulia Boato"
        ],
        "title": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation",
        "abstract": "arXiv:2508.08765v1 Announce Type: new  Abstract: The growing presence of AI-generated videos on social networks poses new challenges for deepfake detection, as detectors trained under controlled conditions often fail to generalize to real-world scenarios. A key factor behind this gap is the aggressive, proprietary compression applied by platforms like YouTube and Facebook, which launder low-level forensic cues. However, replicating these transformations at scale is difficult due to API limitations and data-sharing constraints. For these reasons, we propose a first framework that emulates the video sharing pipelines of social networks by estimating compression and resizing parameters from a small set of uploaded videos. These parameters enable a local emulator capable of reproducing platform-specific artifacts on large datasets without direct API access. Experiments on FaceForensics++ videos shared via social networks demonstrate that our emulated data closely matches the degradation patterns of real uploads. Furthermore, detectors fine-tuned on emulated videos achieve comparable performance to those trained on actual shared media. Our approach offers a scalable and practical solution for bridging the gap between lab-based training and real-world deployment of deepfake detectors, particularly in the underexplored domain of compressed video content.",
        "arxiv_id": "2508.08765",
        "ARXIVID": "2508.08765",
        "COMMENT": "Does not match any specific criterion; focuses on deepfake detection and social network compression emulation, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08992": {
        "authors": [
            "Rui Wang",
            "Qihan Lin",
            "Jiayu Liu",
            "Qing Zong",
            "Tianshi Zheng",
            "Weiqi Wang",
            "Yangqiu Song"
        ],
        "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty",
        "abstract": "arXiv:2508.08992v1 Announce Type: new  Abstract: Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT.",
        "arxiv_id": "2508.08992",
        "ARXIVID": "2508.08992",
        "COMMENT": "This paper investigates decision-making in LLMs under uncertainty, but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about cognitive modeling and LLM behavior.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.09094": {
        "authors": [
            "Oleksandr Kuznetsov",
            "Emanuele Frontoni",
            "Luca Romeo",
            "Riccardo Rosati",
            "Andrea Maranesi",
            "Alessandro Muscatello"
        ],
        "title": "Deep Learning Models for Robust Facial Liveness Detection",
        "abstract": "arXiv:2508.09094v1 Announce Type: new  Abstract: In the rapidly evolving landscape of digital security, biometric authentication systems, particularly facial recognition, have emerged as integral components of various security protocols. However, the reliability of these systems is compromised by sophisticated spoofing attacks, where imposters gain unauthorized access by falsifying biometric traits. Current literature reveals a concerning gap: existing liveness detection methodologies - designed to counteract these breaches - fall short against advanced spoofing tactics employing deepfakes and other artificial intelligence-driven manipulations. This study introduces a robust solution through novel deep learning models addressing the deficiencies in contemporary anti-spoofing techniques. By innovatively integrating texture analysis and reflective properties associated with genuine human traits, our models distinguish authentic presence from replicas with remarkable precision. Extensive evaluations were conducted across five diverse datasets, encompassing a wide range of attack vectors and environmental conditions. Results demonstrate substantial advancement over existing systems, with our best model (AttackNet V2.2) achieving 99.9% average accuracy when trained on combined data. Moreover, our research unveils critical insights into the behavioral patterns of impostor attacks, contributing to a more nuanced understanding of their evolving nature. The implications are profound: our models do not merely fortify the authentication processes but also instill confidence in biometric systems across various sectors reliant on secure access.",
        "arxiv_id": "2508.09094",
        "ARXIVID": "2508.09094",
        "COMMENT": "Presents new deep learning models for facial liveness detection, focusing on anti-spoofing. While relevant to computer vision, it does not match any of the specific criteria (spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in a novel way).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08537": {
        "authors": [
            "Anh Le",
            "Asanobu Kitamoto"
        ],
        "title": "Training Kindai OCR with parallel textline images and self-attention feature distance-based loss",
        "abstract": "arXiv:2508.08537v1 Announce Type: new  Abstract: Kindai documents, written in modern Japanese from the late 19th to early 20th century, hold significant historical value for researchers studying societal structures, daily life, and environmental conditions of that period. However, transcribing these documents remains a labor-intensive and time-consuming task, resulting in limited annotated data for training optical character recognition (OCR) systems. This research addresses this challenge of data scarcity by leveraging parallel textline images - pairs of original Kindai text and their counterparts in contemporary Japanese fonts - to augment training datasets. We introduce a distance-based objective function that minimizes the gap between self-attention features of the parallel image pairs. Specifically, we explore Euclidean distance and Maximum Mean Discrepancy (MMD) as domain adaptation metrics. Experimental results demonstrate that our method reduces the character error rate (CER) by 2.23% and 3.94% over a Transformer-based OCR baseline when using Euclidean distance and MMD, respectively. Furthermore, our approach improves the discriminative quality of self-attention representations, leading to more effective OCR performance for historical documents.",
        "arxiv_id": "2508.08537",
        "ARXIVID": "2508.08537",
        "COMMENT": "This paper presents a new loss function for OCR on historical Japanese documents using self-attention feature distance. While it is a clever statistical trick in computer vision, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.09009": {
        "authors": [
            "Luyang Cao",
            "Han Xu",
            "Jian Zhang",
            "Lei Qi",
            "Jiayi Ma",
            "Yinghuan Shi",
            "Yang Gao"
        ],
        "title": "Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement",
        "abstract": "arXiv:2508.09009v1 Announce Type: new  Abstract: In low-light image enhancement, Retinex-based deep learning methods have garnered significant attention due to their exceptional interpretability. These methods decompose images into mutually independent illumination and reflectance components, allows each component to be enhanced separately. In fact, achieving perfect decomposition of illumination and reflectance components proves to be quite challenging, with some residuals still existing after decomposition. In this paper, we formally name these residuals as inter-component residuals (ICR), which has been largely underestimated by previous methods. In our investigation, ICR not only affects the accuracy of the decomposition but also causes enhanced components to deviate from the ideal outcome, ultimately reducing the final synthesized image quality. To address this issue, we propose a novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the decomposition and enhancement stage. In the decomposition stage, we leverage inter-component residual reduction module to reduce the feature similarity between illumination and reflectance components. In the enhancement stage, we utilize the feature similarity between the two components to detect and mitigate the impact of ICR within each enhancement unit. Extensive experiments on three low-light benchmark datasets demonstrated that by reducing ICR, our method outperforms state-of-the-art approaches both qualitatively and quantitatively.",
        "arxiv_id": "2508.09009",
        "ARXIVID": "2508.09009",
        "COMMENT": "This paper proposes a new method for low-light image enhancement using a novel inter-component correction in Retinex-based models. While it is a methodological improvement in computer vision, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08795": {
        "authors": [
            "Amir Mohammad Salehoof",
            "Ali Ramezani",
            "Yadollah Yaghoobzadeh",
            "Majid Nili Ahmadabadi"
        ],
        "title": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions",
        "abstract": "arXiv:2508.08795v1 Announce Type: new  Abstract: Large language models (LLMs) acquire vast knowledge from large text corpora, but this information can become outdated or inaccurate. Since retraining is computationally expensive, knowledge editing offers an efficient alternative -- modifying internal knowledge without full retraining. These methods aim to update facts precisely while preserving the model's overall capabilities. While existing surveys focus on the mechanism of editing (e.g., parameter changes vs. external memory), they often overlook the function of the knowledge being edited. This survey introduces a novel, complementary function-based taxonomy to provide a more holistic view. We examine how different mechanisms apply to various knowledge types -- factual, temporal, conceptual, commonsense, and social -- highlighting how editing effectiveness depends on the nature of the target knowledge. By organizing our review along these two axes, we map the current landscape, outline the strengths and limitations of existing methods, define the problem formally, survey evaluation tasks and datasets, and conclude with open challenges and future directions.",
        "arxiv_id": "2508.08795",
        "ARXIVID": "2508.08795",
        "COMMENT": "This is a survey on knowledge editing in LLMs, focusing on a new taxonomy. It does not match any of the four criteria directly, as it is not about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}