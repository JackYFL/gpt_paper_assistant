{
    "2509.14476": {
        "authors": [
            "Jiasen Lu",
            "Liangchen Song",
            "Mingze Xu",
            "Byeongjoo Ahn",
            "Yanjun Wang",
            "Chen Chen",
            "Afshin Dehghan",
            "Yinfei Yang"
        ],
        "title": "AToken: A Unified Tokenizer for Vision",
        "abstract": "arXiv:2509.14476v1 Announce Type: new  Abstract: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.",
        "arxiv_id": "2509.14476",
        "ARXIVID": "2509.14476",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and criterion 2 (new VLLMs/MLLMs, as it enables multimodal LLMs). Introduces AToken, a unified visual tokenizer for images, videos, and 3D assets, supporting both generation and understanding tasks, and enabling next-generation multimodal AI systems.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2509.15212": {
        "authors": [
            "Yuming Jiang",
            "Siteng Huang",
            "Shengke Xue",
            "Yaxi Zhao",
            "Jun Cen",
            "Sicong Leng",
            "Kehan Li",
            "Jiayan Guo",
            "Kexiang Wang",
            "Mingxiu Chen",
            "Fan Wang",
            "Deli Zhao",
            "Xin Li"
        ],
        "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
        "abstract": "arXiv:2509.15212v1 Announce Type: new  Abstract: This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.",
        "arxiv_id": "2509.15212",
        "ARXIVID": "2509.15212",
        "COMMENT": "Matches criteria 2 and 3. Introduces RynnVLA-001, a vision-language-action model for robot manipulation, with a novel two-stage pretraining using large-scale video and human demonstrations, and a new ActionVAE for action representation. Shows strong empirical results in embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.15178": {
        "authors": [
            "Zaiquan Yang",
            "Yuhao Liu",
            "Gerhard Hancke",
            "Rynson W. H. Lau"
        ],
        "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding",
        "abstract": "arXiv:2509.15178v1 Announce Type: new  Abstract: Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
        "arxiv_id": "2509.15178",
        "ARXIVID": "2509.15178",
        "COMMENT": "Matches criteria 1 and 2. Presents a new zero-shot spatio-temporal video grounding method using MLLMs, with novel decomposed spatio-temporal highlighting and temporal-augmented assembling strategies. Shows empirical improvements and new insights into MLLM grounding behavior.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.14981": {
        "authors": [
            "Chuan Fang",
            "Heng Li",
            "Yixun Liang",
            "Jia Zheng",
            "Yongsen Mao",
            "Yuan Liu",
            "Rui Tang",
            "Zihan Zhou",
            "Ping Tan"
        ],
        "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
        "abstract": "arXiv:2509.14981v1 Announce Type: new  Abstract: Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.",
        "arxiv_id": "2509.14981",
        "ARXIVID": "2509.14981",
        "COMMENT": "Matches criterion 1 (spatial understanding for scene generation) and criterion 4 (vision foundation models and applications). Introduces a new large-scale dataset and a multi-modal diffusion model for layout-guided 3D indoor scene generation, with open-sourced data and models. Strong focus on spatial consistency and multi-modal generation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.14565": {
        "authors": [
            "Li Gao",
            "Hongyang Sun",
            "Liu Liu",
            "Yunhao Li",
            "Yang Cai"
        ],
        "title": "DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising",
        "abstract": "arXiv:2509.14565v1 Announce Type: new  Abstract: Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.",
        "arxiv_id": "2509.14565",
        "ARXIVID": "2509.14565",
        "COMMENT": "This paper introduces DiffVL, a diffusion-based visual localization method on 2D maps using BEV-conditioned GPS denoising. It directly matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (application of diffusion models in vision tasks). The approach is novel in treating GPS as a generative prior and using diffusion models for localization.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.15221": {
        "authors": [
            "Zhaoyang Liu",
            "JingJing Xie",
            "Zichen Ding",
            "Zehao Li",
            "Bowen Yang",
            "Zhenyu Wu",
            "Xuehui Wang",
            "Qiushi Sun",
            "Shi Liu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Qingyun Li",
            "Zeyue Tian",
            "Gen Luo",
            "Xiangyu Yue",
            "Biqing Qi",
            "Kai Chen",
            "Bowen Zhou",
            "Yu Qiao",
            "Qifeng Chen",
            "Wenhai Wang"
        ],
        "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data",
        "abstract": "arXiv:2509.15221v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.",
        "arxiv_id": "2509.15221",
        "ARXIVID": "2509.15221",
        "COMMENT": "Matches criteria 2 and 4. Presents ScaleCUA, a large-scale open-source dataset and model for vision-language computer use agents (CUAs) across multiple platforms, showing strong empirical results and advancing vision foundation model applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.14638": {
        "authors": [
            "Mingsong Li",
            "Lin Liu",
            "Hongjun Wang",
            "Haoxing Chen",
            "Xijun Gu",
            "Shizhan Liu",
            "Dong Gong",
            "Junbo Zhao",
            "Zhenzhong Lan",
            "Jianguo Li"
        ],
        "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks",
        "abstract": "arXiv:2509.14638v1 Announce Type: new  Abstract: Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.",
        "arxiv_id": "2509.14638",
        "ARXIVID": "2509.14638",
        "COMMENT": "Matches criteria 2 and 4. Introduces MultiEdit, a large dataset and pipeline for instruction-based image editing using MLLMs, and demonstrates improved performance on challenging editing tasks. Advances applications of vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.14739": {
        "authors": [
            "Jinlong Fan",
            "Bingyu Hu",
            "Xingguang Li",
            "Yuxiang Yang",
            "Jing Zhang"
        ],
        "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction",
        "abstract": "arXiv:2509.14739v1 Announce Type: new  Abstract: Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.",
        "arxiv_id": "2509.14739",
        "ARXIVID": "2509.14739",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new method for 3D monocular avatar reconstruction using mesh-guided 2D Gaussian splatting and distillation from vision foundation models, with a novel coordinated training strategy for multi-modal priors. Strong empirical results and clever optimization tricks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.14958": {
        "authors": [
            "Xiang Tuo",
            "Xu Xuemiao",
            "Liu Bangzhen",
            "Li Jinyi",
            "Li Yong",
            "He Shengfeng"
        ],
        "title": "Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification",
        "abstract": "arXiv:2509.14958v1 Announce Type: new  Abstract: The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP's intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings.",
        "arxiv_id": "2509.14958",
        "ARXIVID": "2509.14958",
        "COMMENT": "Matches criterion 4: Proposes a cross-modal geometric rectification framework leveraging CLIP for 3D few-shot class-incremental learning, applying vision foundation models to 3D recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15217": {
        "authors": [
            "Yue Xin",
            "Wenyuan Wang",
            "Rui Pan",
            "Ruida Wang",
            "Howard Meng",
            "Renjie Pi",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "title": "Generalizable Geometric Image Caption Synthesis",
        "abstract": "arXiv:2509.15217v1 Announce Type: new  Abstract: Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.",
        "arxiv_id": "2509.15217",
        "ARXIVID": "2509.15217",
        "COMMENT": "Matches criterion 2: Improves multimodal large language models (MLLMs) for geometric image captioning and reasoning, with a novel RL-based data synthesis pipeline.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.14966": {
        "authors": [
            "Xingwu Zhang",
            "Guanxuan Li",
            "Zhuocheng Zhang",
            "Zijun Long"
        ],
        "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching",
        "abstract": "arXiv:2509.14966v1 Announce Type: new  Abstract: The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.",
        "arxiv_id": "2509.14966",
        "ARXIVID": "2509.14966",
        "COMMENT": "Matches criterion 1. Proposes a new method for robotic object identification that augments 2D features with selective 3D geometric keypoint matching, improving spatial understanding for embodied agents in challenging real-world settings.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15224": {
        "authors": [
            "Luca Bartolomei",
            "Enrico Mannocci",
            "Fabio Tosi",
            "Matteo Poggi",
            "Stefano Mattoccia"
        ],
        "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation",
        "abstract": "arXiv:2509.15224v1 Announce Type: new  Abstract: Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.",
        "arxiv_id": "2509.15224",
        "ARXIVID": "2509.15224",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a cross-modal distillation paradigm using vision foundation models for event-based monocular depth estimation, including a novel recurrent architecture and competitive results without expensive annotations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.14977": {
        "authors": [
            "Chaoyin She",
            "Ruifang Lu",
            "Lida Chen",
            "Wei Wang",
            "Qinghua Huang"
        ],
        "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence",
        "abstract": "arXiv:2509.14977v1 Announce Type: new  Abstract: Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM.",
        "arxiv_id": "2509.14977",
        "ARXIVID": "2509.14977",
        "COMMENT": "Matches criterion 2: Introduces EchoVLM, a new vision-language model (VLM) with a mixture-of-experts architecture for ultrasound intelligence, showing a new VLLM and its application.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.15185": {
        "authors": [
            "Xiaoyu Yue",
            "Zidong Wang",
            "Yuqing Wang",
            "Wenlong Zhang",
            "Xihui Liu",
            "Wanli Ouyang",
            "Lei Bai",
            "Luping Zhou"
        ],
        "title": "Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation",
        "abstract": "arXiv:2509.15185v1 Announce Type: new  Abstract: Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.",
        "arxiv_id": "2509.15185",
        "ARXIVID": "2509.15185",
        "COMMENT": "Matches criterion 4: Proposes a new self-supervised training framework for autoregressive image generation, improving visual understanding in generative models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.14890": {
        "authors": [
            "Antoine Legrand",
            "Renaud Detry",
            "Christophe De Vleeschouwer"
        ],
        "title": "NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation",
        "abstract": "arXiv:2509.14890v1 Announce Type: new  Abstract: On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target. While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process. This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies. For this purpose, we train a NeRF-based image generator using the gradients back-propagated through the pose estimation network. This enforces the generator to render the main 3D features exploited by the spacecraft pose estimation network. Experiments demonstrate that our method recovers the relevant 3D cues. Furthermore, they offer additional insights on the relationship between the pose estimation network supervision and its implicit representation of the target spacecraft.",
        "arxiv_id": "2509.14890",
        "ARXIVID": "2509.14890",
        "COMMENT": "Matches criterion 1: Proposes a NeRF-based method to visualize 3D cues in data-driven spacecraft pose estimation, providing new insights into spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.14485": {
        "authors": [
            "Marko Tesic",
            "Yue Zhao",
            "Joel Z. Leibo",
            "Rakshit S. Trivedi",
            "Jose Hernandez-Orallo"
        ],
        "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations",
        "abstract": "arXiv:2509.14485v1 Announce Type: new  Abstract: The development and evaluation of social capabilities in AI agents require complex environments where competitive and cooperative behaviours naturally emerge. While game-theoretic properties can explain why certain teams or agent populations outperform others, more abstract behaviours, such as convention following, are harder to control in training and evaluation settings. The Melting Pot contest is a social AI evaluation suite designed to assess the cooperation capabilities of AI systems. In this paper, we apply a Bayesian approach known as Measurement Layouts to infer the capability profiles of multi-agent systems in the Melting Pot contest. We show that these capability profiles not only predict future performance within the Melting Pot suite but also reveal the underlying prosocial abilities of agents. Our analysis indicates that while higher prosocial capabilities sometimes correlate with better performance, this is not a universal trend-some lower-scoring agents exhibit stronger cooperation abilities. Furthermore, we find that top-performing contest submissions are more likely to achieve high scores in scenarios where prosocial capabilities are not required. These findings, together with reports that the contest winner used a hard-coded solution tailored to specific environments, suggest that at least one top-performing team may have optimised for conditions where cooperation was not necessary, potentially exploiting limitations in the evaluation framework. We provide recommendations for improving the annotation of cooperation demands and propose future research directions to account for biases introduced by different testing environments. Our results demonstrate that Measurement Layouts offer both strong predictive accuracy and actionable insights, contributing to a more transparent and generalisable approach to evaluating AI systems in complex social settings.",
        "arxiv_id": "2509.14485",
        "ARXIVID": "2509.14485",
        "COMMENT": "This paper analyzes prosocial ability profiles in multi-agent populations using the Melting Pot contest, and discusses limitations in current evaluation frameworks. It matches criterion 3 (embodied AI, new benchmarks, and methods with novel angles), as it critiques and proposes improvements to social AI evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.14989": {
        "authors": [
            "Benedikt Kolbeinsson",
            "Krystian Mikolajczyk"
        ],
        "title": "UCorr: Wire Detection and Depth Estimation for Autonomous Drones",
        "abstract": "arXiv:2509.14989v1 Announce Type: new  Abstract: In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.",
        "arxiv_id": "2509.14989",
        "ARXIVID": "2509.14989",
        "COMMENT": "Matches criterion 1: Presents a new monocular end-to-end model for wire segmentation and depth estimation for autonomous drones, improving spatial understanding for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15226": {
        "authors": [
            "Abhishek Basu",
            "Fahad Shamshad",
            "Ashshak Sharifdeen",
            "Karthik Nandakumar",
            "Muhammad Haris Khan"
        ],
        "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models",
        "abstract": "arXiv:2509.15226v1 Announce Type: new  Abstract: Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.",
        "arxiv_id": "2509.15226",
        "ARXIVID": "2509.15226",
        "COMMENT": "Matches criterion 2. Proposes CalibPrompt, a calibration-aware prompt learning framework for medical vision-language models (Med-VLMs), addressing confidence calibration in VLMs for medical imaging.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.14998": {
        "authors": [
            "Xiao Wu",
            "Ting-Zhu Huang",
            "Liang-Jian Deng",
            "Yanyuan Qiao",
            "Imran Razzak",
            "Yutong Xie"
        ],
        "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making",
        "abstract": "arXiv:2509.14998v1 Announce Type: new  Abstract: Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.",
        "arxiv_id": "2509.14998",
        "ARXIVID": "2509.14998",
        "COMMENT": "This paper proposes a knowledge-driven adaptive multi-agent collaboration framework for LLMs in medical decision-making. It is related to criterion 2 (multi-modal large language models) and criterion 3 (embodied AI, new methods for agent collaboration), as it introduces a novel multi-agent LLM framework with dynamic team formation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.14777": {
        "authors": [
            "Sunwoo Cho",
            "Yejin Jung",
            "Nam Ik Cho",
            "Jae Woong Soh"
        ],
        "title": "Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models",
        "abstract": "arXiv:2509.14777v1 Announce Type: new  Abstract: Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.",
        "arxiv_id": "2509.14777",
        "ARXIVID": "2509.14777",
        "COMMENT": "This paper proposes a new data distillation method for super-resolution that does not require class labels or pre-trained models. It uses CLIP features and diffusion models, which is relevant to criterion 4 (vision foundation models and applications), as CLIP is a vision foundation model and diffusion models are generative. The method is also novel in its approach to data efficiency.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.14685": {
        "authors": [
            "Kazuma Nagata",
            "Naoshi Kaneko"
        ],
        "title": "DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images",
        "abstract": "arXiv:2509.14685v1 Announce Type: new  Abstract: Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at https://github.com/kzmngt/DACoN.",
        "arxiv_id": "2509.14685",
        "ARXIVID": "2509.14685",
        "COMMENT": "Matches criterion 4: Uses foundation models for semantic feature extraction in anime colorization, showing an application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.15167": {
        "authors": [
            "Pak-Hei Yeung",
            "Jayroop Ramesh",
            "Pengfei Lyu",
            "Ana Namburete",
            "Jagath Rajapakse"
        ],
        "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model",
        "abstract": "arXiv:2509.15167v1 Announce Type: new  Abstract: This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.",
        "arxiv_id": "2509.15167",
        "ARXIVID": "2509.15167",
        "COMMENT": "This paper explores knowledge transfer from 2D vision models to 3D medical segmentation in a semi-supervised setting. While it leverages vision foundation models, the focus is on medical segmentation rather than spatial intelligence, VLLMs/MLLMs, or embodied AI. Some relevance to criterion 4.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15011": {
        "authors": [
            "Vasiliki Ismiroglou",
            "Malte Pedersen",
            "Stefan H. Bengtson",
            "Andreas Aakerberg",
            "Thomas B. Moeslund"
        ],
        "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation",
        "abstract": "arXiv:2509.15011v1 Announce Type: new  Abstract: In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.",
        "arxiv_id": "2509.15011",
        "ARXIVID": "2509.15011",
        "COMMENT": "This paper revisits the underwater image formation model and proposes a new synthetic data generation pipeline, including a new dataset. It is relevant to vision foundation models and their applications (criterion 4), as it improves data for training such models, but does not directly introduce a new foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.14975": {
        "authors": [
            "Xuanhua Yin",
            "Dingxin Zhang",
            "Yu Feng",
            "Shunqi Mao",
            "Jianhui Yu",
            "Weidong Cai"
        ],
        "title": "Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders",
        "abstract": "arXiv:2509.14975v1 Announce Type: new  Abstract: Existing rotation-invariant point cloud masked autoencoders (MAE) rely on random masking strategies that overlook geometric structure and semantic coherence. Random masking treats patches independently, failing to capture spatial relationships consistent across orientations and overlooking semantic object parts that maintain identity regardless of rotation. We propose a dual-stream masking approach combining 3D Spatial Grid Masking and Progressive Semantic Masking to address these fundamental limitations. Grid masking creates structured patterns through coordinate sorting to capture geometric relationships that persist across different orientations, while semantic masking uses attention-driven clustering to discover semantically meaningful parts and maintain their coherence during masking. These complementary streams are orchestrated via curriculum learning with dynamic weighting, progressing from geometric understanding to semantic discovery. Designed as plug-and-play components, our strategies integrate into existing rotation-invariant frameworks without architectural changes, ensuring broad compatibility across different approaches. Comprehensive experiments on ModelNet40, ScanObjectNN, and OmniObject3D demonstrate consistent improvements across various rotation scenarios, showing substantial performance gains over the baseline rotation-invariant methods.",
        "arxiv_id": "2509.14975",
        "ARXIVID": "2509.14975",
        "COMMENT": "Related to spatial understanding and point cloud processing, but focuses on rotation-invariant masked autoencoders. No direct connection to embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.14591": {
        "authors": [
            "Xuan Deng",
            "Xiandong Meng",
            "Longguang Wang",
            "Tiange Zhang",
            "Xiaopeng Fan",
            "Debin Zhao"
        ],
        "title": "Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression",
        "abstract": "arXiv:2509.14591v1 Announce Type: new  Abstract: Dynamic point clouds are widely used in applications such as immersive reality, robotics, and autonomous driving. Efficient compression largely depends on accurate motion estimation and compensation, yet the irregular structure and significant local variations of point clouds make this task highly challenging. Current methods often rely on explicit motion estimation, whose encoded vectors struggle to capture intricate dynamics and fail to fully exploit temporal correlations. To overcome these limitations, we introduce a Feature-aligned Motion Transformation (FMT) framework for dynamic point cloud compression. FMT replaces explicit motion vectors with a spatiotemporal alignment strategy that implicitly models continuous temporal variations, using aligned features as temporal context within a latent-space conditional encoding framework. Furthermore, we design a random access (RA) reference strategy that enables bidirectional motion referencing and layered encoding, thereby supporting frame-level parallel compression. Extensive experiments demonstrate that our method surpasses D-DPCC and AdaDPCC in both encoding and decoding efficiency, while also achieving BD-Rate reductions of 20% and 9.4%, respectively. These results highlight the effectiveness of FMT in jointly improving compression efficiency and processing performance.",
        "arxiv_id": "2509.14591",
        "ARXIVID": "2509.14591",
        "COMMENT": "Related to dynamic point cloud compression, which is relevant to spatial understanding, but does not directly address embodied agents or spatial intelligence in the context of embodied AI. No VLLM/MLLM or vision foundation model angle.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.14773": {
        "authors": [
            "Yuan Gao",
            "Wei Dong"
        ],
        "title": "A Real-Time Multi-Model Parametric Representation of Point Clouds",
        "abstract": "arXiv:2509.14773v1 Announce Type: new  Abstract: In recent years, parametric representations of point clouds have been widely applied in tasks such as memory-efficient mapping and multi-robot collaboration. Highly adaptive models, like spline surfaces or quadrics, are computationally expensive in detection or fitting. In contrast, real-time methods, such as Gaussian mixture models or planes, have low degrees of freedom, making high accuracy with few primitives difficult. To tackle this problem, a multi-model parametric representation with real-time surface detection and fitting is proposed. Specifically, the Gaussian mixture model is first employed to segment the point cloud into multiple clusters. Then, flat clusters are selected and merged into planes or curved surfaces. Planes can be easily fitted and delimited by a 2D voxel-based boundary description method. Surfaces with curvature are fitted by B-spline surfaces and the same boundary description method is employed. Through evaluations on multiple public datasets, the proposed surface detection exhibits greater robustness than the state-of-the-art approach, with 3.78 times improvement in efficiency. Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian mixture models, operating at 36.4 fps on a low-power onboard computer.",
        "arxiv_id": "2509.14773",
        "ARXIVID": "2509.14773",
        "COMMENT": "Related to spatial understanding and point cloud representation, but does not directly address embodied agents, VLLMs/MLLMs, or vision foundation models. Focuses on efficient parametric representation for mapping and robotics.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.14841": {
        "authors": [
            "Hongjun Wang",
            "Jiyuan Chen",
            "Zhengwei Yin",
            "Xuan Song",
            "Yinqiang Zheng"
        ],
        "title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution",
        "abstract": "arXiv:2509.14841v1 Announce Type: new  Abstract: Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.",
        "arxiv_id": "2509.14841",
        "ARXIVID": "2509.14841",
        "COMMENT": "Presents a targeted feature denoising framework for generalizable image super-resolution. While it is a clever statistical trick for vision, it does not directly match the criteria for spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.15208": {
        "authors": [
            "Pierre Fernandez",
            "Tom\\'a\\v{s} Sou\\v{c}ek",
            "Nikola Jovanovi\\'c",
            "Hady Elsahar",
            "Sylvestre-Alvise Rebuffi",
            "Valeriu Lacatusu",
            "Tuan Tran",
            "Alexandre Mourachko"
        ],
        "title": "Geometric Image Synchronization with Deep Watermarking",
        "abstract": "arXiv:2509.15208v1 Announce Type: new  Abstract: Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.",
        "arxiv_id": "2509.15208",
        "ARXIVID": "2509.15208",
        "COMMENT": "This paper introduces SyncSeal, a deep learning-based watermarking method for robust geometric image synchronization. While it is a vision paper with generative and spatial aspects, it does not directly address spatial intelligence in embodied agents or vision foundation models. It is more about image watermarking and synchronization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.14662": {
        "authors": [
            "Ming Li",
            "Nan Zhang",
            "Chenrui Fan",
            "Hong Jiao",
            "Yanbin Fu",
            "Sydney Peters",
            "Qingshu Xu",
            "Robert Lissitz",
            "Tianyi Zhou"
        ],
        "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory",
        "abstract": "arXiv:2509.14662v1 Announce Type: new  Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.",
        "arxiv_id": "2509.14662",
        "ARXIVID": "2509.14662",
        "COMMENT": "This paper introduces a new benchmark and framework for analyzing reasoning traces in large reasoning models, but it is not focused on spatial intelligence, VLLMs, or embodied AI. It is more about interpretability in reasoning models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.14546": {
        "authors": [
            "Zhilun Zhou",
            "Jing Yi Wang",
            "Nicholas Sukiennik",
            "Chen Gao",
            "Fengli Xu",
            "Yong Li",
            "James Evans"
        ],
        "title": "Rationality Check! Benchmarking the Rationality of Large Language Models",
        "abstract": "arXiv:2509.14546v1 Announce Type: new  Abstract: Large language models (LLMs), a recent advance in deep learning and machine intelligence, have manifested astonishing capacities, now considered among the most promising for artificial general intelligence. With human-like capabilities, LLMs have been used to simulate humans and serve as AI assistants across many applications. As a result, great concern has arisen about whether and under what circumstances LLMs think and behave like real human agents. Rationality is among the most important concepts in assessing human behavior, both in thinking (i.e., theoretical rationality) and in taking action (i.e., practical rationality). In this work, we propose the first benchmark for evaluating the omnibus rationality of LLMs, covering a wide range of domains and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental results, and analysis that illuminates where LLMs converge and diverge from idealized human rationality. We believe the benchmark can serve as a foundational tool for both developers and users of LLMs.",
        "arxiv_id": "2509.14546",
        "ARXIVID": "2509.14546",
        "COMMENT": "This paper introduces a new benchmark for evaluating the rationality of LLMs, which is related to criterion 3 (benchmarking for AI models), but it is not focused on embodied AI or spatial intelligence. It is more about language models' rationality.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15031": {
        "authors": [
            "Chau Pham",
            "Quan Dao",
            "Mahesh Bhosale",
            "Yunjie Tian",
            "Dimitris Metaxas",
            "David Doermann"
        ],
        "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing",
        "abstract": "arXiv:2509.15031v1 Announce Type: new  Abstract: Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \\textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.",
        "arxiv_id": "2509.15031",
        "ARXIVID": "2509.15031",
        "COMMENT": "Does not directly match any specific criterion, but is relevant to generative modeling and image editing with clever RL/statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.14609": {
        "authors": [
            "Weitong Wu",
            "Zhaohu Xing",
            "Jing Gong",
            "Qin Peng",
            "Lei Zhu"
        ],
        "title": "HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation",
        "abstract": "arXiv:2509.14609v1 Announce Type: new  Abstract: In the domain of 3D biomedical image segmentation, Mamba exhibits the superior performance for it addresses the limitations in modeling long-range dependencies inherent to CNNs and mitigates the abundant computational overhead associated with Transformer-based frameworks when processing high-resolution medical volumes. However, attaching undue importance to global context modeling may inadvertently compromise critical local structural information, thus leading to boundary ambiguity and regional distortion in segmentation outputs. Therefore, we propose the HybridMamba, an architecture employing dual complementary mechanisms: 1) a feature scanning strategy that progressively integrates representations both axial-traversal and local-adaptive pathways to harmonize the relationship between local and global representations, and 2) a gated module combining spatial-frequency analysis for comprehensive contextual modeling. Besides, we collect a multi-center CT dataset related to lung cancer. Experiments on MRI and CT datasets demonstrate that HybridMamba significantly outperforms the state-of-the-art methods in 3D medical image segmentation.",
        "arxiv_id": "2509.14609",
        "ARXIVID": "2509.14609",
        "COMMENT": "Related to computer vision and deep learning, but focuses on 3D medical image segmentation with a new architecture. Does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.14830": {
        "authors": [
            "Alvaro Lopez Pellicer",
            "Andre Mariucci",
            "Plamen Angelov",
            "Marwan Bukhari",
            "Jemma G. Kerns"
        ],
        "title": "ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification",
        "abstract": "arXiv:2509.14830v1 Announce Type: new  Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.",
        "arxiv_id": "2509.14830",
        "ARXIVID": "2509.14830",
        "COMMENT": "This paper introduces a multi-modal, explainable prototype-based model for bone health classification. While it is multi-modal and explainable, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models in a way that matches the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.14839": {
        "authors": [
            "Miriam Louise Carnot",
            "Jonas Kunze",
            "Erik Fastermann",
            "Eric Peukert",
            "Andr\\'e Ludwig",
            "Bogdan Franczyk"
        ],
        "title": "MapAnything: Mapping Urban Assets using Single Street-View Images",
        "abstract": "arXiv:2509.14839v1 Announce Type: new  Abstract: To maintain an overview of urban conditions, city administrations manage databases of objects like traffic signs and trees, complete with their geocoordinates. Incidents such as graffiti or road damage are also relevant. As digitization increases, so does the need for more data and up-to-date databases, requiring significant manual effort. This paper introduces MapAnything, a module that automatically determines the geocoordinates of objects using individual images. Utilizing advanced Metric Depth Estimation models, MapAnything calculates geocoordinates based on the object's distance from the camera, geometric principles, and camera specifications. We detail and validate the module, providing recommendations for automating urban object and incident mapping. Our evaluation measures the accuracy of estimated distances against LiDAR point clouds in urban environments, analyzing performance across distance intervals and semantic areas like roads and vegetation. The module's effectiveness is demonstrated through practical use cases involving traffic signs and road damage.",
        "arxiv_id": "2509.14839",
        "ARXIVID": "2509.14839",
        "COMMENT": "This paper proposes a module for mapping urban assets from single street-view images using metric depth estimation. While it involves spatial reasoning, it is more of an application paper and does not introduce new methods for spatial intelligence in embodied agents or new benchmarks.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.14619": {
        "authors": [
            "Feng Ding",
            "Haisheng Fu",
            "Soroush Oraki",
            "Jie Liang"
        ],
        "title": "LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition",
        "abstract": "arXiv:2509.14619v1 Announce Type: new  Abstract: Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: https://github.com/xiaobaoxia/LSTC-MDA.",
        "arxiv_id": "2509.14619",
        "ARXIVID": "2509.14619",
        "COMMENT": "This paper proposes a unified framework for temporal convolution and data augmentation in skeleton-based action recognition. While it is a vision paper, it does not directly address spatial intelligence in embodied agents, VLLMs, or vision foundation models. It is more about action recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.14927": {
        "authors": [
            "Tan-Hiep To",
            "Duy-Khang Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation",
        "abstract": "arXiv:2509.14927v1 Announce Type: new  Abstract: Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping consumer perceptions and enhancing brand credibility. However, collaborating with human KOLs often involves high costs and logistical challenges. To address this, we present GenKOL, an interactive system that empowers marketing professionals to efficiently generate high-quality virtual KOL images using generative AI. GenKOL enables users to dynamically compose promotional visuals through an intuitive interface that integrates multiple AI capabilities, including garment generation, makeup transfer, background synthesis, and hair editing. These capabilities are implemented as modular, interchangeable services that can be deployed flexibly on local machines or in the cloud. This modular architecture ensures adaptability across diverse use cases and computational environments. Our system can significantly streamline the production of branded content, lowering costs and accelerating marketing workflows through scalable virtual KOL creation.",
        "arxiv_id": "2509.14927",
        "ARXIVID": "2509.14927",
        "COMMENT": "This paper presents a modular generative AI framework for creating virtual KOLs, focusing on marketing applications. While it uses generative AI and modularity, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models as per the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}