{
    "2511.03334": {
        "authors": [
            "Guozhen Zhang",
            "Zixiang Zhou",
            "Teng Hu",
            "Ziqiao Peng",
            "Youliang Zhang",
            "Yi Chen",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions",
        "abstract": "arXiv:2511.03334v1 Announce Type: new  Abstract: Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.",
        "arxiv_id": "2511.03334",
        "ARXIVID": "2511.03334",
        "COMMENT": "Matches criterion 2 (MLLMs): Proposes UniAVGen, a unified audio-video generation model with novel cross-modal interaction mechanisms and a foundation model approach for multi-modal generation tasks. Strong focus on generative modeling and cross-modal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.03126": {
        "authors": [
            "Hongbo Lan",
            "Zhenlin An",
            "Haoyu Li",
            "Vaibhav Singh",
            "Longfei Shangguan"
        ],
        "title": "Accelerating Physical Property Reasoning for Augmented Visual Cognition",
        "abstract": "arXiv:2511.03126v1 Announce Type: new  Abstract: This paper introduces \\sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \\sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \\sysname reduces the end-to-end latency of this reasoning pipeline from 10--20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \\sysname achieves this 62.9$\\times$--287.2$\\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \\sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \\sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.",
        "arxiv_id": "2511.03126",
        "ARXIVID": "2511.03126",
        "COMMENT": "Matches criterion 1: Presents a system for accelerated physical property reasoning in augmented visual cognition, with optimizations for spatial understanding in embodied agents. Also relevant to criterion 3 (novel method for embodied AI).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.03178": {
        "authors": [
            "Shreyas C. Dhake",
            "Jiayuan Huang",
            "Runlong He",
            "Danyal Z. Khan",
            "Evangelos B. Mazomenos",
            "Sophia Bano",
            "Hani J. Marcus",
            "Danail Stoyanov",
            "Matthew J. Clarkson",
            "Mobarak I. Hoque"
        ],
        "title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention",
        "abstract": "arXiv:2511.03178v1 Announce Type: new  Abstract: Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.",
        "arxiv_id": "2511.03178",
        "ARXIVID": "2511.03178",
        "COMMENT": "Matches criterion 2: Proposes a new video-language model (SurgAnt-ViVQA) for surgical event anticipation, adapting a large language model with temporal cross-attention. Also introduces a new benchmark (criterion 3).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.03163": {
        "authors": [
            "Yun-Chen Lin",
            "Jiayuan Huang",
            "Hanyuan Zhang",
            "Sergi Kavtaradze",
            "Matthew J. Clarkson",
            "Mobarak I. Hoque"
        ],
        "title": "Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation",
        "abstract": "arXiv:2511.03163v1 Announce Type: new  Abstract: Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.",
        "arxiv_id": "2511.03163",
        "ARXIVID": "2511.03163",
        "COMMENT": "Proposes a depth-guided liver landmark segmentation framework using vision foundation encoders (SAM2, DA2) and a novel low-rank gradient projection method. This is a strong match for criterion 4 (vision foundation models and applications), and also involves spatial understanding (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.03325": {
        "authors": [
            "Mauro Orazio Drago",
            "Luca Carlini",
            "Pelinsu Celebi Balyemez",
            "Dennis Pierantozzi",
            "Chiara Lena",
            "Cesare Hassan",
            "Danail Stoyanov",
            "Elena De Momi",
            "Sophia Bano",
            "Mobarak I. Hoque"
        ],
        "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding",
        "abstract": "arXiv:2511.03325v1 Announce Type: new  Abstract: Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\\% on REAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.",
        "arxiv_id": "2511.03325",
        "ARXIVID": "2511.03325",
        "COMMENT": "Proposes a new temporally-grounded video question answering model for surgical scene understanding, using a masked video-text encoder and a new dataset. This is a new method and benchmark for embodied AI with a novel angle (temporal reasoning in surgical video QA). Closely matches criterion 3.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.03666": {
        "authors": [
            "Dongkeun Kim",
            "Minsu Cho",
            "Suha Kwak"
        ],
        "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection",
        "abstract": "arXiv:2511.03666v1 Announce Type: new  Abstract: Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.",
        "arxiv_id": "2511.03666",
        "ARXIVID": "2511.03666",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents): Proposes a part-aware bottom-up group reasoning framework for fine-grained social interaction detection, focusing on spatial cues and subtle social signals, which previous work ignored. Strong methodological improvement in spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02946": {
        "authors": [
            "Srikumar Sastry",
            "Subash Khanal",
            "Aayush Dhakal",
            "Jiayu Lin",
            "Dan Cher",
            "Phoenix Jarosz",
            "Nathan Jacobs"
        ],
        "title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology",
        "abstract": "arXiv:2511.02946v1 Announce Type: new  Abstract: We introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at https://vishu26.github.io/prom3e.",
        "arxiv_id": "2511.02946",
        "ARXIVID": "2511.02946",
        "COMMENT": "Matches criterion 2 (MLLMs): Proposes a new probabilistic masked multimodal embedding model (ProM3E) for any-to-any generation and cross-modal retrieval, with a focus on representation learning and modality fusion. The model is general-purpose and supports modality inversion, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.03156": {
        "authors": [
            "Sagar Shrestha",
            "Gopal Sharma",
            "Luowei Zhou",
            "Suren Kumar"
        ],
        "title": "Finetuning-Free Personalization of Text to Image Generation via Hypernetworks",
        "abstract": "arXiv:2511.03156v1 Announce Type: new  Abstract: Personalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.",
        "arxiv_id": "2511.03156",
        "ARXIVID": "2511.03156",
        "COMMENT": "Matches criterion 4: Proposes a new method for text-to-image personalization using hypernetworks, relevant to vision foundation models and generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.03255": {
        "authors": [
            "Nikolina Tomic Roshni Bhatnagar",
            "Sarthak Jain",
            "Connor Lau",
            "Tien-Yu Liu",
            "Laura Gambini",
            "Rima Arnaout"
        ],
        "title": "Generative deep learning for foundational video translation in ultrasound",
        "abstract": "arXiv:2511.03255v1 Announce Type: new  Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.",
        "arxiv_id": "2511.03255",
        "ARXIVID": "2511.03255",
        "COMMENT": "Presents a generative deep learning method for foundational video translation in ultrasound, demonstrating generalization across clinical domains. This is relevant to vision foundation models and their applications (criterion 4), and also involves generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.03459": {
        "authors": [
            "Kevin Manogue",
            "Tomasz M Schang",
            "Dilara Ku\\c{s}",
            "Jonas M\\\"uller",
            "Stefan Zachow",
            "Agniva Sengupta"
        ],
        "title": "Generalizing Shape-from-Template to Topological Changes",
        "abstract": "arXiv:2511.03459v1 Announce Type: new  Abstract: Reconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.",
        "arxiv_id": "2511.03459",
        "ARXIVID": "2511.03459",
        "COMMENT": "Extends shape-from-template methods to handle topological changes, which is a novel methodological improvement in spatial understanding. Closely matches criterion 1.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02953": {
        "authors": [
            "Sadiq Layi Macaulay",
            "Nimet Kaygusuz",
            "Simon Hadfield"
        ],
        "title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation",
        "abstract": "arXiv:2511.02953v1 Announce Type: new  Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.",
        "arxiv_id": "2511.02953",
        "ARXIVID": "2511.02953",
        "COMMENT": "Introduces a large and diverse event-based dataset for depth estimation, which is relevant to embodied AI benchmarks and vision foundation model applications. Closely matches criterion 3 (new benchmark for embodied AI/simulator-related) and is also relevant to criterion 4.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.03332": {
        "authors": [
            "Yi Yang",
            "Yiming Xu",
            "Timo Kaiser",
            "Hao Cheng",
            "Bodo Rosenhahn",
            "Michael Ying Yang"
        ],
        "title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge",
        "abstract": "arXiv:2511.03332v1 Announce Type: new  Abstract: In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.",
        "arxiv_id": "2511.03332",
        "ARXIVID": "2511.03332",
        "COMMENT": "Matches criterion 2: Uses LLaVA-Video (a multi-modal large language model) for multi-object tracking retrieval in a challenge setting.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.03725": {
        "authors": [
            "Jongseo Lee",
            "Wooil Lee",
            "Gyeong-Moon Park",
            "Seong Tae Kim",
            "Jinwoo Choi"
        ],
        "title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition",
        "abstract": "arXiv:2511.03725v1 Announce Type: new  Abstract: Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.",
        "arxiv_id": "2511.03725",
        "ARXIVID": "2511.03725",
        "COMMENT": "Matches criterion 4: Proposes a concept-based explainable video action recognition framework (DANCE) that leverages large language models for object/scene concept extraction, relevant to vision foundation models and their interpretability.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.03099": {
        "authors": [
            "Yiyi Miao",
            "Taoyu Wu",
            "Tong Chen",
            "Sihao Li",
            "Ji Jiang",
            "Youpeng Yang",
            "Angelos Stefanidis",
            "Limin Yu",
            "Jionglong Su"
        ],
        "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs",
        "abstract": "arXiv:2511.03099v1 Announce Type: new  Abstract: In orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.",
        "arxiv_id": "2511.03099",
        "ARXIVID": "2511.03099",
        "COMMENT": "Matches criterion 4: Applies 3D Gaussian Splatting (a vision foundation model technique) to dental occlusion novel view synthesis, with new methods for sparse-view reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.03267": {
        "authors": [
            "Bingyang Guo",
            "Hongjie Li",
            "Ruiyun Yu",
            "Hanzhe Liang",
            "Jinbao Wang"
        ],
        "title": "IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection",
        "abstract": "arXiv:2511.03267v1 Announce Type: new  Abstract: 3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.",
        "arxiv_id": "2511.03267",
        "ARXIVID": "2511.03267",
        "COMMENT": "Matches criterion 3: introduces a new 3D point cloud anomaly detection dataset (IEC3D-AD) for industrial equipment components, and proposes a novel 3D-AD paradigm (GMANet) with a generative approach and spatial discrepancy optimization. This is a new benchmark and method for embodied AI in industrial settings.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.03070": {
        "authors": [
            "Drago Plecko",
            "Patrik Okanovic",
            "Torsten Hoefler",
            "Elias Bareinboim"
        ],
        "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge",
        "abstract": "arXiv:2511.03070v1 Announce Type: new  Abstract: Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., \"what is the capital of England?\"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., \"what is the sex of a computer science graduate in the US?\"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.",
        "arxiv_id": "2511.03070",
        "ARXIVID": "2511.03070",
        "COMMENT": "Related to criterion 2 (LLM evaluation): Proposes a new benchmark to test LLMs' knowledge of real-world probability distributions, showing surprising empirical results about their limitations. This is insightful for understanding LLMs' capabilities, though not directly about VLLMs or MLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.03093": {
        "authors": [
            "Yi Gong",
            "Xinyuan Zhang",
            "Jichen Chai",
            "Yichen Ding",
            "Yifei Lou"
        ],
        "title": "A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction",
        "abstract": "arXiv:2511.03093v1 Announce Type: new  Abstract: Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.",
        "arxiv_id": "2511.03093",
        "ARXIVID": "2511.03093",
        "COMMENT": "This paper proposes a new computational imaging framework for volumetric light-sheet image reconstruction, which is a methodological improvement in spatial understanding for biological imaging. Closely matches criterion 1 (new methodological improvements to spatial understanding).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.03194": {
        "authors": [
            "Le Xue",
            "Gang Feng",
            "Wenbo Zhang",
            "Yichi Zhang",
            "Lanlan Li",
            "Shuqi Wang",
            "Liling Peng",
            "Sisi Peng",
            "Xin Gao"
        ],
        "title": "PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research",
        "abstract": "arXiv:2511.03194v1 Announce Type: new  Abstract: Publicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed Tomography (PET/CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.",
        "arxiv_id": "2511.03194",
        "ARXIVID": "2511.03194",
        "COMMENT": "Matches criterion 4: Introduces a new multi-modal medical imaging dataset (PET/CT + reports) for research in multi-modal learning and vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.03053": {
        "authors": [
            "Ziyang Xu",
            "Olaf Wysocki",
            "Christoph Holst"
        ],
        "title": "From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth",
        "abstract": "arXiv:2511.03053v1 Announce Type: new  Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds' uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.",
        "arxiv_id": "2511.03053",
        "ARXIVID": "2511.03053",
        "COMMENT": "Presents a learning-based framework for evaluating uncertainty in MLS point clouds, which relates to spatial understanding and intelligence. Closely matches criterion 1.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.03098": {
        "authors": [
            "Miftahur Rahman",
            "Samuel Adebayo",
            "Dorian A. Acevedo-Mejia",
            "David Hester",
            "Daniel McPolin",
            "Karen Rafferty",
            "Debra F. Laefer"
        ],
        "title": "ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly",
        "abstract": "arXiv:2511.03098v1 Announce Type: new  Abstract: The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.",
        "arxiv_id": "2511.03098",
        "ARXIVID": "2511.03098",
        "COMMENT": "Somewhat related to criterion 3 (benchmark/simulator): Introduces a new hybrid dataset for object detection in steel assembly robotics, with a focus on efficient dataset creation and benchmarking for construction-robotics perception. The dataset blends synthetic and real data for improved training.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.03179": {
        "authors": [
            "Varun Kumar",
            "George Em Karniadakis"
        ],
        "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework",
        "abstract": "arXiv:2511.03179v1 Announce Type: new  Abstract: The engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.",
        "arxiv_id": "2511.03179",
        "ARXIVID": "2511.03179",
        "COMMENT": "Somewhat related to criterion 2 (LLMs as agents): Describes a multi-agent framework for engineering design using LLMs for knowledge graph construction and design iteration. Interesting for multi-agent LLM applications, but not directly about VLLMs or MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.03245": {
        "authors": [
            "Liwei Luo",
            "Shuaitengyuan Li",
            "Dongwei Ren",
            "Qilong Wang",
            "Pengfei Zhu",
            "Qinghua Hu"
        ],
        "title": "Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning",
        "abstract": "arXiv:2511.03245v1 Announce Type: new  Abstract: Recently, remarkable progress has been made in large-scale pre-trained model tuning, and inference efficiency is becoming more crucial for practical deployment. Early exiting in conjunction with multi-stage predictors, when cooperated with a parameter-efficient fine-tuning strategy, offers a straightforward way to achieve an inference-efficient model. However, a key challenge remains unresolved: How can early stages provide low-level fundamental features to deep stages while simultaneously supplying high-level discriminative features to early-stage predictors? To address this problem, we propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively decouple the low-level representative ability and high-level discriminative ability in early stages. First, in terms of architecture, we introduce a lightweight bypass module into multi-stage predictors for functional decomposition of shallow features from early stages, while a high-order statistics-based predictor is developed for early stages to effectively enhance their discriminative ability. To reasonably train our multi-predictor architecture, a decoupled optimization is proposed to allocate two-phase loss weights for multi-stage predictors during model tuning, where the initial training phase enables the model to prioritize the acquisition of discriminative ability of deep stages via emphasizing representative ability of early stages, and the latter training phase drives discriminative ability towards earlier stages as much as possible. As such, our DMPO can effectively decouple representative and discriminative abilities in early stages in terms of architecture design and model optimization. Experiments across various datasets and pre-trained backbones demonstrate that DMPO clearly outperforms its counterparts when reducing computational cost.",
        "arxiv_id": "2511.03245",
        "ARXIVID": "2511.03245",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models): Proposes a new optimization method for inference-efficient model tuning, which could be relevant for foundation models but is more about model efficiency than new vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.03004": {
        "authors": [
            "Dakota Hester",
            "Vitor S. Martins",
            "Lucas B. Ferreira",
            "Thainara M. A. Lima"
        ],
        "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning",
        "abstract": "arXiv:2511.03004v1 Announce Type: new  Abstract: Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the \"Bootstrap Your Own Latent\" pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.",
        "arxiv_id": "2511.03004",
        "ARXIVID": "2511.03004",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models): Uses self-supervised learning (BYOL) for label-efficient land cover classification at high spatial resolution. Focus is on application and efficiency, not new foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.03232": {
        "authors": [
            "Sichen Guo",
            "Wenjie Li",
            "Yuanyang Liu",
            "Guangwei Gao",
            "Jian Yang",
            "Chia-Wen Lin"
        ],
        "title": "Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution",
        "abstract": "arXiv:2511.03232v1 Announce Type: new  Abstract: Recently, Mamba-based super-resolution (SR) methods have demonstrated the ability to capture global receptive fields with linear complexity, addressing the quadratic computational cost of Transformer-based SR approaches. However, existing Mamba-based methods lack fine-grained transitions across different modeling scales, which limits the efficiency of feature representation. In this paper, we propose T-PMambaSR, a lightweight SR framework that integrates window-based self-attention with Progressive Mamba. By enabling interactions among receptive fields of different scales, our method establishes a fine-grained modeling paradigm that progressively enhances feature representation with linear complexity. Furthermore, we introduce an Adaptive High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost during Transformer and Mamba processing. Extensive experiments demonstrate that T-PMambaSR progressively enhances the model's receptive field and expressiveness, yielding better performance than recent Transformer- or Mamba-based methods while incurring lower computational cost. Our codes will be released after acceptance.",
        "arxiv_id": "2511.03232",
        "ARXIVID": "2511.03232",
        "COMMENT": "Presents a lightweight image super-resolution framework combining Transformer and Mamba architectures, but does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Some relevance to vision foundation model applications.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.03212": {
        "authors": [
            "Ruting Cheng",
            "Boyuan Feng",
            "Yijiang Zheng",
            "Chuhui Qiu",
            "Aizierjiang Aiersilan",
            "Joaquin A. Calderon",
            "Wentao Zhao",
            "Qing Pan",
            "James K. Hahn"
        ],
        "title": "MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction",
        "abstract": "arXiv:2511.03212v1 Announce Type: new  Abstract: Accurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model's predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model's decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.",
        "arxiv_id": "2511.03212",
        "ARXIVID": "2511.03212",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models): Proposes a multi-view transformer for 3D body scan analysis, but the focus is on a specific medical application (cesarean section prediction) rather than general vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02923": {
        "authors": [
            "Ivan Zvonkov",
            "Gabriel Tseng",
            "Inbal Becker-Reshef",
            "Hannah Kerner"
        ],
        "title": "Cropland Mapping using Geospatial Embeddings",
        "abstract": "arXiv:2511.02923v1 Announce Type: new  Abstract: Accurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.",
        "arxiv_id": "2511.02923",
        "ARXIVID": "2511.02923",
        "COMMENT": "Somewhat related to criterion 4 (vision foundation models): Applies geospatial embeddings (from Presto and AlphaEarth) for cropland mapping, showing their utility in real-world mapping. Focus is on application rather than new methodology.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2511.03106": {
        "authors": [
            "Katherine C. Kellogg",
            "Bingyang Ye",
            "Yifan Hu",
            "Guergana K. Savova",
            "Byron Wallace",
            "Danielle S. Bitterman"
        ],
        "title": "Large language models require a new form of oversight: capability-based monitoring",
        "abstract": "arXiv:2511.03106v1 Announce Type: new  Abstract: The rapid adoption of large language models (LLMs) in healthcare has been accompanied by scrutiny of their oversight. Existing monitoring approaches, inherited from traditional machine learning (ML), are task-based and founded on assumed performance degradation arising from dataset drift. In contrast, with LLMs, inevitable model degradation due to changes in populations compared to the training dataset cannot be assumed, because LLMs were not trained for any specific task in any given population. We therefore propose a new organizing principle guiding generalist LLM monitoring that is scalable and grounded in how these models are developed and used in practice: capability-based monitoring. Capability-based monitoring is motivated by the fact that LLMs are generalist systems whose overlapping internal capabilities are reused across numerous downstream tasks. Instead of evaluating each downstream task independently, this approach organizes monitoring around shared model capabilities, such as summarization, reasoning, translation, or safety guardrails, in order to enable cross-task detection of systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss. We describe considerations for developers, organizational leaders, and professional societies for implementing a capability-based monitoring approach. Ultimately, capability-based monitoring will provide a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist artificial intelligence models in healthcare.",
        "arxiv_id": "2511.03106",
        "ARXIVID": "2511.03106",
        "COMMENT": "This paper discusses a new monitoring paradigm for LLMs in healthcare, but does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}