{
    "2511.18746": {
        "authors": [
            "Hao Li",
            "Qiao Sun"
        ],
        "title": "Any4D: Open-Prompt 4D Generation from Natural Language and Images",
        "abstract": "arXiv:2511.18746v1 Announce Type: new  Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \\textit{\"GPT moment\"} in the embodied domain. There is a naive observation: \\textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \\textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \\textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \\textit{2) reduces} learning complexity, \\textit{3) improves} data efficiency in embodied data collection, and \\textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.",
        "arxiv_id": "2511.18746",
        "ARXIVID": "2511.18746",
        "COMMENT": "Matches criterion 1 and 3: Proposes a new method for spatial understanding in embodied agents (Primitive Embodied World Models) and introduces a novel approach to embodied world modeling with a modular VLM planner and compositional generalization. Also leverages vision-language models for embodied intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.19057": {
        "authors": [
            "Hai Wu",
            "Shuai Tang",
            "Jiale Wang",
            "Longkun Zou",
            "Mingyue Guo",
            "Rongqin Liang",
            "Ke Chen",
            "Yaowei Wang"
        ],
        "title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space",
        "abstract": "arXiv:2511.19057v1 Announce Type: new  Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.",
        "arxiv_id": "2511.19057",
        "ARXIVID": "2511.19057",
        "COMMENT": "Directly matches criterion 3: introduces a new benchmark (LAA3D) for 3D detection and tracking of low-altitude aircraft, with a large-scale dataset and a new monocular 3D detection baseline. The sim-to-real generalization and unified evaluation protocols are novel angles.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18333": {
        "authors": [
            "Xuanke Shi",
            "Boxuan Li",
            "Xiaoyang Han",
            "Zhongang Cai",
            "Lei Yang",
            "Dahua Lin",
            "Quan Wang"
        ],
        "title": "ConsistCompose: Unified Multimodal Layout Control for Image Composition",
        "abstract": "arXiv:2511.18333v1 Announce Type: new  Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.",
        "arxiv_id": "2511.18333",
        "ARXIVID": "2511.18333",
        "COMMENT": "Directly matches criterion 2 and 4: introduces a unified multimodal framework (ConsistCompose) for layout-controllable image generation, embedding layout coordinates into language prompts for spatial control, and provides a large-scale dataset. This is a new VLLM/MLLM approach with strong spatial intelligence and vision foundation model application.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18011": {
        "authors": [
            "Jun Zhang",
            "Jie Feng",
            "Long Chen",
            "Junhui Wang",
            "Zhicheng Liu",
            "Depeng Jin",
            "Yong Li"
        ],
        "title": "RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios",
        "abstract": "arXiv:2511.18011v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.",
        "arxiv_id": "2511.18011",
        "ARXIVID": "2511.18011",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI) and criterion 2 (evaluates MLLMs on spatial reasoning). RoadBench is a systematic benchmark for fine-grained spatial understanding and reasoning in urban road scenarios, revealing shortcomings in current MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.17792": {
        "authors": [
            "Dingrui Wang",
            "Hongyuan Ye",
            "Zhihao Liang",
            "Zhexiao Sun",
            "Zhaowei Lu",
            "Yuchen Zhang",
            "Yuyu Zhao",
            "Yuan Gao",
            "Marvin Seegert",
            "Finn Sch\\\"afer",
            "Haotong Qin",
            "Wei Li",
            "Luigi Palmieri",
            "Felix Jahncke",
            "Mattia Piccinini",
            "Johannes Betz"
        ],
        "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?",
        "abstract": "arXiv:2511.17792v1 Announce Type: new  Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.",
        "arxiv_id": "2511.17792",
        "ARXIVID": "2511.17792",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark for mapless path planning with world models). Introduces a new benchmark (Target-Bench) for evaluating world models in mapless path planning with semantic targets, and provides surprising empirical results about current model limitations.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18823": {
        "authors": [
            "Fufangchen Zhao",
            "Liao Zhang",
            "Daiqi Shi",
            "Yuanjun Gao",
            "Chen Ye",
            "Yang Cai",
            "Jian Gao",
            "Danfeng Yan"
        ],
        "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models",
        "abstract": "arXiv:2511.18823v1 Announce Type: new  Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.",
        "arxiv_id": "2511.18823",
        "ARXIVID": "2511.18823",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes VideoPerceiver, a novel video multimodal large language model (VMLLM) with a new training framework for fine-grained temporal perception in video understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18173": {
        "authors": [
            "Enrico Pallotta",
            "Sina Mokhtarzadeh Azar",
            "Lars Doorenbos",
            "Serdar Ozsoy",
            "Umar Iqbal",
            "Juergen Gall"
        ],
        "title": "EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses",
        "abstract": "arXiv:2511.18173v1 Announce Type: new  Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.",
        "arxiv_id": "2511.18173",
        "ARXIVID": "2511.18173",
        "COMMENT": "Matches criterion 1 (spatial understanding on embodied agents) and criterion 3 (embodied AI, new methods for egocentric video generation with pose control). Introduces a pose-controllable video diffusion model for egocentric video generation, enabling controllable embodied video simulation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18305": {
        "authors": [
            "Raja Kumar",
            "Arka Sadhu",
            "Ram Nevatia"
        ],
        "title": "DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition",
        "abstract": "arXiv:2511.18305v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\\textbf{DiVE-k}$, $\\textbf{Di}$fferential $\\textbf{V}$isual r$\\textbf{E}$asoning using top-$\\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.",
        "arxiv_id": "2511.18305",
        "ARXIVID": "2511.18305",
        "COMMENT": "Directly matches criterion 2 (new VLLMs/MLLMs). Proposes DiVE-k, a new method for fine-grained image recognition using large vision-language models, with a novel RL-based training signal for differential reasoning. Shows significant empirical improvements and generalization.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.17750": {
        "authors": [
            "Zhimin Shao",
            "Abhay Yadav",
            "Rama Chellappa",
            "Cheng Peng"
        ],
        "title": "SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration",
        "abstract": "arXiv:2511.17750v1 Announce Type: new  Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.",
        "arxiv_id": "2511.17750",
        "ARXIVID": "2511.17750",
        "COMMENT": "Directly matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes SPIDER, a universal feature matching framework that leverages vision foundation models for robust spatial correspondence, and introduces a new benchmark for unconstrained scenarios.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18507": {
        "authors": [
            "Kai Jiang",
            "Siqi Huang",
            "Xiangyu Chen",
            "Jiawei Shao",
            "Hongyuan Zhang",
            "Xuelong Li"
        ],
        "title": "Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives",
        "abstract": "arXiv:2511.18507v1 Announce Type: new  Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.",
        "arxiv_id": "2511.18507",
        "ARXIVID": "2511.18507",
        "COMMENT": "Directly matches criterion 2: focuses on continual learning in Multimodal Large Language Models (MLLMs), introduces a new dataset (MSVQA) and a new method (UNIFIER) for mitigating catastrophic forgetting across scenarios. Strong relevance to MLLMs and multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.18005": {
        "authors": [
            "Shengyuan Wang",
            "Zhiheng Zheng",
            "Yu Shang",
            "Lixuan He",
            "Yangcheng Yu",
            "Fan Hangyu",
            "Jie Feng",
            "Qingmin Liao",
            "Yong Li"
        ],
        "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
        "abstract": "arXiv:2511.18005v1 Announce Type: new  Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \\textbf{R}eality-\\textbf{A}ligned \\textbf{I}ntelligent \\textbf{S}ynthesis \\textbf{E}ngine that creates detailed, \\textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.",
        "arxiv_id": "2511.18005",
        "ARXIVID": "2511.18005",
        "COMMENT": "Matches criterion 3 (embodied AI, new agentic framework for city-scale 3D world generation) and criterion 4 (vision foundation models and applications). RAISECity introduces a multimodal agent framework for reality-aligned 3D world generation, with strong empirical results and a novel agentic design.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.18817": {
        "authors": [
            "Siyuan Wei",
            "Chunjie Wang",
            "Xiao Liu",
            "Xiaosheng Yan",
            "Zhishan Zhou",
            "Rui Huang"
        ],
        "title": "Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring",
        "abstract": "arXiv:2511.18817v1 Announce Type: new  Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.",
        "arxiv_id": "2511.18817",
        "ARXIVID": "2511.18817",
        "COMMENT": "Matches criterion 2 (3D MLLMs) and criterion 3 (new dataset/benchmark for embodied AI). Proposes a fully automated pipeline for high-quality 3D dialog data, addressing ambiguities in 3D scene understanding, and demonstrates empirical improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.17839": {
        "authors": [
            "Yujiang Pu",
            "Zhanbo Huang",
            "Vishnu Boddeti",
            "Yu Kong"
        ],
        "title": "Show Me: Unifying Instructional Image and Video Generation with Diffusion Models",
        "abstract": "arXiv:2511.17839v1 Announce Type: new  Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.",
        "arxiv_id": "2511.17839",
        "ARXIVID": "2511.17839",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a unified framework for instructional image and video generation using diffusion models, with structure and motion consistency rewards, and demonstrates benefits for world simulators and action-object state transformation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.17932": {
        "authors": [
            "Yan Xu",
            "Yixing Wang",
            "Stella X. Yu"
        ],
        "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion",
        "abstract": "arXiv:2511.17932v1 Announce Type: new  Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \\emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \\emph{completing a natural video} unfolding through space.   We recast the task as \\emph{test-time natural video completion}, using powerful priors from \\emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \\emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \\emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \\emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.   The result is coherent, high-fidelity renderings from sparse inputs \\emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.",
        "arxiv_id": "2511.17932",
        "ARXIVID": "2511.17932",
        "COMMENT": "This paper proposes a novel method for sparse-input novel view synthesis using pretrained video diffusion models and 3D Gaussian Splatting, which is a methodological improvement in spatial understanding and generative modeling (criterion 1, and partially 4). The iterative feedback loop between 3D geometry and 2D synthesis is a clever trick.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.17735": {
        "authors": [
            "Samuel Stevens",
            "Jacob Beattie",
            "Tanya Berger-Wolf",
            "Yu Su"
        ],
        "title": "Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders",
        "abstract": "arXiv:2511.17735v1 Announce Type: new  Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.",
        "arxiv_id": "2511.17735",
        "ARXIVID": "2511.17735",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Explores open-ended scientific discovery using sparse autoencoders on vision foundation model representations, with a focus on unsupervised feature discovery and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.18082": {
        "authors": [
            "Wencheng Ye",
            "Tianshi Wang",
            "Lei Zhu",
            "Fengling Li",
            "Guoli Yang"
        ],
        "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models",
        "abstract": "arXiv:2511.18082v1 Announce Type: new  Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.",
        "arxiv_id": "2511.18082",
        "ARXIVID": "2511.18082",
        "COMMENT": "Matches criterion 1 and 3: proposes a new method (ActDistill) for efficient vision-language-action models for embodied agents, focusing on action-guided distillation and efficient embodied intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.19004": {
        "authors": [
            "Wentao Qu",
            "Guofeng Mei",
            "Yang Wu",
            "Yongshun Gong",
            "Xiaoshui Huang",
            "Liang Xiao"
        ],
        "title": "A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation",
        "abstract": "arXiv:2511.19004v1 Announce Type: new  Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.",
        "arxiv_id": "2511.19004",
        "ARXIVID": "2511.19004",
        "COMMENT": "Matches criterion 3: proposes a new Text-to-LiDAR diffusion model and introduces a new benchmark (T2nuScenes) for text-to-LiDAR scene generation, with a focus on controllability and prompt analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.18050": {
        "authors": [
            "Tian Ye",
            "Song Fei",
            "Lei Zhu"
        ],
        "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
        "abstract": "arXiv:2511.18050v1 Announce Type: new  Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.",
        "arxiv_id": "2511.18050",
        "ARXIVID": "2511.18050",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes UltraFlux, a new data-model co-design for high-quality 4K text-to-image generation, with several novel technical components and strong empirical results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.19062": {
        "authors": [
            "Qiyang Yu",
            "Yu Fang",
            "Tianrui Li",
            "Xuemei Cao",
            "Yan Chen",
            "Jianghao Li",
            "Fan Min",
            "Yi Zhang"
        ],
        "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation",
        "abstract": "arXiv:2511.19062v1 Announce Type: new  Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.",
        "arxiv_id": "2511.19062",
        "ARXIVID": "2511.19062",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding in segmentation) and criterion 4 (vision foundation models, as it builds on SAM). Proposes a granular computing-driven, prompt-free segmentation method with multi-granularity attention, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18873": {
        "authors": [
            "Yiming Wang",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Siyu Tang"
        ],
        "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction",
        "abstract": "arXiv:2511.18873v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.",
        "arxiv_id": "2511.18873",
        "ARXIVID": "2511.18873",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes Neural Texture Splatting, a new expressive 3D Gaussian Splatting method for view synthesis, geometry, and dynamic reconstruction, with a global neural field for improved generalization and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18570": {
        "authors": [
            "Samarth Chopra",
            "Jing Liang",
            "Gershom Seneviratne",
            "Dinesh Manocha"
        ],
        "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation",
        "abstract": "arXiv:2511.18570v1 Announce Type: new  Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.",
        "arxiv_id": "2511.18570",
        "ARXIVID": "2511.18570",
        "COMMENT": "Matches criterion 1 (spatial intelligence on embodied agents) and criterion 4 (vision foundation models and applications). Introduces PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting for dense physical property estimation, unifying 3D reconstruction, uncertainty modeling, and physical reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.17965": {
        "authors": [
            "Yangyang Liu",
            "Yuhao Wang",
            "Pingping Zhang"
        ],
        "title": "Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification",
        "abstract": "arXiv:2511.17965v1 Announce Type: new  Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.",
        "arxiv_id": "2511.17965",
        "ARXIVID": "2511.17965",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a new multi-modal object re-identification framework with selective interaction and global-local alignment, and demonstrates effectiveness on several benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18208": {
        "authors": [
            "Ahmed Gomaa",
            "Annette Schwarz",
            "Ludwig Singer",
            "Arnd D\\\"orfler",
            "Matthias Stefan May",
            "Pluvio Stephan",
            "Ishita Sheth",
            "Juliane Szkitsak",
            "Katharina Breininger",
            "Yixing Huang",
            "Benjamin Frey",
            "Oliver Schnell",
            "Daniel Delev",
            "Roland Coras",
            "Daniel H\\\"ofler",
            "Philipp Schubert",
            "Jenny Stritzelberger",
            "Sabine Semrau",
            "Andreas Maier",
            "Dieter H Heiland",
            "Udo S. Gaipl",
            "Andrea Wittig",
            "Rainer Fietkau",
            "Christoph Bert",
            "Stefanie Corradini",
            "Florian Putz"
        ],
        "title": "Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI",
        "abstract": "arXiv:2511.18208v1 Announce Type: new  Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.",
        "arxiv_id": "2511.18208",
        "ARXIVID": "2511.18208",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications). Uses large-scale self-supervised pre-training of a Vision Transformer for multi-modal medical imaging, showing strong empirical results and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.19199": {
        "authors": [
            "Teodora Popordanoska",
            "Jiameng Li",
            "Matthew B. Blaschko"
        ],
        "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
        "abstract": "arXiv:2511.19199v1 Announce Type: new  Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
        "arxiv_id": "2511.19199",
        "ARXIVID": "2511.19199",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (CLASH) for cross-modal contradiction detection, focusing on a novel angle (contradiction detection) that previous multimodal benchmarks ignored.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18701": {
        "authors": [
            "Mustafa Munir",
            "Harsh Goel",
            "Xiwen Wei",
            "Minkyu Choi",
            "Sahil Shah",
            "Kartikeya Bhardwaj",
            "Paul Whatmough",
            "Sandeep Chinchali",
            "Radu Marculescu"
        ],
        "title": "ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction",
        "abstract": "arXiv:2511.18701v1 Announce Type: new  Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed \"consistent\" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.",
        "arxiv_id": "2511.18701",
        "ARXIVID": "2511.18701",
        "COMMENT": "Matches criterion 4: Uses vision foundation models (CLIP, SAM) in a neuro-symbolic framework for object consistency verification and correction in video editing. Introduces a novel neuro-symbolic verifier and adaptive frame repair.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.19071": {
        "authors": [
            "Fangda Chen",
            "Jintao Tang",
            "Pancheng Wang",
            "Ting Wang",
            "Shasha Li",
            "Ting Deng"
        ],
        "title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation",
        "abstract": "arXiv:2511.19071v1 Announce Type: new  Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.",
        "arxiv_id": "2511.19071",
        "ARXIVID": "2511.19071",
        "COMMENT": "Matches criterion 4: Applies and extends the Segment Anything Model (SAM, a vision foundation model) to 3D medical image segmentation with novel decoder and auto-prompting modules to address spatial feature loss and manual prompt limitations.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.18136": {
        "authors": [
            "Chunming He",
            "Rihan Zhang",
            "Longxiang Tang",
            "Ziyun Yang",
            "Kai Li",
            "Deng-Ping Fan",
            "Sina Farsiu"
        ],
        "title": "SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation",
        "abstract": "arXiv:2511.18136v1 Announce Type: new  Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \\textbf{Phase \\uppercase\\expandafter{\\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \\textbf{Phase \\uppercase\\expandafter{\\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.",
        "arxiv_id": "2511.18136",
        "ARXIVID": "2511.18136",
        "COMMENT": "Matches criterion 4: Vision foundation models (SAM) and its application to label-deficient concealed object segmentation. Also introduces a collaborative training paradigm that enhances both lightweight segmenters and large foundation models under label-scarce conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.18028": {
        "authors": [
            "Chenyu Li",
            "Danfeng Hong",
            "Bing Zhang",
            "Zhaojie Pan",
            "Naoto Yokoya",
            "Jocelyn Chanussot"
        ],
        "title": "MambaX: Image Super-Resolution with State Predictive Control",
        "abstract": "arXiv:2511.18028v1 Announce Type: new  Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \\textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.",
        "arxiv_id": "2511.18028",
        "ARXIVID": "2511.18028",
        "COMMENT": "Proposes MambaX, a nonlinear state predictive control model for image super-resolution, introducing a new paradigm for multimodal SR fusion and dynamic spectrum-state representation. This is a methodological improvement in spatial understanding and multi-modal learning (criterion 1).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.17979": {
        "authors": [
            "Bo Yin",
            "Xiaobin Hu",
            "Xingyu Zhou",
            "Peng-Tao Jiang",
            "Yue Liao",
            "Junwei Zhu",
            "Jiangning Zhang",
            "Ying Tai",
            "Chengjie Wang",
            "Shuicheng Yan"
        ],
        "title": "FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning",
        "abstract": "arXiv:2511.17979v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.",
        "arxiv_id": "2511.17979",
        "ARXIVID": "2511.17979",
        "COMMENT": "Presents a new frequency-driven fine-tuning framework (FeRA) for diffusion models, which is a clever statistical trick for generative modeling and adaptation (criterion 4). The frequency energy mechanism and routing are novel.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.18870": {
        "authors": [
            "Bing Wu",
            "Chang Zou",
            "Changlin Li",
            "Duojun Huang",
            "Fang Yang",
            "Hao Tan",
            "Jack Peng",
            "Jianbing Wu",
            "Jiangfeng Xiong",
            "Jie Jiang",
            "Linus",
            "Patrol",
            "Peizhen Zhang",
            "Peng Chen",
            "Penghao Zhao",
            "Qi Tian",
            "Songtao Liu",
            "Weijie Kong",
            "Weiyan Wang",
            "Xiao He",
            "Xin Li",
            "Xinchi Deng",
            "Xuefei Zhe",
            "Yang Li",
            "Yanxin Long",
            "Yuanbo Peng",
            "Yue Wu",
            "Yuhong Liu",
            "Zhenyu Wang",
            "Zuozhuo Dai",
            "Bo Peng",
            "Coopers Li",
            "Gu Gong",
            "Guojian Xiao",
            "Jiahe Tian",
            "Jiaxin Lin",
            "Jie Liu",
            "Jihong Zhang",
            "Jiesong Lian",
            "Kaihang Pan",
            "Lei Wang",
            "Lin Niu",
            "Mingtao Chen",
            "Mingyang Chen",
            "Mingzhe Zheng",
            "Miles Yang",
            "Qiangqiang Hu",
            "Qi Yang",
            "Qiuyong Xiao",
            "Runzhou Wu",
            "Ryan Xu",
            "Rui Yuan",
            "Shanshan Sang",
            "Shisheng Huang",
            "Siruis Gong",
            "Shuo Huang",
            "Weiting Guo",
            "Xiang Yuan",
            "Xiaojia Chen",
            "Xiawei Hu",
            "Wenzhi Sun",
            "Xiele Wu",
            "Xianshun Ren",
            "Xiaoyan Yuan",
            "Xiaoyue Mi",
            "Yepeng Zhang",
            "Yifu Sun",
            "Yiting Lu",
            "Yitong Li",
            "You Huang",
            "Yu Tang",
            "Yixuan Li",
            "Yuhang Deng",
            "Yuan Zhou",
            "Zhichao Hu",
            "Zhiguang Liu",
            "Zhihe Yang",
            "Zilin Yang",
            "Zhenzhi Lu",
            "Zixiang Zhou",
            "Zhao Zhong"
        ],
        "title": "HunyuanVideo 1.5 Technical Report",
        "abstract": "arXiv:2511.18870v1 Announce Type: new  Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
        "arxiv_id": "2511.18870",
        "ARXIVID": "2511.18870",
        "COMMENT": "This is a new open-source video generation model (HunyuanVideo 1.5) with a compact architecture and state-of-the-art performance, relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.18396": {
        "authors": [
            "Jinhao Li",
            "Sarah M. Erfani",
            "Lei Feng",
            "James Bailey",
            "Feng Liu"
        ],
        "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification",
        "abstract": "arXiv:2511.18396v1 Announce Type: new  Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.",
        "arxiv_id": "2511.18396",
        "ARXIVID": "2511.18396",
        "COMMENT": "This paper explores weak-to-strong generalization for CLIP-based classification, extending a known concept from language models to vision-language models (criterion 2). It proposes a new method (CPL) for improving CLIP classification under weak supervision.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.17967": {
        "authors": [
            "Hao Li",
            "Yuhao Wang",
            "Xiantao Hu",
            "Wenning Hao",
            "Pingping Zhang",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking",
        "abstract": "arXiv:2511.17967v1 Announce Type: new  Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.",
        "arxiv_id": "2511.17967",
        "ARXIVID": "2511.17967",
        "COMMENT": "This paper proposes a new method for robust RGBT tracking using contextual aggregation and deformable alignment, which is a methodological improvement in spatial understanding for embodied agents (criterion 1). The use of Mamba-based feature interaction and deformable alignment is novel for cross-modal tracking.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.18399": {
        "authors": [
            "Yuxiang Nie",
            "Han Wang",
            "Yongjie Ye",
            "Haiyang Yu",
            "Weitao Jia",
            "Tao Zeng",
            "Hao Feng",
            "Xiang Fei",
            "Yang Li",
            "Xiaohui Lv",
            "Guozhi Tang",
            "Jingqun Tang",
            "Jinghui Lu",
            "Zehui Dai",
            "Jiacong Wang",
            "Dingkang Yang",
            "An-Lan Wang",
            "Can Huang"
        ],
        "title": "ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering",
        "abstract": "arXiv:2511.18399v1 Announce Type: new  Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.",
        "arxiv_id": "2511.18399",
        "ARXIVID": "2511.18399",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmark for MLLMs in video QA). Introduces ChineseVideoBench, a new benchmark for evaluating MLLMs on Chinese video question answering, with a focus on nuanced linguistic and cultural awareness.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.18786": {
        "authors": [
            "Junyang Chen",
            "Jiangxin Dong",
            "Long Sun",
            "Yixin Yang",
            "Jinshan Pan"
        ],
        "title": "STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution",
        "abstract": "arXiv:2511.18786v1 Announce Type: new  Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.",
        "arxiv_id": "2511.18786",
        "ARXIVID": "2511.18786",
        "COMMENT": "Presents a new video super-resolution framework using a pre-trained video diffusion model, with novel techniques for temporal and structural consistency. While it uses diffusion models (related to vision foundation models), it does not directly address spatial intelligence or embodied agents, nor does it introduce a new VLLM/MLLM. Closest to criterion 4.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.18591": {
        "authors": [
            "Wei Dong",
            "Han Zhou",
            "Junwei Lin",
            "Jun Chen"
        ],
        "title": "Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation",
        "abstract": "arXiv:2511.18591v1 Announce Type: new  Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.",
        "arxiv_id": "2511.18591",
        "ARXIVID": "2511.18591",
        "COMMENT": "Matches criterion 4: leverages vision-language model (VLM) priors for image restoration, showing an application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.18822": {
        "authors": [
            "Zhennan Chen",
            "Junwei Zhu",
            "Xu Chen",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Hanzhen Zhao",
            "Chengjie Wang",
            "Jian Yang",
            "Ying Tai"
        ],
        "title": "DiP: Taming Diffusion Models in Pixel Space",
        "abstract": "arXiv:2511.18822v1 Announce Type: new  Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\\times$256.",
        "arxiv_id": "2511.18822",
        "ARXIVID": "2511.18822",
        "COMMENT": "Does not directly match any specific criterion. Proposes a new pixel-space diffusion model (DiP) for efficient image generation, which is relevant to generative modeling in vision but not specifically to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 7
    },
    "2511.18254": {
        "authors": [
            "Siyi Li",
            "Qingwen Zhang",
            "Ishan Khatri",
            "Kyle Vedder",
            "Deva Ramanan",
            "Neehar Peri"
        ],
        "title": "UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization",
        "abstract": "arXiv:2511.18254v1 Announce Type: new  Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.",
        "arxiv_id": "2511.18254",
        "ARXIVID": "2511.18254",
        "COMMENT": "Relevant to computer vision and generalization in LiDAR scene flow, but does not directly match any specific criterion (not about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.17876": {
        "authors": [
            "Mukul Singh",
            "Ananya Singha",
            "Aishni Parab",
            "Pronita Mehrotra",
            "Sumit Gulwani"
        ],
        "title": "Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models",
        "abstract": "arXiv:2511.17876v1 Announce Type: new  Abstract: Associative thinking--the ability to connect seemingly unrelated ideas--is a foundational element of human creativity and problem-solving. This paper explores whether reinforcement learning (RL) guided by associative thinking principles can enhance a model's performance across diverse generative tasks, including story writing, code generation, and chart creation. We introduce a reinforcement learning framework that uses a prompt-based evaluation mechanism, incorporating established divergent thinking metrics from creativity research. A base language model is fine-tuned using this framework to reward outputs demonstrating higher novelty through higher degrees of conceptual connectivity. Interestingly, the experimental results suggest that RL-based associative thinking-trained models not only generate more original and coherent stories but also exhibit improved abstraction and flexibility in tasks such as programming and data visualization. Our findings provide initial evidence that modeling cognitive creativity principles through reinforcement learning can yield more adaptive and generative AI.",
        "arxiv_id": "2511.17876",
        "ARXIVID": "2511.17876",
        "COMMENT": "Does not directly match any specific criterion. Focuses on reinforcement learning for creative thinking in language models, which is interesting for generative modeling but not specifically about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19351": {
        "authors": [
            "Abdurahman Ali Mohammed",
            "Catherine Fonder",
            "Ying Wei",
            "Wallapak Tavanapong",
            "Donald S Sakaguchi",
            "Qi Li",
            "Surya K. Mallapragada"
        ],
        "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting",
        "abstract": "arXiv:2511.19351v1 Announce Type: new  Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.",
        "arxiv_id": "2511.19351",
        "ARXIVID": "2511.19351",
        "COMMENT": "Presents a new large-scale fluorescence microscopy dataset and benchmarks for cell counting, including an adaptation of the Segment Anything Model (SAM). While relevant to computer vision, it does not directly match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.17824": {
        "authors": [
            "Pranay Meshram",
            "Yash Turkar",
            "Kartikeya Singh",
            "Praveen Raj Masilamani",
            "Charuvahan Adhivarahan",
            "Karthik Dantu"
        ],
        "title": "QAL: A Loss for Recall Precision Balance in 3D Reconstruction",
        "abstract": "arXiv:2511.17824v1 Announce Type: new  Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.   Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.   QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines",
        "arxiv_id": "2511.17824",
        "ARXIVID": "2511.17824",
        "COMMENT": "Proposes a new loss function for 3D reconstruction, improving recall-precision balance. While relevant to 3D vision, it does not directly match the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18788": {
        "authors": [
            "Shiyi Mu",
            "Zichong Gu",
            "Zhiqi Ai",
            "Anqi Liu",
            "Yilin Gao",
            "Shugong Xu"
        ],
        "title": "StereoDETR: Stereo-based Transformer for 3D Object Detection",
        "abstract": "arXiv:2511.18788v1 Announce Type: new  Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.",
        "arxiv_id": "2511.18788",
        "ARXIVID": "2511.18788",
        "COMMENT": "Presents a new stereo-based transformer for 3D object detection, improving speed and accuracy. While relevant to computer vision, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.17619": {
        "authors": [
            "Qinghao Meng",
            "Junbo Yin",
            "Jianbing Shen",
            "Yunde Jia"
        ],
        "title": "Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds",
        "abstract": "arXiv:2511.17619v1 Announce Type: new  Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.",
        "arxiv_id": "2511.17619",
        "ARXIVID": "2511.17619",
        "COMMENT": "Addresses 3D object detection from point clouds with a new corner-aware regression strategy, but does not directly match any of the four criteria. It is relevant to computer vision and 3D understanding, but not specifically to spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18089": {
        "authors": [
            "Wenjing Liu",
            "Qin Ren",
            "Wen Zhang",
            "Yuewei Lin",
            "Chenyu You"
        ],
        "title": "Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective",
        "abstract": "arXiv:2511.18089v1 Announce Type: new  Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.",
        "arxiv_id": "2511.18089",
        "ARXIVID": "2511.18089",
        "COMMENT": "Presents a new min-max optimization framework for multi-modal survival analysis, focusing on balancing alignment and distinctiveness. While it is a multi-modal method, it does not introduce a new VLLM/MLLM or foundation model, nor is it focused on spatial intelligence or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19320": {
        "authors": [
            "Jiaming Zhang",
            "Shengming Cao",
            "Rui Li",
            "Xiaotong Zhao",
            "Yutao Cui",
            "Xinglin Hou",
            "Gangshan Wu",
            "Haolan Chen",
            "Yu Xu",
            "Limin Wang",
            "Kai Ma"
        ],
        "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
        "abstract": "arXiv:2511.19320v1 Announce Type: new  Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
        "arxiv_id": "2511.19320",
        "ARXIVID": "2511.19320",
        "COMMENT": "Proposes a new framework for human image animation with first-frame preservation. While it is a novel method in image-to-video generation, it does not directly match any of the four criteria, though it is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18090": {
        "authors": [
            "Mingwei He",
            "Tongda Xu",
            "Xingtong Ge",
            "Ming Sun",
            "Chao Zhou",
            "Yan Wang"
        ],
        "title": "Versatile Recompression-Aware Perceptual Image Super-Resolution",
        "abstract": "arXiv:2511.18090v1 Announce Type: new  Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.",
        "arxiv_id": "2511.18090",
        "ARXIVID": "2511.18090",
        "COMMENT": "This paper proposes a new method for perceptual image super-resolution that is aware of downstream recompression, using a diffusion model as a codec simulator. This is a novel angle in image restoration, but does not directly match any of the four criteria, though it is tangentially related to vision foundation models (criterion 4).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18105": {
        "authors": [
            "Purvish Jajal",
            "Nick John Eliopoulos",
            "Benjamin Shiue-Hal Chou",
            "George K. Thiruvathukal",
            "Yung-Hsiang Lu",
            "James C. Davis"
        ],
        "title": "AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens",
        "abstract": "arXiv:2511.18105v1 Announce Type: new  Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.",
        "arxiv_id": "2511.18105",
        "ARXIVID": "2511.18105",
        "COMMENT": "Related to transformer adaptivity and efficient computation in vision models, but does not directly match any of the four criteria. No explicit focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18656": {
        "authors": [
            "Harrison Bagley",
            "Will Meakin",
            "Simon Lucey",
            "Yee Wei Law",
            "Tat-Jun Chin"
        ],
        "title": "Robust Physical Adversarial Patches Using Dynamically Optimized Clusters",
        "abstract": "arXiv:2511.18656v1 Announce Type: new  Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.",
        "arxiv_id": "2511.18656",
        "ARXIVID": "2511.18656",
        "COMMENT": "Related to adversarial robustness in vision, but does not directly match any of the four criteria. Focuses on physical adversarial patches and scale-resilient structures.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19434": {
        "authors": [
            "Yasin Esfandiari",
            "Stefan Bauer",
            "Sebastian U. Stich",
            "Andrea Dittadi"
        ],
        "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
        "abstract": "arXiv:2511.19434v1 Announce Type: new  Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
        "arxiv_id": "2511.19434",
        "ARXIVID": "2511.19434",
        "COMMENT": "Relevant to generative modeling and diffusion models, but does not directly match any of the four criteria. Proposes a plug-and-play method for merging pretrained diffusion experts.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18942": {
        "authors": [
            "Zong-Wei Hong",
            "Jing-lun Li",
            "Lin-Ze Li",
            "Shen Zhang",
            "Yao Tang"
        ],
        "title": "VeCoR - Velocity Contrastive Regularization for Flow Matching",
        "abstract": "arXiv:2511.18942v1 Announce Type: new  Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.   To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both \"where to go\" and \"where not to go.\" To be formal, we propose \\textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.   On ImageNet-1K 256$\\times$256, VeCoR yields 22\\% and 35\\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/",
        "arxiv_id": "2511.18942",
        "ARXIVID": "2511.18942",
        "COMMENT": "Relevant to generative modeling in computer vision, but does not directly match any of the four criteria. It proposes a new regularization for flow-based generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18888": {
        "authors": [
            "Qian Jiang",
            "Qianqian Wang",
            "Xin Jin",
            "Michal Wozniak",
            "Shaowen Yao",
            "Wei Zhou"
        ],
        "title": "MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model",
        "abstract": "arXiv:2511.18888v1 Announce Type: new  Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.",
        "arxiv_id": "2511.18888",
        "ARXIVID": "2511.18888",
        "COMMENT": "Does not directly match any specific criterion. Proposes a multi-function network for panchromatic image resolution restoration, which is a vision model but not specifically about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18976": {
        "authors": [
            "Huaming Ling",
            "Ying Wang",
            "Si Chen",
            "Junfeng Fan"
        ],
        "title": "Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs",
        "abstract": "arXiv:2511.18976v1 Announce Type: new  Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.",
        "arxiv_id": "2511.18976",
        "ARXIVID": "2511.18976",
        "COMMENT": "Does not directly match any specific criterion. Focuses on FHE inference for CNNs, which is relevant to efficient vision model deployment but not to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18763": {
        "authors": [
            "Xuanzhao Dong",
            "Wenhui Zhu",
            "Yujian Xiong",
            "Xiwen Chen",
            "Hao Wang",
            "Xin Li",
            "Jiajun Cheng",
            "Zhipeng Wang",
            "Shao Tang",
            "Oana Dumitrascu",
            "Yalin Wang"
        ],
        "title": "VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement",
        "abstract": "arXiv:2511.18763v1 Announce Type: new  Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT",
        "arxiv_id": "2511.18763",
        "ARXIVID": "2511.18763",
        "COMMENT": "Does not directly match any specific criterion. Proposes a vessel-aware optimal transport method for retinal fundus enhancement, which is an application of computer vision but not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18436": {
        "authors": [
            "Hao Shen",
            "Jikang Cheng",
            "Renye Yan",
            "Zhongyuan Wang",
            "Wei Peng",
            "Baojin Huang"
        ],
        "title": "When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection",
        "abstract": "arXiv:2511.18436v1 Announce Type: new  Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.",
        "arxiv_id": "2511.18436",
        "ARXIVID": "2511.18436",
        "COMMENT": "Does not directly match any specific criterion. Focuses on generative replay for incremental face forgery detection, which is related to generative modeling but not specifically to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19183": {
        "authors": [
            "Carsten T. L\\\"uth",
            "Jeremias Traub",
            "Kim-Celine Kahl",
            "Till J. Bungert",
            "Lukas Klein",
            "Lars Kr\\\"amer",
            "Paul F. Jaeger",
            "Fabian Isensee",
            "Klaus Maier-Hein"
        ],
        "title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation",
        "abstract": "arXiv:2511.19183v1 Announce Type: new  Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive",
        "arxiv_id": "2511.19183",
        "ARXIVID": "2511.19183",
        "COMMENT": "Relevant to active learning and 3D biomedical segmentation, but does not match any specific criterion (not about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18858": {
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Xinyue Li",
            "Wengang Zhou",
            "Hongsheng Li",
            "Houqiang Li"
        ],
        "title": "Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling",
        "abstract": "arXiv:2511.18858v1 Announce Type: new  Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.",
        "arxiv_id": "2511.18858",
        "ARXIVID": "2511.18858",
        "COMMENT": "Does not match any specific criterion. Focuses on dataset distillation for long-tailed distributions, which is not directly about spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19024": {
        "authors": [
            "Long Tang",
            "Guoquan Zhen",
            "Jie Hao",
            "Jianbo Zhang",
            "Huiyu Duan",
            "Liang Yuan",
            "Guangtao Zhai"
        ],
        "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling",
        "abstract": "arXiv:2511.19024v1 Announce Type: new  Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.",
        "arxiv_id": "2511.19024",
        "ARXIVID": "2511.19024",
        "COMMENT": "Related to vision foundation models and image quality assessment, but does not directly match any of the four criteria. Focuses on BIQA with GCN and MoE.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}