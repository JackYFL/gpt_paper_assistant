{
    "2508.14893": {
        "authors": [
            "Qinhong Zhou",
            "Hongxin Zhang",
            "Xiangye Lin",
            "Zheyuan Zhang",
            "Yutian Chen",
            "Wenjun Liu",
            "Zunzhe Zhang",
            "Sunli Chen",
            "Lixing Fang",
            "Qiushi Lyu",
            "Xinyu Sun",
            "Jincheng Yang",
            "Zeyuan Wang",
            "Bao Chi Dang",
            "Zhehuan Chen",
            "Daksha Ladia",
            "Jiageng Liu",
            "Chuang Gan"
        ],
        "title": "Virtual Community: An Open World for Humans, Robots, and Society",
        "abstract": "arXiv:2508.14893v1 Announce Type: new  Abstract: The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.",
        "arxiv_id": "2508.14893",
        "ARXIVID": "2508.14893",
        "COMMENT": "Matches criterion 3: Introduces Virtual Community, a new open-world, multi-agent physics simulator for studying embodied social intelligence, human-robot interaction, and community planning. Proposes new challenges and benchmarks for embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2508.14153": {
        "authors": [
            "Lianghui Zhu",
            "Bin Ouyang",
            "Yuxuan Zhang",
            "Tianheng Cheng",
            "Rui Hu",
            "Haocheng Shen",
            "Longjin Ran",
            "Xiaoxin Chen",
            "Li Yu",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "title": "LENS: Learning to Segment Anything with Unified Reinforced Reasoning",
        "abstract": "arXiv:2508.14153v1 Announce Type: new  Abstract: Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at https://github.com/hustvl/LENS.",
        "arxiv_id": "2508.14153",
        "ARXIVID": "2508.14153",
        "COMMENT": "Matches criteria 2 and 4: introduces LENS, a new method for text-prompted image segmentation using a large vision-language model (Qwen2.5-VL-3B-Instruct) and reinforcement learning for chain-of-thought reasoning, improving generalization and segmentation quality.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.14891": {
        "authors": [
            "Licheng Shen",
            "Saining Zhang",
            "Honghan Li",
            "Peilin Yang",
            "Zihao Huang",
            "Zongzheng Zhang",
            "Hao Zhao"
        ],
        "title": "GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects",
        "abstract": "arXiv:2508.14891v1 Announce Type: new  Abstract: Reconstructing articulated objects is essential for building digital twins of interactive environments. However, prior methods typically decouple geometry and motion by first reconstructing object shape in distinct states and then estimating articulation through post-hoc alignment. This separation complicates the reconstruction pipeline and restricts scalability, especially for objects with complex, multi-part articulation. We introduce a unified representation that jointly models geometry and motion using articulated 3D Gaussians. This formulation improves robustness in motion decomposition and supports articulated objects with up to 20 parts, significantly outperforming prior approaches that often struggle beyond 2--3 parts due to brittle initialization. To systematically assess scalability and generalization, we propose MPArt-90, a new benchmark consisting of 90 articulated objects across 20 categories, each with diverse part counts and motion configurations. Extensive experiments show that our method consistently achieves superior accuracy in part-level geometry reconstruction and motion estimation across a broad range of object types. We further demonstrate applicability to downstream tasks such as robotic simulation and human-scene interaction modeling, highlighting the potential of unified articulated representations in scalable physical modeling.",
        "arxiv_id": "2508.14891",
        "ARXIVID": "2508.14891",
        "COMMENT": "Matches criterion 3 (new benchmark and method for articulated object modeling in embodied AI). Proposes a unified geometry-motion representation and introduces a new benchmark (MPArt-90) for articulated objects, with applications to robotic simulation and human-scene interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.14527": {
        "authors": [
            "Jiangfan Liu",
            "Yongkang Guo",
            "Fangzhi Zhong",
            "Tianyuan Zhang",
            "Zonglei Jing",
            "Siyuan Liang",
            "Jiakai Wang",
            "Mingchuan Zhang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "title": "Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles",
        "abstract": "arXiv:2508.14527v1 Announce Type: new  Abstract: The generation of safety-critical scenarios in simulation has become increasingly crucial for safety evaluation in autonomous vehicles prior to road deployment in society. However, current approaches largely rely on predefined threat patterns or rule-based strategies, which limit their ability to expose diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a framework that can generate plentiful safety-critical scenarios by reasoning novel adversarial cases and then amplifying them with complex traffic flows. Given a simple prompt of a benign scene, it first performs Meta-Scenario Generation, where a large language model, grounded in structured driving knowledge, infers an adversarial agent whose behavior poses a threat that is both plausible and deliberately challenging. This meta-scenario is then specified in executable code for precise in-simulator control. Subsequently, Complex Scenario Evolution uses background vehicles to amplify the core threat introduced by Meta-Scenario. It builds an adversarial collaborator graph to identify key agent trajectories for optimization. These perturbations are designed to simultaneously reduce the ego vehicle's maneuvering space and create critical occlusions. Extensive experiments conducted on multiple reinforcement learning based AV models show that ScenGE uncovers more severe collision cases (+31.96%) on average than SoTA baselines. Additionally, our ScenGE can be applied to large model based AV systems and deployed on different simulators; we further observe that adversarial training on our scenarios improves the model robustness. Finally, we validate our framework through real-world vehicle tests and human evaluation, confirming that the generated scenarios are both plausible and critical. We hope our paper can build up a critical step towards building public trust and ensuring their safe deployment.",
        "arxiv_id": "2508.14527",
        "ARXIVID": "2508.14527",
        "COMMENT": "Matches criterion 3: introduces ScenGE, a new framework for generating safety-critical scenarios in simulators for autonomous vehicles, with a novel adversarial-collaborative approach and validation on real-world and simulated systems.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.14811": {
        "authors": [
            "Canyu Zhao",
            "Xiaoman Li",
            "Tianjian Feng",
            "Zhiyue Zhao",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization",
        "abstract": "arXiv:2508.14811v1 Announce Type: new  Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing. Project webpage: https://aim-uofa.github.io/Tinker",
        "arxiv_id": "2508.14811",
        "ARXIVID": "2508.14811",
        "COMMENT": "Matches criterion 4: Presents Tinker, a diffusion-based, multi-view consistent 3D editing framework using pretrained vision foundation models, with a new large-scale dataset and novel editing pipeline. Strong generative modeling and vision foundation model application.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.14278": {
        "authors": [
            "Elena Alegret Regalado",
            "Kunyi Li",
            "Sen Wang",
            "Siyun Liang",
            "Michael Niemeyer",
            "Stefano Gasperini",
            "Nassir Navab",
            "Federico Tombari"
        ],
        "title": "GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting",
        "abstract": "arXiv:2508.14278v1 Announce Type: new  Abstract: 3D scene reconstruction and understanding have gained increasing popularity, yet existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images. In this paper, we present GALA, a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. To extend to generalized language feature fields, we introduce the core contribution of GALA, a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning. Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.",
        "arxiv_id": "2508.14278",
        "ARXIVID": "2508.14278",
        "COMMENT": "Matches criterion 4: Proposes GALA, a new method for open-vocabulary 3D scene understanding using 3D Gaussian Splatting and language alignment, enabling 2D/3D open-vocabulary queries. Strong vision foundation model and multi-modal learning angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.14264": {
        "authors": [
            "Thanh-Dat Truong",
            "Huu-Thien Tran",
            "Tran Thai Son",
            "Bhiksha Raj",
            "Khoa Luu"
        ],
        "title": "Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models",
        "abstract": "arXiv:2508.14264v1 Announce Type: new  Abstract: Large multimodal models (LMMs) have gained impressive performance due to their outstanding capability in various understanding tasks. However, these models still suffer from some fundamental limitations related to robustness and generalization due to the alignment and correlation between visual and textual features. In this paper, we introduce a simple but efficient learning mechanism for improving the robust alignment between visual and textual modalities by solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual understanding, and cross-modality alignment by introducing two new tasks: reconstructing the image order and the text order into the LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to capture visual and textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we introduce a new Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses. The proposed approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic task-oriented and instruction-following LMM benchmarks.",
        "arxiv_id": "2508.14264",
        "ARXIVID": "2508.14264",
        "COMMENT": "Matches criterion 2 (new method for improving multi-modal large language models, LMMs/VLLMs). Proposes a robust alignment approach for LMMs, introducing new tasks and loss functions for better visual-textual alignment and reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.14704": {
        "authors": [
            "Ziyang Luo",
            "Zhiqi Shen",
            "Wenzhuo Yang",
            "Zirui Zhao",
            "Prathyusha Jwalapuram",
            "Amrita Saha",
            "Doyen Sahoo",
            "Silvio Savarese",
            "Caiming Xiong",
            "Junnan Li"
        ],
        "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers",
        "abstract": "arXiv:2508.14704v1 Announce Type: new  Abstract: The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.",
        "arxiv_id": "2508.14704",
        "ARXIVID": "2508.14704",
        "COMMENT": "Matches criterion 3: introduces MCP-Universe, a new benchmark for evaluating LLMs in realistic, tool-using, multi-domain settings, with a focus on real-world server interaction and long-horizon reasoning. Also relevant to embodied AI and agent evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.14461": {
        "authors": [
            "Shanlin Sun",
            "Yifan Wang",
            "Hanwen Zhang",
            "Yifeng Xiong",
            "Qin Ren",
            "Ruogu Fang",
            "Xiaohui Xie",
            "Chenyu You"
        ],
        "title": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering",
        "abstract": "arXiv:2508.14461v1 Announce Type: new  Abstract: While multi-step diffusion models have advanced both forward and inverse rendering, existing approaches often treat these problems independently, leading to cycle inconsistency and slow inference speed. In this work, we present Ouroboros, a framework composed of two single-step diffusion models that handle forward and inverse rendering with mutual reinforcement. Our approach extends intrinsic decomposition to both indoor and outdoor scenes and introduces a cycle consistency mechanism that ensures coherence between forward and inverse rendering outputs. Experimental results demonstrate state-of-the-art performance across diverse scenes while achieving substantially faster inference speed compared to other diffusion-based methods. We also demonstrate that Ouroboros can transfer to video decomposition in a training-free manner, reducing temporal inconsistency in video sequences while maintaining high-quality per-frame inverse rendering.",
        "arxiv_id": "2508.14461",
        "ARXIVID": "2508.14461",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Introduces Ouroboros, a novel single-step diffusion model framework for cycle-consistent forward and inverse rendering, with state-of-the-art results and fast inference. The cycle consistency and transfer to video decomposition are clever methodological contributions.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2508.14537": {
        "authors": [
            "Yonghan Shin",
            "SeungKyu Kim",
            "Won-Ki Jeong"
        ],
        "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion",
        "abstract": "arXiv:2508.14537v1 Announce Type: new  Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major computational challenge due to their gigapixel scale, often requiring the processing of tens to hundreds of thousands of high-resolution patches per slide. This results in prohibitive encoding costs, with preprocessing and training times extending to days or even weeks-making WSI encoding the most significant bottleneck in real-world deployment. In this work, we propose WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain vision-language models and large language models to address this challenge by selectively processing diagnostically relevant regions. WISE-FUSE first computes similarity scores between low-resolution patches and class-specific textual descriptions using a knowledge distillation mechanism that preserves fine-grained diagnostic features. Based on these similarity scores, we select a small subset of informative regions for the target task, which quickly eliminates irrelevant patches at the coarse level. The corresponding high-resolution patches are then selectively encoded and fused with textual embeddings to reinforce diagnostic context. Extensive experiments demonstrate that WISE-FUSE reduces WSI encoding time by over threefold while achieving diagnostic performance comparable to or surpassing that of exhaustive patch processing, offering a scalable and practical solution for CPath.",
        "arxiv_id": "2508.14537",
        "ARXIVID": "2508.14537",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs): Proposes WISE-FUSE, an adaptive WSI encoding framework leveraging pathology-domain vision-language models and large language models for efficient whole slide image encoding. Also relevant to criterion 4 (vision foundation models and applications) due to the use of VLMs and LLMs in a practical medical imaging context.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14767": {
        "authors": [
            "Fabian Holst",
            "Emre G\\\"ulsoylu",
            "Simone Frintrop"
        ],
        "title": "Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels",
        "abstract": "arXiv:2508.14767v1 Announce Type: new  Abstract: The paper presents a novel technique for creating a 6D pose estimation dataset for marine vessels by fusing monocular RGB images with Automatic Identification System (AIS) data. The proposed technique addresses the limitations of relying purely on AIS for location information, caused by issues like equipment reliability, data manipulation, and transmission delays. By combining vessel detections from monocular RGB images, obtained using an object detection network (YOLOX-X), with AIS messages, the technique generates 3D bounding boxes that represent the vessels' 6D poses, i.e. spatial and rotational dimensions. The paper evaluates different object detection models to locate vessels in image space. We also compare two transformation methods (homography and Perspective-n-Point) for aligning AIS data with image coordinates. The results of our work demonstrate that the Perspective-n-Point (PnP) method achieves a significantly lower projection error compared to homography-based approaches used before, and the YOLOX-X model achieves a mean Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold of 0.5 for relevant vessel classes. We show indication that our approach allows the creation of a 6D pose estimation dataset without needing manual annotation. Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a publicly available dataset comprising 3753 images with 3D bounding box annotations for pose estimation, created by our data fusion approach. This dataset can be used for training and evaluating 6D pose estimation networks. In addition we introduce a set of 1000 images with 2D bounding box annotations for ship detection from the same scene.",
        "arxiv_id": "2508.14767",
        "ARXIVID": "2508.14767",
        "COMMENT": "Matches criterion 3 (new benchmark/dataset for 6D pose estimation, relevant to embodied AI and spatial understanding). Introduces a novel data fusion method and a new public dataset (BONK-pose) for marine vessel pose estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14502": {
        "authors": [
            "Thanh-Nhan Vo",
            "Trong-Thuan Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran"
        ],
        "title": "SATURN: Autoregressive Image Generation Guided by Scene Graphs",
        "abstract": "arXiv:2508.14502v1 Announce Type: new  Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but often struggle to capture the layout and object relationships implied by complex prompts. Scene graphs provide a natural structural prior, yet previous graph-guided approaches have typically relied on heavy GAN or diffusion pipelines, which lag behind modern autoregressive architectures in both speed and fidelity. We introduce SATURN (Structured Arrangement of Triplets for Unified Rendering Networks), a lightweight extension to VAR-CLIP that translates a scene graph into a salience-ordered token sequence, enabling a frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from 56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78, outperforming prior methods such as SG2IM and SGDiff without requiring extra modules or multi-stage training. Qualitative results further confirm improvements in object count fidelity and spatial relation accuracy, showing that SATURN effectively combines structural awareness with state-of-the-art autoregressive fidelity.",
        "arxiv_id": "2508.14502",
        "ARXIVID": "2508.14502",
        "COMMENT": "Matches criterion 1 (spatial understanding via scene graphs for image generation) and criterion 4 (uses vision foundation models, i.e., CLIP). SATURN introduces a new method for autoregressive image generation guided by scene graphs, improving spatial relation accuracy and object layout fidelity.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14483": {
        "authors": [
            "Haoran Bai",
            "Xiaoxu Chen",
            "Canqian Yang",
            "Zongyao He",
            "Sibin Deng",
            "Ying Chen"
        ],
        "title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration",
        "abstract": "arXiv:2508.14483v1 Announce Type: new  Abstract: We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at https://github.com/csbhr/Vivid-VR.",
        "arxiv_id": "2508.14483",
        "ARXIVID": "2508.14483",
        "COMMENT": "Matches criterion 4: builds on a text-to-video diffusion transformer (a vision foundation model) for photorealistic video restoration, with novel concept distillation and control mechanisms.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14197": {
        "authors": [
            "Tinghan Yang",
            "Md Ashiqur Rahman",
            "Raymond A. Yeh"
        ],
        "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
        "abstract": "arXiv:2508.14197v1 Announce Type: new  Abstract: Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and $G$-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.",
        "arxiv_id": "2508.14197",
        "ARXIVID": "2508.14197",
        "COMMENT": "Matches criterion 4: leverages CLIP (a vision foundation model) for symmetry detection, introducing a novel prompting technique and rotation-equivariant decoder.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14504": {
        "authors": [
            "Bernd Hofmann",
            "Albert Scheck",
            "Joerg Franke",
            "Patrick Bruendl"
        ],
        "title": "PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments",
        "abstract": "arXiv:2508.14504v1 Announce Type: new  Abstract: The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.",
        "arxiv_id": "2508.14504",
        "ARXIVID": "2508.14504",
        "COMMENT": "Matches criterion 4: PB-IAD leverages multimodal foundation models for industrial anomaly detection, focusing on prompt-based adaptation and semantic instructions. Strong application of vision-language foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14280": {
        "authors": [
            "Ali Rasekh",
            "Sepehr Kazemi Ranjbar",
            "Simon Gottschalk"
        ],
        "title": "Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference",
        "abstract": "arXiv:2508.14280v1 Announce Type: new  Abstract: Explainable object recognition using vision-language models such as CLIP involves predicting accurate category labels supported by rationales that justify the decision-making process. Existing methods typically rely on prompt-based conditioning, which suffers from limitations in CLIP's text encoder and provides weak conditioning on explanatory structures. Additionally, prior datasets are often restricted to single, and frequently noisy, rationales that fail to capture the full diversity of discriminative image features. In this work, we introduce a multi-rationale explainable object recognition benchmark comprising datasets in which each image is annotated with multiple ground-truth rationales, along with evaluation metrics designed to offer a more comprehensive representation of the task. To overcome the limitations of previous approaches, we propose a contrastive conditional inference (CCI) framework that explicitly models the probabilistic relationships among image embeddings, category labels, and rationales. Without requiring any training, our framework enables more effective conditioning on rationales to predict accurate object categories. Our approach achieves state-of-the-art results on the multi-rationale explainable object recognition benchmark, including strong zero-shot performance, and sets a new standard for both classification accuracy and rationale quality. Together with the benchmark, this work provides a more complete framework for evaluating future models in explainable object recognition. The code will be made available online.",
        "arxiv_id": "2508.14280",
        "ARXIVID": "2508.14280",
        "COMMENT": "Matches criterion 4: Introduces a new benchmark and method (CCI) for explainable object recognition with vision-language models, focusing on multi-rationale explanations and zero-shot performance. Strong vision-language and explainability angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.14187": {
        "authors": [
            "Md Ashiqur Rahman",
            "Chiao-An Yang",
            "Michael N. Cheng",
            "Lim Jun Hao",
            "Jeremiah Jiang",
            "Teck-Yian Lim",
            "Raymond A. Yeh"
        ],
        "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
        "abstract": "arXiv:2508.14187v1 Announce Type: new  Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the same class can have different sizes, and their perceived size is further affected by the distance from the camera. These variations are local to the objects, i.e., different object sizes may change differently within the same image. To effectively handle scale variations, we present a deep equilibrium canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can be easily incorporated into existing network architectures and can be adapted to a pre-trained model. Notably, we show that on the competitive ImageNet benchmark, DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our code is available at https://github.com/ashiq24/local-scale-equivariance.",
        "arxiv_id": "2508.14187",
        "ARXIVID": "2508.14187",
        "COMMENT": "Matches criterion 1: proposes a deep equilibrium canonicalizer to improve local scale equivariance in vision models, with demonstrated improvements on popular architectures.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.14358": {
        "authors": [
            "Zhujun Li",
            "Shuo Zhang",
            "Ioannis Stamos"
        ],
        "title": "Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation",
        "abstract": "arXiv:2508.14358v1 Announce Type: new  Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size of objects within given categories. Existing approaches for this task rely solely on 6D poses as supervisory signals without explicitly capturing the intrinsic continuity of poses, leading to inconsistencies in predictions and reduced generalization to unseen poses. To address this limitation, we propose HRC-Pose, a novel depth-only framework for category-level object pose estimation, which leverages contrastive learning to learn point cloud representations that preserve the continuity of 6D poses. HRC-Pose decouples object pose into rotation and translation components, which are separately encoded and leveraged throughout the network. Specifically, we introduce a contrastive learning strategy for multi-task, multi-category scenarios based on our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds from multiple categories by considering rotational and translational differences as well as categorical information. We further design pose estimation modules that separately process the learned rotation-aware and translation-aware embeddings. Our experiments demonstrate that HRC-Pose successfully learns continuous feature spaces. Results on REAL275 and CAMERA25 benchmarks show that our method consistently outperforms existing depth-only state-of-the-art methods and runs in real-time, demonstrating its effectiveness and potential for real-world applications. Our code is at https://github.com/zhujunli1993/HRC-Pose.",
        "arxiv_id": "2508.14358",
        "ARXIVID": "2508.14358",
        "COMMENT": "Matches criterion 1: proposes HRC-Pose, a new method for learning point cloud representations with pose continuity for 6D object pose estimation, using contrastive learning and hierarchical ranking.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.14343": {
        "authors": [
            "Dian Ning",
            "Dong Seog Han"
        ],
        "title": "Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates",
        "abstract": "arXiv:2508.14343v1 Announce Type: new  Abstract: In one-stage multi-object detection tasks, various intersection over union (IoU)-based solutions aim at smooth and stable convergence near the targets during training. However, IoU-based losses fail to correctly update the gradient of small objects due to an extremely flat gradient. During the update of multiple objects, the learning of small objects' gradients suffers more because of insufficient gradient updates. Therefore, we propose an inter-class relational loss to efficiently update the gradient of small objects while not sacrificing the learning efficiency of other objects based on the simple fact that an object has a spatial relationship to another object (e.g., a car plate is attached to a car in a similar position). When the predicted car plate's bounding box is not within its car, a loss punishment is added to guide the learning, which is inversely proportional to the overlapped area of the car's and predicted car plate's bounding box. By leveraging the spatial relationship at the inter-class level, the loss guides small object predictions using larger objects and enhances latent information in deeper feature maps. In this paper, we present twofold contributions using license plate detection as a case study: (1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse real-world scenarios with high-quality annotations; and (2) a novel inter-class relational loss function designed to promote effective detection performance. We highlight the proposed ICR loss penalty can be easily added to existing IoU-based losses and enhance the performance. These contributions improve the standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6% in mAP$^{\\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without any additional hyperparameter tuning. Code and dataset will be available soon.",
        "arxiv_id": "2508.14343",
        "ARXIVID": "2508.14343",
        "COMMENT": "Matches criterion 1 (spatial understanding in object detection via inter-class spatial relationships). Proposes a novel loss leveraging spatial relationships for small object detection, with a new dataset.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.14563": {
        "authors": [
            "Xingyuan Yang",
            "Min Wei"
        ],
        "title": "GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels",
        "abstract": "arXiv:2508.14563v1 Announce Type: new  Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally limited by inherent ambiguity. Although NeRF-based methods achieve high-fidelity reconstruction via dense-ray sampling, their computational cost is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction efficiency but exhibits limitations under specular reflections. Multi-view inconsistencies introduce high-frequency surface noise and structural artifacts, while simplified rendering equations obscure material properties, leading to implausible relighting results. To address these issues, we propose GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we establish robust surface reconstruction through physics-based rendering with split-sum approximation, enhanced by geometric priors from foundation models. Second, we perform material decomposition by leveraging Monte Carlo importance sampling of the full rendering equation, modeling indirect illumination via differentiable 2D Gaussian ray tracing and refining high-frequency specular details through spherical mipmap-based directional encoding that captures anisotropic highlights. Extensive experiments demonstrate state-of-the-art performance in geometry reconstruction, material separation, and photorealistic relighting under novel illuminations, outperforming existing inverse rendering approaches.",
        "arxiv_id": "2508.14563",
        "ARXIVID": "2508.14563",
        "COMMENT": "Matches criterion 4 (vision foundation models for geometry and relighting). Uses geometric priors from foundation models for high-fidelity inverse rendering of glossy objects.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.14437": {
        "authors": [
            "Gabriel Tjio",
            "Jie Zhang",
            "Xulei Yang",
            "Yun Xing",
            "Nhat Chung",
            "Xiaofeng Cao",
            "Ivor W. Tsang",
            "Chee Keong Kwoh",
            "Qing Guo"
        ],
        "title": "FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation",
        "abstract": "arXiv:2508.14437v1 Announce Type: new  Abstract: Test-time adaptation enables models to adapt to evolving domains. However, balancing the tradeoff between preserving knowledge and adapting to domain shifts remains challenging for model adaptation methods, since adapting to domain shifts can induce forgetting of task-relevant knowledge. To address this problem, we propose FOCUS, a novel frequency-based conditioning approach within a diffusion-driven input-adaptation framework. Utilising learned, spatially adaptive frequency priors, our approach conditions the reverse steps during diffusion-driven denoising to preserve task-relevant semantic information for dense prediction.   FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network (Y-FPN) that disentangles high and low frequency information from noisy images. This minimizes the computational costs involved in implementing our approach in a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data augmentation method that perturbs the images across diverse frequency bands, which improves the robustness of our approach to diverse corruptions.   We demonstrate the effectiveness of FOCUS for semantic segmentation and monocular depth estimation across 15 corruption types and three datasets, achieving state-of-the-art averaged performance. In addition to improving standalone performance, FOCUS complements existing model adaptation methods since we can derive pseudo labels from FOCUS-denoised images for additional supervision. Even under limited, intermittent supervision with the pseudo labels derived from the FOCUS denoised images, we show that FOCUS mitigates catastrophic forgetting for recent model adaptation methods.",
        "arxiv_id": "2508.14437",
        "ARXIVID": "2508.14437",
        "COMMENT": "Somewhat relevant to criterion 4: Proposes FOCUS, a frequency-based conditioning method for diffusion models to mitigate catastrophic forgetting in test-time adaptation for vision tasks. Interesting statistical trick for dense prediction.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.14567": {
        "authors": [
            "Walter Zimmer",
            "Ross Greer",
            "Xingcheng Zhou",
            "Rui Song",
            "Marc Pavel",
            "Daniel Lehmberg",
            "Ahmed Ghita",
            "Akshay Gopalkrishnan",
            "Mohan Trivedi",
            "Alois Knoll"
        ],
        "title": "Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset",
        "abstract": "arXiv:2508.14567v1 Announce Type: new  Abstract: Even though a significant amount of work has been done to increase the safety of transportation networks, accidents still occur regularly. They must be understood as an unavoidable and sporadic outcome of traffic networks. We present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of real-world highway accidents. It contains ten sequences of vehicle crashes at high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and track IDs within 48,144 labeled frames recorded from four roadside cameras and LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the OpenLABEL format. We propose Accid3nD, an accident detection model that combines a rule-based approach with a learning-based one. Experiments and ablation studies on our dataset show the robustness of our proposed method. The dataset, model, and code are available on our project website: https://tum-traffic-dataset.github.io/tumtraf-a.",
        "arxiv_id": "2508.14567",
        "ARXIVID": "2508.14567",
        "COMMENT": "Partially matches criterion 3: introduces a new real-world traffic accident dataset (TUMTraf-A) and a hybrid accident detection model, which could be useful for embodied AI benchmarking.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.14660": {
        "authors": [
            "Muhammad Ibraheem Siddiqui",
            "Muhammad Umer Sheikh",
            "Hassan Abid",
            "Kevin Henry",
            "Muhammad Haris Khan"
        ],
        "title": "Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images",
        "abstract": "arXiv:2508.14660v1 Announce Type: new  Abstract: Segmentation in dense visual scenes poses significant challenges due to occlusions, background clutter, and scale variations. To address this, we introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot framework for Personalized instance Segmentation in dense images. PerSense employs a novel Instance Detection Module (IDM) that leverages density maps (DMs) to generate instance-level candidate point prompts, followed by a Point Prompt Selection Module (PPSM) that filters false positives via adaptive thresholding and spatial gating. A feedback mechanism further enhances segmentation by automatically selecting effective exemplars to improve DM quality. We additionally present PerSense++, an enhanced variant that incorporates three additional components to improve robustness in cluttered scenes: (i) a diversity-aware exemplar selection strategy that leverages feature and scale diversity for better DM generation; (ii) a hybrid IDM combining contour and peak-based prompt generation for improved instance separation within complex density patterns; and (iii) an Irrelevant Mask Rejection Module (IMRM) that discards spatially inconsistent masks using outlier analysis. Finally, to support this underexplored task, we introduce PerSense-D, a dedicated benchmark for personalized segmentation in dense images. Extensive experiments across multiple benchmarks demonstrate that PerSense++ outperforms existing methods in dense settings.",
        "arxiv_id": "2508.14660",
        "ARXIVID": "2508.14660",
        "COMMENT": "Somewhat relevant to criterion 4: Proposes PerSense++, a training-free, model-agnostic instance segmentation method with a new benchmark for dense images. Focuses on segmentation, not directly on foundation models or spatial intelligence for embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.14286": {
        "authors": [
            "Anushka A. Kore",
            "Frank G. te Nijenhuis",
            "Matthijs van der Sluijs",
            "Wim van Zwam",
            "Charles Majoie",
            "Geert Lycklama \\`a Nijeholt",
            "Danny Ruijters",
            "Frans Vos",
            "Sandra Cornelissen",
            "Ruisheng Su",
            "Theo van Walsum"
        ],
        "title": "OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA",
        "abstract": "arXiv:2508.14286v1 Announce Type: new  Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at https://github.com/anushka-kore/OccluNet.git",
        "arxiv_id": "2508.14286",
        "ARXIVID": "2508.14286",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1): Proposes OccluNet, a spatio-temporal deep learning model for occlusion detection in medical imaging, using transformer-based temporal attention. While not directly about embodied agents, it is a novel spatial-temporal model for complex visual tasks.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.14465": {
        "authors": [
            "Weitao Wang",
            "Zichen Wang",
            "Hongdeng Shen",
            "Yulei Lu",
            "Xirui Fan",
            "Suhui Wu",
            "Jun Zhang",
            "Haoqian Wang",
            "Hao Zhang"
        ],
        "title": "DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing",
        "abstract": "arXiv:2508.14465v1 Announce Type: new  Abstract: With the rapid progress of video generation, demand for customized video editing is surging, where subject swapping constitutes a key component yet remains under-explored. Prevailing swapping approaches either specialize in narrow domains--such as human-body animation or hand-object interaction--or rely on some indirect editing paradigm or ambiguous text prompts that compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided, subject-agnostic, end-to-end framework that swaps any subject in any video for customization with a user-specified mask and reference image. To inject fine-grained guidance, we introduce multiple conditions and a dedicated condition fusion module that integrates them efficiently. In addition, an adaptive mask strategy is designed to accommodate subjects of varying scales and attributes, further improving interactions between the swapped subject and its surrounding context. Through our elaborate two-phase dataset construction and training scheme, our DreamSwapV outperforms existing methods, as validated by comprehensive experiments on VBench indicators and our first introduced DreamSwapV-Benchmark.",
        "arxiv_id": "2508.14465",
        "ARXIVID": "2508.14465",
        "COMMENT": "Somewhat related to criterion 4 (video editing with mask-guided subject swapping, possibly using vision foundation models), but the focus is on application rather than methodological innovation in spatial intelligence or VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.14856": {
        "authors": [
            "Lakshmi Annamalai",
            "Chetan Singh Thakur"
        ],
        "title": "EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention",
        "abstract": "arXiv:2508.14856v1 Announce Type: new  Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low latency and low compute solutions using frame based cameras remains a challenge. Event cameras offer a promising alternative. To leverage their low power sensing, we introduce EventSSEG, a method for road segmentation that uses event only computing and a probabilistic attention mechanism. Event only computing poses a challenge in transferring pretrained weights from the conventional camera domain, requiring abundant labeled data, which is scarce. To overcome this, EventSSEG employs event-based self supervised learning, eliminating the need for extensive labeled data. Experiments on DSEC-Semantic and DDD17 show that EventSSEG achieves state of the art performance with minimal labeled events. This approach maximizes event cameras capabilities and addresses the lack of labeled events.",
        "arxiv_id": "2508.14856",
        "ARXIVID": "2508.14856",
        "COMMENT": "Somewhat related to spatial intelligence (criterion 1) via event-driven segmentation for autonomous vehicles, but not focused on embodied agents or spatial reasoning per se. Interesting self-supervised approach for event cameras.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.14779": {
        "authors": [
            "Mengliang Zhang",
            "Jacob M. Luber"
        ],
        "title": "Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification",
        "abstract": "arXiv:2508.14779v1 Announce Type: new  Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in whole-slide image (WSI) diagnosis. However, pathology images from different hospitals often vary due to differences in scanning hardware and preprocessing styles, which may lead PFMs to inadvertently learn hospital-specific features, posing risks for clinical deployment. In this work, we present the first systematic study of domain bias in PFMs arising from hospital source characteristics. Specifically, we (1) construct a pipeline for quantifying domain bias in PFMs, (2) evaluate and compare the performance of multiple models, and (3) propose a lightweight adversarial framework that removes latent hospital-specific features from frozen representations without modifying the encoder itself. By introducing a trainable adapter and a domain classifier connected through a gradient reversal layer (GRL), our method learns task-discriminative yet domain-invariant representations. Experiments on multi-center histopathology datasets demonstrate that our approach substantially reduces domain predictability while maintaining or even improving disease classification performance, particularly in out-of-domain (unseen hospital) scenarios. Further analyses, including hospital detection and feature space visualization, confirm the effectiveness of our method in mitigating hospital bias. We will provide our code based on acceptance.",
        "arxiv_id": "2508.14779",
        "ARXIVID": "2508.14779",
        "COMMENT": "Somewhat related to criterion 4 (applies foundation models in pathology), but focus is on domain adaptation and adversarial learning for hospital-invariant features, not spatial intelligence or VLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2508.14374": {
        "authors": [
            "Wenyong Zhou",
            "Boyu Li",
            "Jiachen Ren",
            "Taiqiang Wu",
            "Zhilin Ai",
            "Zhengwu Liu",
            "Ngai Wong"
        ],
        "title": "QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation",
        "abstract": "arXiv:2508.14374v1 Announce Type: new  Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously while addressing spectral bias through activation functions (AFs). Previous approaches mitigate this bias by employing complex AFs, which often incur significant hardware overhead. To tackle this challenge, we introduce QuadINR, a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve superior performance with dramatic reductions in hardware consumption. The quadratic functions encompass rich harmonic content in their Fourier series, delivering enhanced expressivity for high-frequency signals, as verified through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage pipeline framework that facilitates efficient hardware implementation of various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform and an ASIC implementation in a 28nm process. Experiments across images and videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior work, with an area of only 1914$\\mu$m$^2$ and a dynamic power of 6.14mW, reducing resource and power consumption by up to 97\\% and improving latency by up to 93\\% vs existing baselines.",
        "arxiv_id": "2508.14374",
        "ARXIVID": "2508.14374",
        "COMMENT": "Does not directly match any criterion; focuses on hardware-efficient implicit neural representations, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.14373": {
        "authors": [
            "Runshi Zhang",
            "Bimeng Jie",
            "Yang He",
            "Junchen Wang"
        ],
        "title": "TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network",
        "abstract": "arXiv:2508.14373v1 Announce Type: new  Abstract: Computer-aided surgical simulation is a critical component of orthognathic surgical planning, where accurately simulating face-bone shape transformations is significant. The traditional biomechanical simulation methods are limited by their computational time consumption levels, labor-intensive data processing strategies and low accuracy. Recently, deep learning-based simulation methods have been proposed to view this problem as a point-to-point transformation between skeletal and facial point clouds. However, these approaches cannot process large-scale points, have limited receptive fields that lead to noisy points, and employ complex preprocessing and postprocessing operations based on registration. These shortcomings limit the performance and widespread applicability of such methods. Therefore, we propose a Transformer-based coarse-to-fine point movement network (TCFNet) to learn unique, complicated correspondences at the patch and point levels for dense face-bone point cloud transformations. This end-to-end framework adopts a Transformer-based network and a local information aggregation network (LIA-Net) in the first and second stages, respectively, which reinforce each other to generate precise point movement paths. LIA-Net can effectively compensate for the neighborhood precision loss of the Transformer-based network by modeling local geometric structures (edges, orientations and relative position features). The previous global features are employed to guide the local displacement using a gated recurrent unit. Inspired by deformable medical image registration, we propose an auxiliary loss that can utilize expert knowledge for reconstructing critical organs.Compared with the existing state-of-the-art (SOTA) methods on gathered datasets, TCFNet achieves outstanding evaluation metrics and visualization results. The code is available at https://github.com/Runshi-Zhang/TCFNet.",
        "arxiv_id": "2508.14373",
        "ARXIVID": "2508.14373",
        "COMMENT": "Does not match any specific criterion. Focuses on point cloud transformation for surgical simulation, not spatial intelligence in embodied agents, VLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.14588": {
        "authors": [
            "Sofi\\`ene Boutaj",
            "Marin Scalbert",
            "Pierre Marza",
            "Florent Couzinie-Devy",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis"
        ],
        "title": "Controllable Latent Space Augmentation for Digital Pathology",
        "abstract": "arXiv:2508.14588v1 Announce Type: new  Abstract: Whole slide image (WSI) analysis in digital pathology presents unique challenges due to the gigapixel resolution of WSIs and the scarcity of dense supervision signals. While Multiple Instance Learning (MIL) is a natural fit for slide-level tasks, training robust models requires large and diverse datasets. Even though image augmentation techniques could be utilized to increase data variability and reduce overfitting, implementing them effectively is not a trivial task. Traditional patch-level augmentation is prohibitively expensive due to the large number of patches extracted from each WSI, and existing feature-level augmentation methods lack control over transformation semantics. We introduce HistAug, a fast and efficient generative model for controllable augmentations in the latent space for digital pathology. By conditioning on explicit patch-level transformations (e.g., hue, erosion), HistAug generates realistic augmented embeddings while preserving initial semantic information. Our method allows the processing of a large number of patches in a single forward pass efficiently, while at the same time consistently improving MIL model performance. Experiments across multiple slide-level tasks and diverse organs show that HistAug outperforms existing methods, particularly in low-data regimes. Ablation studies confirm the benefits of learned transformations over noise-based perturbations and highlight the importance of uniform WSI-wise augmentation. Code is available at https://github.com/MICS-Lab/HistAug.",
        "arxiv_id": "2508.14588",
        "ARXIVID": "2508.14588",
        "COMMENT": "Does not directly match any criterion; focuses on controllable latent space augmentation for digital pathology, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.14486": {
        "authors": [
            "Toqi Tahamid Sarker",
            "Khaled R Ahmed",
            "Taminul Islam",
            "Cristiana Bernardi Rankrape",
            "Karla Gage"
        ],
        "title": "WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification",
        "abstract": "arXiv:2508.14486v1 Announce Type: new  Abstract: Weed management represents a critical challenge in agriculture, significantly impacting crop yields and requiring substantial resources for control. Effective weed monitoring and analysis strategies are crucial for implementing sustainable agricultural practices and site-specific management approaches. We introduce WeedSense, a novel multi-task learning architecture for comprehensive weed analysis that jointly performs semantic segmentation, height estimation, and growth stage classification. We present a unique dataset capturing 16 weed species over an 11-week growth cycle with pixel-level annotations, height measurements, and temporal labels. WeedSense leverages a dual-path encoder incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated Decoder with transformer-based feature fusion to generate multi-scale features and enable simultaneous prediction across multiple tasks. WeedSense outperforms other state-of-the-art models on our comprehensive evaluation. On our multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm MAE for height estimation, and 99.99% accuracy for growth stage classification while maintaining real-time inference at 160 FPS. Our multitask approach achieves 3$\\times$ faster inference than sequential single-task execution and uses 32.4% fewer parameters. Please see our project page at weedsense.github.io.",
        "arxiv_id": "2508.14486",
        "ARXIVID": "2508.14486",
        "COMMENT": "Does not directly match any criterion; multi-task learning for weed analysis is not directly related to spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.14349": {
        "authors": [
            "Sean Fletcher",
            "Gabby Scott",
            "Douglas Currie",
            "Xin Zhang",
            "Yuqi Song",
            "Bruce MacLeod"
        ],
        "title": "Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model",
        "abstract": "arXiv:2508.14349v1 Announce Type: new  Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular level is critical for both clinical evaluation and biomedical research. However, existing detection methods require specialized equipment, skilled personnel, and extensive sample preparation, making them expensive, labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep learning approaches have shown great promise in medical and biological image analysis, enabling automated, high-throughput assessment of cellular morphology. Yet, no publicly available dataset currently exists for automated morphological analysis of cellular responses to Taxol exposure. To address this gap, we introduce a new microscopy image dataset capturing C6 glioma cells treated with varying concentrations of Taxol. To provide an effective solution for Taxol concentration classification and establish a benchmark for future studies on this dataset, we propose a baseline model named ResAttention-KNN, which combines a ResNet-50 with Convolutional Block Attention Modules and uses a k-Nearest Neighbors classifier in the learned embedding space. This model integrates attention-based refinement and non-parametric classification to enhance robustness and interpretability. Both the dataset and implementation are publicly released to support reproducibility and facilitate future research in vision-based biomedical analysis.",
        "arxiv_id": "2508.14349",
        "ARXIVID": "2508.14349",
        "COMMENT": "Does not match any specific criterion. Focuses on biomedical image analysis and dataset/model for Taxol exposure, not spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}