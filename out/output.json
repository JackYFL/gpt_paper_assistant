{
    "2509.12060": {
        "authors": [
            "Wei Cai",
            "Shujuan Liu",
            "Jian Zhao",
            "Ziyan Shi",
            "Yusheng Zhao",
            "Yuchen Yuan",
            "Tianle Zhang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models",
        "abstract": "arXiv:2509.12060v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit reasoning risk, wherein innocuous unimodal inputs synergistically assemble into risky multimodal data that produce harmful outputs. We attribute this vulnerability to the difficulty of MLLMs maintaining safety alignment through long-chain reasoning. To address this issue, we introduce Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring interpretable reasoning paths tailored for such a cross-modal challenge. A novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is also designed based on the SSUI dataset to align the MLLM's internal reasoning process with human safety values. Experimental results show that our SRPO-trained models achieve state-of-the-art results on key safety benchmarks, including the proposed Reasoning Path Benchmark (RSBench), significantly outperforming both open-source and top-tier commercial MLLMs.",
        "arxiv_id": "2509.12060",
        "ARXIVID": "2509.12060",
        "COMMENT": "This paper addresses cross-modal safety in MLLMs, introduces a new dataset (SSUI) and a training framework (SRPO) for safety-aware reasoning. It directly matches criterion 2 (new MLLMs) and is also relevant for surprising empirical results in vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.11090": {
        "authors": [
            "Chao Chen",
            "Shunyu Yao",
            "Yuanwu He",
            "Tao Feng",
            "Ruojing Song",
            "Yuliang Guo",
            "Xinyu Huang",
            "Chenxu Wu",
            "Ren Liu",
            "Chen Feng"
        ],
        "title": "End-to-End Visual Autonomous Parking via Control-Aided Attention",
        "abstract": "arXiv:2509.11090v1 Announce Type: new  Abstract: Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \\titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at https://github.com/Joechencc/CAAPolicy.",
        "arxiv_id": "2509.11090",
        "ARXIVID": "2509.11090",
        "COMMENT": "This paper proposes CAA-Policy, an end-to-end visual autonomous parking system with a novel control-aided attention mechanism, trained in a self-supervised manner. It is evaluated in the CARLA simulator and introduces a new method for embodied AI with a focus on spatial attention and control synergy, matching criterion 1 (spatial intelligence on embodied agents) and criterion 3 (new embodied AI methods with novel angles).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.11292": {
        "authors": [
            "Ziling Liu",
            "Ziwei Chen",
            "Mingqi Gao",
            "Jinyu Yang",
            "Feng Zheng"
        ],
        "title": "Leveraging Geometric Priors for Unaligned Scene Change Detection",
        "abstract": "arXiv:2509.11292v1 Announce Type: new  Abstract: Unaligned Scene Change Detection aims to detect scene changes between image pairs captured at different times without assuming viewpoint alignment. To handle viewpoint variations, current methods rely solely on 2D visual cues to establish cross-image correspondence to assist change detection. However, large viewpoint changes can alter visual observations, causing appearance-based matching to drift or fail. Additionally, supervision limited to 2D change masks from small-scale SCD datasets restricts the learning of generalizable multi-view knowledge, making it difficult to reliably identify visual overlaps and handle occlusions. This lack of explicit geometric reasoning represents a critical yet overlooked limitation. In this work, we are the first to leverage geometric priors from a Geometric Foundation Model to address the core challenges of unaligned SCD, including reliable identification of visual overlaps, robust correspondence establishment, and explicit occlusion detection. Building on these priors, we propose a training-free framework that integrates them with the powerful representations of a visual foundation model to enable reliable change detection under viewpoint misalignment. Through extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achieves superior and robust performance. Our code will be released at https://github.com/ZilingLiu/GeoSCD.",
        "arxiv_id": "2509.11292",
        "ARXIVID": "2509.11292",
        "COMMENT": "This paper leverages geometric priors from a Geometric Foundation Model and integrates them with a visual foundation model for unaligned scene change detection. It directly matches criterion 4 (vision foundation models and applications) and also brings a novel angle by using geometric priors for spatial understanding (criterion 1).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.11752": {
        "authors": [
            "Hongyuan Zhang",
            "Yuheng Wu",
            "Mingyang Zhao",
            "Zhiwei Chen",
            "Rebecca Li",
            "Fei Zhu",
            "Haohan Zhao",
            "Xiaohua Yuan",
            "Meng Yang",
            "Chunli Qiu",
            "Xiang Cong",
            "Haiyan Chen",
            "Lina Luan",
            "Randolph H. L. Wong",
            "Huai Liao",
            "Colin A Graham",
            "Shi Chang",
            "Guowei Tao",
            "Dong Yi",
            "Zhen Lei",
            "Nassir Navab",
            "Sebastien Ourselin",
            "Jiebo Luo",
            "Hongbin Liu",
            "Gaofeng Meng"
        ],
        "title": "A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications",
        "abstract": "arXiv:2509.11752v1 Announce Type: new  Abstract: Artificial intelligence (AI) that can effectively learn ultrasound representations by integrating multi-source data holds significant promise for advancing clinical care. However, the scarcity of large labeled datasets in real-world clinical environments and the limited generalizability of task-specific models have hindered the development of generalizable clinical AI models for ultrasound applications. In this study, we present EchoCare, a novel ultrasound foundation model for generalist clinical use, developed via self-supervised learning on our curated, publicly available, large-scale dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images, sourced from over 23 countries across 5 continents and acquired via a diverse range of distinct imaging devices, thus encompassing global cohorts that are multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt off-the-shelf vision foundation model architectures, we introduce a hierarchical classifier into EchoCare to enable joint learning of pixel-level and representation-level features, capturing both global anatomical contexts and local ultrasound characteristics. With minimal training, EchoCare outperforms state-of-the-art comparison models across 10 representative ultrasound benchmarks of varying diagnostic difficulties, spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement and report generation. The code and pretrained model are publicly released, rendering EchoCare accessible for fine-tuning and local adaptation, supporting extensibility to additional applications. EchoCare provides a fully open and generalizable foundation model to boost the development of AI technologies for diverse clinical ultrasound applications.",
        "arxiv_id": "2509.11752",
        "ARXIVID": "2509.11752",
        "COMMENT": "Matches criterion 4: introduces EchoCare, a new vision foundation model for ultrasound, with a novel hierarchical classifier and a large, diverse dataset. The model is open and generalizable, and the work is directly about vision foundation models and their applications.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.10748": {
        "authors": [
            "Jecia Z. Y. Mao",
            "Francis X Creighton",
            "Russell H Taylor",
            "Manish Sahu"
        ],
        "title": "SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation",
        "abstract": "arXiv:2509.10748v1 Announce Type: new  Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene is crucial to enable context-aware intraoperative assistance and decision making. Current solutions remain tethered to domain-specific, supervised models that rely on labeled data and required domain-specific data to adapt to new surgical scenarios and beyond predefined label categories. Recent advances in prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot segmentation across heterogeneous medical images. However, dependence of these models on manual visual or textual cues restricts their deployment in introperative surgical settings. We introduce a speech-guided collaborative perception (SCOPE) framework that integrates reasoning capabilities of large language model (LLM) with perception capabilities of open-set VFMs to support on-the-fly segmentation, labeling and tracking of surgical instruments and anatomy in intraoperative video streams. A key component of this framework is a collaborative perception agent, which generates top candidates of VFM-generated segmentation and incorporates intuitive speech feedback from clinicians to guide the segmentation of surgical instruments in a natural human-machine collaboration paradigm. Afterwards, instruments themselves serve as interactive pointers to label additional elements of the surgical scene. We evaluated our proposed framework on a subset of publicly available Cataract1k dataset and an in-house ex-vivo skull-base dataset to demonstrate its potential to generate on-the-fly segmentation and tracking of surgical scene. Furthermore, we demonstrate its dynamic capabilities through a live mock ex-vivo experiment. This human-AI collaboration paradigm showcase the potential of developing adaptable, hands-free, surgeon-centric tools for dynamic operating-room environments.",
        "arxiv_id": "2509.10748",
        "ARXIVID": "2509.10748",
        "COMMENT": "This paper presents a speech-guided collaborative perception framework that integrates LLMs and vision foundation models for surgical scene segmentation. It matches criterion 4 (vision foundation models and applications) and also leverages multi-modal interaction (speech, vision, language), which is of high interest.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.12146": {
        "authors": [
            "Nishank Singla",
            "Krisztian Koos",
            "Farzin Haddadpour",
            "Amin Honarmandi Shandiz",
            "Lovish Chum",
            "Xiaojian Xu",
            "Qing Jin",
            "Erhan Bas"
        ],
        "title": "Multi Anatomy X-Ray Foundation Model",
        "abstract": "arXiv:2509.12146v1 Announce Type: new  Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.",
        "arxiv_id": "2509.12146",
        "ARXIVID": "2509.12146",
        "COMMENT": "This paper introduces XR-0, a multi-anatomy X-ray vision foundation model evaluated on diverse tasks. It is a direct match for criterion 4 (vision foundation models and applications), and demonstrates the importance of anatomical diversity for robust medical vision models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.11866": {
        "authors": [
            "Meng Luo",
            "Shengqiong Wu",
            "Liqiang Jing",
            "Tianjie Ju",
            "Li Zheng",
            "Jinxiang Lai",
            "Tianlong Wu",
            "Xinya Du",
            "Jian Li",
            "Siyuan Yan",
            "Jiebo Luo",
            "William Yang Wang",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding",
        "abstract": "arXiv:2509.11866v1 Announce Type: new  Abstract: Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/Eurekaleo/Dr.V.",
        "arxiv_id": "2509.11866",
        "ARXIVID": "2509.11866",
        "COMMENT": "This paper introduces Dr.V, a hierarchical framework for diagnosing video hallucination in large video models (LVMs) using fine-grained spatial-temporal grounding. It provides a new benchmark (Dr.V-Bench) and a diagnostic agent (Dr.V-Agent) for video understanding, focusing on spatial-temporal grounding and interpretability. This closely matches criterion 3 (new embodied AI benchmark/methods with novel angles, especially spatial-temporal grounding for hallucination detection in video LLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.10704": {
        "authors": [
            "Xingchen Wan",
            "Han Zhou",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Ke Jiang",
            "Rajarishi Sinha",
            "Sercan \\\"O. Ar{\\i}k"
        ],
        "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration",
        "abstract": "arXiv:2509.10704v1 Announce Type: new  Abstract: Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.",
        "arxiv_id": "2509.10704",
        "ARXIVID": "2509.10704",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes Maestro, a self-improving text-to-image system using MLLM agents for prompt evolution and image critique, with strong empirical results. The agentic orchestration and self-evolution loop is a novel approach for T2I generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.11662": {
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "abstract": "arXiv:2509.11662v1 Announce Type: new  Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs. Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers, which enables it to process images at their original variable resolutions. This design avoids the degradation caused by fixed-resolution tiling while preserving fine-grained details and global layouts, which is crucial for visually dense content such as complex charts and diagrams. To ensure the smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a distributed multimodal training framework tailored for Ascend NPUs. To maintain training accuracy, we implement equivalent replacements for certain operators. MindVL undergoes a three-phase training process, namely the warm-up phase, multitask training phase, and supervised instruction tuning phase, to gradually enhance its capabilities. This process starts with basic visual and multimodal pre-training, followed by large-scale multiask trainging and instruction tuning. We also adopt multimodal data packaging and hybrid parallelism techniques, which significantly improve end-to-end training speed. To further boost model performance, we specifically introduce test-time resolution search and model weight averaging. Notably, despite using about 1/10 of the training data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL in evaluations of general multimodal understanding and document/table comprehension. Beyond overall scores, MindVL also delivers leading performance in OCR assessments.",
        "arxiv_id": "2509.11662",
        "ARXIVID": "2509.11662",
        "COMMENT": "Matches criterion 2: introduces MindVL, a new multimodal large language model (MLLM) trained on Ascend NPUs, with architectural and training innovations for efficient and effective multimodal learning. Also discusses performance on vision-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.11548": {
        "authors": [
            "Weiming Li",
            "Yan Shao",
            "Jing Yang",
            "Yujing Lu",
            "Ling Zhong",
            "Yuhan Wang",
            "Manni Duan"
        ],
        "title": "How Auxiliary Reasoning Unleashes GUI Grounding in VLMs",
        "abstract": "arXiv:2509.11548v1 Announce Type: new  Abstract: Graphical user interface (GUI) grounding is a fundamental task for building GUI agents. However, general vision-language models (VLMs) struggle with this task due to a lack of specific optimization. We identify a key gap in this paper: while VLMs exhibit significant latent grounding potential, as demonstrated by their performance measured by Pointing Game, they underperform when tasked with outputting explicit coordinates. To address this discrepancy, and bypass the high data and annotation costs of current fine-tuning approaches, we propose three zero-shot auxiliary reasoning methods. By providing explicit spatial cues such as axes, grids and labeled intersections as part of the input image, these methods enable VLMs to articulate their implicit spatial understanding capabilities. We evaluate these methods on four GUI grounding benchmarks across seven open-source and proprietary VLMs. The evaluation results demonstrate that the proposed methods substantially improve the performance of GUI grounding.",
        "arxiv_id": "2509.11548",
        "ARXIVID": "2509.11548",
        "COMMENT": "Matches criterion 1 (spatial understanding in VLMs for GUI grounding) and criterion 2 (evaluates and improves VLMs' spatial reasoning via auxiliary cues). Proposes clever zero-shot methods to improve explicit spatial grounding in VLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.11097": {
        "authors": [
            "Nhut Le",
            "Ehsan Karimi",
            "Maryam Rahnemoonfar"
        ],
        "title": "3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment",
        "abstract": "arXiv:2509.11097v1 Announce Type: new  Abstract: Timely assessment of structural damage is critical for disaster response and recovery. However, most prior work in natural disaster analysis relies on 2D imagery, which lacks depth, suffers from occlusions, and provides limited spatial context. 3D semantic segmentation offers a richer alternative, but existing 3D benchmarks focus mainly on urban or indoor scenes, with little attention to disaster-affected areas. To address this gap, we present 3DAeroRelief--the first 3D benchmark dataset specifically designed for post-disaster assessment. Collected using low-cost unmanned aerial vehicles (UAVs) over hurricane-damaged regions, the dataset features dense 3D point clouds reconstructed via Structure-from-Motion and Multi-View Stereo techniques. Semantic annotations were produced through manual 2D labeling and projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D large-scale outdoor environments with fine-grained structural damage in real-world disaster contexts. UAVs enable affordable, flexible, and safe data collection in hazardous areas, making them particularly well-suited for emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate several state-of-the-art 3D segmentation models on the dataset to highlight both the challenges and opportunities of 3D scene understanding in disaster response. Our dataset serves as a valuable resource for advancing robust 3D vision systems in real-world applications for post-disaster scenarios.",
        "arxiv_id": "2509.11097",
        "ARXIVID": "2509.11097",
        "COMMENT": "Matches criterion 3 (new 3D benchmark for embodied AI, specifically for post-disaster assessment with UAVs). The dataset is novel for its focus on disaster-affected outdoor scenes and 3D semantic segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.11360": {
        "authors": [
            "Wan Xu",
            "Feng Zhu",
            "Yihan Zeng",
            "Yuanfan Guo",
            "Ming Liu",
            "Hang Xu",
            "Wangmeng Zuo"
        ],
        "title": "GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration",
        "abstract": "arXiv:2509.11360v1 Announce Type: new  Abstract: Video detailed captioning aims to generate comprehensive video descriptions to facilitate video understanding. Recently, most efforts in the video detailed captioning community have been made towards a local-to-global paradigm, which first generates local captions from video clips and then summarizes them into a global caption. However, we find this paradigm leads to less detailed and contextual-inconsistent captions, which can be attributed to (1) no mechanism to ensure fine-grained captions, and (2) weak interaction between local and global captions. To remedy the above two issues, we propose GLaVE-Cap, a Global-Local aligned framework with Vision Expert integration for Captioning, which consists of two core modules: TrackFusion enables comprehensive local caption generation, by leveraging vision experts to acquire cross-frame visual prompts, coupled with a dual-stream structure; while CaptionBridge establishes a local-global interaction, by using global context to guide local captioning, and adaptively summarizing local captions into a coherent global caption. Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark featuring 5X more queries per video than existing benchmarks, covering diverse visual dimensions to facilitate reliable evaluation. We further provide a training dataset GLaVE-1.2M containing 16K high-quality fine-grained video captions and 1.2M related question-answer pairs. Extensive experiments on four benchmarks show that our GLaVE-Cap achieves state-of-the-art performance. Besides, the ablation studies and student model analyses further validate the effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the video understanding community. The source code, model weights, benchmark, and dataset will be open-sourced.",
        "arxiv_id": "2509.11360",
        "ARXIVID": "2509.11360",
        "COMMENT": "Matches criterion 3 (new benchmark for video captioning, GLaVE-Bench, and new method for global-local alignment in video captioning). Also relevant to criterion 2 (multi-modal models for video understanding).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.11840": {
        "authors": [
            "Tim Lebailly",
            "Vijay Veerabadran",
            "Satwik Kottur",
            "Karl Ridgeway",
            "Michael Louis Iuzzolino"
        ],
        "title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation",
        "abstract": "arXiv:2509.11840v1 Announce Type: new  Abstract: Generative vision-language models (VLMs) exhibit strong high-level image understanding but lack spatially dense alignment between vision and language modalities, as our findings indicate. Orthogonal to advancements in generative VLMs, another line of research has focused on representation learning for vision-language alignment, targeting zero-shot inference for dense tasks like segmentation. In this work, we bridge these two directions by densely aligning images with synthetic descriptions generated by VLMs. Synthetic captions are inexpensive, scalable, and easy to generate, making them an excellent source of high-level semantic understanding for dense alignment methods. Empirically, our approach outperforms prior work on standard zero-shot open-vocabulary segmentation benchmarks/datasets, while also being more data-efficient.",
        "arxiv_id": "2509.11840",
        "ARXIVID": "2509.11840",
        "COMMENT": "Matches criterion 1 (spatial understanding in VLMs for segmentation) and criterion 2 (leverages generative VLMs for dense alignment). The paper proposes a new method to bridge generative VLMs and dense spatial tasks using synthetic captions, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.11301": {
        "authors": [
            "Matthias W\\\"uest",
            "Francis Engelmann",
            "Ondrej Miksik",
            "Marc Pollefeys",
            "Daniel Barath"
        ],
        "title": "UnLoc: Leveraging Depth Uncertainties for Floorplan Localization",
        "abstract": "arXiv:2509.11301v1 Announce Type: new  Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera localization within floorplans. Floorplan data is readily available, long-term persistent, and robust to changes in visual appearance. We address key limitations of recent methods, such as the lack of uncertainty modeling in depth predictions and the necessity for custom depth networks trained for each environment. We introduce a novel probabilistic model that incorporates uncertainty estimation, modeling depth predictions as explicit probability distributions. By leveraging off-the-shelf pre-trained monocular depth models, we eliminate the need to rely on per-environment-trained depth networks, enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale synthetic and real-world datasets, demonstrating significant improvements over existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$ times higher localization recall on long sequences (100 frames) and $16.7$ times higher on short ones (15 frames) than the state of the art on the challenging LaMAR HGE dataset.",
        "arxiv_id": "2509.11301",
        "ARXIVID": "2509.11301",
        "COMMENT": "This paper introduces UnLoc, a probabilistic model for camera localization within floorplans, leveraging depth uncertainty and off-the-shelf monocular depth models. This is a methodological improvement in spatial understanding for embodied agents (criterion 1), and is relevant for embodied AI (criterion 3) as it addresses localization in real and synthetic environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11880": {
        "authors": [
            "Carlos Celemin",
            "Joseph Brennan",
            "Pierluigi Vito Amadori",
            "Tim Bradley"
        ],
        "title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning",
        "abstract": "arXiv:2509.11880v1 Announce Type: new  Abstract: This paper introduces a novel application of Supervised Contrastive Learning (SupCon) to Imitation Learning (IL), with a focus on learning more effective state representations for agents in video game environments. The goal is to obtain latent representations of the observations that capture better the action-relevant factors, thereby modeling better the cause-effect relationship from the observations that are mapped to the actions performed by the demonstrator, for example, the player jumps whenever an obstacle appears ahead. We propose an approach to integrate the SupCon loss with continuous output spaces, enabling SupCon to operate without constraints regarding the type of actions of the environment. Experiments on the 3D games Astro Bot and Returnal, and multiple 2D Atari games show improved representation quality, faster learning convergence, and better generalization compared to baseline models trained only with supervised action prediction loss functions.",
        "arxiv_id": "2509.11880",
        "ARXIVID": "2509.11880",
        "COMMENT": "This paper introduces supervised contrastive learning to imitation learning for video game agents, improving state representations. This is a methodological improvement for spatial understanding in embodied agents (criterion 1), and is relevant for embodied AI (criterion 3) as it proposes a new learning method for agents in simulated environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11853": {
        "authors": [
            "Yi-Hsin Li",
            "Thomas Sikora",
            "Sebastian Knorr",
            "M{\\aa}arten Sj\\\"ostr\\\"om"
        ],
        "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
        "abstract": "arXiv:2509.11853v1 Announce Type: new  Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.",
        "arxiv_id": "2509.11853",
        "ARXIVID": "2509.11853",
        "COMMENT": "This paper proposes a segmentation-driven initialization for 3D Gaussian Splatting, improving sparse-view 3D scene reconstruction. It is a methodological improvement in spatial understanding (criterion 1), especially for 3D scene representation and efficient rendering, which is relevant for embodied agents and vision foundation model applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11719": {
        "authors": [
            "Bingqing Wei",
            "Lianmin Chen",
            "Zhongyu Xia",
            "Yongtao Wang"
        ],
        "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction",
        "abstract": "arXiv:2509.11719v1 Announce Type: new  Abstract: Multi-agent trajectory prediction in autonomous driving requires a comprehensive understanding of complex social dynamics. Existing methods, however, often struggle to capture the full richness of these dynamics, particularly the co-existence of multi-scale interactions and the diverse behaviors of heterogeneous agents. To address these challenges, this paper introduces HeLoFusion, an efficient and scalable encoder for modeling heterogeneous and multi-scale agent interactions. Instead of relying on global context, HeLoFusion constructs local, multi-scale graphs centered on each agent, allowing it to effectively model both direct pairwise dependencies and complex group-wise interactions (\\textit{e.g.}, platooning vehicles or pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of agent heterogeneity through an aggregation-decomposition message-passing scheme and type-specific feature networks, enabling it to learn nuanced, type-dependent interaction patterns. This locality-focused approach enables a principled representation of multi-level social context, yielding powerful and expressive agent embeddings. On the challenging Waymo Open Motion Dataset, HeLoFusion achieves state-of-the-art performance, setting new benchmarks for key metrics including Soft mAP and minADE. Our work demonstrates that a locality-grounded architecture, which explicitly models multi-scale and heterogeneous interactions, is a highly effective strategy for advancing motion forecasting.",
        "arxiv_id": "2509.11719",
        "ARXIVID": "2509.11719",
        "COMMENT": "Matches criterion 1 (spatial understanding in multi-agent trajectory prediction for autonomous driving) and criterion 3 (novel method for modeling heterogeneous and multi-scale interactions). Proposes a new encoder architecture for trajectory prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11480": {
        "authors": [
            "Amir Taherin",
            "Juyi Lin",
            "Arash Akbari",
            "Arman Akbari",
            "Pu Zhao",
            "Weiwei Chen",
            "David Kaeli",
            "Yanzhi Wang"
        ],
        "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
        "abstract": "arXiv:2509.11480v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
        "arxiv_id": "2509.11480",
        "ARXIVID": "2509.11480",
        "COMMENT": "Matches criterion 3 (embodied AI, benchmarking and empirical analysis of VLA models) and criterion 4 (vision foundation models and applications). Provides a cross-platform evaluation of vision-language-action models for robotics, with surprising empirical results about edge vs. datacenter hardware. The empirical insights and benchmarking are valuable.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11247": {
        "authors": [
            "Robert Long",
            "Rongxin Jiang",
            "Mingrui Yan"
        ],
        "title": "Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States",
        "abstract": "arXiv:2509.11247v1 Announce Type: new  Abstract: Person Re-Identification (ReID) has several challenges in real-world surveillance systems due to clothing changes (CCReID) and the need for maintaining continual learning (LReID). Previous existing methods either develop models specifically for one application, which is mostly a same-cloth (SC) setting or treat CCReID as its own separate sub-problem. In this work, we will introduce the LReID-Hybrid task with the goal of developing a model to achieve both SC and CC while learning in a continual setting. Mismatched representations and forgetting from one task to the next are significant issues, we address this with CMLReID, a CLIP-based framework composed of two novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive prompts, and also incorporates context to align richly multi-grained visual cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection (AKFP) which produces robust SC/CC prototypes through the use of a dual-path learner that aligns features with our Clothing-State-Aware Projection Loss. Experiments performed on a wide range of datasets and illustrate that CMLReID outperforms all state-of-the-art methods with strong robustness and generalization despite clothing variations and a sophisticated process of sequential learning.",
        "arxiv_id": "2509.11247",
        "ARXIVID": "2509.11247",
        "COMMENT": "This paper introduces a CLIP-based framework for lifelong person re-identification under clothing changes, using contextualized multimodal learning and prompt engineering. It is a new method in multi-modal learning and leverages vision foundation models (criterion 4), and also touches on continual learning in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10759": {
        "authors": [
            "Yi-Ruei Liu",
            "You-Zhe Xie",
            "Yu-Hsiang Hsu",
            "I-Sheng Fang",
            "Yu-Lun Liu",
            "Jun-Cheng Chen"
        ],
        "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation",
        "abstract": "arXiv:2509.10759v1 Announce Type: new  Abstract: Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.",
        "arxiv_id": "2509.10759",
        "ARXIVID": "2509.10759",
        "COMMENT": "This paper presents a new 4D Gaussian Ray Tracing pipeline for generating data with realistic camera effects and introduces a new benchmark for evaluating camera effect robustness. This is a novel simulator/data generation method for vision tasks, matching criterion 3.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10841": {
        "authors": [
            "Simone Mosco",
            "Daniel Fusaro",
            "Wanmeng Li",
            "Emanuele Menegatti",
            "Alberto Pretto"
        ],
        "title": "Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios",
        "abstract": "arXiv:2509.10841v1 Announce Type: new  Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D environments in applications such as autonomous driving and robotics. Recent methods achieve strong performance by exploiting different point cloud representations or incorporating data from other sensors, such as cameras or external datasets. However, these approaches often suffer from high computational complexity and require large amounts of training data, limiting their generalization in data-scarce scenarios. In this paper, we improve the performance of point-based methods by effectively learning features from 2D representations through point-plane projections, enabling the extraction of complementary information while relying solely on LiDAR data. Additionally, we introduce a geometry-aware technique for data augmentation that aligns with LiDAR sensor properties and mitigates class imbalance. We implemented and evaluated our method that applies point-plane projections onto multiple informative 2D representations of the point cloud. Experiments demonstrate that this approach leads to significant improvements in limited-data scenarios, while also achieving competitive results on two publicly available standard datasets, as SemanticKITTI and PandaSet. The code of our method is available at https://github.com/SiMoM0/3PNet",
        "arxiv_id": "2509.10841",
        "ARXIVID": "2509.10841",
        "COMMENT": "This paper introduces a new method for LiDAR semantic segmentation using point-plane projections and geometry-aware augmentation, which is a methodological improvement for spatial understanding in embodied agents (criterion 1).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.11067": {
        "authors": [
            "Liangxuan Guo",
            "Bin Zhu",
            "Qingqian Tao",
            "Kangning Liu",
            "Xun Zhao",
            "Xianzhe Qin",
            "Jin Gao",
            "Guangfu Hao"
        ],
        "title": "Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration",
        "abstract": "arXiv:2509.11067v1 Announce Type: new  Abstract: Autonomous agents for desktop automation struggle with complex multi-step tasks due to poor coordination and inadequate quality control. We introduce \\textsc{Agentic Lybic}, a novel multi-agent system where the entire architecture operates as a finite-state machine (FSM). This core innovation enables dynamic orchestration. Our system comprises four components: a Controller, a Manager, three Workers (Technician for code-based operations, Operator for GUI interactions, and Analyst for decision support), and an Evaluator. The critical mechanism is the FSM-based routing between these components, which provides flexibility and generalization by dynamically selecting the optimal execution strategy for each subtask. This principled orchestration, combined with robust quality gating, enables adaptive replanning and error recovery. Evaluated officially on the OSWorld benchmark, \\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\\% success rate in 50 steps, substantially outperforming existing methods. Results demonstrate that principled multi-agent orchestration with continuous quality control provides superior reliability for generalized desktop automation in complex computing environments.",
        "arxiv_id": "2509.11067",
        "ARXIVID": "2509.11067",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for agent orchestration and benchmarking). Proposes a multi-agent system for desktop automation with a finite-state machine architecture, achieving SOTA on the OSWorld benchmark. The FSM-based orchestration and quality control is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.11944": {
        "authors": [
            "Susanta Mitra"
        ],
        "title": "Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare",
        "abstract": "arXiv:2509.11944v1 Announce Type: new  Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal data for reasoning and diagnosing multiple diseases. Although some multimodal reasoning models have emerged for reasoning complex tasks in scientific domains, their applications in the healthcare domain remain limited and fall short in correct reasoning for diagnosis. To address the challenges of multimodal medical reasoning for correct diagnosis and assist the healthcare professionals, a novel temporal graph-based reasoning process modelled through a directed graph has been proposed in the current work. It helps in accommodating dynamic changes in reasons through backtracking, refining the reasoning content, and creating new or deleting existing reasons to reach the best recommendation or answer. Again, consideration of multimodal data at different time points can enable tracking and analysis of patient health and disease progression. Moreover, the proposed multi-agent temporal reasoning framework provides task distributions and a cross-validation mechanism to further enhance the accuracy of reasoning outputs. A few basic experiments and analysis results justify the novelty and practical utility of the proposed preliminary approach.",
        "arxiv_id": "2509.11944",
        "ARXIVID": "2509.11944",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding/spatial intelligence on embodied agents) and criterion 2 (MLLMs) due to the proposal of a temporal graph-based reasoning process for multimodal data and a multi-agent framework, with application to healthcare. The use of temporal graphs and agentic reasoning is a novel angle for multimodal reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.11624": {
        "authors": [
            "Wending Liu",
            "Siyun Liang",
            "Huy H. Nguyen",
            "Isao Echizen"
        ],
        "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
        "abstract": "arXiv:2509.11624v1 Announce Type: new  Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
        "arxiv_id": "2509.11624",
        "ARXIVID": "2509.11624",
        "COMMENT": "Related to criterion 4: proposes a new 3D deepfake generation framework using Gaussian Splatting, bridging 3D modeling and generative vision models. The method is novel in its use of 3D Gaussian Splatting for controllable, multi-view deepfakes.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.12194": {
        "authors": [
            "Thomas A. Buckley",
            "Riccardo Conci",
            "Peter G. Brodeur",
            "Jason Gusdorf",
            "Sourik Beltr\\'an",
            "Bita Behrouzi",
            "Byron Crowe",
            "Jacob Dockterman",
            "Muzzammil Muhammad",
            "Sarah Ohnigian",
            "Andrew Sanchez",
            "James A. Diao",
            "Aashna P. Shah",
            "Daniel Restrepo",
            "Eric S. Rosenberg",
            "Andrew S. Lea",
            "Marinka Zitnik",
            "Scott H. Podolsky",
            "Zahir Kanjee",
            "Raja-Elie E. Abdulnour",
            "Jacob M. Koshy",
            "Adam Rodman",
            "Arjun K. Manrai"
        ],
        "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
        "abstract": "arXiv:2509.12194v1 Announce Type: new  Abstract: BACKGROUND: For over a century, the New England Journal of Medicine Clinicopathological Conferences (CPCs) have tested the reasoning of expert physicians and, recently, artificial intelligence (AI). However, prior AI evaluations have focused on final diagnoses without addressing the multifaceted reasoning and presentation skills required of expert discussants.   METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025), we conducted extensive physician annotation and automated processing to create CPC-Bench, a physician-validated benchmark spanning 10 text-based and multimodal tasks, against which we evaluated leading large language models (LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce written and slide-based video presentations using only the case presentation, modeling the role of the human expert in these cases.   RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the final diagnosis first in 60% of cases and within the top ten in 84% of cases, outperforming a 20-physician baseline; next-test selection accuracy reached 98%. Event-level physician annotations quantified AI diagnostic accuracy per unit of information. Performance was lower on literature search and image tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image challenges. In blinded comparisons of CaBot vs. human expert-generated text, physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions. To promote research, we are releasing CaBot and CPC-Bench.   CONCLUSIONS: LLMs exceed physician performance on complex text-based differential diagnosis and convincingly emulate expert medical presentations, but image interpretation and literature retrieval remain weaker. CPC-Bench and CaBot may enable transparent and continued tracking of progress in medical AI.",
        "arxiv_id": "2509.12194",
        "ARXIVID": "2509.12194",
        "COMMENT": "Matches criterion 3: introduces CPC-Bench, a new benchmark for evaluating LLMs and multimodal models on complex medical reasoning, and releases CaBot, an embodied AI discussant. The benchmark is multimodal and focuses on reasoning and presentation, which is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10710": {
        "authors": [
            "Sven Schreiber",
            "Noha Sarhan",
            "Simone Frintrop",
            "Christian Wilms"
        ],
        "title": "SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition",
        "abstract": "arXiv:2509.10710v1 Announce Type: new  Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB data or signer pose information. However, combining these modalities often results in the loss of crucial details, such as hand shape and orientation, due to imprecise representations like bounding boxes. Therefore, we propose the ISLR system SegSLR, which combines RGB and pose information through promptable zero-shot video segmentation. Given the rough localization of the hands and the signer's body from pose information, we segment the respective parts through the video to maintain all relevant shape information. Subsequently, the segmentations focus the processing of the RGB data on the most relevant body parts for ISLR. This effectively combines RGB and pose information. Our evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR outperforms state-of-the-art methods. Furthermore, ablation studies indicate that SegSLR strongly benefits from focusing on the signer's body and hands, justifying our design choices.",
        "arxiv_id": "2509.10710",
        "ARXIVID": "2509.10710",
        "COMMENT": "This paper proposes a new method for sign language recognition by combining RGB and pose information through promptable zero-shot video segmentation. It is a methodological improvement in spatial understanding for agents (criterion 1), as it focuses on better spatial representation of hand and body parts for recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11102": {
        "authors": [
            "Nhi Kieu",
            "Kien Nguyen",
            "Arnold Wiliem",
            "Clinton Fookes",
            "Sridha Sridharan"
        ],
        "title": "Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation",
        "abstract": "arXiv:2509.11102v1 Announce Type: new  Abstract: Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.",
        "arxiv_id": "2509.11102",
        "ARXIVID": "2509.11102",
        "COMMENT": "This paper proposes a new generative-enhanced multimodal network for missing modality in remote sensing semantic segmentation, with novel hybrid feature extraction and fusion. It is a new method in multi-modal learning for vision (criterion 4), and uses generative modeling.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.10683": {
        "authors": [
            "Felicia Liu",
            "Jay J. Yoo",
            "Farzad Khalvati"
        ],
        "title": "A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI",
        "abstract": "arXiv:2509.10683v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown strong performance in text-based healthcare tasks. However, their utility in image-based applications remains unexplored. We investigate the effectiveness of LLMs for medical imaging tasks, specifically glioma classification and segmentation, and compare their performance to that of traditional convolutional neural networks (CNNs). Using the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and balanced precision and recall. The general LLM reached 76% accuracy but suffered from a specificity of only 18%, often misclassifying Low-Grade tumors. Fine-tuning improved specificity to 55%, but overall performance declined (e.g., accuracy dropped to 72%). For segmentation, three methods - center point, bounding box, and polygon extraction, were implemented. CNNs accurately localized gliomas, though small tumors were sometimes missed. In contrast, LLMs consistently clustered predictions near the image center, with no distinction of glioma size, location, or placement. Fine-tuning improved output formatting but failed to meaningfully enhance spatial accuracy. The bounding polygon method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in both tasks. LLMs showed limited spatial understanding and minimal improvement from fine-tuning, indicating that, in their current form, they are not well-suited for image-based tasks. More rigorous fine-tuning or alternative training strategies may be needed for LLMs to achieve better performance, robustness, and utility in the medical space.",
        "arxiv_id": "2509.10683",
        "ARXIVID": "2509.10683",
        "COMMENT": "This paper compares LLMs and CNNs for medical image classification and segmentation, and finds LLMs have limited spatial understanding. It provides empirical insight into the spatial limitations of VLLMs (criterion 2), and is relevant for those interested in surprising empirical results in vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.10620": {
        "authors": [
            "Emily Kaczmarek",
            "Justin Szeto",
            "Brennan Nichyporuk",
            "Tal Arbel"
        ],
        "title": "Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses",
        "abstract": "arXiv:2509.10620v1 Announce Type: new  Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly acquired in clinical settings to monitor a wide range of neurological conditions, including neurodegenerative disorders and stroke. While deep learning models have shown promising results analyzing 3D MRI across a number of brain imaging tasks, most are highly tailored for specific tasks with limited labeled data, and are not able to generalize across tasks and/or populations. The development of self-supervised learning (SSL) has enabled the creation of large medical foundation models that leverage diverse, unlabeled datasets ranging from healthy to diseased data, showing significant success in 2D medical imaging applications. However, even the very few foundation models for 3D brain MRI that have been developed remain limited in resolution, scope, or accessibility. In this work, we present a general, high-resolution SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on 18,759 patients (44,958 scans) from 11 publicly available datasets spanning diverse neurological diseases. We compare our model to Masked Autoencoders (MAE), as well as two supervised baselines, on four diverse downstream prediction tasks in both in-distribution and out-of-distribution settings. Our fine-tuned SimCLR model outperforms all other models across all tasks. Notably, our model still achieves superior performance when fine-tuned using only 20% of labeled training samples for predicting Alzheimer's disease. We use publicly available code and data, and release our trained model at https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly applicable and accessible foundation model for clinical brain MRI analysis.",
        "arxiv_id": "2509.10620",
        "ARXIVID": "2509.10620",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Presents a general, high-resolution SimCLR-based self-supervised foundation model for 3D brain MRI, outperforming other models and released for public use. The focus is on medical imaging, but the foundation model aspect is relevant.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.11116": {
        "authors": [
            "Ashkan Taghipour",
            "Vahid Naghshin",
            "Benjamin Southwell",
            "Farid Boussaid",
            "Hamid Laga",
            "Mohammed Bennamoun"
        ],
        "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting",
        "abstract": "arXiv:2509.11116v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
        "arxiv_id": "2509.11116",
        "ARXIVID": "2509.11116",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is tangentially related to criterion 1 (spatial understanding) as it proposes a new regularization method for 3D Gaussian Splatting, which is important for real-time 3D perception in robotics and AR/VR. The spatially variant regularizer is a methodological improvement for spatial representation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.12039": {
        "authors": [
            "Zilong Zhang",
            "Chujie Qin",
            "Chunle Guo",
            "Yong Zhang",
            "Chao Xue",
            "Ming-Ming Cheng",
            "Chongyi Li"
        ],
        "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration",
        "abstract": "arXiv:2509.12039v1 Announce Type: new  Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at https://github.com/DragonisCV/RAM",
        "arxiv_id": "2509.12039",
        "ARXIVID": "2509.12039",
        "COMMENT": "This paper proposes a new method for robust image restoration using adaptive semantic-aware masking and leverages DINOv2 representations, which is related to vision foundation models (criterion 4). However, it is not directly about spatial intelligence or embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.11406": {
        "authors": [
            "Christoph F\\\"urb\\\"ock",
            "Paul Weiser",
            "Branko Mitic",
            "Philipp Seeb\\\"ock",
            "Thomas Helbich",
            "Georg Langs"
        ],
        "title": "No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data",
        "abstract": "arXiv:2509.11406v1 Announce Type: new  Abstract: In real world clinical environments, training and applying deep learning models on multi-modal medical imaging data often struggles with partially incomplete data. Standard approaches either discard missing samples, require imputation or repurpose dropout learning schemes, limiting robustness and generalizability. To address this, we propose a hypernetwork-based method that dynamically generates task-specific classification models conditioned on the set of available modalities. Instead of training a fixed model, a hypernetwork learns to predict the parameters of a task model adapted to available modalities, enabling training and inference on all samples, regardless of completeness. We compare this approach with (1) models trained only on complete data, (2) state of the art channel dropout methods, and (3) an imputation-based method, using artificially incomplete datasets to systematically analyze robustness to missing modalities. Results demonstrate superior adaptability of our method, outperforming state of the art approaches with an absolute increase in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of training data with missing modalities). By enabling a single model to generalize across all modality configurations, our approach provides an efficient solution for real-world multi-modal medical data analysis.",
        "arxiv_id": "2509.11406",
        "ARXIVID": "2509.11406",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) and is tangentially related to criterion 2 (multi-modal models). Proposes a hypernetwork-based method for dynamic model generation with incomplete multi-modal medical data, outperforming state-of-the-art methods for missing modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10761": {
        "authors": [
            "Marcelo Sandoval-Castaneda",
            "Bryan Russell",
            "Josef Sivic",
            "Gregory Shakhnarovich",
            "Fabian Caba Heilbron"
        ],
        "title": "EditDuet: A Multi-Agent System for Video Non-Linear Editing",
        "abstract": "arXiv:2509.10761v1 Announce Type: new  Abstract: Automated tools for video editing and assembly have applications ranging from filmmaking and advertisement to content creation for social media. Previous video editing work has mainly focused on either retrieval or user interfaces, leaving actual editing to the user. In contrast, we propose to automate the core task of video editing, formulating it as sequential decision making process. Ours is a multi-agent approach. We design an Editor agent and a Critic agent. The Editor takes as input a collection of video clips together with natural language instructions and uses tools commonly found in video editing software to produce an edited sequence. On the other hand, the Critic gives natural language feedback to the editor based on the produced sequence or renders it if it is satisfactory. We introduce a learning-based approach for enabling effective communication across specialized agents to address the language-driven video editing task. Finally, we explore an LLM-as-a-judge metric for evaluating the quality of video editing system and compare it with general human preference. We evaluate our system's output video sequences qualitatively and quantitatively through a user study and find that our system vastly outperforms existing approaches in terms of coverage, time constraint satisfaction, and human preference.",
        "arxiv_id": "2509.10761",
        "ARXIVID": "2509.10761",
        "COMMENT": "Somewhat related to criterion 2: proposes a multi-agent system for video editing using LLMs for language-driven editing, but not a new VLLM or MLLM itself. Interesting for multi-modal learning and LLM-as-a-judge evaluation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.11114": {
        "authors": [
            "Yuqiu Liu",
            "Jialin Song",
            "Manolis Savva",
            "Wuyang Chen"
        ],
        "title": "WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild",
        "abstract": "arXiv:2509.11114v1 Announce Type: new  Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from a single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at [https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).",
        "arxiv_id": "2509.11114",
        "ARXIVID": "2509.11114",
        "COMMENT": "This paper presents a pipeline for reconstructing dynamic 3D smoke assets from single in-the-wild videos, enabling interactive simulation and editing. It is relevant to vision and simulation, but does not directly address embodied AI benchmarks, spatial intelligence, or foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.11661": {
        "authors": [
            "Lifei Hao",
            "Yue Cheng",
            "Baoqi Huang",
            "Bing Jia",
            "Xuandong Zhao"
        ],
        "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition",
        "abstract": "arXiv:2509.11661v1 Announce Type: new  Abstract: Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.",
        "arxiv_id": "2509.11661",
        "ARXIVID": "2509.11661",
        "COMMENT": "Applies generative diffusion models for few-shot data augmentation in a specific industrial vision task (dirty tableware recognition). Some relevance to vision foundation models and generative modeling, but not a direct match to the criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.12052": {
        "authors": [
            "Yuchen Deng",
            "Xiuyang Wu",
            "Hai-Tao Zheng",
            "Suiyang Zhang",
            "Yi He",
            "Yuxing Han"
        ],
        "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective",
        "abstract": "arXiv:2509.12052v1 Announce Type: new  Abstract: Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate \"Divide and Conquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.",
        "arxiv_id": "2509.12052",
        "ARXIVID": "2509.12052",
        "COMMENT": "This paper introduces a new autoregressive framework for talking-head animation, focusing on controllable and efficient generation. While it is a novel generative modeling approach for video, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about generative modeling for animation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11892": {
        "authors": [
            "Akito Shinohara",
            "Kohei Fukuda",
            "Hiroaki Aizawa"
        ],
        "title": "Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection",
        "abstract": "arXiv:2509.11892v1 Announce Type: new  Abstract: The ability to detect out-of-distribution data is essential not only for ensuring robustness against unknown or unexpected input data but also for improving the generalization performance of the model. Among various out-of-distribution detection methods, Outlier Exposure and Mixture Outlier Exposure are promising approaches that enhance out-of-distribution detection performance by exposing the outlier data during training. However, even with these sophisticated techniques, it remains challenging for models to learn the relationships between classes effectively and to distinguish data sampling from in-distribution and out-of-distribution clearly. Therefore, we focus on the logit space, where the properties between class-wise distributions are distinctly separated from those in the input or feature spaces. Specifically, we propose a linear interpolation technique in the logit space that mixes in-distribution and out-of-distribution data to facilitate smoothing logits between classes and improve the out-of-distribution detection performance, particularly for out-of-distribution data that lie close to the in-distribution data. Additionally, we enforce consistency between the logits obtained through mixing in the logit space and those generated via mixing in the input space. Our experiments demonstrate that our logit-space mixing technique reduces the abrupt fluctuations in the model outputs near the decision boundaries, resulting in smoother and more reliable separation between in-distribution and out-of-distribution data. Furthermore, we evaluate the effectiveness of the proposed method on a fine-grained out-of-distribution detection task.",
        "arxiv_id": "2509.11892",
        "ARXIVID": "2509.11892",
        "COMMENT": "This paper proposes a logit-space mixing technique for out-of-distribution detection, focusing on fine-grained OOD tasks. While it introduces a clever statistical trick, it does not directly match any of the four criteria, but is relevant to general machine learning and robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11838": {
        "authors": [
            "Navid Hashemi",
            "Samuel Sasaki",
            "Diego Manzanas Lopez",
            "Ipek Oguz",
            "Meiyi Ma",
            "Taylor T. Johnson"
        ],
        "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network",
        "abstract": "arXiv:2509.11838v1 Announce Type: new  Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as medical imaging, autonomous driving, and environmental monitoring, where safety hinges on reliable model behavior under uncertainty. Yet, existing probabilistic verification approaches struggle to scale with the complexity and dimensionality of modern segmentation tasks, often yielding guarantees that are too conservative to be practical. We introduce a probabilistic verification framework that is both architecture-agnostic and scalable to high-dimensional outputs. Our approach combines sampling-based reachability analysis with conformal inference (CI) to deliver provable guarantees while avoiding the excessive conservatism of prior methods. To counteract CI's limitations in high-dimensional settings, we propose novel strategies that reduce conservatism without compromising rigor. Empirical evaluation on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates that our method provides reliable safety guarantees while substantially tightening bounds compared to SOTA. We also provide a toolbox implementing this technique, available on Github.",
        "arxiv_id": "2509.11838",
        "ARXIVID": "2509.11838",
        "COMMENT": "This paper introduces a new probabilistic verification framework for semantic segmentation networks, focusing on robustness in high-dimensional spaces. While it presents methodological improvements for spatial understanding in segmentation, it does not focus on embodied agents or spatial intelligence in embodied settings (criterion 1). It is relevant to computer vision and robustness, but not a direct match to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11344": {
        "authors": [
            "Huaiyuan Qin",
            "Muli Yang",
            "Siyuan Hu",
            "Peng Hu",
            "Yu Zhang",
            "Chen Gong",
            "Hongyuan Zhu"
        ],
        "title": "Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning",
        "abstract": "arXiv:2509.11344v1 Announce Type: new  Abstract: Self-supervised learning (SSL) conventionally relies on the instance consistency paradigm, assuming that different views of the same image can be treated as positive pairs. However, this assumption breaks down for non-iconic data, where different views may contain distinct objects or semantic information. In this paper, we investigate the effectiveness of SSL when instance consistency is not guaranteed. Through extensive ablation studies, we demonstrate that SSL can still learn meaningful representations even when positive pairs lack strict instance consistency. Furthermore, our analysis further reveals that increasing view diversity, by enforcing zero overlapping or using smaller crop scales, can enhance downstream performance on classification and dense prediction tasks. However, excessive diversity is found to reduce effectiveness, suggesting an optimal range for view diversity. To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to measure mutual information between views, finding that moderate EMD values correlate with improved SSL learning, providing insights for future SSL framework design. We validate our findings across a range of settings, highlighting their robustness and applicability on diverse data sources.",
        "arxiv_id": "2509.11344",
        "ARXIVID": "2509.11344",
        "COMMENT": "Relevant to general self-supervised learning in vision, but does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. Focuses on view diversity in SSL.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11385": {
        "authors": [
            "Akhil Padmanabha",
            "Arpit Agarwal",
            "Catherine Li",
            "Austin Williams",
            "Dinesh K. Patel",
            "Sankalp Chopkar",
            "Achu Wilson",
            "Ahmet Ozkan",
            "Wenzhen Yuan",
            "Sonal Choudhary",
            "Arash Mostaghimi",
            "Zackory Erickson",
            "Carmel Majidi"
        ],
        "title": "In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing",
        "abstract": "arXiv:2509.11385v1 Announce Type: new  Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for objective and quantitative dermatological assessment, but no portable, high-resolution device exists that has been validated and used for depth reconstruction across various body locations. We present a compact 3-D skin reconstruction probe based on GelSight tactile imaging with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation. Our probe, integrated into a handheld probe with force sensing for consistent contact, achieves a mean absolute error of 12.55 micron on wrinkle-like test objects. In a study with 15 participants without skin disorders, we provide the first validated wrinkle depth metrics across multiple body regions. We further demonstrate statistically significant reductions in wrinkle height at three locations following over-the-counter moisturizer application. Our work offers a validated tool for clinical and cosmetic skin analysis, with potential applications in diagnosis, treatment monitoring, and skincare efficacy evaluation.",
        "arxiv_id": "2509.11385",
        "ARXIVID": "2509.11385",
        "COMMENT": "This paper presents a new tactile sensing device and learning-based method for 3D skin surface reconstruction. While it is an interesting application of computer vision and learning, it does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11178": {
        "authors": [
            "Chengde Lin",
            "Xuezhu Gong",
            "Shuxue Ding",
            "Mingzhe Yang",
            "Xijun Lu",
            "Chengjun Mo"
        ],
        "title": "StegOT: Trade-offs in Steganography via Optimal Transport",
        "abstract": "arXiv:2509.11178v1 Announce Type: new  Abstract: Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on https://github.com/Rss1124/StegOT.",
        "arxiv_id": "2509.11178",
        "ARXIVID": "2509.11178",
        "COMMENT": "This paper is about steganography using optimal transport and generative models. It does not match any of the specific criteria, though it is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11811": {
        "authors": [
            "Mehwish Mehmood",
            "Shahzaib Iqbal",
            "Tariq Mahmood Khan",
            "Ivor Spence",
            "Muhammad Fahim"
        ],
        "title": "LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio",
        "abstract": "arXiv:2509.11811v1 Announce Type: new  Abstract: Retinal vessel segmentation is critical for the early diagnosis of vision-threatening and systemic diseases, especially in real-world clinical settings with limited computational resources. Although significant improvements have been made in deep learning-based segmentation methods, current models still face challenges in extracting tiny vessels and suffer from high computational costs. In this study, we present LFRA-Net by incorporating focal modulation attention at the encoder-decoder bottleneck and region-aware attention in the selective skip connections. LFRA-Net is a lightweight network optimized for precise and effective retinal vascular segmentation. It enhances feature representation and regional focus by efficiently capturing local and global dependencies. LFRA-Net outperformed many state-of-the-art models while maintaining lightweight characteristics with only 0.17 million parameters, 0.66 MB memory size, and 10.50 GFLOPs. We validated it on three publicly available datasets: DRIVE, STARE, and CHASE\\_DB. It performed better in terms of Dice score (84.28\\%, 88.44\\%, and 85.50\\%) and Jaccard index (72.86\\%, 79.31\\%, and 74.70\\%) on the DRIVE, STARE, and CHASE\\_DB datasets, respectively. LFRA-Net provides an ideal ratio between segmentation accuracy and computational cost compared to existing deep learning methods, which makes it suitable for real-time clinical applications in areas with limited resources. The code can be found at https://github.com/Mehwish4593/LFRA-Net.",
        "arxiv_id": "2509.11811",
        "ARXIVID": "2509.11811",
        "COMMENT": "This paper presents LFRA-Net, a lightweight attention network for retinal vessel segmentation. While it introduces a new architecture for spatial understanding in medical images, it does not focus on embodied agents, VLLMs/MLLMs, or foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11720": {
        "authors": [
            "Nikolaos Livathinos",
            "Christoph Auer",
            "Ahmed Nassar",
            "Rafael Teixeira de Lima",
            "Maksym Lysak",
            "Brown Ebouky",
            "Cesar Berrospi",
            "Michele Dolfi",
            "Panagiotis Vagenas",
            "Matteo Omenetti",
            "Kasper Dinkla",
            "Yusik Kim",
            "Valery Weber",
            "Lucas Morin",
            "Ingmar Meijer",
            "Viktor Kuropiatnyk",
            "Tim Strohmeyer",
            "A. Said Gurbuz",
            "Peter W. J. Staar"
        ],
        "title": "Advanced Layout Analysis Models for Docling",
        "abstract": "arXiv:2509.11720v1 Announce Type: new  Abstract: This technical report documents the development of novel Layout Analysis models integrated into the Docling document-conversion pipeline. We trained several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly available and proprietary). Post-processing steps were applied to the raw detections to make them more applicable to the document conversion task. We evaluated the effectiveness of the layout analysis on various document benchmarks using different methodologies while also measuring the runtime performance across different environments (CPU, Nvidia and Apple GPUs). We introduce five new document layout models achieving 20.6% - 23.9% mAP improvement over Docling's previous baseline, with comparable or better runtime. Our best model, \"heron-101\", attains 78% mAP with 28 ms/image inference time on a single NVIDIA A100 GPU. Extensive quantitative and qualitative experiments establish best practices for training, evaluating, and deploying document-layout detectors, providing actionable guidance for the document conversion community. All trained checkpoints, code, and documentation are released under a permissive license on HuggingFace.",
        "arxiv_id": "2509.11720",
        "ARXIVID": "2509.11720",
        "COMMENT": "Relevant to document layout analysis and object detection, but not directly to spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models in a general sense. Application is document conversion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11078": {
        "authors": [
            "Yunghwei Lai",
            "Weizhi Ma",
            "Yang Liu"
        ],
        "title": "Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation",
        "abstract": "arXiv:2509.11078v1 Announce Type: new  Abstract: Synthetic data generation using large language models (LLMs) has emerged as a promising solution across various domains, particularly in medical field, to mitigate data collection challenges. However, existing studies mainly utilize LLMs to rewrite and complete existing medical records, where the limitations in data privacy, accuracy, and diversity sill exist, and additionally lack the ability to interact like real patients. To address these issues, we propose a realistic patient generation framework, Patient-Zero, which requires no real medical records. Patient-Zero first introduces a medically-aligned multi-step generation architecture, which builds comprehensive patient records through hierarchical medical knowledge injection without real medical records. Then, to optimize the virtual patient's interaction abilities with humans, Patient-Zero designs a dynamic updating mechanism to improve the consistency and conversational performance. Our framework enables the generation of contextually diverse patient records while maintaining strict medical coherence, supported by adaptive dialogue strategies and real-time clinical plausibility verification. Experimental results demonstrate that our model achieves good performance in accuracy, diversity, and consistency. After training with our generated virtual patients, existing models show significant improvements on the MedQA dataset.",
        "arxiv_id": "2509.11078",
        "ARXIVID": "2509.11078",
        "COMMENT": "Does not directly match any criteria, but is tangentially related to criterion 2 (MLLMs) as it uses LLMs for synthetic patient data generation. Focus is on medical data simulation, not on spatial intelligence or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11727": {
        "authors": [
            "Tae Kyeong Jeong",
            "Garam Kim",
            "Juyoun Park"
        ],
        "title": "Microsurgical Instrument Segmentation for Robot-Assisted Surgery",
        "abstract": "arXiv:2509.11727v1 Announce Type: new  Abstract: Accurate segmentation of thin structures is critical for microsurgical scene understanding but remains challenging due to resolution loss, low contrast, and class imbalance. We propose Microsurgery Instrument Segmentation for Robotic Assistance(MISRA), a segmentation framework that augments RGB input with luminance channels, integrates skip attention to preserve elongated features, and employs an Iterative Feedback Module(IFM) for continuity restoration across multiple passes. In addition, we introduce a dedicated microsurgical dataset with fine-grained annotations of surgical instruments including thin objects, providing a benchmark for robust evaluation Dataset available at https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate that MISRA achieves competitive performance, improving the mean class IoU by 5.37% over competing methods, while delivering more stable predictions at instrument contacts and overlaps. These results position MISRA as a promising step toward reliable scene parsing for computer-assisted and robotic microsurgery.",
        "arxiv_id": "2509.11727",
        "ARXIVID": "2509.11727",
        "COMMENT": "Does not directly match any criteria, but is relevant to computer vision and segmentation. Proposes a new segmentation framework for microsurgical instruments and introduces a new dataset. Not focused on embodied AI, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12090": {
        "authors": [
            "Yihong Chen",
            "Jiancheng Yang",
            "Deniz Sayin Mercadier",
            "Hieu Le",
            "Juerg Schwitter",
            "Pascal Fua"
        ],
        "title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI",
        "abstract": "arXiv:2509.12090v1 Announce Type: new  Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.",
        "arxiv_id": "2509.12090",
        "ARXIVID": "2509.12090",
        "COMMENT": "Does not directly match any criteria, but is relevant to computer vision and medical imaging. Proposes a new framework for 4D heart mesh recovery from MRI, with innovations in feature assembly and motion learning. Not directly about spatial intelligence in embodied agents or VLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11941": {
        "authors": [
            "Ilia Kopanichuk",
            "Petr Anokhin",
            "Vladimir Shaposhnikov",
            "Vladimir Makharev",
            "Ekaterina Tsapieva",
            "Iaroslav Bespalov",
            "Dmitry V. Dylov",
            "Ivan Oseledets"
        ],
        "title": "How to Evaluate Medical AI",
        "abstract": "arXiv:2509.11941v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into medical diagnostic workflows requires robust and consistent evaluation methods to ensure reliability, clinical relevance, and the inherent variability in expert judgments. Traditional metrics like precision and recall often fail to account for the inherent variability in expert judgments, leading to inconsistent assessments of AI performance. Inter-rater agreement statistics like Cohen's Kappa are more reliable but they lack interpretability. We introduce Relative Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new evaluation metrics that compare AI outputs against multiple expert opinions rather than a single reference. By normalizing performance against inter-expert disagreement, these metrics provide a more stable and realistic measure of the quality of predicted diagnosis. In addition to the comprehensive analysis of diagnostic quality measures, our study contains a very important side result. Our evaluation methodology allows us to avoid selecting diagnoses from a limited list when evaluating a given case. Instead, both the models being tested and the examiners verifying them arrive at a free-form diagnosis. In this automated methodology for establishing the identity of free-form clinical diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our approach using 360 medical dialogues, comparing multiple large language models (LLMs) against a panel of physicians. Large-scale study shows that top-performing models, such as DeepSeek-V3, achieve consistency on par with or exceeding expert consensus. Moreover, we demonstrate that expert judgments exhibit significant variability - often greater than that between AI and humans. This finding underscores the limitations of any absolute metrics and supports the need to adopt relative metrics in medical AI.",
        "arxiv_id": "2509.11941",
        "ARXIVID": "2509.11941",
        "COMMENT": "Does not match any specific criterion. Focuses on evaluation metrics for medical AI and LLMs, but not on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10995": {
        "authors": [
            "Nisha Pillai",
            "Aditi Virupakshaiah",
            "Harrison W. Smith",
            "Amanda J. Ashworth",
            "Prasanna Gowda",
            "Phillip R. Owens",
            "Adam R. Rivers",
            "Bindu Nanduri",
            "Mahalingam Ramkumar"
        ],
        "title": "Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring",
        "abstract": "arXiv:2509.10995v1 Announce Type: new  Abstract: Animal health monitoring and population management are critical aspects of wildlife conservation and livestock management that increasingly rely on automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV) based systems combined with computer vision offer promising solutions for non-invasive animal monitoring across challenging terrains, limited availability of labeled training data remains an obstacle in developing effective deep learning (DL) models for these applications. Transfer learning has emerged as a potential solution, allowing models trained on large datasets to be adapted for resource-limited scenarios such as those with limited data. However, the vast landscape of pre-trained neural network architectures makes it challenging to select optimal models, particularly for researchers new to the field. In this paper, we propose a reinforcement learning (RL)-based transfer learning framework that employs an upper confidence bound (UCB) algorithm to automatically select the most suitable pre-trained model for animal detection tasks. Our approach systematically evaluates and ranks candidate models based on their performance, streamlining the model selection process. Experimental results demonstrate that our framework achieves a higher detection rate while requiring significantly less computational time compared to traditional methods.",
        "arxiv_id": "2509.10995",
        "ARXIVID": "2509.10995",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1) as it uses RL for model selection in animal monitoring, but not directly about embodied agents or spatial intelligence methods. More about transfer learning and model selection.",
        "RELEVANCE": 4,
        "NOVELTY": 4
    },
    "2509.11184": {
        "authors": [
            "Partha Shah",
            "Durva Sankhe",
            "Maariyah Rashid",
            "Zakaa Khaled",
            "Esther Puyol-Ant\\'on",
            "Tiarna Lee",
            "Maram Alqarni",
            "Sweta Rai",
            "Andrew P. King"
        ],
        "title": "The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models",
        "abstract": "arXiv:2509.11184v1 Announce Type: new  Abstract: Artificial intelligence (AI) models to automatically classify skin lesions from dermatology images have shown promising performance but also susceptibility to bias by skin tone. The most common way of representing skin tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has been criticised for having greater granularity in its skin tone categories for lighter-skinned subjects. This paper conducts an investigation of the impact (on performance and bias) on AI classification models of granularity in the FST scale. By training multiple AI models to classify benign vs. malignant lesions using FST-specific data of differing granularity, we show that: (i) when training models using FST-specific data based on three groups (FST 1/2, 3/4 and 5/6), performance is generally better for models trained on FST-specific data compared to a general model trained on FST-balanced data; (ii) reducing the granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a detrimental effect on performance. Our results highlight the importance of the granularity of FST groups when training lesion classification models. Given the question marks over possible human biases in the choice of categories in the FST scale, this paper provides evidence for a move away from the FST scale in fair AI research and a transition to an alternative scale that better represents the diversity of human skin tones.",
        "arxiv_id": "2509.11184",
        "ARXIVID": "2509.11184",
        "COMMENT": "This paper studies the impact of skin tone label granularity on fairness in dermatology image classification. It is not directly related to any of the criteria, though it is in the general area of computer vision and fairness.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.10961": {
        "authors": [
            "Farhan Sadik",
            "Christopher L. Newman",
            "Stuart J. Warden",
            "Rachel K. Surowiec"
        ],
        "title": "Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging",
        "abstract": "arXiv:2509.10961v1 Announce Type: new  Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular smearing, hinder in vivo assessment of bone microstructures in high-resolution peripheral quantitative computed tomography (HR-pQCT). Despite various motion grading techniques, no motion correction methods exist due to the lack of standardized degradation models. We optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, creating paired datasets of motion-corrupted images and their corresponding ground truth, which enables seamless integration into supervised learning frameworks for motion correction. As such, we propose an Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts in both simulated (source) and real-world (target) datasets. The model incorporates edge-enhancing skip connections to preserve trabecular edges and self-attention mechanisms to capture long-range dependencies, facilitating motion correction. A visual geometry group (VGG)-based perceptual loss is used to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean signal-to-noise ratio (SNR) of 26.78, structural similarity index measure (SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source dataset, while showing improved performance on the target dataset with an SNR of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a simplified representation of real-world motion that may not fully capture the complexity of in vivo motion artifacts. Nevertheless, because motion artifacts present one of the foremost challenges to more widespread adoption of this modality, these methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT.",
        "arxiv_id": "2509.10961",
        "ARXIVID": "2509.10961",
        "COMMENT": "Does not match any specific criterion. Focuses on deep learning for motion artifact correction in bone imaging, not on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.10767": {
        "authors": [
            "Sajad Amiri",
            "Shahram Taeb",
            "Sara Gharibi",
            "Setareh Dehghanfard",
            "Somayeh Sadat Mehrnia",
            "Mehrdad Oveisi",
            "Ilker Hacihaliloglu",
            "Arman Rahmim",
            "Mohammad R. Salmanpour"
        ],
        "title": "Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging",
        "abstract": "arXiv:2509.10767v1 Announce Type: new  Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but raise safety, cost, and accessibility concerns. Predicting contrast enhancement from non-contrast MRI using machine learning (ML) offers a safer alternative, as enhancement reflects tumor aggressiveness and informs treatment planning. Yet scanner and cohort variability hinder robust model selection. We propose a stability-aware framework to identify reproducible ML pipelines for multicenter prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG). Non-contrast T1WI served as input, with enhancement derived from paired post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were extracted and combined with 48 dimensionality reduction methods and 25 classifiers, yielding 1,200 pipelines. Rotational validation was trained on three datasets and tested on the fourth. Cross-validation prediction accuracies ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM), 0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1, precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr pipeline consistently ranked highest, balancing accuracy and stability. This framework demonstrates that stability-aware model selection enables reliable prediction of contrast enhancement from non-contrast glioma MRI, reducing reliance on GBCAs and improving generalizability across centers. It provides a scalable template for reproducible ML in neuro-oncology and beyond.",
        "arxiv_id": "2509.10767",
        "ARXIVID": "2509.10767",
        "COMMENT": "Does not match any specific criterion. Focuses on stability-aware ML for MRI imaging, but not on spatial intelligence, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}