{
    "2511.00710": {
        "authors": [
            "Minghe Shen",
            "Zhuo Zhi",
            "Chonghan Liu",
            "Shuo Xing",
            "Zhengzhong Tu",
            "Che Liu"
        ],
        "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries",
        "abstract": "arXiv:2511.00710v1 Announce Type: new  Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.",
        "arxiv_id": "2511.00710",
        "ARXIVID": "2511.00710",
        "COMMENT": "Matches criterion 1: New methodological improvements to spatial understanding, spatial intelligence on embodied agents, and criterion 2: Shows new VLLMs (visual large language models). Ariadne introduces a controllable framework for probing and extending VLM reasoning boundaries, especially for spatial reasoning, and demonstrates surprising empirical results in expanding VLM capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.01914": {
        "authors": [
            "Yuan Zhang",
            "Chenyu Xue",
            "Wenjie Xu",
            "Chao Ji",
            "Jiajia wu",
            "Jia Pan"
        ],
        "title": "iFlyBot-VLA Technical Report",
        "abstract": "arXiv:2511.01914v1 Announce Type: new  Abstract: We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community",
        "arxiv_id": "2511.01914",
        "ARXIVID": "2511.01914",
        "COMMENT": "Matches criteria 1, 2, and 3: This paper introduces iFlyBot-VLA, a large-scale Vision-Language-Action model with a novel dual-level action representation and mixed training strategy to enhance 3D perceptual and reasoning capabilities for embodied agents. It also benchmarks on LIBERO Franka and plans to open-source data, making it highly relevant to spatial intelligence, VLLMs/MLLMs, and embodied AI benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.01824": {
        "authors": [
            "Yuetai Li",
            "Huseyin A Inan",
            "Xiang Yue",
            "Wei-Ning Chen",
            "Lukas Wutschitz",
            "Janardhan Kulkarni",
            "Radha Poovendran",
            "Robert Sim",
            "Saravan Rajmohan"
        ],
        "title": "Simulating Environments with Reasoning Models for Agent Training",
        "abstract": "arXiv:2511.01824v1 Announce Type: new  Abstract: LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.",
        "arxiv_id": "2511.01824",
        "ARXIVID": "2511.01824",
        "COMMENT": "Matches criterion 3: This paper proposes a novel method for agent training by simulating environments using LLMs, removing the need for bespoke environment engineering. This is a new angle for embodied AI and agent training, focusing on scalable, flexible simulation for RL and SFT without real environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.02607": {
        "authors": [
            "Xu Zhang",
            "Danyang Li",
            "Xiaohang Dong",
            "Tianhao Wu",
            "Hualong Yu",
            "Jianye Wang",
            "Qicheng Li",
            "Xiang Li"
        ],
        "title": "UniChange: Unifying Change Detection with Multimodal Large Language Model",
        "abstract": "arXiv:2511.02607v1 Announce Type: new  Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.",
        "arxiv_id": "2511.02607",
        "ARXIVID": "2511.02607",
        "COMMENT": "Matches criterion 2 and 4: This paper introduces UniChange, the first MLLM-based unified change detection model, leveraging generative language abilities and text prompts for both binary and semantic change detection. It is directly about MLLMs and vision foundation models for remote sensing change detection.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.02778": {
        "authors": [
            "Kevin Qinghong Lin",
            "Yuhao Zheng",
            "Hangyu Ran",
            "Dantong Zhu",
            "Dongxing Mao",
            "Linjie Li",
            "Philip Torr",
            "Alex Jinpeng Wang"
        ],
        "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation",
        "abstract": "arXiv:2511.02778v1 Announce Type: new  Abstract: Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.",
        "arxiv_id": "2511.02778",
        "ARXIVID": "2511.02778",
        "COMMENT": "Matches criterion 2 and 4: This paper introduces VCode, a multimodal coding benchmark using SVG as a symbolic visual representation, and proposes VCoder, an agentic framework for visual code generation with VLMs. It is directly about VLLMs/MLLMs and vision foundation models, with a novel benchmark and method.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.02779": {
        "authors": [
            "Yiyang Zhou",
            "Haoqin Tu",
            "Zijun Wang",
            "Zeyu Wang",
            "Niklas Muennighoff",
            "Fan Nie",
            "Yejin Choi",
            "James Zou",
            "Chaorui Deng",
            "Shen Yan",
            "Haoqi Fan",
            "Cihang Xie",
            "Huaxiu Yao",
            "Qinghao Ye"
        ],
        "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought",
        "abstract": "arXiv:2511.02779v1 Announce Type: new  Abstract: We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.",
        "arxiv_id": "2511.02779",
        "ARXIVID": "2511.02779",
        "COMMENT": "Matches criterion 2 and 4. Proposes MIRA, a new benchmark for visual chain-of-thought reasoning, specifically evaluating multimodal large language models (MLLMs) on tasks requiring intermediate visual reasoning. The benchmark highlights the importance of visual intermediate steps for reasoning, which is a novel angle in VLLM/MLLM evaluation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.02712": {
        "authors": [
            "Zhicheng Zhang",
            "Weicheng Wang",
            "Yongjie Zhu",
            "Wenyu Qin",
            "Pengfei Wan",
            "Di Zhang",
            "Jufeng Yang"
        ],
        "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models",
        "abstract": "arXiv:2511.02712v1 Announce Type: new  Abstract: Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.",
        "arxiv_id": "2511.02712",
        "ARXIVID": "2511.02712",
        "COMMENT": "Matches criterion 4: This paper introduces VidEmo, a family of video emotion foundation models, and a large-scale emotion-centric dataset. It is directly about vision foundation models and their application to emotion reasoning in videos.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.02650": {
        "authors": [
            "Tianfan Peng",
            "Yuntao Du",
            "Pengzhou Ji",
            "Shijie Dong",
            "Kailin Jiang",
            "Mingchuan Ma",
            "Yijun Tian",
            "Jinhe Bi",
            "Qian Li",
            "Wei Du",
            "Feng Xiao",
            "Lizhen Cui"
        ],
        "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models",
        "abstract": "arXiv:2511.02650v1 Announce Type: new  Abstract: Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.",
        "arxiv_id": "2511.02650",
        "ARXIVID": "2511.02650",
        "COMMENT": "Matches criterion 2 (VLLMs/MLLMs): This paper introduces UniPruneBench, a unified benchmark for visual token pruning in large multimodal models (LMMs), and provides empirical insights into token compression. It is directly about improving efficiency in VLLMs/MLLMs and provides a new benchmark for this purpose.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.02384": {
        "authors": [
            "Jiahe Song",
            "Chuang Wang",
            "Bowen Jiang",
            "Yinfan Wang",
            "Hao Zheng",
            "Xingjian Wei",
            "Chengjin Liu",
            "Junyuan Gao",
            "Yubin Wang",
            "Lijun Wu",
            "Jiang Wu",
            "Qian Yu",
            "Conghui He"
        ],
        "title": "RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning",
        "abstract": "arXiv:2511.02384v1 Announce Type: new  Abstract: Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed \"BBox and Index as Visual Prompt\" (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.",
        "arxiv_id": "2511.02384",
        "ARXIVID": "2511.02384",
        "COMMENT": "Matches criterion 2 and 4: This paper reformulates chemical reaction diagram parsing as a visual prompt-guided captioning task using large vision-language models (LVLMs), and introduces a new dataset. It leverages vision foundation models for structured information extraction, and proposes a novel visual prompting strategy.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.02415": {
        "authors": [
            "Duo Xu",
            "Hao Cheng",
            "Xin Lin",
            "Zhen Xie",
            "Hao Wang"
        ],
        "title": "ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
        "abstract": "arXiv:2511.02415v1 Announce Type: new  Abstract: Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.",
        "arxiv_id": "2511.02415",
        "ARXIVID": "2511.02415",
        "COMMENT": "Matches criterion 2 and 4. Proposes ChartM$^3$, a code-driven pipeline for generating multi-dimensional, multi-step visual reasoning datasets for chart comprehension, specifically targeting MLLMs. The dataset and pipeline enable improved reasoning and generalization in chart understanding, which is a novel contribution for vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.02483": {
        "authors": [
            "Xilong Zhou",
            "Jianchun Chen",
            "Pramod Rao",
            "Timo Teufel",
            "Linjie Lyu",
            "Tigran Minasian",
            "Oleksandr Sotnychenko",
            "Xiaoxiao Long",
            "Marc Habermann",
            "Christian Theobalt"
        ],
        "title": "OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control",
        "abstract": "arXiv:2511.02483v1 Announce Type: new  Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.",
        "arxiv_id": "2511.02483",
        "ARXIVID": "2511.02483",
        "COMMENT": "Matches criterion 4: Vision foundation models related and its applications. OLATverse is a large-scale, real-world object dataset with precise lighting control, enabling new benchmarks and applications in inverse rendering and relighting, which are core vision foundation model tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02473": {
        "authors": [
            "Taiga Yamane",
            "Satoshi Suzuki",
            "Ryo Masumura",
            "Shotaro Tora"
        ],
        "title": "MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer",
        "abstract": "arXiv:2511.02473v1 Announce Type: new  Abstract: Multi-view action recognition aims to recognize human actions using multiple camera views and deals with occlusion caused by obstacles or crowds. In this task, cooperation among views, which generates a joint representation by combining multiple views, is vital. Previous studies have explored promising cooperation methods for improving performance. However, since their methods focus only on the task setting of recognizing a single action from an entire video, they are not applicable to the recently popular spatio-temporal action recognition~(STAR) setting, in which each person's action is recognized sequentially. To address this problem, this paper proposes a multi-view action recognition method for the STAR setting, called MVAFormer. In MVAFormer, we introduce a novel transformer-based cooperation module among views. In contrast to previous studies, which utilize embedding vectors with lost spatial information, our module utilizes the feature map for effective cooperation in the STAR setting, which preserves the spatial information. Furthermore, in our module, we divide the self-attention for the same and different views to model the relationship between multiple views effectively. The results of experiments using a newly collected dataset demonstrate that MVAFormer outperforms the comparison baselines by approximately $4.4$ points on the F-measure.",
        "arxiv_id": "2511.02473",
        "ARXIVID": "2511.02473",
        "COMMENT": "Matches criterion 1: This paper proposes MVAFormer, a transformer-based method for multi-view spatio-temporal action recognition, with a novel cooperation module that preserves spatial information. It is a methodological improvement in spatial understanding for action recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02830": {
        "authors": [
            "Dmitrii Pozdeev",
            "Alexey Artemov",
            "Ananta R. Bhattarai",
            "Artem Sevastopolsky"
        ],
        "title": "Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks",
        "abstract": "arXiv:2511.02830v1 Announce Type: new  Abstract: We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face/head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.",
        "arxiv_id": "2511.02830",
        "ARXIVID": "2511.02830",
        "COMMENT": "Matches criterion 4. Proposes DenseMarks, a new learned representation for dense correspondences in human head images using a Vision Transformer. The method enables robust, geometry-aware point matching and tracking, and is a novel application of vision foundation models to dense spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00162": {
        "authors": [
            "Michael D. Moffitt"
        ],
        "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus",
        "abstract": "arXiv:2511.00162v2 Announce Type: new  Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\\langle$ input, output $\\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.",
        "arxiv_id": "2511.00162",
        "ARXIVID": "2511.00162",
        "COMMENT": "Matches criterion 3. Introduces ARC-GEN, a procedural benchmark generator for the Abstraction and Reasoning Corpus (ARC), which is widely used for evaluating generalization and skill acquisition in AI. The generator is exhaustive and mimetic, covering all ARC tasks and closely matching the original distribution, which is a novel contribution for embodied AI and benchmarking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00509": {
        "authors": [
            "Yifan Xia",
            "Guorui Chen",
            "Wenqian Yu",
            "Zhijiang Li",
            "Philip Torr",
            "Jindong Gu"
        ],
        "title": "Reimagining Safety Alignment with An Image",
        "abstract": "arXiv:2511.00509v1 Announce Type: new  Abstract: Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.",
        "arxiv_id": "2511.00509",
        "ARXIVID": "2511.00509",
        "COMMENT": "Matches criterion 2. Proposes Magic Image, a visual prompt framework for safety alignment in multimodal large language models (MLLMs), addressing over-refusal and harmful content generation. The method is a novel approach to safety alignment in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.02495": {
        "authors": [
            "Zixuan Liu",
            "Siavash H. Khajavi",
            "Guangkai Jiang"
        ],
        "title": "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding",
        "abstract": "arXiv:2511.02495v1 Announce Type: new  Abstract: Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890",
        "arxiv_id": "2511.02495",
        "ARXIVID": "2511.02495",
        "COMMENT": "Matches criterion 2: Shows new MLLMs (multi-modal large language models) and criterion 4: Vision foundation models related and its applications. Introduces a large-scale multi-modal dataset for fire understanding, enabling vision-language reasoning and synthetic data generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.00609": {
        "authors": [
            "Shengqi Xu",
            "Xinpeng Zhou",
            "Yabo Zhang",
            "Ming Liu",
            "Tao Liang",
            "Tianyu Zhang",
            "Yalong Bai",
            "Zuxuan Wu",
            "Wangmeng Zuo"
        ],
        "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment",
        "abstract": "arXiv:2511.00609v1 Announce Type: new  Abstract: Personalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \\textit{predict-then-assess} paradigm: it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.",
        "arxiv_id": "2511.00609",
        "ARXIVID": "2511.00609",
        "COMMENT": "Somewhat related to criterion 2: Shows new VLLMs or MLLMs, as it proposes a reasoning-based personalized image preference assessment framework using multi-modal data and chain-of-thought reasoning. However, it is more focused on preference modeling than general VLLM/MLLM advances.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.02563": {
        "authors": [
            "Akash Sharma",
            "Chinmay Mhatre",
            "Sankalp Gawali",
            "Ruthvik Bokkasam",
            "Brij Kishore",
            "Vishwajeet Pattanaik",
            "Tarun Rambha",
            "Abdul R. Pinjari",
            "Vijay Kovvali",
            "Anirban Chakraborty",
            "Punit Rathore",
            "Raghu Krishnapuram",
            "Yogesh Simmhan"
        ],
        "title": "The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic",
        "abstract": "arXiv:2511.02563v1 Announce Type: new  Abstract: This report describes the UVH-26 dataset, the first public release by AIM@IISc of a large-scale dataset of annotated traffic-camera images from India. The dataset comprises 26,646 high-resolution (1080p) images sampled from 2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently annotated through a crowdsourced hackathon involving 565 college students from across India. In total, 1.8 million bounding boxes were labeled across 14 vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler (Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller, Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k consensus ground truth bounding boxes and labels were derived for distinct objects in the 26k images using Majority Voting and STAPLE algorithms. Further, we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X, and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50, mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in mAP50:95 over equivalent baseline models trained on COCO dataset, with RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40 for COCO-trained weights for common classes (Car, Bus, and Truck). This demonstrates the benefits of domain-specific training data for Indian traffic scenarios. The release package provides the 26k images with consensus annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the 6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the heterogeneity of Indian urban mobility directly from operational traffic-camera streams, UVH-26 addresses a critical gap in existing global benchmarks, and offers a foundation for advancing detection, classification, and deployment of intelligent transportation systems in emerging nations with complex traffic conditions.",
        "arxiv_id": "2511.02563",
        "ARXIVID": "2511.02563",
        "COMMENT": "Partially matches criterion 4: Vision foundation models related and its applications, as it provides a new large-scale dataset for traffic understanding in India, which can be used to train and evaluate vision models. However, it is more of a dataset paper than a methodological advance.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2511.02247": {
        "authors": [
            "Hao Li",
            "Daiwei Lu",
            "Jesse d'Almeida",
            "Dilara Isik",
            "Ehsan Khodapanah Aghdam",
            "Nick DiSanto",
            "Ayberk Acar",
            "Susheela Sharma",
            "Jie Ying Wu",
            "Robert J. Webster III",
            "Ipek Oguz"
        ],
        "title": "Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency",
        "abstract": "arXiv:2511.02247v1 Announce Type: new  Abstract: Monocular depth estimation (MDE) is a critical task to guide autonomous medical robots. However, obtaining absolute (metric) depth from an endoscopy camera in surgical scenes is difficult, which limits supervised learning of depth on real endoscopic images. Current image-level unsupervised domain adaptation methods translate synthetic images with known depth maps into the style of real endoscopic frames and train depth networks using these translated images with their corresponding depth maps. However a domain gap often remains between real and translated synthetic images. In this paper, we present a latent feature alignment method to improve absolute depth estimation by reducing this domain gap in the context of endoscopic videos of the central airway. Our methods are agnostic to the image translation process and focus on the depth estimation itself. Specifically, the depth network takes translated synthetic and real endoscopic frames as input and learns latent domain-invariant features via adversarial learning and directional feature consistency. The evaluation is conducted on endoscopic videos of central airway phantoms with manually aligned absolute depth maps. Compared to state-of-the-art MDE methods, our approach achieves superior performance on both absolute and relative depth metrics, and consistently improves results across various backbones and pretrained weights. Our code is available at https://github.com/MedICL-VU/MDE.",
        "arxiv_id": "2511.02247",
        "ARXIVID": "2511.02247",
        "COMMENT": "This paper proposes a domain-invariant feature learning method for monocular depth estimation in endoscopy. While it is a methodological improvement in spatial understanding for medical robots, it does not focus on embodied agents in the general sense, VLLMs/MLLMs, or vision foundation models. It is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2511.02349": {
        "authors": [
            "Jiankai Tang",
            "Tao Zhang",
            "Jia Li",
            "Yiru Zhang",
            "Mingyu Zhang",
            "Kegang Wang",
            "Yuming Hao",
            "Bolin Wang",
            "Haiyang Li",
            "Xingyao Wang",
            "Yuanchun Shi",
            "Yuntao Wang",
            "Sichong Qian"
        ],
        "title": "M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings",
        "abstract": "arXiv:2511.02349v1 Announce Type: new  Abstract: Portable physiological monitoring is essential for early detection and management of cardiovascular disease, but current methods often require specialized equipment that limits accessibility or impose impractical postures that patients cannot maintain. Video-based photoplethysmography on smartphones offers a convenient noninvasive alternative, yet it still faces reliability challenges caused by motion artifacts, lighting variations, and single-view constraints. Few studies have demonstrated reliable application to cardiovascular patients, and no widely used open datasets exist for cross-device accuracy. To address these limitations, we introduce the M3PD dataset, the first publicly available dual-view mobile photoplethysmography dataset, comprising synchronized facial and fingertip videos captured simultaneously via front and rear smartphone cameras from 60 participants (including 47 cardiovascular patients). Building on this dual-view setting, we further propose F3Mamba, which fuses the facial and fingertip views through Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to 30.2 percent over existing single-view baselines while improving robustness in challenging real-world scenarios. Data and code: https://github.com/Health-HCI-Group/F3Mamba.",
        "arxiv_id": "2511.02349",
        "ARXIVID": "2511.02349",
        "COMMENT": "This paper introduces a new dual-view photoplethysmography dataset and a model for physiological monitoring. While it is a new dataset and model in computer vision, it does not address spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models. It is not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.02277": {
        "authors": [
            "Giorgos Sfikas",
            "Konstantina Nikolaidou",
            "Foteini Papadopoulou",
            "George Retsinas",
            "Anastasios L. Kesidis"
        ],
        "title": "Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?",
        "abstract": "arXiv:2511.02277v1 Announce Type: new  Abstract: Object pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.",
        "arxiv_id": "2511.02277",
        "ARXIVID": "2511.02277",
        "COMMENT": "This paper explores the use of Euler angles as a parameterization for normalizing flows in 3D pose estimation. It is a methodological study in 3D computer vision, but does not focus on spatial intelligence for embodied agents, VLLMs/MLLMs, or vision foundation models. It is relevant to spatial understanding, but not a direct match to the criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.00640": {
        "authors": [
            "Zicheng Xu",
            "Guanchu Wang",
            "Yu-Neng Chuang",
            "Guangyao Zheng",
            "Alexander S. Szalay",
            "Zirui Liu",
            "Vladimir Braverman"
        ],
        "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching",
        "abstract": "arXiv:2511.00640v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.",
        "arxiv_id": "2511.00640",
        "ARXIVID": "2511.00640",
        "COMMENT": "This paper proposes a new decoding framework (DTS) for large reasoning models, focusing on efficient and accurate reasoning path selection. While it is a methodological improvement for reasoning models, it does not directly address spatial understanding, VLLMs/MLLMs, embodied agents, or vision foundation models. It is more about language model inference efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.01998": {
        "authors": [
            "Benjamin Walder",
            "Daniel Toader",
            "Robert Nuster",
            "G\\\"unther Paltauf",
            "Peter Burgholzer",
            "Gregor Langer",
            "Lukas Krainer",
            "Markus Haltmeier"
        ],
        "title": "Locally-Supervised Global Image Restoration",
        "abstract": "arXiv:2511.01998v1 Announce Type: new  Abstract: We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.",
        "arxiv_id": "2511.01998",
        "ARXIVID": "2511.01998",
        "COMMENT": "This paper proposes a new method for image reconstruction from incomplete measurements using local supervision and invariances. While it is a methodological improvement in image restoration, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.00457": {
        "authors": [
            "Chunyu Wei",
            "Wenji Hu",
            "Xingjia Hao",
            "Xin Wang",
            "Yifan Yang",
            "Yueguo Chen",
            "Yang Tian",
            "Yunhai Wang"
        ],
        "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining",
        "abstract": "arXiv:2511.00457v1 Announce Type: new  Abstract: Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.",
        "arxiv_id": "2511.00457",
        "ARXIVID": "2511.00457",
        "COMMENT": "This paper introduces GraphChain, a framework for using LLMs in large-scale graph analysis via tool chaining. While it is a novel application of LLMs, it does not focus on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about graph reasoning with LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.01033": {
        "authors": [
            "Tiberiu Musat",
            "Tiago Pimentel",
            "Lorenzo Noci",
            "Alessandro Stolfo",
            "Mrinmaya Sachan",
            "Thomas Hofmann"
        ],
        "title": "On the Emergence of Induction Heads for In-Context Learning",
        "abstract": "arXiv:2511.01033v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.",
        "arxiv_id": "2511.01033",
        "ARXIVID": "2511.01033",
        "COMMENT": "This paper investigates the emergence of induction heads in transformers for in-context learning. While it is a methodological study of transformer internals, it does not directly address spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models. It is more about language model interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.02558": {
        "authors": [
            "Ali Farki",
            "Elaheh Moradi",
            "Deepika Koundal",
            "Jussi Tohka"
        ],
        "title": "Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction",
        "abstract": "arXiv:2511.02558v1 Announce Type: new  Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.",
        "arxiv_id": "2511.02558",
        "ARXIVID": "2511.02558",
        "COMMENT": "This paper focuses on longitudinal brain MRI prediction using deep learning. While it is a methodological improvement in medical imaging, it does not address spatial intelligence for embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01444": {
        "authors": [
            "Huiting Huang",
            "Tieliang Gong",
            "Kai He",
            "Jialun Wu",
            "Erik Cambria",
            "Mengling Feng"
        ],
        "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck",
        "abstract": "arXiv:2511.01444v1 Announce Type: new  Abstract: Multimodal sentiment analysis has received significant attention across diverse research domains. Despite advancements in algorithm design, existing approaches suffer from two critical limitations: insufficient learning of noise-contaminated unimodal data, leading to corrupted cross-modal interactions, and inadequate fusion of multimodal representations, resulting in discarding discriminative unimodal information while retaining multimodal redundant information. To address these challenges, this paper proposes a Double Information Bottleneck (DIB) strategy to obtain a powerful, unified compact multimodal representation. Implemented within the framework of low-rank Renyi's entropy functional, DIB offers enhanced robustness against diverse noise sources and computational tractability for high-dimensional data, as compared to the conventional Shannon entropy-based methods. The DIB comprises two key modules: 1) learning a sufficient and compressed representation of individual unimodal data by maximizing the task-relevant information and discarding the superfluous information, and 2) ensuring the discriminative ability of multimodal representation through a novel attention bottleneck fusion mechanism. Consequently, DIB yields a multimodal representation that effectively filters out noisy information from unimodal data while capturing inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI, CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI respectively.",
        "arxiv_id": "2511.01444",
        "ARXIVID": "2511.01444",
        "COMMENT": "This paper introduces a Double Information Bottleneck for robust multimodal sentiment analysis. It is a methodological improvement in multimodal learning, but does not focus on spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models. It is relevant to multi-modal learning, but not a direct match to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01320": {
        "authors": [
            "Ziqi Wang",
            "Hailiang Zhao",
            "Yuhao Yang",
            "Daojiang Hu",
            "Cheng Bao",
            "Mingyi Liu",
            "Kai Di",
            "Schahram Dustdar",
            "Zhongjie Wang",
            "Shuiguang Deng"
        ],
        "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance",
        "abstract": "arXiv:2511.01320v1 Announce Type: new  Abstract: Accurate and timely prediction of tool conditions is critical for intelligent manufacturing systems, where unplanned tool failures can lead to quality degradation and production downtime. In modern industrial environments, predictive maintenance is increasingly implemented as an intelligent service that integrates sensing, analysis, and decision support across production processes. To meet the demand for reliable and service-oriented operation, we present OmniFuser, a multimodal learning framework for predictive maintenance of milling tools that leverages both visual and sensor data. It performs parallel feature extraction from high-resolution tool images and cutting-force signals, capturing complementary spatiotemporal patterns across modalities. To effectively integrate heterogeneous features, OmniFuser employs a contamination-free cross-modal fusion mechanism that disentangles shared and modality-specific components, allowing for efficient cross-modal interaction. Furthermore, a recursive refinement pathway functions as an anchor mechanism, consistently retaining residual information to stabilize fusion dynamics. The learned representations can be encapsulated as reusable maintenance service modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled) and multi-step force signal forecasting. Experiments on real-world milling datasets demonstrate that OmniFuser consistently outperforms state-of-the-art baselines, providing a dependable foundation for building intelligent industrial maintenance services.",
        "arxiv_id": "2511.01320",
        "ARXIVID": "2511.01320",
        "COMMENT": "This paper presents OmniFuser, a multimodal fusion framework for predictive maintenance in manufacturing. While it uses visual and sensor data, it is not about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01052": {
        "authors": [
            "Yeawon Lee",
            "Christopher C. Yang",
            "Chia-Hsuan Chang",
            "Grace Lu-Yao"
        ],
        "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports",
        "abstract": "arXiv:2511.01052v1 Announce Type: new  Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.",
        "arxiv_id": "2511.01052",
        "ARXIVID": "2511.01052",
        "COMMENT": "This paper uses LLMs for interpretable cancer stage identification from pathology reports, focusing on knowledge elicitation. While it uses LLMs, it is not about VLLMs/MLLMs, spatial intelligence, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02505": {
        "authors": [
            "Yaosen Chen",
            "Wei Wang",
            "Xuming Wen",
            "Han Yang",
            "Yanru Zhang"
        ],
        "title": "ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing",
        "abstract": "arXiv:2511.02505v1 Announce Type: new  Abstract: Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly.To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com",
        "arxiv_id": "2511.02505",
        "ARXIVID": "2511.02505",
        "COMMENT": "This paper proposes an energy-based optimization method for video shot assembly, leveraging visual-semantic matching and energy-based models. While it uses visual-semantic matching (possibly with LLMs), it is not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about video editing automation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00993": {
        "authors": [
            "Tianming Liu",
            "Jirong Yang",
            "Yafeng Yin",
            "Manzi Li",
            "Linghao Wang",
            "Zheng Zhu"
        ],
        "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach",
        "abstract": "arXiv:2511.00993v1 Announce Type: new  Abstract: Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.",
        "arxiv_id": "2511.00993",
        "ARXIVID": "2511.00993",
        "COMMENT": "This paper presents a dual-agent LLM framework for simulating human learning and adaptation in transportation systems. While it uses LLMs as agents and discusses simulation, it is not focused on spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models. It is more about behavioral modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02564": {
        "authors": [
            "Md Rashidunnabi",
            "Kailash A. Hambarde",
            "Vasco Lopes",
            "Joao C. Neves",
            "Hugo Proenca"
        ],
        "title": "Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification",
        "abstract": "arXiv:2511.02564v1 Announce Type: new  Abstract: Video-based person re-identification (ReID) in cross-view domains (for example, aerial-ground surveillance) remains an open problem because of extreme viewpoint shifts, scale disparities, and temporal inconsistencies. To address these challenges, we propose MTF-CVReID, a parameter-efficient framework that introduces seven complementary modules over a ViT-B/16 backbone. Specifically, we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment (IVFA) for perspective-invariant representation alignment; (6) Hierarchical Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities; and (7) Multi-View Identity Consistency Learning (MVICL) that enforces cross-view identity coherence using a contrastive learning paradigm. Despite adding only about 2 million parameters and 0.7 GFLOPs over the baseline, MTF-CVReID maintains real-time efficiency (189 FPS) and achieves state-of-the-art performance on the AG-VPReID benchmark across all altitude levels, with strong cross-dataset generalization to G2A-VReID and MARS datasets. These results show that carefully designed adapter-based modules can substantially enhance cross-view robustness and temporal consistency without compromising computational efficiency. The source code is available at https://github.com/MdRashidunnabi/MTF-CVReID",
        "arxiv_id": "2511.02564",
        "ARXIVID": "2511.02564",
        "COMMENT": "This paper proposes a new method for video-based person re-identification with a focus on cross-view and temporal consistency. While it is a strong computer vision paper, it does not directly address spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. It is not a new benchmark or simulator for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00267": {
        "authors": [
            "Christian Prothmann",
            "Vijay Gadepally",
            "Jeremy Kepner",
            "Koley Borchard",
            "Luca Carlone",
            "Zachary Folcik",
            "J. Daniel Grith",
            "Michael Houle",
            "Jonathan P. How",
            "Nathan Hughes",
            "Ifueko Igbinedion",
            "Hayden Jananthan",
            "Tejas Jayashankar",
            "Michael Jones",
            "Sertac Karaman",
            "Binoy G. Kurien",
            "Alejandro Lancho",
            "Giovanni Lavezzi",
            "Gary C. F. Lee",
            "Charles E. Leiserson",
            "Richard Linares",
            "Lindsey McEvoy",
            "Peter Michaleas",
            "Chasen Milner",
            "Alex Pentland",
            "Yury Polyanskiy",
            "Jovan Popovich",
            "Jeffrey Price",
            "Tim W. Reid",
            "Stephanie Riley",
            "Siddharth Samsi",
            "Peter Saunders",
            "Olga Simek",
            "Mark S. Veillette",
            "Amir Weiss",
            "Gregory W. Wornell",
            "Daniela Rus",
            "Scott T. Ruppel"
        ],
        "title": "Advancing AI Challenges for the United States Department of the Air Force",
        "abstract": "arXiv:2511.00267v1 Announce Type: new  Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.",
        "arxiv_id": "2511.00267",
        "ARXIVID": "2511.00267",
        "COMMENT": "This paper describes AI challenge problems for the US Air Force, focusing on datasets and open challenges. While it is relevant to AI research and datasets, it does not directly address spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}