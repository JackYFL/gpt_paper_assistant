{
    "2507.15807": {
        "authors": [
            "Shuo Chen",
            "Jianzhe Liu",
            "Zhen Han",
            "Yan Xia",
            "Daniel Cremers",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context",
        "abstract": "arXiv:2507.15807v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm .",
        "arxiv_id": "2507.15807",
        "ARXIVID": "2507.15807",
        "COMMENT": "Matches criterion 2: focuses on MLLMs and their true multimodal in-context learning abilities, introduces a new fine-tuning strategy (DARA) and a dedicated dataset (TrueMICL) to improve and evaluate multimodal adaptation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.15028": {
        "authors": [
            "Yuanhan Zhang",
            "Yunice Chew",
            "Yuhao Dong",
            "Aria Leo",
            "Bo Hu",
            "Ziwei Liu"
        ],
        "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding",
        "abstract": "arXiv:2507.15028v1 Announce Type: new  Abstract: Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.",
        "arxiv_id": "2507.15028",
        "ARXIVID": "2507.15028",
        "COMMENT": "Matches criterion 3 (new embodied AI benchmark): Introduces Video Thinking Test (Video-TT), a new benchmark for advanced video reasoning and robustness in video LLMs, focusing on gaps between models and human intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.15852": {
        "authors": [
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Songxin He",
            "Jianfan Lin",
            "Junsong Tang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction",
        "abstract": "arXiv:2507.15852v1 Announce Type: new  Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.",
        "arxiv_id": "2507.15852",
        "ARXIVID": "2507.15852",
        "COMMENT": "Matches criterion 2 (shows new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes SeC, a concept-driven segmentation framework using LVLMs for video object segmentation, and introduces a new benchmark (SeCVOS), matching criterion 3 as well.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.15130": {
        "authors": [
            "Ce Zhang",
            "Yale Song",
            "Ruta Desai",
            "Michael Louis Iuzzolino",
            "Joseph Tighe",
            "Gedas Bertasius",
            "Satwik Kottur"
        ],
        "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction",
        "abstract": "arXiv:2507.15130v1 Announce Type: new  Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user actions required to achieve a specified goal based on a video showing the user's progress. Although recent advances in multimodal large language models (MLLMs) have shown promising results in video understanding, long-horizon visual planning remains a challenging problem. We identify two challenges in training large MLLMs for video-based planning tasks: (1) scarcity of procedural annotations, limiting the model's ability to learn procedural task dynamics effectively, and (2) inefficiency of next-token prediction objective to explicitly capture the structured action space for visual planning when compared to free-form, natural language. To tackle data scarcity, we introduce Auxiliary Task Augmentation. We design and train our model on auxiliary tasks relevant to long-horizon video-based planning (e.g., goal prediction) to augment the model's planning ability. To more explicitly model the structured action space unique to visual planning tasks, we leverage Multi-token Prediction, extending traditional next-token prediction by using multiple heads to predict multiple future tokens during training. Our approach, VideoPlan, achieves state-of-the-art VPA performance on the COIN and CrossTask datasets, surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3 future actions. We further extend our method to the challenging Ego4D Long-term Action Anticipation task, and show that it is on par with the state-of-the-art approaches despite not using specialized egocentric features. Code will be made available.",
        "arxiv_id": "2507.15130",
        "ARXIVID": "2507.15130",
        "COMMENT": "Directly matches criterion 1 (methodological improvements to spatial understanding and planning in embodied agents) and criterion 2 (MLLMs for visual planning). The use of auxiliary tasks and multi-token prediction for long-horizon visual planning is a novel approach.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.15597": {
        "authors": [
            "Hao Luo",
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Sipeng Zheng",
            "Ye Wang",
            "Haoqi Yuan",
            "Jiazheng Liu",
            "Chaoyi Xu",
            "Qin Jin",
            "Zongqing Lu"
        ],
        "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
        "abstract": "arXiv:2507.15597v1 Announce Type: new  Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.",
        "arxiv_id": "2507.15597",
        "ARXIVID": "2507.15597",
        "COMMENT": "Matches criterion 1 and 2: introduces Being-H0, a new vision-language-action (VLA) model trained on large-scale human videos for dexterous manipulation, with novel physical instruction tuning and part-level motion tokenization. This is a new VLLM/MLLM for embodied agents with spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.15824": {
        "authors": [
            "Enes Sanli",
            "Baris Sarper Tezcan",
            "Aykut Erdem",
            "Erkut Erdem"
        ],
        "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models",
        "abstract": "arXiv:2507.15824v1 Announce Type: new  Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models.",
        "arxiv_id": "2507.15824",
        "ARXIVID": "2507.15824",
        "COMMENT": "Matches criterion 3: introduces a new benchmark (PhysVidBench) for evaluating physical commonsense in video generation models, focusing on tool use and physical plausibility, which are often overlooked in current T2V evaluations.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.14976": {
        "authors": [
            "Hao Zheng",
            "Shunzhi Yang",
            "Zhuoxin He",
            "Jinfeng Yang",
            "Zhenhua Huang"
        ],
        "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models",
        "abstract": "arXiv:2507.14976v1 Announce Type: new  Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent generalization abilities. However, adapting these large-scale models to downstream tasks while preserving their generalization capabilities remains challenging. Although prompt learning methods have shown promise, they suffer from two fundamental bottlenecks that limit generalization: (a) modality isolation, and (b) hierarchical semantic decay. To address these limitations, we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that establishes bidirectional knowledge flow between text and vision modalities, enabling them to refine their semantics mutually. HiCroPL routes knowledge flows by leveraging the complementary strengths of text and vision. In early layers, text prompts inject relatively clear semantics into visual prompts through a hierarchical knowledge mapper, enhancing the representation of low-level visual semantics. In later layers, visual prompts encoding specific task-relevant objects flow back to refine text prompts, enabling deeper alignment. Crucially, our hierarchical knowledge mapper allows representations at multi-scales to be fused, ensuring that deeper representations retain transferable shallow semantics thereby enhancing generalization. We further introduce a lightweight layer-specific knowledge proxy to enable efficient cross-modal interactions. Extensive evaluations across four tasks demonstrate HiCroPL's superior performance, achieving state-of-the-art results on 11 benchmarks with significant improvements. Code is available at: https://github.com/zzeoZheng/HiCroPL.",
        "arxiv_id": "2507.14976",
        "ARXIVID": "2507.14976",
        "COMMENT": "Directly matches criterion 4 (vision foundation models and applications) and criterion 2 (new method for adapting VLMs via hierarchical cross-modal prompt learning). The bidirectional knowledge flow and hierarchical mapping are novel methodological contributions.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.14293": {
        "authors": [
            "Boyuan Zheng",
            "Zeyi Liao",
            "Scott Salisbury",
            "Zeyuan Liu",
            "Michael Lin",
            "Qinyuan Zheng",
            "Zifan Wang",
            "Xiang Deng",
            "Dawn Song",
            "Huan Sun",
            "Yu Su"
        ],
        "title": "WebGuard: Building a Generalizable Guardrail for Web Agents",
        "abstract": "arXiv:2507.14293v1 Announce Type: new  Abstract: The rapid development of autonomous web agents powered by Large Language Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of taking unintended or harmful actions. This situation underscores an urgent need for effective safety measures, akin to access controls for human users. To address this critical challenge, we introduce WebGuard, the first comprehensive dataset designed to support the assessment of web agent action risks and facilitate the development of guardrails for real-world online environments. In doing so, WebGuard specifically focuses on predicting the outcome of state-changing actions and contains 4,939 human-annotated actions from 193 websites across 22 diverse domains, including often-overlooked long-tail websites. These actions are categorized using a novel three-tier risk schema: SAFE, LOW, and HIGH. The dataset includes designated training and test splits to support evaluation under diverse generalization settings. Our initial evaluations reveal a concerning deficiency: even frontier LLMs achieve less than 60% accuracy in predicting action outcomes and less than 60% recall in lagging HIGH-risk actions, highlighting the risks of deploying current-generation agents without dedicated safeguards. We therefore investigate fine-tuning specialized guardrail models using WebGuard. We conduct comprehensive evaluations across multiple generalization settings and find that a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from 20% to 76%. Despite these improvements, the performance still falls short of the reliability required for high-stakes deployment, where guardrails must approach near-perfect accuracy and recall.",
        "arxiv_id": "2507.14293",
        "ARXIVID": "2507.14293",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied/web agents): Introduces WebGuard, a comprehensive dataset for assessing web agent action risks and developing guardrails, with empirical evaluation of LLMs and VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.14520": {
        "authors": [
            "Xinyi Chen",
            "Yifei Yuan",
            "Jiaang Li",
            "Serge Belongie",
            "Maarten de Rijke",
            "Anders S{\\o}gaard"
        ],
        "title": "What if Othello-Playing Language Models Could See?",
        "abstract": "arXiv:2507.14520v1 Announce Type: new  Abstract: Language models are often said to face a symbol grounding problem. While some argue that world understanding can emerge from text alone, others suggest grounded learning is more efficient. We explore this through Othello, where the board state defines a simplified, rule-based world. Building on prior work, we introduce VISOTHELLO, a multi-modal model trained on move histories and board images. Using next-move prediction, we compare it to mono-modal baselines and test robustness to semantically irrelevant perturbations. We find that multi-modal training improves both performance and the robustness of internal representations. These results suggest that grounding language in visual input helps models infer structured world representations.",
        "arxiv_id": "2507.14520",
        "ARXIVID": "2507.14520",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs): Introduces VISOTHELLO, a multi-modal model trained on move histories and board images, and compares to mono-modal baselines. Also provides empirical insights into symbol grounding and robustness in multi-modal models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.14899": {
        "authors": [
            "Jiale Liu",
            "Huan Wang",
            "Yue Zhang",
            "Xiaoyu Luo",
            "Jiaxiang Hu",
            "Zhiliang Liu",
            "Min Xie"
        ],
        "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis",
        "abstract": "arXiv:2507.14899v1 Announce Type: new  Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address these shortcomings, this paper proposes InsightX Agent, a novel LMM-based agentic framework designed to deliver reliable, interpretable, and interactive X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect region proposals for multi-scale feature maps and sparsifies them through Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in X-ray images while maintaining computational efficiency. The EGR tool guides the LMM agent through a chain-of-thought-inspired review process, incorporating context assessment, individual defect analysis, false positive elimination, confidence recalibration and quality assurance to validate and refine the SDMSD's initial proposals. By strategically employing and intelligently using tools, InsightX Agent moves beyond passive data processing to active reasoning, enhancing diagnostic reliability and providing interpretations that integrate diverse information sources. Experimental evaluations on the GDXray+ dataset demonstrate that InsightX Agent not only achieves a high object detection F1-score of 96.35% but also offers significantly improved interpretability and trustworthiness in its analyses, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks.",
        "arxiv_id": "2507.14899",
        "ARXIVID": "2507.14899",
        "COMMENT": "Matches criterion 2 (new LMM-based agentic framework for X-ray NDT analysis) and criterion 4 (application of LMMs to industrial vision tasks). Also relevant to criterion 1 (spatial intelligence in embodied agents) due to the agentic reasoning and spatial defect detection. The integration of a multi-scale detector and evidence-grounded reflection tool is a novel methodological improvement.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.14497": {
        "authors": [
            "Weimin Lyu",
            "Qingqiao Hu",
            "Kehan Qi",
            "Zhan Shi",
            "Wentao Huang",
            "Saumya Gupta",
            "Chao Chen"
        ],
        "title": "Efficient Whole Slide Pathology VQA via Token Compression",
        "abstract": "arXiv:2507.14497v1 Announce Type: new  Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000 pixels, posing significant challenges for multimodal large language model (MLLM) due to long context length and high computational demands. Previous methods typically focus on patch-level analysis or slide-level classification using CLIP-based models with multi-instance learning, but they lack the generative capabilities needed for visual question answering (VQA). More recent MLLM-based approaches address VQA by feeding thousands of patch tokens directly into the language model, which leads to excessive resource consumption. To address these limitations, we propose Token Compression Pathology LLaVA (TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token compression. TCP-LLaVA introduces a set of trainable compression tokens that aggregate visual and textual information through a modality compression module, inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are forwarded to the LLM for answer generation, significantly reducing input length and computational cost. Experiments on ten TCGA tumor subtypes show that TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing training resource consumption by a substantial margin.",
        "arxiv_id": "2507.14497",
        "ARXIVID": "2507.14497",
        "COMMENT": "Matches criterion 2 (new MLLMs/VLLMs) and criterion 4 (vision foundation models and applications). Proposes a new MLLM architecture (TCP-LLaVA) for whole slide pathology VQA, introducing token compression for efficient large-scale visual question answering.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.14801": {
        "authors": [
            "Xiangyu Chen",
            "Kaiwen Zhu",
            "Yuandong Pu",
            "Shuo Cao",
            "Xiaohui Li",
            "Wenlong Zhang",
            "Yihao Liu",
            "Yu Qiao",
            "Jiantao Zhou",
            "Chao Dong"
        ],
        "title": "Exploring Scalable Unified Modeling for General Low-Level Vision",
        "abstract": "arXiv:2507.14801v1 Announce Type: new  Abstract: Low-level vision involves a wide spectrum of tasks, including image restoration, enhancement, stylization, and feature extraction, which differ significantly in both task formulation and output domains. To address the challenge of unified modeling across such diverse tasks, we propose a Visual task Prompt-based Image Processing (VPIP) framework that leverages input-target image pairs as visual prompts to guide the model in performing a variety of low-level vision tasks. The framework comprises an end-to-end image processing backbone, a prompt encoder, and a prompt interaction module, enabling flexible integration with various architectures and effective utilization of task-specific visual representations. Based on this design, we develop a unified low-level vision model, GenLV, and evaluate its performance across multiple representative tasks. To explore the scalability of this approach, we extend the framework along two dimensions: model capacity and task diversity. We construct a large-scale benchmark consisting of over 100 low-level vision tasks and train multiple versions of the model with varying scales. Experimental results show that the proposed method achieves considerable performance across a wide range of tasks. Notably, increasing the number of training tasks enhances generalization, particularly for tasks with limited data, indicating the model's ability to learn transferable representations through joint training. Further evaluations in zero-shot generalization, few-shot transfer, and task-specific fine-tuning scenarios demonstrate the model's strong adaptability, confirming the effectiveness, scalability, and potential of the proposed framework as a unified foundation for general low-level vision modeling.",
        "arxiv_id": "2507.14801",
        "ARXIVID": "2507.14801",
        "COMMENT": "Matches criterion 4: proposes a unified low-level vision model (GenLV) and a large-scale benchmark for general low-level vision tasks, leveraging foundation model principles for vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15856": {
        "authors": [
            "Jiawei Yang",
            "Tianhong Li",
            "Lijie Fan",
            "Yonglong Tian",
            "Yue Wang"
        ],
        "title": "Latent Denoising Makes Good Visual Tokenizers",
        "abstract": "arXiv:2507.15856v1 Announce Type: new  Abstract: Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.",
        "arxiv_id": "2507.15856",
        "ARXIVID": "2507.15856",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications): Proposes a new tokenizer design principle (latent denoising) for generative modeling, with empirical improvements across multiple models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14505": {
        "authors": [
            "Jiahao Ma",
            "Tianyu Wang",
            "Miaomiao Liu",
            "David Ahmedt-Aristizabal",
            "Chuong Nguyen"
        ],
        "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection",
        "abstract": "arXiv:2507.14505v1 Announce Type: new  Abstract: Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.",
        "arxiv_id": "2507.14505",
        "ARXIVID": "2507.14505",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied agents): Proposes DCHM, a depth-consistent human modeling framework for multiview pedestrian detection, improving 3D spatial modeling without human-labeled annotations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14965": {
        "authors": [
            "Yaojie Zhang",
            "Tianlun Huang",
            "Weijun Wang",
            "Wei Feng"
        ],
        "title": "Decision PCR: Decision version of the Point Cloud Registration task",
        "abstract": "arXiv:2507.14965v1 Announce Type: new  Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in 3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become ineffective under extremely low inlier ratios. In this paper, we revisit the registration result evaluation problem and identify the Decision version of the PCR task as the fundamental problem. To address this Decision PCR task, we propose a data-driven approach. First, we construct a corresponding dataset based on the 3DMatch dataset. Then, a deep learning-based classifier is trained to reliably assess registration quality, overcoming the limitations of traditional metrics. To our knowledge, this is the first comprehensive study to address this task through a deep learning framework. We incorporate this classifier into standard PCR pipelines. When integrated with our approach, existing state-of-the-art PCR methods exhibit significantly enhanced registration performance. For example, combining our framework with GeoTransformer achieves a new SOTA registration recall of 86.97\\% on the challenging 3DLoMatch benchmark. Our method also demonstrates strong generalization capabilities on the unseen outdoor ETH dataset.",
        "arxiv_id": "2507.14965",
        "ARXIVID": "2507.14965",
        "COMMENT": "Matches criterion 1 (methodological improvement in spatial understanding): Proposes a new deep learning-based classifier for point cloud registration evaluation, addressing low-overlap scenarios and improving spatial intelligence in 3D vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15683": {
        "authors": [
            "Boni Hu",
            "Zhenyu Xia",
            "Lin Chen",
            "Pengcheng Han",
            "Shuhui Bu"
        ],
        "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing",
        "abstract": "arXiv:2507.15683v1 Announce Type: new  Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera pose from query images, is fundamental to remote sensing and UAV applications. Existing methods face inherent trade-offs: image-based retrieval and pose regression approaches lack precision, while structure-based methods that register queries to Structure-from-Motion (SfM) models suffer from computational complexity and limited scalability. These challenges are particularly pronounced in remote sensing scenarios due to large-scale scenes, high altitude variations, and domain gaps of existing visual priors. To overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel scene representation that compactly encodes both 3D geometry and appearance. We introduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting the rich semantic information and geometric constraints inherent in Gaussian primitives. To handle large-scale remote sensing scenarios, we incorporate partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic memory management strategies. Our approach consists of two stages: (1) a sparse stage featuring a Gaussian-specific consistent render-aware sampling strategy and landmark-guided detector for robust and accurate initial pose estimation, and (2) a dense stage that iteratively refines poses through coarse-to-fine dense rasterization matching while incorporating reliability verification. Through comprehensive evaluation on simulation data, public datasets, and real flight experiments, we demonstrate that our method delivers competitive localization accuracy, recall rate, and computational efficiency while effectively filtering unreliable pose estimates. The results confirm the effectiveness of our approach for practical remote sensing applications.",
        "arxiv_id": "2507.15683",
        "ARXIVID": "2507.15683",
        "COMMENT": "Matches criterion 1 (spatial understanding in embodied/remote agents) and criterion 3 (new methods for spatial relocalization in remote sensing, with novel use of 3D Gaussian Splatting and hierarchical matching).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15655": {
        "authors": [
            "Aniket Pal",
            "Ajoy Mondal",
            "Minesh Mathew",
            "C. V. Jawahar"
        ],
        "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark",
        "abstract": "arXiv:2507.15655v1 Announce Type: new  Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA) benchmarks augments the capabilities of large language models (LLMs) and multi-modal LLMs, thereby enabling them to adeptly capture the intricate linguistic subtleties and visual complexities inherent across diverse languages. Despite its potential, the current MLVQA model struggles to fully utilize its capabilities when dealing with the extensive variety of handwritten documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark meticulously crafted to mitigate the dearth of authentic Multilingual Handwritten document comprehension. HW-MLVQA encompasses an extensive collection of 1,600 handwritten Pages complemented by 2,400 question-answers. Furthermore, it provides a robust benchmark evaluation framework spanning three distinct modalities: text, image, and an integrated image & text modality. To simulate authentic real-world contexts devoid of ground truth textual transcriptions, we facilitates a rigorous assessment of proprietary and open-source OCR models. The benchmark aspires to facilitate pivotal advancements in multilingual handwritten document interpretation, fostering innovation and scholarly inquiry within this specialized domain.",
        "arxiv_id": "2507.15655",
        "ARXIVID": "2507.15655",
        "COMMENT": "Directly matches criterion 3 (new benchmark for embodied AI, specifically for multilingual handwritten document VQA). The benchmark is comprehensive and covers multiple modalities, which is a novel contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14897": {
        "authors": [
            "Renxi Wang",
            "Rifo Ahmad Genadi",
            "Bilal El Bouardi",
            "Yongxin Wang",
            "Fajri Koto",
            "Zhengzhong Liu",
            "Timothy Baldwin",
            "Haonan Li"
        ],
        "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents",
        "abstract": "arXiv:2507.14897v1 Announce Type: new  Abstract: Language model (LM) agents have gained significant attention for their ability to autonomously complete tasks through interactions with environments, tools, and APIs. LM agents are primarily built with prompt engineering or supervised finetuning. At the same time, reinforcement learning (RL) has been explored to enhance LM's capabilities, such as reasoning and factuality. However, the combination of the LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study. To this end, we built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms. Our framework supports multi-turn interactions by adapting traditional RL methods with token-level masking. It features a decorator-based interface for defining tools and reward functions, enabling seamless extension and ease of use. To support high-throughput training, we implement asynchronous execution of tool calls and reward computations, and design a centralized resource management system for scalable environment coordination. We also provide a suite of prebuilt tools and environments, demonstrating the framework's effectiveness through successful agent training across multiple tasks.",
        "arxiv_id": "2507.14897",
        "ARXIVID": "2507.14897",
        "COMMENT": "Relevant to criterion 3 (embodied AI, new RL framework for LM agents, with extensibility and scalability for agentic tasks). The focus on RL for LM agents and the framework's design is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15321": {
        "authors": [
            "Zhenyu Li",
            "Haotong Lin",
            "Jiashi Feng",
            "Peter Wonka",
            "Bingyi Kang"
        ],
        "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?",
        "abstract": "arXiv:2507.15321v1 Announce Type: new  Abstract: Depth estimation is a fundamental task in computer vision with diverse applications. Recent advancements in deep learning have led to powerful depth foundation models (DFMs), yet their evaluation remains challenging due to inconsistencies in existing protocols. Traditional benchmarks rely on alignment-based metrics that introduce biases, favor certain depth representations, and complicate fair comparisons. In this work, we propose BenchDepth, a new benchmark that evaluates DFMs through five carefully selected downstream proxy tasks: depth completion, stereo matching, monocular feed-forward 3D scene reconstruction, SLAM, and vision-language spatial understanding. Unlike conventional evaluation protocols, our approach assesses DFMs based on their practical utility in real-world applications, bypassing problematic alignment procedures. We benchmark eight state-of-the-art DFMs and provide an in-depth analysis of key findings and observations. We hope our work sparks further discussion in the community on best practices for depth model evaluation and paves the way for future research and advancements in depth estimation.",
        "arxiv_id": "2507.15321",
        "ARXIVID": "2507.15321",
        "COMMENT": "Matches criterion 3 and 4: introduces BenchDepth, a new benchmark for depth foundation models, including a vision-language spatial understanding task. Focuses on novel evaluation protocols for vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14675": {
        "authors": [
            "Yuchen Duan",
            "Zhe Chen",
            "Yusong Hu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Botian Shi",
            "Lewei Lu",
            "Qibin Hou",
            "Tong Lu",
            "Hongsheng Li",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
        "abstract": "arXiv:2507.14675v1 Announce Type: new  Abstract: Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets. While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents. This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents. Building on the dataset, we develop a native multimodal model, Docopilot, which can accurately handle document-level dependencies without relying on RAG. Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. Data, code, and models are released at https://github.com/OpenGVLab/Docopilot",
        "arxiv_id": "2507.14675",
        "ARXIVID": "2507.14675",
        "COMMENT": "Matches criterion 2: introduces Docopilot, a new multimodal large language model (MLLM) for document-level understanding, and a large-scale document-level dataset. Advances MLLMs for complex document tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14705": {
        "authors": [
            "Sai Wang",
            "Senthilnathan Subramanian",
            "Mudit Sahni",
            "Praneeth Gone",
            "Lingjie Meng",
            "Xiaochen Wang",
            "Nicolas Ferradas Bertoli",
            "Tingxian Cheng",
            "Jun Xu"
        ],
        "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents",
        "abstract": "arXiv:2507.14705v1 Announce Type: new  Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.   We present Neo, a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.   Applied to a production-grade Seller Financial Assistant chatbot, Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.   Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent interfaces, state controller and feedback loops are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. We release the framework to facilitate reproducible, high-fidelity testing of emerging agentic systems.",
        "arxiv_id": "2507.14705",
        "ARXIVID": "2507.14705",
        "COMMENT": "Matches criterion 3: introduces Neo, a new configurable multi-agent framework for scalable and realistic testing of LLM-based agents, with a simulator-like environment for agentic evaluation. Focuses on novel, automated, and stochastic evaluation for embodied/agentic systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15035": {
        "authors": [
            "Zhijun Zeng",
            "Youjia Zheng",
            "Hao Hu",
            "Zeyuan Dong",
            "Yihang Zheng",
            "Xinliang Liu",
            "Jinzhuo Wang",
            "Zuoqiang Shi",
            "Linfeng Zhang",
            "Yubing Li",
            "He Sun"
        ],
        "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography",
        "abstract": "arXiv:2507.15035v1 Announce Type: new  Abstract: Accurate and efficient simulation of wave equations is crucial in computational wave imaging applications, such as ultrasound computed tomography (USCT), which reconstructs tissue material properties from observed scattered waves. Traditional numerical solvers for wave equations are computationally intensive and often unstable, limiting their practical applications for quasi-real-time image reconstruction. Neural operators offer an innovative approach by accelerating PDE solving using neural networks; however, their effectiveness in realistic imaging is limited because existing datasets oversimplify real-world complexity. In this paper, we present OpenBreastUS, a large-scale wave equation dataset designed to bridge the gap between theoretical equations and practical imaging applications. OpenBreastUS includes 8,000 anatomically realistic human breast phantoms and over 16 million frequency-domain wave simulations using real USCT configurations. It enables a comprehensive benchmarking of popular neural operators for both forward simulation and inverse imaging tasks, allowing analysis of their performance, scalability, and generalization capabilities. By offering a realistic and extensive dataset, OpenBreastUS not only serves as a platform for developing innovative neural PDE solvers but also facilitates their deployment in real-world medical imaging problems. For the first time, we demonstrate efficient in vivo imaging of the human breast using neural operator solvers.",
        "arxiv_id": "2507.15035",
        "ARXIVID": "2507.15035",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmarks/simulators) as it introduces a large-scale, realistic benchmark (OpenBreastUS) for neural operator evaluation in wave imaging, enabling new research in neural PDE solvers for medical imaging.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2507.14456": {
        "authors": [
            "Chi Wan",
            "Yixin Cui",
            "Jiatong Du",
            "Shuo Yang",
            "Yulong Bai",
            "Yanjun Huang"
        ],
        "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving",
        "abstract": "arXiv:2507.14456v1 Announce Type: new  Abstract: End-to-end autonomous driving requires adaptive and robust handling of complex and diverse traffic environments. However, prevalent single-mode planning methods attempt to learn an overall policy while struggling to acquire diversified driving skills to handle diverse scenarios. Therefore, this paper proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a Dual-aware Router. Specifically, the Global Expert is trained on the overall dataset, possessing robust performance. The Scene-Adaptive Experts are trained on corresponding scene subsets, achieving adaptive performance. The Dual-aware Router simultaneously considers scenario-level features and routing uncertainty to dynamically activate expert modules. Through the effective coupling of the Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router, GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS outperforms existing methods in the Bench2Drive closed-loop benchmark and achieves state-of-the-art performance in Driving Score and Success Rate, even with only monocular vision input. Furthermore, ablation studies demonstrate significant improvements over the original single-expert baseline: 7.67% in Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The code will be available at https://github.com/newbrains1/GEMINUS.",
        "arxiv_id": "2507.14456",
        "ARXIVID": "2507.14456",
        "COMMENT": "Matches criterion 3 (embodied AI, new methods for simulators/benchmarks) as it proposes a novel mixture-of-experts framework for end-to-end autonomous driving, with a dual-aware router for scenario adaptation. This is a new method for embodied AI agents in complex environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14426": {
        "authors": [
            "Zhou Chen",
            "Joe Lin",
            "Sathyanarayanan N. Aakur"
        ],
        "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding",
        "abstract": "arXiv:2507.14426v1 Announce Type: new  Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance grounding, which identifies the objects in a scene that enable a given action (e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet and language models with visual evidence from CLIP, using an energy-based reasoning loop to refine predictions iteratively. This process yields transparent, goal-driven decisions to ground symbolic and perceptual structures. Experiments in multi-object, label-free settings demonstrate that CRAFT enhances accuracy while improving interpretability, providing a step toward robust and trustworthy scene understanding.",
        "arxiv_id": "2507.14426",
        "ARXIVID": "2507.14426",
        "COMMENT": "Matches criterion 1: New methodological improvements to spatial understanding. CRAFT is a neuro-symbolic framework for visual functional affordance grounding, integrating commonsense priors and visual evidence for interpretable scene understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14851": {
        "authors": [
            "Muhammad Kamran Janjua",
            "Amirhosein Ghasemabadi",
            "Kunlin Zhang",
            "Mohammad Salameh",
            "Chao Gao",
            "Di Niu"
        ],
        "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration",
        "abstract": "arXiv:2507.14851v1 Announce Type: new  Abstract: In this work, we propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance. Unlike prior art, our method assumes no degradation knowledge in train or test time and learns an approximation to the grounded knowledge such that the foundation model can be safely disentangled during inference adding no extra cost. Further, we call for standardization of benchmarks in all-in-one video restoration, and propose two benchmarks in multi-degradation setting, three-task (3D) and four-task (4D), and two time-varying composite degradation benchmarks; one of the latter being our proposed dataset with varying snow intensity, simulating how weather degradations affect videos naturally. We compare our method with prior works and report state-of-the-art performance on all benchmarks.",
        "arxiv_id": "2507.14851",
        "ARXIVID": "2507.14851",
        "COMMENT": "Matches criterion 4: proposes an all-in-one video restoration framework using foundation models and introduces new benchmarks for multi-degradation video restoration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.15140": {
        "authors": [
            "Mohammad Mashayekhi",
            "Sara Ahmadi Majd",
            "Arian AmirAmjadi",
            "Parsa Hosseini"
        ],
        "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis",
        "abstract": "arXiv:2507.15140v1 Announce Type: new  Abstract: The diagnosis of oral diseases presents a problematic clinical challenge, characterized by a wide spectrum of pathologies with overlapping symptomatology. To address this, we developed Clinical Semantic Intelligence (CSI), a novel artificial intelligence framework that diagnoses 118 different oral diseases by computationally modeling the cognitive processes of an expert clinician. Our core hypothesis is that moving beyond simple pattern matching to emulate expert reasoning is critical to building clinically useful diagnostic aids.   CSI's architecture integrates a fine-tuned multimodal CLIP model with a specialized ChatGLM-6B language model. This system executes a Hierarchical Diagnostic Reasoning Tree (HDRT), a structured framework that distills the systematic, multi-step logic of differential diagnosis. The framework operates in two modes: a Fast Mode for rapid screening and a Standard Mode that leverages the full HDRT for an interactive and in-depth diagnostic workup.   To train and validate our system, we curated a primary dataset of 4,310 images, supplemented by an external hold-out set of 176 images for final validation. A clinically-informed augmentation strategy expanded our training data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the HDRT-driven Standard Mode. The performance gain is directly attributable to the hierarchical reasoning process. Herein, we detail the architectural philosophy, development, and rigorous evaluation of the CSI framework.",
        "arxiv_id": "2507.15140",
        "ARXIVID": "2507.15140",
        "COMMENT": "Matches criterion 2 (new MLLMs): Proposes Clinical Semantic Intelligence (CSI), a novel multimodal framework integrating a fine-tuned CLIP model and a language model for oral disease diagnosis, with a hierarchical reasoning process.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.14798": {
        "authors": [
            "Xinyi Wu",
            "Steven Landgraf",
            "Markus Ulrich",
            "Rongjun Qin"
        ],
        "title": "An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks",
        "abstract": "arXiv:2507.14798v1 Announce Type: new  Abstract: State-of-the-art 3D computer vision algorithms continue to advance in handling sparse, unordered image sets. Recently developed foundational models for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction (DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry Grounded Transformer (VGGT), have attracted attention due to their ability to handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical aerial images matters, as these models may handle extremely low image overlaps, stereo occlusions, and textureless regions. For redundant collections, they can accelerate 3D reconstruction by using extremely sparsified image sets. Despite tests on various computer vision benchmarks, their potential on photogrammetric aerial blocks remains unexplored. This paper conducts a comprehensive evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of the UseGeo dataset for pose estimation and dense 3D reconstruction. Results show these methods can accurately reconstruct dense point clouds from very sparse image sets (fewer than 10 images, up to 518 pixels resolution), with completeness gains up to +50% over COLMAP. VGGT also demonstrates higher computational efficiency, scalability, and more reliable camera pose estimation. However, all exhibit limitations with high-resolution images and large sets, as pose reliability declines with more images and geometric complexity. These findings suggest transformer-based methods cannot fully replace traditional SfM and MVS, but offer promise as complementary approaches, especially in challenging, low-resolution, and sparse scenarios.",
        "arxiv_id": "2507.14798",
        "ARXIVID": "2507.14798",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Evaluates new 3D vision foundation models (DUSt3R, MASt3R, VGGT) on aerial photogrammetry, with surprising empirical results on sparse data.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.14787": {
        "authors": [
            "Xi Xiao",
            "Aristeidis Tsaris",
            "Anika Tabassum",
            "John Lagergren",
            "Larry M. York",
            "Tianyang Wang",
            "Xiao Wang"
        ],
        "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra",
        "abstract": "arXiv:2507.14787v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous wavelength bands, making it a powerful tool in biology, agriculture, and environmental monitoring. However, interpreting Vision Transformers (ViTs) in this setting remains largely unexplored due to two key challenges: (1) existing saliency methods struggle to capture meaningful spectral cues, often collapsing attention onto the class token, and (2) full-spectrum ViTs are computationally prohibitive for interpretability, given the high-dimensional nature of HSI data. We present FOCUS, the first framework that enables reliable and efficient spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core components: class-specific spectral prompts that guide attention toward semantically meaningful wavelength groups, and a learnable [SINK] token trained with an attraction loss to absorb noisy or redundant attention. Together, these designs make it possible to generate stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without any gradient backpropagation or backbone modification. FOCUS improves band-level IoU by 15 percent, reduces attention collapse by over 40 percent, and produces saliency results that align closely with expert annotations. With less than 1 percent parameter overhead, our method makes high-resolution ViT interpretability practical for real-world hyperspectral applications, bridging a long-standing gap between black-box modeling and trustworthy HSI decision-making.",
        "arxiv_id": "2507.14787",
        "ARXIVID": "2507.14787",
        "COMMENT": "Matches criterion 1 (spatial intelligence/understanding in embodied agents) and criterion 4 (vision foundation models and applications). FOCUS introduces a new interpretability framework for ViTs in hyperspectral imaging, with clever statistical tricks (class-specific spectral prompts, [SINK] token, attraction loss) for spatial-spectral interpretability.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.15606": {
        "authors": [
            "Ru Jia",
            "Xiaozhuang Ma",
            "Jianji Wang",
            "Nanning Zheng"
        ],
        "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation",
        "abstract": "arXiv:2507.15606v1 Announce Type: new  Abstract: While the proposal of the Tri-plane representation has advanced the development of the 3D-aware image generative models, problems rooted in its inherent structure, such as multi-face artifacts caused by sharing the same features in symmetric regions, limit its ability to generate 360$^\\circ$ view images. In this paper, we propose CylinderPlane, a novel implicit representation based on Cylindrical Coordinate System, to eliminate the feature ambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different from the inevitable feature entanglement in Cartesian coordinate-based Tri-plane representation, the cylindrical coordinate system explicitly separates features at different angles, allowing our cylindrical representation possible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis. We further introduce the nested cylinder representation that composites multiple cylinders at different scales, thereby enabling the model more adaptable to complex geometry and varying resolutions. The combination of cylinders with different resolutions can effectively capture more critical locations and multi-scale features, greatly facilitates fine detail learning and robustness to different resolutions. Moreover, our representation is agnostic to implicit rendering methods and can be easily integrated into any neural rendering pipeline. Extensive experiments on both synthetic dataset and unstructured in-the-wild images demonstrate that our proposed representation achieves superior performance over previous methods.",
        "arxiv_id": "2507.15606",
        "ARXIVID": "2507.15606",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a new implicit representation (CylinderPlane) for 3D-aware image generation, addressing multi-view consistency and feature ambiguity, which is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.14823": {
        "authors": [
            "Dong Shu",
            "Haoyang Yuan",
            "Yuchen Wang",
            "Yanguang Liu",
            "Huopu Zhang",
            "Haiyan Zhao",
            "Mengnan Du"
        ],
        "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models",
        "abstract": "arXiv:2507.14823v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) have made significant progress in chart understanding. However, financial charts, characterized by complex temporal structures and domain-specific terminology, remain notably underexplored. We introduce FinChart-Bench, the first benchmark specifically focused on real-world financial charts. FinChart-Bench comprises 1,200 financial chart images collected from 2015 to 2024, each annotated with True/False (TF), Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016 questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs on FinChart-Bench. Our evaluation reveals critical insights: (1) the performance gap between open-source and closed-source models is narrowing, (2) performance degradation occurs in upgraded models within families, (3) many models struggle with instruction following, (4) both advanced models show significant limitations in spatial reasoning abilities, and (5) current LVLMs are not reliable enough to serve as automated evaluators. These findings highlight important limitations in current LVLM capabilities for financial chart understanding. The FinChart-Bench dataset is available at https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.",
        "arxiv_id": "2507.14823",
        "ARXIVID": "2507.14823",
        "COMMENT": "Matches criterion 4: introduces FinChart-Bench, a new benchmark for financial chart comprehension in vision-language models, and provides empirical insights into LVLMs' spatial reasoning abilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.14613": {
        "authors": [
            "Guoping Xu",
            "Christopher Kabat",
            "You Zhang"
        ],
        "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2",
        "abstract": "arXiv:2507.14613v1 Announce Type: new  Abstract: Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.",
        "arxiv_id": "2507.14613",
        "ARXIVID": "2507.14613",
        "COMMENT": "Matches criterion 4: proposes an adapter-based method for adapting the Segment Anything Model 2 (SAM2) to medical video segmentation and tracking, leveraging vision foundation models for new applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.14811": {
        "authors": [
            "Jiaji Zhang",
            "Ruichao Sun",
            "Hailiang Zhao",
            "Jiaju Wu",
            "Peng Chen",
            "Hao Li",
            "Xinkui Zhao",
            "Kingsum Chow",
            "Gang Xiong",
            "Lin Ye",
            "Shuiguang Deng"
        ],
        "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models",
        "abstract": "arXiv:2507.14811v1 Announce Type: new  Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.",
        "arxiv_id": "2507.14811",
        "ARXIVID": "2507.14811",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a generalizable quantization framework for diffusion models, which are a type of vision foundation model. The method is broadly applicable and addresses deployment challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.15365": {
        "authors": [
            "Fatemeh Saleh",
            "Sadegh Aliakbarian",
            "Charlie Hewitt",
            "Lohit Petikam",
            "Xiao-Xian",
            "Antonio Criminisi",
            "Thomas J. Cashman",
            "Tadas Baltru\\v{s}aitis"
        ],
        "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data",
        "abstract": "arXiv:2507.15365v1 Announce Type: new  Abstract: The state of the art in human-centric computer vision achieves high accuracy and robustness across a diverse range of tasks. The most effective models in this domain have billions of parameters, thus requiring extremely large datasets, expensive training regimes, and compute-intensive inference. In this paper, we demonstrate that it is possible to train models on much smaller but high-fidelity synthetic datasets, with no loss in accuracy and higher efficiency. Using synthetic training data provides us with excellent levels of detail and perfect labels, while providing strong guarantees for data provenance, usage rights, and user consent. Procedural data synthesis also provides us with explicit control on data diversity, that we can use to address unfairness in the models we train. Extensive quantitative assessment on real input images demonstrates accuracy of our models on three dense prediction tasks: depth estimation, surface normal estimation, and soft foreground segmentation. Our models require only a fraction of the cost of training and inference when compared with foundational models of similar accuracy. Our human-centric synthetic dataset and trained models are available at https://aka.ms/DAViD.",
        "arxiv_id": "2507.15365",
        "ARXIVID": "2507.15365",
        "COMMENT": "Matches criterion 4: Vision foundation models and their applications. The paper presents a data-efficient approach to training vision models using synthetic data, achieving comparable accuracy to foundation models but with less data and compute.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.14481": {
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Tian Zhang",
            "Jianquan Liu",
            "Chuang Hu"
        ],
        "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning",
        "abstract": "arXiv:2507.14481v1 Announce Type: new  Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers (ViTs) without requiring access to data, allowing for the deployment of ViTs on devices with limited resources. In DFQ, the quantization model must be calibrated using synthetic samples, making the quality of these synthetic samples crucial. Existing methods fail to fully capture and balance the global and local features within the samples, resulting in limited synthetic data quality. Moreover, we have found that during inference, there is a significant difference in the distributions of intermediate layer activations between the quantized and full-precision models. These issues lead to a severe performance degradation of the quantized model. To address these problems, we propose a pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT). Specifically, we synthesize samples in order of increasing difficulty, effectively enhancing the quality of synthetic data. During the calibration and inference stage, we introduce the activation correction matrix for the quantized model to align the intermediate layer activations with those of the full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves remarkable superiority over existing DFQ methods and its performance is on par with models quantized through real data. For example, the performance of DeiT-T with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our method eliminates the need for fine-tuning, which not only reduces computational overhead but also lowers the deployment barriers for edge devices. This characteristic aligns with the principles of Green Learning by improving energy efficiency and facilitating real-world applications in resource-constrained environments.",
        "arxiv_id": "2507.14481",
        "ARXIVID": "2507.14481",
        "COMMENT": "Related to vision foundation models (criterion 4): presents a data-free quantization method for Vision Transformers, improving deployment on edge devices.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.15748": {
        "authors": [
            "Jisu Shin",
            "Richard Shaw",
            "Seunghyun Shin",
            "Anton Pelykh",
            "Zhensong Zhang",
            "Hae-Gon Jeon",
            "Eduardo Perez-Pellitero"
        ],
        "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS",
        "abstract": "arXiv:2507.15748v1 Announce Type: new  Abstract: Modern camera pipelines apply extensive on-device processing, such as exposure adjustment, white balance, and color correction, which, while beneficial individually, often introduce photometric inconsistencies across views. These appearance variations violate multi-view consistency and degrade the quality of novel view synthesis. Joint optimization of scene representations and per-image appearance embeddings has been proposed to address this issue, but at the cost of increased computational complexity and slower training. In this work, we propose a transformer-based method that predicts spatially adaptive bilateral grids to correct photometric variations in a multi-view consistent manner, enabling robust cross-scene generalization without the need for scene-specific retraining. By incorporating the learned grids into the 3D Gaussian Splatting pipeline, we improve reconstruction quality while maintaining high training efficiency. Extensive experiments show that our approach outperforms or matches existing scene-specific optimization methods in reconstruction fidelity and convergence speed.",
        "arxiv_id": "2507.15748",
        "ARXIVID": "2507.15748",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a transformer-based method for appearance harmonization in 3D Gaussian Splatting, improving multi-view consistency and generalization in vision pipelines.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.15216": {
        "authors": [
            "Yuping Qiu",
            "Rui Zhu",
            "Ying-cong Chen"
        ],
        "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise",
        "abstract": "arXiv:2507.15216v1 Announce Type: new  Abstract: Self-supervised learning has become an incredibly successful method for feature learning, widely applied to many downstream tasks. It has proven especially effective for discriminative tasks, surpassing the trending generative models. However, generative models perform better in image generation and detail enhancement. Thus, it is natural for us to find a connection between SSL and generative models to further enhance the representation capacity of SSL. As generative models can create new samples by approximating the data distribution, such modeling should also lead to a semantic understanding of the raw visual data, which is necessary for recognition tasks. This enlightens us to combine the core principle of the diffusion model: diffusion noise, with SSL to learn a competitive recognition model. Specifically, diffusion noise can be viewed as a particular state of mask that reveals a close relationship between masked image modeling (MIM) and diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to incorporate diffusion noise into MIM by the position embedding of masked tokens. The multi-level noise schedule is a series of feature augmentations to further enhance the robustness of our model. We perform a comprehensive study to confirm its effectiveness in the classification of downstream tasks. Codes will be released soon in public.",
        "arxiv_id": "2507.15216",
        "ARXIVID": "2507.15216",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications) as it proposes a new method to combine diffusion noise (from generative models) with self-supervised learning for improved visual representation, which is relevant to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.15518": {
        "authors": [
            "Sizhou Chen",
            "Shufan Jiang",
            "Chi Zhang",
            "Xiao-Lei Zhang",
            "Xuelong Li"
        ],
        "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics",
        "abstract": "arXiv:2507.15518v1 Announce Type: new  Abstract: Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences. Our code, dataset and models are available at https://github.com/HAMLET-2025/HAMLET.",
        "arxiv_id": "2507.15518",
        "ARXIVID": "2507.15518",
        "COMMENT": "Somewhat related to embodied AI (criterion 3), as it presents a multi-agent framework (HAMLET) for interactive drama with embodied agents, but the focus is more on narrative and interaction than spatial intelligence or new simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.14549": {
        "authors": [
            "Haotian Deng",
            "Chi Zhang",
            "Chen Wei",
            "Quanying Liu"
        ],
        "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions",
        "abstract": "arXiv:2507.14549v1 Announce Type: new  Abstract: A fundamental challenge in affective cognitive science is to develop models that accurately capture the relationship between external emotional stimuli and human internal experiences. While ANNs have demonstrated remarkable accuracy in facial expression recognition, their ability to model inter-individual differences in human perception remains underexplored. This study investigates the phenomenon of high perceptual variability-where individuals exhibit significant differences in emotion categorization even when viewing the same stimulus. Inspired by the similarity between ANNs and human perception, we hypothesize that facial expression samples that are ambiguous for ANN classifiers also elicit divergent perceptual judgments among human observers. To examine this hypothesis, we introduce a novel perceptual boundary sampling method to generate facial expression stimuli that lie along ANN decision boundaries. These ambiguous samples form the basis of the varEmotion dataset, constructed through large-scale human behavioral experiments. Our analysis reveals that these ANN-confusing stimuli also provoke heightened perceptual uncertainty in human participants, highlighting shared computational principles in emotion perception. Finally, by fine-tuning ANN representations using behavioral data, we achieve alignment between ANN predictions and both group-level and individual-level human perceptual patterns. Our findings establish a systematic link between ANN decision boundaries and human perceptual variability, offering new insights into personalized modeling of emotional interpretation.",
        "arxiv_id": "2507.14549",
        "ARXIVID": "2507.14549",
        "COMMENT": "This paper is relevant to vision foundation models (criterion 4) and explores the intersection of ANN decision boundaries and human perceptual variability, which is a clever empirical study. However, it does not introduce a new VLLM/MLLM or embodied agent method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15085": {
        "authors": [
            "Peirong Zhang",
            "Haowei Xu",
            "Jiaxin Zhang",
            "Guitao Xu",
            "Xuhan Zheng",
            "Zhenhua Yang",
            "Junle Liu",
            "Yuyi Zhang",
            "Lianwen Jin"
        ],
        "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR",
        "abstract": "arXiv:2507.15085v1 Announce Type: new  Abstract: Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\\emph{e.g.}, Flux-series) and unified generative models (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \\& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.",
        "arxiv_id": "2507.15085",
        "ARXIVID": "2507.15085",
        "COMMENT": "Empirical evaluation of generative models (including GPT-4o) for OCR and text image generation/editing. This is relevant to criterion 4 (vision foundation models and applications), and provides surprising empirical results about model weaknesses.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.14559": {
        "authors": [
            "Zixuan Hu",
            "Xiaotong Li",
            "Shixiang Tang",
            "Jun Liu",
            "Yichun Hu",
            "Ling-Yu Duan"
        ],
        "title": "LEAD: Exploring Logit Space Evolution for Model Selection",
        "abstract": "arXiv:2507.14559v1 Announce Type: new  Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a proliferation of available pre-trained models for vision tasks. This surge presents a significant challenge in efficiently choosing the most suitable pre-trained models for downstream tasks. The critical aspect of this challenge lies in effectively predicting the model transferability by considering the underlying fine-tuning dynamics. Existing methods often model fine-tuning dynamics in feature space with linear transformations, which do not precisely align with the fine-tuning objective and fail to grasp the essential nonlinearity from optimization. To this end, we present LEAD, a finetuning-aligned approach based on the network output of logits. LEAD proposes a theoretical framework to model the optimization process and derives an ordinary differential equation (ODE) to depict the nonlinear evolution toward the final logit state. Additionally, we design a class-aware decomposition method to consider the varying evolution dynamics across classes and further ensure practical applicability. Integrating the closely aligned optimization objective and nonlinear modeling capabilities derived from the differential equation, our method offers a concise solution to effectively bridge the optimization gap in a single step, bypassing the lengthy fine-tuning process. The comprehensive experiments on 24 supervised and self-supervised pre-trained models across 10 downstream datasets demonstrate impressive performances and showcase its broad adaptability even in low-data scenarios.",
        "arxiv_id": "2507.14559",
        "ARXIVID": "2507.14559",
        "COMMENT": "Presents a new method for model selection in vision tasks using logit space evolution. While relevant to vision foundation models, it is more about model selection than applications or new models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.14267": {
        "authors": [
            "Ziqi Wang",
            "Hongshuo Huang",
            "Hancheng Zhao",
            "Changwen Xu",
            "Shang Zhu",
            "Jan Janssen",
            "Venkatasubramanian Viswanathan"
        ],
        "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation",
        "abstract": "arXiv:2507.14267v1 Announce Type: new  Abstract: Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1\\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.",
        "arxiv_id": "2507.14267",
        "ARXIVID": "2507.14267",
        "COMMENT": "This paper introduces a multi-agent LLM-based framework for automating DFT simulations in materials science. While it is an agentic framework, it is not focused on spatial intelligence in embodied agents or vision-language models, but rather on scientific simulation. Some relevance to criterion 3 (agentic frameworks), but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.14959": {
        "authors": [
            "Saeid Ghafouri",
            "Mohsen Fayyaz",
            "Xiangchen Li",
            "Deepu John",
            "Bo Ji",
            "Dimitrios Nikolopoulos",
            "Hans Vandierendonck"
        ],
        "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices",
        "abstract": "arXiv:2507.14959v1 Announce Type: new  Abstract: Real-time multi-label video classification on embedded devices is constrained by limited compute and energy budgets. Yet, video streams exhibit structural properties such as label sparsity, temporal continuity, and label co-occurrence that can be leveraged for more efficient inference. We introduce Polymorph, a context-aware framework that activates a minimal set of lightweight Low Rank Adapters (LoRA) per frame. Each adapter specializes in a subset of classes derived from co-occurrence patterns and is implemented as a LoRA weight over a shared backbone. At runtime, Polymorph dynamically selects and composes only the adapters needed to cover the active labels, avoiding full-model switching and weight merging. This modular strategy improves scalability while reducing latency and energy overhead. Polymorph achieves 40% lower energy consumption and improves mAP by 9 points over strong baselines on the TAO dataset. Polymorph is open source at https://github.com/inference-serving/polymorph/.",
        "arxiv_id": "2507.14959",
        "ARXIVID": "2507.14959",
        "COMMENT": "Presents a new energy-efficient multi-label classification framework for video streams on embedded devices. While relevant to embodied AI and efficient inference, it does not introduce new benchmarks or focus on spatial intelligence.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.15064": {
        "authors": [
            "Shuyuan Tu",
            "Zhen Xing",
            "Xintong Han",
            "Zhi-Qi Cheng",
            "Qi Dai",
            "Chong Luo",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation",
        "abstract": "arXiv:2507.15064v1 Announce Type: new  Abstract: Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.",
        "arxiv_id": "2507.15064",
        "ARXIVID": "2507.15064",
        "COMMENT": "Presents a new method for human image animation using diffusion models with learnable pose alignment and identity preservation. This is relevant to generative modeling in vision, but does not directly address spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2507.15618": {
        "authors": [
            "Weiyu Ma",
            "Jiwen Jiang",
            "Haobo Fu",
            "Haifeng Zhang"
        ],
        "title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II",
        "abstract": "arXiv:2507.15618v1 Announce Type: new  Abstract: We present an adapter-based approach for tactical conditioning of StarCraft II AI agents. Current agents, while powerful, lack the ability to adapt their strategies based on high-level tactical directives. Our method freezes a pre-trained policy network (DI-Star) and attaches lightweight adapter modules to each action head, conditioned on a tactical tensor that encodes strategic preferences. By training these adapters with KL divergence constraints, we ensure the policy maintains core competencies while exhibiting tactical variations. Experimental results show our approach successfully modulates agent behavior across tactical dimensions including aggression, expansion patterns, and technology preferences, while maintaining competitive performance. Our method enables flexible tactical control with minimal computational overhead, offering practical strategy customization for complex real-time strategy games.",
        "arxiv_id": "2507.15618",
        "ARXIVID": "2507.15618",
        "COMMENT": "Adapter-based tactical conditioning for StarCraft II agents. While interesting for embodied AI, it does not introduce a new benchmark or focus on spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.15844": {
        "authors": [
            "Shangke Lyu",
            "Linjuan Wu",
            "Yuchen Yan",
            "Xingyu Wu",
            "Hao Li",
            "Yongliang Shen",
            "Peisheng Jiang",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
        "abstract": "arXiv:2507.15844v1 Announce Type: new  Abstract: Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.",
        "arxiv_id": "2507.15844",
        "ARXIVID": "2507.15844",
        "COMMENT": "Presents a new RL framework for adaptive reasoning in large models. While interesting for efficiency and reasoning, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.15520": {
        "authors": [
            "Hanting Li",
            "Fei Zhou",
            "Xin Sun",
            "Yang Hua",
            "Jungong Han",
            "Liang-Jie Zhang"
        ],
        "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement",
        "abstract": "arXiv:2507.15520v1 Announce Type: new  Abstract: Recent Transformer-based low-light enhancement methods have made promising progress in recovering global illumination. However, they still struggle with non-uniform lighting scenarios, such as backlit and shadow, appearing as over-exposure or inadequate brightness restoration. To address this challenge, we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer) framework that enables accurate illumination restoration. Specifically, we propose a dynamic integral image representation to model the spatially-varying illumination, and further construct a novel Spatially-Adaptive Integral Illumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which leverages the illumination to calibrate the lightness-relevant features toward visual-pleased illumination enhancement. Extensive experiments on five standard low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our SAIGFormer significantly outperforms state-of-the-art methods in both quantitative and qualitative metrics. In particular, our method achieves superior performance in non-uniform illumination enhancement while exhibiting strong generalization capabilities across multiple datasets. Code is available at https://github.com/LHTcode/SAIGFormer.git.",
        "arxiv_id": "2507.15520",
        "ARXIVID": "2507.15520",
        "COMMENT": "This paper presents a transformer-based method for low-light image enhancement with spatially-adaptive illumination, which is a vision foundation model application (criterion 4), but not directly related to VLLMs/MLLMs or embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.15008": {
        "authors": [
            "Jiasheng Xu",
            "Yewang Chen"
        ],
        "title": "FastSmoothSAM: A Fast Smooth Method For Segment Anything Model",
        "abstract": "arXiv:2507.15008v1 Announce Type: new  Abstract: Accurately identifying and representing object edges is a challenging task in computer vision and image processing. The Segment Anything Model (SAM) has significantly influenced the field of image segmentation, but suffers from high memory consumption and long inference times, limiting its efficiency in real-time applications. To address these limitations, Fast Segment Anything (FastSAM) was proposed, achieving real-time segmentation. However, FastSAM often generates jagged edges that deviate from the true object shapes. Therefore, this paper introduces a novel refinement approach using B-Spline curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the robust shape control and flexible geometric construction of B-Splines, a four-stage refining process involving two rounds of curve fitting is employed to effectively smooth jagged edges. This approach significantly improves the visual quality and analytical accuracy of object edges without compromising critical geometric information. The proposed method improves the practical utility of FastSAM by improving segmentation accuracy while maintaining real-time processing capabilities. This advancement unlocks greater potential for FastSAM technology in various real-world scenarios, such as industrial automation, medical imaging, and autonomous systems, where precise and efficient edge recognition is crucial.",
        "arxiv_id": "2507.15008",
        "ARXIVID": "2507.15008",
        "COMMENT": "This paper is an application of B-Spline curve fitting to improve FastSAM segmentation, which is a vision foundation model application (criterion 4), but the novelty is incremental (edge refinement).",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2507.14432": {
        "authors": [
            "Han Gong",
            "Qiyue Li",
            "Zhi Liu",
            "Hao Zhou",
            "Peng Yuan Zhou",
            "Zhu Li",
            "Jie Li"
        ],
        "title": "Adaptive 3D Gaussian Splatting Video Streaming",
        "abstract": "arXiv:2507.14432v1 Announce Type: new  Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.",
        "arxiv_id": "2507.14432",
        "ARXIVID": "2507.14432",
        "COMMENT": "This paper is about 3D Gaussian splatting for video streaming, which is a vision foundation model application (criterion 4), but it is more about compression and streaming than spatial intelligence or VLLMs/MLLMs.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.15351": {
        "authors": [
            "Zijian Zhao",
            "Sen Li"
        ],
        "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms",
        "abstract": "arXiv:2507.15351v1 Announce Type: new  Abstract: On-demand ride-sharing platforms face the fundamental challenge of dynamically bundling passengers with diverse origins and destinations and matching them with vehicles in real time, all under significant uncertainty. Recently, MARL has emerged as a promising solution for this problem, leveraging decentralized learning to address the curse of dimensionality caused by the large number of agents in the ride-hailing market and the resulting expansive state and action spaces. However, conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions. To address these challenges, we propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.",
        "arxiv_id": "2507.15351",
        "ARXIVID": "2507.15351",
        "COMMENT": "Somewhat related to criterion 3: proposes new MARL methods for ride-sharing, but not focused on spatial intelligence or embodied AI benchmarks/simulators. More about RL optimization than spatial/embodied aspects.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.15758": {
        "authors": [
            "Xingyu Wu",
            "Yuchen Yan",
            "Shangke Lyu",
            "Linjuan Wu",
            "Yiwen Qiu",
            "Yongliang Shen",
            "Weiming Lu",
            "Jian Shao",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
        "abstract": "arXiv:2507.15758v1 Announce Type: new  Abstract: Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.",
        "arxiv_id": "2507.15758",
        "ARXIVID": "2507.15758",
        "COMMENT": "Presents a novel length-adaptive policy optimization for large reasoning models, focusing on efficient reasoning. While interesting for LLMs, it does not directly address spatial intelligence, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.14833": {
        "authors": [
            "Haoxuan Zhang",
            "Wenju Cui",
            "Yuzhu Cao",
            "Tao Tan",
            "Jie Liu",
            "Yunsong Peng",
            "Jian Zheng"
        ],
        "title": "Paired Image Generation with Diffusion-Guided Diffusion Models",
        "abstract": "arXiv:2507.14833v1 Announce Type: new  Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.",
        "arxiv_id": "2507.14833",
        "ARXIVID": "2507.14833",
        "COMMENT": "Relevant to generative modeling and vision, but focuses on medical image segmentation and paired image generation. No direct match to criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15000": {
        "authors": [
            "Chaoyun Wang",
            "I-Chao Shen",
            "Takeo Igarashi",
            "Nanning Zheng",
            "Caigui Jiang"
        ],
        "title": "Axis-Aligned Document Dewarping",
        "abstract": "arXiv:2507.15000v1 Announce Type: new  Abstract: Document dewarping is crucial for many applications. However, existing learning-based methods primarily rely on supervised regression with annotated data without leveraging the inherent geometric properties in physical documents to the dewarping process. Our key insight is that a well-dewarped document is characterized by transforming distorted feature lines into axis-aligned ones. This property aligns with the inherent axis-aligned nature of the discrete grid geometry in planar documents. In the training phase, we propose an axis-aligned geometric constraint to enhance document dewarping. In the inference phase, we propose an axis alignment preprocessing strategy to reduce the dewarping difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned Distortion (AAD), that not only incorporates geometric meaning and aligns with human visual perception but also demonstrates greater robustness. As a result, our method achieves SOTA results on multiple existing benchmarks and achieves 18.2%~34.5% improvements on the AAD metric.",
        "arxiv_id": "2507.15000",
        "ARXIVID": "2507.15000",
        "COMMENT": "Relevant to computer vision and geometric modeling, but does not directly match any specific criterion (no new foundation model, MLLM, or embodied AI focus).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14855": {
        "authors": [
            "Xingshu Chen",
            "Sicheng Yu",
            "Chong Cheng",
            "Hao Wang",
            "Ting Tian"
        ],
        "title": "An Uncertainty-aware DETR Enhancement Framework for Object Detection",
        "abstract": "arXiv:2507.14855v1 Announce Type: new  Abstract: This paper investigates the problem of object detection with a focus on improving both the localization accuracy of bounding boxes and explicitly modeling prediction uncertainty. Conventional detectors rely on deterministic bounding box regression, ignoring uncertainty in predictions and limiting model robustness. In this paper, we propose an uncertainty-aware enhancement framework for DETR-based object detectors. We model bounding boxes as multivariate Gaussian distributions and incorporate the Gromov-Wasserstein distance into the loss function to better align the predicted and ground-truth distributions. Building on this, we derive a Bayes Risk formulation to filter high-risk information and improve detection reliability. We also propose a simple algorithm to quantify localization uncertainty via confidence intervals. Experiments on the COCO benchmark show that our method can be effectively integrated into existing DETR variants, enhancing their performance. We further extend our framework to leukocyte detection tasks, achieving state-of-the-art results on the LISC and WBCDD datasets. These results confirm the scalability of our framework across both general and domain-specific detection tasks. Code page: https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.",
        "arxiv_id": "2507.14855",
        "ARXIVID": "2507.14855",
        "COMMENT": "Presents an uncertainty-aware enhancement for DETR-based object detectors. While relevant to computer vision, it does not match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15809": {
        "authors": [
            "Roberto Miele",
            "Niklas Linde"
        ],
        "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion",
        "abstract": "arXiv:2507.15809v1 Announce Type: new  Abstract: Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.",
        "arxiv_id": "2507.15809",
        "ARXIVID": "2507.15809",
        "COMMENT": "Related to generative modeling (diffusion models) and vision, but not to any specific criterion. Focuses on subsurface modeling and inversion, not spatial intelligence or VLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15577": {
        "authors": [
            "Hugo Carlesso",
            "Maria Eliza Patulea",
            "Moncef Garouani",
            "Radu Tudor Ionescu",
            "Josiane Mothe"
        ],
        "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation",
        "abstract": "arXiv:2507.15577v1 Announce Type: new  Abstract: Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at https://github.com/hugocarlesso/GeMix to foster reproducibility and further research.",
        "arxiv_id": "2507.15577",
        "ARXIVID": "2507.15577",
        "COMMENT": "Related to general generative modeling and vision, but does not match any specific criterion. Proposes a GAN-based mixup for medical image augmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15686": {
        "authors": [
            "Wenjie Huang",
            "Qi Yang",
            "Shuting Xia",
            "He Huang",
            "Zhu Li",
            "Yiling Xu"
        ],
        "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression",
        "abstract": "arXiv:2507.15686v1 Announce Type: new  Abstract: Existing AI-based point cloud compression methods struggle with dependence on specific training data distributions, which limits their real-world deployment. Implicit Neural Representation (INR) methods solve the above problem by encoding overfitted network parameters to the bitstream, resulting in more distribution-agnostic results. However, due to the limitation of encoding time and decoder size, current INR based methods only consider lossy geometry compression. In this paper, we propose the first INR based lossless point cloud geometry compression method called Lossless Implicit Neural Representations for Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we design a group of point clouds level coding framework with an effective network initialization strategy, which can reduce around 60% encoding time. A lightweight coding network based on multiscale SparseConv, consisting of scale context extraction, child node prediction, and model compression modules, is proposed to realize fast inference and compact decoder size. Experimental results show that our method consistently outperforms traditional and AI-based methods: for example, with the convergence time in the MVUB dataset, our method reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and 21.95% compared to SparsePCGC. Our project can be seen on https://huangwenjie2023.github.io/LINR-PCGC/.",
        "arxiv_id": "2507.15686",
        "ARXIVID": "2507.15686",
        "COMMENT": "Does not match any specific criterion. Focuses on point cloud geometry compression using implicit neural representations, which is not directly related to spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14447": {
        "authors": [
            "Guancheng Zeng",
            "Xueyi Chen",
            "Jiawang Hu",
            "Shaohua Qi",
            "Yaxuan Mao",
            "Zhantao Wang",
            "Yifan Nie",
            "Shuang Li",
            "Qiuyang Feng",
            "Pengxu Qiu",
            "Yujia Wang",
            "Wenqiang Han",
            "Linyan Huang",
            "Gang Li",
            "Jingjing Mo",
            "Haowen Hu"
        ],
        "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise",
        "abstract": "arXiv:2507.14447v1 Announce Type: new  Abstract: The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.",
        "arxiv_id": "2507.14447",
        "ARXIVID": "2507.14447",
        "COMMENT": "Describes a structural planning framework for LLM agent systems in enterprise. While relevant to agent systems, it does not focus on spatial intelligence, VLLMs/MLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14826": {
        "authors": [
            "Fu-Jen Tsai",
            "Yan-Tsung Peng",
            "Yen-Yu Lin",
            "Chia-Wen Lin"
        ],
        "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing",
        "abstract": "arXiv:2507.14826v1 Announce Type: new  Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although previous research has collected paired real-world hazy and haze-free images to improve dehazing models' performance in real-world scenarios, these models often experience significant performance drops when handling unseen real-world hazy images due to limited training data. This issue motivates us to develop a flexible domain adaptation method to enhance dehazing performance during testing. Observing that predicting haze patterns is generally easier than recovering clean content, we propose the Physics-guided Haze Transfer Network (PHATNet) which transfers haze patterns from unseen target domains to source-domain haze-free images, creating domain-specific fine-tuning sets to update dehazing models for effective domain adaptation. Additionally, we introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to enhance PHATNet's disentanglement ability. Experimental results demonstrate that PHATNet significantly boosts state-of-the-art dehazing models on benchmark real-world image dehazing datasets.",
        "arxiv_id": "2507.14826",
        "ARXIVID": "2507.14826",
        "COMMENT": "Presents a physics-guided network for domain-adaptive image dehazing. While it is a novel method in vision, it does not match the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15037": {
        "authors": [
            "Zhaotong Yang",
            "Yuhui Li",
            "Shengfeng He",
            "Xinzhe Li",
            "Yangyang Xu",
            "Junyu Dong",
            "Yong Du"
        ],
        "title": "OmniVTON: Training-Free Universal Virtual Try-On",
        "abstract": "arXiv:2507.15037v1 Announce Type: new  Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised in-shop approaches, which ensure high fidelity but struggle with cross-domain generalization, or unsupervised in-the-wild methods, which improve adaptability but remain constrained by data biases and limited universality. A unified, training-free solution that works across both scenarios remains an open challenge. We propose OmniVTON, the first training-free universal VTON framework that decouples garment and pose conditioning to achieve both texture fidelity and pose consistency across diverse settings. To preserve garment details, we introduce a garment prior generation mechanism that aligns clothing with the body, followed by continuous boundary stitching technique to achieve fine-grained texture retention. For precise pose alignment, we utilize DDIM inversion to capture structural cues while suppressing texture interference, ensuring accurate body alignment independent of the original image textures. By disentangling garment and pose constraints, OmniVTON eliminates the bias inherent in diffusion models when handling multiple conditions simultaneously. Experimental results demonstrate that OmniVTON achieves superior performance across diverse datasets, garment types, and application scenarios. Notably, it is the first framework capable of multi-human VTON, enabling realistic garment transfer across multiple individuals in a single scene. Code is available at https://github.com/Jerome-Young/OmniVTON",
        "arxiv_id": "2507.15037",
        "ARXIVID": "2507.15037",
        "COMMENT": "Presents a new training-free universal virtual try-on framework, which is an application of generative modeling in vision. While interesting, it does not directly match any of the four criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14575": {
        "authors": [
            "Andrea Moschetto",
            "Lemuel Puglisi",
            "Alec Sargood",
            "Pierluigi Dell'Acqua",
            "Francesco Guarnera",
            "Sebastiano Battiato",
            "Daniele Rav\\`i"
        ],
        "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation",
        "abstract": "arXiv:2507.14575v1 Announce Type: new  Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.",
        "arxiv_id": "2507.14575",
        "ARXIVID": "2507.14575",
        "COMMENT": "Relevant to generative modeling in multi-modal learning (MRI image-to-image translation), but does not introduce new methods or benchmarks, rather benchmarks existing ones. No direct match to criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.15297": {
        "authors": [
            "Zhiyu Pan",
            "Xiongjun Guan",
            "Yongjie Duan",
            "Jianjiang Feng",
            "Jie Zhou"
        ],
        "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching",
        "abstract": "arXiv:2507.15297v1 Announce Type: new  Abstract: Fingerprint matching under diverse capture conditions remains a fundamental challenge in biometric recognition. To achieve robust and accurate performance in such scenarios, we propose DMD, a minutiae-anchored local dense representation which captures both fine-grained ridge textures and discriminative minutiae features in a spatially structured manner. Specifically, descriptors are extracted from local patches centered and oriented on each detected minutia, forming a three-dimensional tensor, where two dimensions represent spatial locations on the fingerprint plane and the third encodes semantic features. This representation explicitly captures abstract features of local image patches, enabling a multi-level, fine-grained description that aggregates information from multiple minutiae and their surrounding ridge structures. Furthermore, thanks to its strong spatial correspondence with the patch image, DMD allows for the use of foreground segmentation masks to identify valid descriptor regions. During matching, comparisons are then restricted to overlapping foreground areas, improving efficiency and robustness. Extensive experiments on rolled, plain, parital, contactless, and latent fingerprint datasets demonstrate the effectiveness and generalizability of the proposed method. It achieves state-of-the-art accuracy across multiple benchmarks while maintaining high computational efficiency, showing strong potential for large-scale fingerprint recognition. Corresponding code is available at https://github.com/Yu-Yy/DMD.",
        "arxiv_id": "2507.15297",
        "ARXIVID": "2507.15297",
        "COMMENT": "Does not match any specific criterion. Focuses on fingerprint matching and local dense representations, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}