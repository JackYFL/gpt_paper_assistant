{
    "2509.07825": {
        "authors": [
            "Zhuoxu Huang",
            "Mingqi Gao",
            "Jungong Han"
        ],
        "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model",
        "abstract": "arXiv:2509.07825v1 Announce Type: new  Abstract: 3D object segmentation with Large Language Models (LLMs) has become a prevailing paradigm due to its broad semantics, task flexibility, and strong generalization. However, this paradigm is hindered by representation misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds convey only dense geometric structures. In prior methods, misalignment limits both input and output. At the input stage, dense point patches require heavy pre-alignment, weakening object-level semantics and confusing similar distractors. At the output stage, predictions depend only on dense features without explicit geometric cues, leading to a loss of fine-grained accuracy. To address these limitations, we present the Point Linguist Model (PLM), a general framework that bridges the representation gap between LLMs and dense 3D point clouds without requiring large-scale pre-alignment between 3D-text or 3D-images. Specifically, we introduce Object-centric Discriminative Representation (OcDR), which learns object-centric tokens that capture target semantics and scene relations under a hard negative-aware training objective. This mitigates the misalignment between LLM tokens and 3D points, enhances resilience to distractors, and facilitates semantic-level reasoning within LLMs. For accurate segmentation, we introduce the Geometric Reactivation Decoder (GRD), which predicts masks by combining OcDR tokens carrying LLM-inferred geometry with corresponding dense features, preserving comprehensive dense features throughout the pipeline. Extensive experiments show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and +6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness of comprehensive object-centric reasoning for robust 3D understanding.",
        "arxiv_id": "2509.07825",
        "ARXIVID": "2509.07825",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Introduces the Point Linguist Model, a novel framework bridging LLMs and 3D point clouds for object segmentation, with new object-centric representations and a geometric decoder. Strong empirical results on 3D vision-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.07969": {
        "authors": [
            "Xin Lai",
            "Junyi Li",
            "Wei Li",
            "Tao Liu",
            "Tianjian Li",
            "Hengshuang Zhao"
        ],
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
        "abstract": "arXiv:2509.07969v1 Announce Type: new  Abstract: Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
        "arxiv_id": "2509.07969",
        "ARXIVID": "2509.07969",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 3 (embodied AI, new methods for visual reasoning in simulators). Proposes a system for deep, multi-turn reasoning in visual search, introduces a new dataset and a novel training strategy for scaling reasoning patterns.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.07447": {
        "authors": [
            "Taiying Peng",
            "Jiacheng Hua",
            "Miao Liu",
            "Feng Lu"
        ],
        "title": "In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting",
        "abstract": "arXiv:2509.07447v1 Announce Type: new  Abstract: The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings.",
        "arxiv_id": "2509.07447",
        "ARXIVID": "2509.07447",
        "COMMENT": "Matches criterion 3 (new benchmark for embodied AI: egocentric video intent understanding with gaze-guided prompting for MLLMs). Also relevant to criterion 2 (MLLMs for egocentric video).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.07680": {
        "authors": [
            "Sachit Menon",
            "Ahmet Iscen",
            "Arsha Nagrani",
            "Tobias Weyand",
            "Carl Vondrick",
            "Cordelia Schmid"
        ],
        "title": "CAViAR: Critic-Augmented Video Agentic Reasoning",
        "abstract": "arXiv:2509.07680v1 Announce Type: new  Abstract: Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.",
        "arxiv_id": "2509.07680",
        "ARXIVID": "2509.07680",
        "COMMENT": "Matches criterion 1 (methodological improvements to spatial understanding and reasoning in embodied agents) and criterion 2 (agentic reasoning with LLMs and video modules, i.e., VLLM-like agent). Also relevant to criterion 3 (novel agent+critic method for video reasoning benchmarks).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.07530": {
        "authors": [
            "Kiet T. Nguyen",
            "Chanhuyk Lee",
            "Donggyun Kim",
            "Dong Hoon Lee",
            "Seunghoon Hong"
        ],
        "title": "Universal Few-Shot Spatial Control for Diffusion Models",
        "abstract": "arXiv:2509.07530v1 Announce Type: new  Abstract: Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.",
        "arxiv_id": "2509.07530",
        "ARXIVID": "2509.07530",
        "COMMENT": "Matches criterion 1 and 4: New methodological improvements to spatial understanding (spatial control in diffusion models) and vision foundation models (diffusion models). Proposes a universal few-shot spatial control adapter for diffusion models, enabling generalization to novel spatial conditions.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.07295": {
        "authors": [
            "Ji Xie",
            "Trevor Darrell",
            "Luke Zettlemoyer",
            "XuDong Wang"
        ],
        "title": "Reconstruction Alignment Improves Unified Multimodal Models",
        "abstract": "arXiv:2509.07295v1 Announce Type: new  Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit 6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs",
        "arxiv_id": "2509.07295",
        "ARXIVID": "2509.07295",
        "COMMENT": "Matches criterion 2 (new MLLMs) and criterion 4 (vision foundation models and applications). Proposes a general post-training alignment method for unified multimodal models that improves both understanding and generation, with strong empirical results.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.07864": {
        "authors": [
            "Tiancheng Yang",
            "Lin Zhang",
            "Jiaye Lin",
            "Guimin Hu",
            "Di Wang",
            "Lijie Hu"
        ],
        "title": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics",
        "abstract": "arXiv:2509.07864v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Results show our D-LEAF delivers a 53% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4%, substantially suppressing hallucinations while preserving efficiency.",
        "arxiv_id": "2509.07864",
        "ARXIVID": "2509.07864",
        "COMMENT": "Matches criterion 2 (new MLLMs) and is highly relevant to clever statistical tricks for hallucination correction in MLLMs. Introduces new diagnostics and a dynamic attention fusion method to localize and correct hallucinations in multimodal LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.07538": {
        "authors": [
            "Peijin Xie",
            "Shun Qian",
            "Bingquan Liu",
            "Dexin Wang",
            "Lin Sun",
            "Xiangzheng Zhang"
        ],
        "title": "TextlessRAG: End-to-End Visual Document RAG by Speech Without Text",
        "abstract": "arXiv:2509.07538v1 Announce Type: new  Abstract: Document images encapsulate a wealth of knowledge, while the portability of spoken queries enables broader and flexible application scenarios. Yet, no prior work has explored knowledge base question answering over visual document images with queries provided directly in speech. We propose TextlessRAG, the first end-to-end framework for speech-based question answering over large-scale document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR, directly interpreting speech, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. To further boost performance, we integrate a layout-aware reranking mechanism to refine retrieval. Experiments demonstrate substantial improvements in both efficiency and accuracy. To advance research in this direction, we also release the first bilingual speech--document RAG dataset, featuring Chinese and English voice queries paired with multimodal document content. Both the dataset and our pipeline will be made available at repository:https://github.com/xiepeijinhit-hue/textlessrag",
        "arxiv_id": "2509.07538",
        "ARXIVID": "2509.07538",
        "COMMENT": "Matches criterion 2 (new VLLMs/MLLMs) and criterion 4 (vision foundation models and applications). Proposes an end-to-end visual document RAG system that directly processes speech and document images without text, and releases a new bilingual multimodal dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.07894": {
        "authors": [
            "Fangchen Yu",
            "Haiyuan Wan",
            "Qianjia Cheng",
            "Yuchen Zhang",
            "Jiacheng Chen",
            "Fujun Han",
            "Yulun Wu",
            "Junchi Yao",
            "Ruilizhen Hu",
            "Ning Ding",
            "Yu Cheng",
            "Tao Chen",
            "Lei Bai",
            "Dongzhan Zhou",
            "Yun Luo",
            "Ganqu Cui",
            "Peng Ye"
        ],
        "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?",
        "abstract": "arXiv:2509.07894v1 Announce Type: new  Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight a substantial performance gap between open-source models and top students, the strong physical reasoning capabilities of closed-source reasoning models, and the fact that there is still significant room for improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused benchmark for advancing multimodal physical reasoning, is open-source and available at https://github.com/SciYu/HiPhO.",
        "arxiv_id": "2509.07894",
        "ARXIVID": "2509.07894",
        "COMMENT": "Matches criterion 3 (embodied AI, new benchmarks) and criterion 2 (MLLMs). Introduces a new benchmark for evaluating (M)LLMs on high school physics Olympiad problems with human-aligned evaluation, enabling direct comparison with human contestants.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07385": {
        "authors": [
            "Shibang Liu",
            "Xuemei Xie",
            "Guangming Shi"
        ],
        "title": "Parse Graph-Based Visual-Language Interaction for Human Pose Estimation",
        "abstract": "arXiv:2509.07385v1 Announce Type: new  Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and hierarchies, yet prior work mostly focuses on single modality modeling, ignoring the potential of multimodal fusion. Notably, language offers rich HPE priors like spatial relations for occluded scenes, but existing visual-language fusion via global feature integration weakens occluded region responses and causes alignment and location failures. To address this issue, we propose Parse Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module (GM). In PGVL, low-level nodes focus on local features, maximizing the maintenance of responses in occluded areas and high-level nodes integrate global features to infer occluded or invisible parts. GM enables high semantic nodes to guide the feature update of low semantic nodes that have undergone cross attention. It ensuring effective fusion of diverse information. PGVL includes top-down decomposition and bottom-up composition. In the first stage, modality specific parse graphs are constructed. Next stage. recursive bidirectional cross-attention is used, purified by GM. We also design network based on PGVL. The PGVL and our network is validated on major pose estimation datasets. We will release the code soon.",
        "arxiv_id": "2509.07385",
        "ARXIVID": "2509.07385",
        "COMMENT": "Matches criterion 1 (methodological improvement to spatial understanding via parse graph-based visual-language interaction for pose estimation). Also relevant to criterion 2 (multi-modal fusion for spatial reasoning).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07450": {
        "authors": [
            "Xudong Lu",
            "Zhi Zheng",
            "Yi Wan",
            "Yongxiang Yao",
            "Annan Wang",
            "Renrui Zhang",
            "Panwang Xia",
            "Qiong Wu",
            "Qingyun Li",
            "Weifeng Lin",
            "Xiangyu Zhao",
            "Xue Yang",
            "Hongsheng Li"
        ],
        "title": "GLEAM: Learning to Match and Explain in Cross-View Geo-Localization",
        "abstract": "arXiv:2509.07450v1 Announce Type: new  Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they merely predict whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/GLEAM.",
        "arxiv_id": "2509.07450",
        "ARXIVID": "2509.07450",
        "COMMENT": "Matches criterion 2 (shows new MLLM for explainable cross-view geo-localization) and criterion 4 (vision foundation models and their application to geo-localization). Also introduces a new benchmark (GLEAM-X) for explainable reasoning, partially matching criterion 3.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06992": {
        "authors": [
            "Kun Zhai",
            "Siheng Chen",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "title": "FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models",
        "abstract": "arXiv:2509.06992v1 Announce Type: new  Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client collaborative fine-tuning of large Vision-Language Models (VLMs). However, models tuned using FPT are vulnerable to adversarial attacks, leading to misclassification in downstream tasks. In this work, we introduce Federated Adversarial Prompt Tuning (\\textbf{FedAPT}), a novel method designed to enhance the adversarial robustness of FPT. We identify a key issue in FedAPT under non-independent and identically distributed (non-IID) settings: a \\textit{class information gap} between clients and the global model. Clients rely solely on limited local label information to generate adversarial samples for training, while the global model must defend against adversarial attacks from global labels. To address this issue, we propose a \\textbf{class-aware prompt generator} that generates visual prompts from text prompts. This generator is guided by a \\emph{Global Label Embedding} (serving as a ``beacon\") which encodes cross-client label information to create more globally-aligned visual prompts. Additionally, we propose a \\textbf{cross-layer generator sharing} strategy to enhance prompt coupling across different layers of the model, further boosting adversarial robustness. Extensive experiments on multiple image classification datasets demonstrate the superiority of FedAPT in improving adversarial robustness, outperforming existing methods by a large margin. FedAPT also exhibits exceptional generalization in cross-domain and cross-dataset scenarios, indicating its effectiveness in real-world applications.",
        "arxiv_id": "2509.06992",
        "ARXIVID": "2509.06992",
        "COMMENT": "Matches criterion 2: Shows new VLLMs or MLLMs. Proposes a federated adversarial prompt tuning method for vision-language models, improving adversarial robustness and generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.06996": {
        "authors": [
            "Jie Zhang",
            "Ting Xu",
            "Gelei Deng",
            "Runyi Hu",
            "Han Qiu",
            "Tianwei Zhang",
            "Qing Guo",
            "Ivor Tsang"
        ],
        "title": "Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems",
        "abstract": "arXiv:2509.06996v1 Announce Type: new  Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.",
        "arxiv_id": "2509.06996",
        "ARXIVID": "2509.06996",
        "COMMENT": "Matches criterion 2: Shows new VLLMs or MLLMs. Systematically analyzes VLMs' failure on perturbed text, providing surprising empirical results and suggesting new directions for model design.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07920": {
        "authors": [
            "Ao Li",
            "Jinpeng Liu",
            "Yixuan Zhu",
            "Yansong Tang"
        ],
        "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion",
        "abstract": "arXiv:2509.07920v1 Announce Type: new  Abstract: Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.",
        "arxiv_id": "2509.07920",
        "ARXIVID": "2509.07920",
        "COMMENT": "Matches criterion 1: New methodological improvements to spatial understanding on embodied agents. Proposes a diffusion-based optimizer for physically plausible human-object interaction reconstruction, which is a spatial intelligence problem.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07488": {
        "authors": [
            "Xiao Li",
            "Bharat Gandhi",
            "Ming Zhan",
            "Mohit Nehra",
            "Zhicheng Zhang",
            "Yuchen Sun",
            "Meijia Song",
            "Naisheng Zhang",
            "Xi Wang"
        ],
        "title": "Fine-Tuning Vision-Language Models for Visual Navigation Assistance",
        "abstract": "arXiv:2509.07488v1 Announce Type: new  Abstract: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.",
        "arxiv_id": "2509.07488",
        "ARXIVID": "2509.07488",
        "COMMENT": "Matches criterion 1 (new methods for spatial intelligence on embodied agents) and criterion 2 (fine-tuning VLLMs for navigation). Proposes fine-tuning BLIP-2 for vision-language-driven indoor navigation assistance, with a new evaluation metric for navigation instructions.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.07435": {
        "authors": [
            "Ze-Xin Yin",
            "Jiaxiong Qiu",
            "Liu Liu",
            "Xinjie Wang",
            "Wei Sui",
            "Zhizhong Su",
            "Jian Yang",
            "Jin Xie"
        ],
        "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation",
        "abstract": "arXiv:2509.07435v1 Announce Type: new  Abstract: The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.",
        "arxiv_id": "2509.07435",
        "ARXIVID": "2509.07435",
        "COMMENT": "Matches criterion 4 (vision foundation models and their application to 3D asset generation with multi-view diffusion models). Also relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.07047": {
        "authors": [
            "Kamyar Barakati",
            "Utkarsh Pratiush",
            "Sheryl L. Sanchez",
            "Aditya Raghavan",
            "Delia J. Milliron",
            "Mahshid Ahmadi",
            "Philip D. Rack",
            "Sergei V. Kalinin"
        ],
        "title": "SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards",
        "abstract": "arXiv:2509.07047v1 Announce Type: new  Abstract: Image segmentation is a critical task in microscopy, essential for accurately analyzing and interpreting complex visual data. This task can be performed using custom models trained on domain-specific datasets, transfer learning from pre-trained models, or foundational models that offer broad applicability. However, foundational models often present a considerable number of non-transparent tuning parameters that require extensive manual optimization, limiting their usability for real-time streaming data analysis. Here, we introduce a reward function-based optimization to fine-tune foundational models and illustrate this approach for SAM (Segment Anything Model) framework by Meta. The reward functions can be constructed to represent the physics of the imaged system, including particle size distributions, geometries, and other criteria. By integrating a reward-driven optimization framework, we enhance SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$, that better aligns with the requirements of diverse segmentation tasks and particularly allows for real-time streaming data segmentation. We demonstrate the effectiveness of this approach in microscopy imaging, where precise segmentation is crucial for analyzing cellular structures, material interfaces, and nanoscale features.",
        "arxiv_id": "2509.07047",
        "ARXIVID": "2509.07047",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a reward-based optimization for the Segment Anything Model (SAM), adapting it for real-time, physics-guided segmentation in microscopy.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.07596": {
        "authors": [
            "Yusuke Hirota",
            "Ryo Hachiuma",
            "Boyi Li",
            "Ximing Lu",
            "Michael Ross Boone",
            "Boris Ivanovic",
            "Yejin Choi",
            "Marco Pavone",
            "Yu-Chiang Frank Wang",
            "Noa Garcia",
            "Yuta Nakashima",
            "Chao-Han Huck Yang"
        ],
        "title": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation",
        "abstract": "arXiv:2509.07596v1 Announce Type: new  Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about their safe deployment and is typically evaluated using benchmarks with gender annotations on real-world images. However, as these benchmarks often contain spurious correlations between gender and non-gender features, such as objects and backgrounds, we identify a critical oversight in gender bias evaluation: Do spurious features distort gender bias evaluation? To address this question, we systematically perturb non-gender features across four widely used benchmarks (COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact on bias evaluation. Our findings reveal that even minimal perturbations, such as masking just 10% of objects or weakly blurring backgrounds, can dramatically alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in CLIP variants. This suggests that current bias evaluations often reflect model responses to spurious features rather than gender bias, undermining their reliability. Since creating spurious feature-free benchmarks is fundamentally challenging, we recommend reporting bias metrics alongside feature-sensitivity measurements to enable a more reliable bias assessment.",
        "arxiv_id": "2509.07596",
        "ARXIVID": "2509.07596",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Provides a critical empirical analysis of gender bias benchmarks for VLMs, showing how spurious features distort evaluation and proposing new reporting standards.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.07327": {
        "authors": [
            "Shucong Li",
            "Zhenyu Liu",
            "Zijie Hong",
            "Zhiheng Zhou",
            "Xianghai Cao"
        ],
        "title": "DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion",
        "abstract": "arXiv:2509.07327v1 Announce Type: new  Abstract: Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material.",
        "arxiv_id": "2509.07327",
        "ARXIVID": "2509.07327",
        "COMMENT": "Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 4 (vision foundation models and applications). Proposes a new multispectral object detector for UAVs with novel dual-domain enhancement and a priority-guided fusion module, leveraging Mamba for efficient transformer-like modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.07027": {
        "authors": [
            "Jisung Hwang",
            "Jaihoon Kim",
            "Minhyuk Sung"
        ],
        "title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models",
        "abstract": "arXiv:2509.07027v1 Announce Type: new  Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.",
        "arxiv_id": "2509.07027",
        "ARXIVID": "2509.07027",
        "COMMENT": "Matches criterion 4: Vision foundation models and its applications. Proposes a new regularization for text-to-image generative models, improving reward alignment and convergence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.07525": {
        "authors": [
            "Gianluca Amprimo",
            "Alberto Ancilotto",
            "Alessandro Savino",
            "Fabio Quazzolo",
            "Claudia Ferraris",
            "Gabriella Olmo",
            "Elisabetta Farella",
            "Stefano Di Carlo"
        ],
        "title": "EHWGesture -- A dataset for multimodal understanding of clinical gestures",
        "abstract": "arXiv:2509.07525v1 Announce Type: new  Abstract: Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.",
        "arxiv_id": "2509.07525",
        "ARXIVID": "2509.07525",
        "COMMENT": "Matches criterion 3: Embodied AI papers on building new benchmarks. Introduces a new multimodal dataset for clinical gesture understanding, with precise ground-truth and multi-view data, serving as a benchmark for multimodal gesture understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.07120": {
        "authors": [
            "Chung-Shien Brian Wang",
            "Christian Schmidt",
            "Jens Piekenbrinck",
            "Bastian Leibe"
        ],
        "title": "Faster VGGT with Block-Sparse Global Attention",
        "abstract": "arXiv:2509.07120v1 Announce Type: new  Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been an important task in computer vision. Recent transformer-based models like VGGT and $\\pi^3$ have achieved impressive results with simple architectures, yet they face an inherent runtime bottleneck, due to the quadratic complexity of the global attention layers, that limits the scalability to large image sets. In this paper, we empirically analyze the global attention matrix of these models and observe that probability mass concentrates on a small subset of patch-patch interactions that correspond to cross-view geometric matches. Motivated by the structured attention and inspired by recent advancement in large language models, we propose a replacement for the dense global attention operation based on highly optimized block-sparse kernels, yielding up to $4\\times$ faster inference with comparable task performance. Our retrofit requires no retraining of the backbone, extends to both VGGT and $\\pi^3$, and supports large image collections. Evaluations on a comprehensive suite of multi-view benchmarks demonstrate the effectiveness of our approach.",
        "arxiv_id": "2509.07120",
        "ARXIVID": "2509.07120",
        "COMMENT": "Matches criterion 4 (vision foundation models and applications). Proposes a block-sparse global attention mechanism for transformer-based multi-view reconstruction, improving scalability and efficiency.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.07809": {
        "authors": [
            "Mahtab Dahaghin",
            "Milind G. Padalkar",
            "Matteo Toso",
            "Alessio Del Bue"
        ],
        "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting",
        "abstract": "arXiv:2509.07809v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.",
        "arxiv_id": "2509.07809",
        "ARXIVID": "2509.07809",
        "COMMENT": "Novel 3D scene inpainting method using depth-guided Gaussian Splatting; relevant to vision foundation models and generative modeling, but not directly to spatial intelligence in embodied agents or VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.07552": {
        "authors": [
            "Peng Li",
            "Yisheng He",
            "Yingdong Hu",
            "Yuan Dong",
            "Weihao Yuan",
            "Yuan Liu",
            "Zilong Dong",
            "Yike Guo"
        ],
        "title": "PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image",
        "abstract": "arXiv:2509.07552v1 Announce Type: new  Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.",
        "arxiv_id": "2509.07552",
        "ARXIVID": "2509.07552",
        "COMMENT": "Somewhat relevant to criterion 4 (vision foundation models and applications), as it proposes a new method for 3D head synthesis using pretrained 3D GANs and transformer blocks, but not a foundation model itself.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.07852": {
        "authors": [
            "Seyd Teymoor Seydi"
        ],
        "title": "Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets",
        "abstract": "arXiv:2509.07852v1 Announce Type: new  Abstract: Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset.",
        "arxiv_id": "2509.07852",
        "ARXIVID": "2509.07852",
        "COMMENT": "Matches criterion 4: Vision foundation models and applications. Uses AlphaEarth foundation dataset and deep learning for burned area mapping, showing application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.07335": {
        "authors": [
            "Haiqing Ren",
            "Zhongkai Luo",
            "Heng Fan",
            "Xiaohui Yuan",
            "Guanchen Wang",
            "Libo Zhang"
        ],
        "title": "G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition",
        "abstract": "arXiv:2509.07335v1 Announce Type: new  Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for skeleton-based action recognition, primarily due to their ability to leverage graph topology for feature aggregation, a key factor in extracting meaningful representations. However, despite their success, GCNs often struggle to effectively distinguish between ambiguous actions, revealing limitations in the representation of learned topological and spatial features. To address this challenge, we propose a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates a Gaussian filter to refine the skeleton topology graph, improving the representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs) are integrated into the GCN framework to enhance information propagation between skeleton points. Our method shows strong generalization across various GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples.",
        "arxiv_id": "2509.07335",
        "ARXIVID": "2509.07335",
        "COMMENT": "Skeleton-based action recognition with improved graph convolutional networks; relevant to spatial understanding, but not embodied agents or VLLMs/MLLMs. Some relevance to criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.07665": {
        "authors": [
            "Adem Kikaj",
            "Giuseppe Marra",
            "Floris Geerts",
            "Robin Manhaeve",
            "Luc De Raedt"
        ],
        "title": "DeepGraphLog for Layered Neurosymbolic AI",
        "abstract": "arXiv:2509.07665v1 Announce Type: new  Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural networks with the interpretability and structure of symbolic reasoning. However, current NeSy frameworks like DeepProbLog enforce a fixed flow where symbolic reasoning always follows neural processing. This restricts their ability to model complex dependencies, especially in irregular data structures such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables multi-layer neural-symbolic reasoning, allowing neural and symbolic components to be layered in arbitrary order. In contrast to DeepProbLog, which cannot handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic representations as graphs, enabling them to be processed by Graph Neural Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in planning, knowledge graph completion with distant supervision, and GNN expressivity. Our results demonstrate that DeepGraphLog effectively captures complex relational dependencies, overcoming key limitations of existing NeSy systems. By broadening the applicability of neurosymbolic AI to graph-structured domains, DeepGraphLog offers a more expressive and flexible framework for neural-symbolic integration.",
        "arxiv_id": "2509.07665",
        "ARXIVID": "2509.07665",
        "COMMENT": "Neurosymbolic AI with graph neural networks; not directly about spatial intelligence in embodied agents or VLLMs/MLLMs, but relevant to general interest in multi-modal learning and clever statistical tricks.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.07581": {
        "authors": [
            "Barkin Buyukcakir",
            "Rocharles Cavalcante Fontenele",
            "Reinhilde Jacobs",
            "Jannick De Tobel",
            "Patrick Thevissen",
            "Dirk Vandermeulen",
            "Peter Claes"
        ],
        "title": "Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks",
        "abstract": "arXiv:2509.07581v1 Announce Type: new  Abstract: Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments.",
        "arxiv_id": "2509.07581",
        "ARXIVID": "2509.07581",
        "COMMENT": "Graph attention networks for 3D shape classification; interpretable models, but not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, or vision foundation models. Some relevance to general interest in explainable models.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2509.07654": {
        "authors": [
            "Guoyi Zhang",
            "Siyang Chen",
            "Guangsheng Xu",
            "Zhihua Shen",
            "Han Wang",
            "Xiaohu Zhang"
        ],
        "title": "Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection",
        "abstract": "arXiv:2509.07654v1 Announce Type: new  Abstract: Small moving target detection is crucial for many defense applications but remains highly challenging due to low signal-to-noise ratios, ambiguous visual cues, and cluttered backgrounds. In this work, we propose a novel deep learning framework that differs fundamentally from existing approaches, which often rely on target-specific features or motion cues and tend to lack robustness in complex environments. Our key insight is that small target detection and background discrimination are inherently coupled, even cluttered video backgrounds often exhibit strong low-rank structures that can serve as stable priors for detection. We reformulate the task as a tensor-based low-rank and sparse decomposition problem and conduct a theoretical analysis of the background, target, and noise components to guide model design. Building on these insights, we introduce TenRPCANet, a deep neural network that requires minimal assumptions about target characteristics. Specifically, we propose a tokenization strategy that implicitly enforces multi-order tensor low-rank priors through a self-attention mechanism. This mechanism captures both local and non-local self-similarity to model the low-rank background without relying on explicit iterative optimization. In addition, inspired by the sparse component update in tensor RPCA, we design a feature refinement module to enhance target saliency. The proposed method achieves state-of-the-art performance on two highly distinct and challenging tasks: multi-frame infrared small target detection and space object detection. These results demonstrate both the effectiveness and the generalizability of our approach.",
        "arxiv_id": "2509.07654",
        "ARXIVID": "2509.07654",
        "COMMENT": "Does not match any specific criteria. Focuses on small moving target detection with a novel deep learning framework, but not directly on spatial intelligence for embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.07879": {
        "authors": [
            "Daniel DeAlcala",
            "Aythami Morales",
            "Julian Fierrez",
            "Gonzalo Mancera",
            "Ruben Tolosana",
            "Javier Ortega-Garcia"
        ],
        "title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning",
        "abstract": "arXiv:2509.07879v1 Announce Type: new  Abstract: Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection.",
        "arxiv_id": "2509.07879",
        "ARXIVID": "2509.07879",
        "COMMENT": "Does not match any specific criterion. Focuses on model auditability and membership inference, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.07917": {
        "authors": [
            "Chunlin Wen",
            "Yu Zhang",
            "Jie Fan",
            "Hongyuan Zhu",
            "Xiu-Shen Wei",
            "Yijun Wang",
            "Zhiqiang Kou",
            "Shuzhou Sun"
        ],
        "title": "Object-level Correlation for Few-Shot Segmentation",
        "abstract": "arXiv:2509.07917v1 Announce Type: new  Abstract: Few-shot semantic segmentation (FSS) aims to segment objects of novel categories in the query images given only a few annotated support samples. Existing methods primarily build the image-level correlation between the support target object and the entire query image. However, this correlation contains the hard pixel noise, \\textit{i.e.}, irrelevant background objects, that is intractable to trace and suppress, leading to the overfitting of the background. To address the limitation of this correlation, we imitate the biological vision process to identify novel objects in the object-level information. Target identification in the general objects is more valid than in the entire image, especially in the low-data regime. Inspired by this, we design an Object-level Correlation Network (OCNet) by establishing the object-level correlation between the support target object and query general objects, which is mainly composed of the General Object Mining Module (GOMM) and Correlation Construction Module (CCM). Specifically, GOMM constructs the query general object feature by learning saliency and high-level similarity cues, where the general objects include the irrelevant background objects and the target foreground object. Then, CCM establishes the object-level correlation by allocating the target prototypes to match the general object feature. The generated object-level correlation can mine the query target feature and suppress the hard pixel noise for the final prediction. Extensive experiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model achieves the state-of-the-art performance.",
        "arxiv_id": "2509.07917",
        "ARXIVID": "2509.07917",
        "COMMENT": "Does not match any specific criteria. Focuses on few-shot segmentation with object-level correlation, which is not directly about spatial intelligence in embodied agents, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.07260": {
        "authors": [
            "Xin Wang",
            "Ting Dang",
            "Xinyu Zhang",
            "Vassilis Kostakos",
            "Michael J. Witbrock",
            "Hong Jia"
        ],
        "title": "HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring",
        "abstract": "arXiv:2509.07260v1 Announce Type: new  Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.",
        "arxiv_id": "2509.07260",
        "ARXIVID": "2509.07260",
        "COMMENT": "Does not match any specific criterion. Focuses on benchmarking small language models for healthcare monitoring, not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.07706": {
        "authors": [
            "Yildiray Kabak",
            "Gokce B. Laleci Erturkmen",
            "Mert Gencturk",
            "Tuncay Namli",
            "A. Anil Sinaci",
            "Ruben Alcantud Corcoles",
            "Cristina Gomez Ballesteros",
            "Pedro Abizanda",
            "Asuman Dogac"
        ],
        "title": "FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support",
        "abstract": "arXiv:2509.07706v1 Announce Type: new  Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a Retrieval-Augmented Generation (RAG)-based system to improve personalized medical decision support on evidence-based clinical guidelines, emphasizing the need for research in practical applications. In the evolving landscape of medical decision support systems, integrating advanced technologies such as RAG and HL7 FHIR can significantly enhance clinical decision-making processes. Despite the potential of these technologies, there is limited research on their integration in practical applications.",
        "arxiv_id": "2509.07706",
        "ARXIVID": "2509.07706",
        "COMMENT": "Does not match any specific criterion. Focuses on medical decision support and integration of FHIR with RAG-based LLMs, which is outside the main criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.07477": {
        "authors": [
            "Patrick Wienholt",
            "Christiane Kuhl",
            "Jakob Nikolas Kather",
            "Sven Nebelung",
            "Daniel Truhn"
        ],
        "title": "MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification",
        "abstract": "arXiv:2509.07477v1 Announce Type: new  Abstract: Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet",
        "arxiv_id": "2509.07477",
        "ARXIVID": "2509.07477",
        "COMMENT": "Does not match any specific criteria. Focuses on explainable AI for medical imaging, not spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.07054": {
        "authors": [
            "Edgar Dobriban"
        ],
        "title": "Statistical Methods in Generative AI",
        "abstract": "arXiv:2509.07054v1 Announce Type: new  Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.   In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.",
        "arxiv_id": "2509.07054",
        "ARXIVID": "2509.07054",
        "COMMENT": "General review of statistical methods in generative AI. While relevant to your friend's interests, it does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}