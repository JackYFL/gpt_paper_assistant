{
    "2508.11058": {
        "authors": [
            "Wentao Mo",
            "Qingchao Chen",
            "Yuxin Peng",
            "Siyuan Huang",
            "Yang Liu"
        ],
        "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset",
        "abstract": "arXiv:2508.11058v1 Announce Type: new  Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several limitations in existing 3D VL datasets: they rarely necessitate reasoning beyond a close range of objects in single viewpoint, and annotations often link instructions to single objects, missing richer contextual alignments between multiple objects. This significantly curtails the development of models capable of deep, multi-view 3D scene understanding over distant objects. To address these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset where 68% of questions explicitly require integrating information from multiple views (compared to less than 7% in existing datasets), thereby rigorously testing multi-view compositional reasoning. To facilitate the training of models for such demanding scenarios, we present TripAlign dataset, a large-scale and low-cost 2D-3D-language pre-training corpus containing 1M  triplets that explicitly aligns groups of contextually related objects with text, providing richer, view-grounded multi-object multimodal alignment signals than previous single-object annotations. We further develop LEGO, a baseline method for the multi-view reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign achieves state-of-the-art performance not only on the proposed MV-ScanQA, but also on existing benchmarks for 3D dense captioning and question answering. Datasets and code are available at https://matthewdm0816.github.io/tripalign-mvscanqa.",
        "arxiv_id": "2508.11058",
        "ARXIVID": "2508.11058",
        "COMMENT": "Matches criteria 1 (new methodological improvements to spatial understanding on embodied agents), 3 (new benchmark for embodied AI with novel multi-view reasoning focus), and 4 (vision foundation models and applications). Introduces a new 3D QA benchmark (MV-ScanQA) that requires multi-view reasoning, a new pretraining dataset (TripAlign) for richer 2D-3D-language alignment, and a baseline method (LEGO) for transferring 2D LVLM knowledge to 3D. This is a strong fit for your interests in spatial intelligence, benchmarks, and vision-language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11433": {
        "authors": [
            "Qian Liang",
            "Yujia Wu",
            "Kuncheng Li",
            "Jiwei Wei",
            "Shiyuan He",
            "Jinyu Guo",
            "Ning Xie"
        ],
        "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation",
        "abstract": "arXiv:2508.11433v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.",
        "arxiv_id": "2508.11433",
        "ARXIVID": "2508.11433",
        "COMMENT": "This paper introduces MM-R1, a unified multimodal large language model (MLLM) framework for personalized image generation, using cross-modal chain-of-thought reasoning and GRPO alignment. It directly matches criterion 2 (new MLLMs) and is highly relevant to your friend's interest in generative modeling in multi-modal learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.10936": {
        "authors": [
            "Cheng Chen",
            "Hao Huang",
            "Saurabh Bagchi"
        ],
        "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction",
        "abstract": "arXiv:2508.10936v1 Announce Type: new  Abstract: Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.",
        "arxiv_id": "2508.10936",
        "ARXIVID": "2508.10936",
        "COMMENT": "This paper proposes a vision-only Gaussian splatting method for collaborative semantic occupancy prediction, which is highly relevant to criterion 1 (spatial understanding on embodied agents) and criterion 3 (novel methods for embodied AI, especially in collaborative/simulator settings). The use of sparse, object-centric messages and cross-agent fusion is a novel angle.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11446": {
        "authors": [
            "Daniel Airinei",
            "Elena Burceanu",
            "Marius Leordeanu"
        ],
        "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation",
        "abstract": "arXiv:2508.11446v1 Announce Type: new  Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS access, forcing solutions to rely on other sources of information. While significant progress continues to be made in this area, deployment to production applications is still lacking, given the complexity and additional requirements of current solutions. Here, we introduce an efficient, real-time and easily deployable deep learning approach, based on visual input only, that can predict the direction towards a target from images captured by a mobile device. Our technical approach, based on a novel graph-based path generation method, combined with explainable data augmentation and curriculum learning, includes contributions that make the process of data collection, annotation and training, as automatic as possible, efficient and robust. On the practical side, we introduce a novel largescale dataset, with video footage inside a relatively large shopping mall, in which each frame is annotated with the correct next direction towards different specific target destinations. Different from current methods, ours relies solely on vision, avoiding the need of special sensors, additional markers placed along the path, knowledge of the scene map or internet access. We also created an easy to use application for Android, which we plan to make publicly available. We make all our data and code available along with visual demos on our project site",
        "arxiv_id": "2508.11446",
        "ARXIVID": "2508.11446",
        "COMMENT": "Matches criterion 1 and 3: Proposes a novel graph-based path generation method for visual indoor navigation, with explainable data augmentation and curriculum learning. Also introduces a new large-scale dataset and an Android app, focusing on spatial intelligence for embodied agents and a new benchmark.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11630": {
        "authors": [
            "Yi-Fan Zhang",
            "Xingyu Lu",
            "Shukang Yin",
            "Chaoyou Fu",
            "Wei Chen",
            "Xiao Hu",
            "Bin Wen",
            "Kaiyu Jiang",
            "Changyi Liu",
            "Tianke Zhang",
            "Haonan Fan",
            "Kaibing Chen",
            "Jiankang Chen",
            "Haojie Ding",
            "Kaiyu Tang",
            "Zhang Zhang",
            "Liang Wang",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "title": "Thyme: Think Beyond Images",
        "abstract": "arXiv:2508.11630v1 Announce Type: new  Abstract: Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
        "arxiv_id": "2508.11630",
        "ARXIVID": "2508.11630",
        "COMMENT": "Thyme introduces a new paradigm for MLLMs to autonomously generate and execute image processing and computational operations via code, with a novel RL algorithm for training. This is a direct match to criterion 2 (new MLLMs) and also relates to criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11256": {
        "authors": [
            "Junjie Wang",
            "Keyu Chen",
            "Yulin Li",
            "Bin Chen",
            "Hengshuang Zhao",
            "Xiaojuan Qi",
            "Zhuotao Tian"
        ],
        "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception",
        "abstract": "arXiv:2508.11256v1 Announce Type: new  Abstract: Dense visual perception tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense perception often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. \\revise{The context features are enhanced by jointly distilling semantic correlations from Vision Foundation Models (VFMs) and object integrity cues from diffusion models, thereby enhancing spatial consistency. In parallel, the content features are aligned with image crop representations and constrained by region correlations from VFMs to improve local discriminability. Extensive experiments demonstrate that DeCLIP establishes a solid foundation for open-vocabulary dense perception, consistently achieving state-of-the-art performance across a broad spectrum of tasks, including 2D detection and segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.} Code is available at https://github.com/xiaomoguhz/DeCLIP",
        "arxiv_id": "2508.11256",
        "ARXIVID": "2508.11256",
        "COMMENT": "DeCLIP proposes a new framework for open-vocabulary dense perception by enhancing CLIP with decoupled self-attention and distillation from vision foundation models and diffusion models. This is a direct match to criterion 4 (vision foundation models and applications) and also relates to criterion 2 (VLLMs/MLLMs).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11360": {
        "authors": [
            "Songqin Nong",
            "Jingxuan Xu",
            "Sheng Zhou",
            "Jianfeng Chen",
            "Xiaoxuan Tang",
            "Tao Jiang",
            "Wenhao Xu"
        ],
        "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks",
        "abstract": "arXiv:2508.11360v1 Announce Type: new  Abstract: As autonomous agents become adept at understanding and interacting with graphical user interface (GUI) environments, a new era of automated task execution is emerging. Recent studies have demonstrated that Reinforcement Learning (RL) can effectively enhance agents' performance in dynamic interactive GUI environments. However, these methods face two key limitations: (1) they overlook the significant variation in difficulty across different GUI tasks by treating the entire training data as a uniform set, which hampers the agent's ability to adapt its learning process; and (2) most approaches collapse task-specific nuances into a single, coarse reward, leaving the agent with a uniform signal that yields inefficient policy updates. To address these limitations, we propose CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO) that explicitly accounts for the varying difficulty across trajectories. To enable more fine-grained policy optimization, we design a reward function that combines simple rule-based signals with model-judged evaluation, providing richer and more nuanced feedback during training. Experimental results demonstrate that our method achieves significant improvements over previous state-of-the-art approaches, outperforming them by 5.6% on public benchmarks Android Control and 10.3% on our internal online benchmarks, respectively. These findings empirically validate the effectiveness of integrating reinforcement learning with curriculum learning in GUI interaction tasks.",
        "arxiv_id": "2508.11360",
        "ARXIVID": "2508.11360",
        "COMMENT": "This paper proposes a new curriculum learning framework for GUI-based embodied agents, introducing Group Relative Policy Optimization and a nuanced reward function. It matches criterion 1 (spatial intelligence on embodied agents) and criterion 3 (new methods for embodied AI with novel angles, focusing on curriculum and reward design for GUI tasks).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.11021": {
        "authors": [
            "Zisheng Liang",
            "Kidus Zewde",
            "Rudra Pratap Singh",
            "Disha Patil",
            "Zexi Chen",
            "Jiayu Xue",
            "Yao Yao",
            "Yifei Chen",
            "Qinzhe Liu",
            "Simiao Ren"
        ],
        "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?",
        "abstract": "arXiv:2508.11021v1 Announce Type: new  Abstract: Document fraud poses a significant threat to industries reliant on secure and verifiable documentation, necessitating robust detection mechanisms. This study investigates the efficacy of state-of-the-art multi-modal large language models (LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus, Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and 3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against each other and prior work on document fraud detection techniques using a standard dataset with real transactional documents. Through prompt optimization and detailed analysis of the models' reasoning processes, we evaluate their ability to identify subtle indicators of fraud, such as tampered text, misaligned formatting, and inconsistent transactional sums. Our results reveal that top-performing multi-modal LLMs demonstrate superior zero-shot generalization, outperforming conventional methods on out-of-distribution datasets, while several vision LLMs exhibit inconsistent or subpar performance. Notably, model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical. This study underscores the potential of multi-modal LLMs in enhancing document fraud detection systems and provides a foundation for future research into interpretable and scalable fraud mitigation strategies.",
        "arxiv_id": "2508.11021",
        "ARXIVID": "2508.11021",
        "COMMENT": "This paper benchmarks a wide range of state-of-the-art multi-modal large language models (MLLMs) and vision-language models (VLLMs) for document fraud detection, providing empirical insights into their reasoning and generalization abilities. It directly matches criterion 2 (new VLLMs/MLLMs) and also offers surprising empirical results.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.11431": {
        "authors": [
            "Simona Kocour",
            "Assia Benbihi",
            "Torsten Sattler"
        ],
        "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting",
        "abstract": "arXiv:2508.11431v1 Announce Type: new  Abstract: Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre/post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com/spatial-intelligence-ai/Remove360.git. Data are available at huggingface.co/datasets/simkoc/Remove360.",
        "arxiv_id": "2508.11431",
        "ARXIVID": "2508.11431",
        "COMMENT": "Remove360 introduces a new benchmark and evaluation framework for measuring semantic residuals after object removal in 3D Gaussian Splatting, with a new dataset. This is a direct match to criterion 3 (new benchmark for embodied AI/simulator-related, with a novel angle on semantic residuals in 3D scene editing).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.10934": {
        "authors": [
            "Jiahui Huang",
            "Qunjie Zhou",
            "Hesam Rabeti",
            "Aleksandr Korovko",
            "Huan Ling",
            "Xuanchi Ren",
            "Tianchang Shen",
            "Jun Gao",
            "Dmitry Slepichev",
            "Chen-Hsuan Lin",
            "Jiawei Ren",
            "Kevin Xie",
            "Joydeep Biswas",
            "Laura Leal-Taixe",
            "Sanja Fidler"
        ],
        "title": "ViPE: Video Pose Engine for 3D Geometric Perception",
        "abstract": "arXiv:2508.10934v1 Announce Type: new  Abstract: Accurate 3D geometric perception is an important prerequisite for a wide range of spatial AI systems. While state-of-the-art methods depend on large-scale training data, acquiring consistent and precise 3D annotations from in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a handy and versatile video processing engine designed to bridge this gap. ViPE efficiently estimates camera intrinsics, camera motion, and dense, near-metric depth maps from unconstrained raw videos. It is robust to diverse scenarios, including dynamic selfie videos, cinematic shots, or dashcams, and supports various camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to annotate a large-scale collection of videos. This collection includes around 100K real-world internet videos, 1M high-quality AI-generated videos, and 2K panoramic videos, totaling approximately 96M frames -- all annotated with accurate camera poses and dense depth maps. We open-source ViPE and the annotated dataset with the hope of accelerating the development of spatial AI systems.",
        "arxiv_id": "2508.10934",
        "ARXIVID": "2508.10934",
        "COMMENT": "ViPE introduces a new engine for 3D geometric perception from unconstrained videos, robust to various camera models and scenarios. It benchmarks and open-sources a large annotated dataset, directly advancing spatial understanding for spatial AI systems. This matches criterion 1 (methodological improvements to spatial understanding for embodied agents) and criterion 3 (benchmark/simulator for embodied AI).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.11452": {
        "authors": [
            "Kangyu Wang",
            "Hongliang He",
            "Lin Liu",
            "Ruiqi Liang",
            "Zhenzhong Lan",
            "Jianguo Li"
        ],
        "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps",
        "abstract": "arXiv:2508.11452v1 Announce Type: new  Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.",
        "arxiv_id": "2508.11452",
        "ARXIVID": "2508.11452",
        "COMMENT": "Matches criterion 4 and partially criterion 2: Introduces a new open platform for evaluating large foundation models (LLMs and MLLMs) with real-world app feedback, using innovative ranking and sampling strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.11484": {
        "authors": [
            "Xiaoxue Wu",
            "Bingjie Gao",
            "Yu Qiao",
            "Yaohui Wang",
            "Xinyuan Chen"
        ],
        "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models",
        "abstract": "arXiv:2508.11484v1 Announce Type: new  Abstract: Despite significant advances in video synthesis, research into multi-shot video generation remains in its infancy. Even with scaled-up models and massive datasets, the shot transition capabilities remain rudimentary and unstable, largely confining generated videos to single-shot sequences. In this work, we introduce CineTrans, a novel framework for generating coherent multi-shot videos with cinematic, film-style transitions. To facilitate insights into the film editing style, we construct a multi-shot video-text dataset Cine250K with detailed shot annotations. Furthermore, our analysis of existing video diffusion models uncovers a correspondence between attention maps in the diffusion model and shot boundaries, which we leverage to design a mask-based control mechanism that enables transitions at arbitrary positions and transfers effectively in a training-free setting. After fine-tuning on our dataset with the mask mechanism, CineTrans produces cinematic multi-shot sequences while adhering to the film editing style, avoiding unstable transitions or naive concatenations. Finally, we propose specialized evaluation metrics for transition control, temporal consistency and overall quality, and demonstrate through extensive experiments that CineTrans significantly outperforms existing baselines across all criteria.",
        "arxiv_id": "2508.11484",
        "ARXIVID": "2508.11484",
        "COMMENT": "CineTrans introduces a new framework for multi-shot video generation with cinematic transitions using masked diffusion models, and a new dataset and evaluation metrics. This is relevant to vision foundation models (criterion 4) and generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.11350": {
        "authors": [
            "Zhenhao Zhang",
            "Hanqing Wang",
            "Xiangyu Zeng",
            "Ziyu Cheng",
            "Jiaxin Liu",
            "Haoyu Yan",
            "Zhirui Liu",
            "Kaiyang Ji",
            "Tianxiang Gui",
            "Ke Hu",
            "Kangyi Chen",
            "Yahao Fan",
            "Mokai Pan"
        ],
        "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model",
        "abstract": "arXiv:2508.11350v1 Announce Type: new  Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal application in AR/VR and robotics. Recent open-vocabulary HOI detection approaches depend exclusively on large language models for richer textual prompts, neglecting their inherent 3D spatial understanding capabilities. To address this shortcoming, we introduce HOID-R1, the first HOI detection framework that integrates chain-of-thought (CoT) guided supervised fine-tuning (SFT) with group relative policy optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model with essential reasoning capabilities, forcing the model to articulate its thought process in the output. Subsequently, we integrate GRPO to leverage multi-reward signals for policy optimization, thereby enhancing alignment across diverse modalities. To mitigate hallucinations in the CoT reasoning, we introduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs, further improving generalization. Extensive experiments show that HOID-R1 achieves state-of-the-art performance on HOI detection benchmarks and outperforms existing methods in open-world generalization to novel scenarios.",
        "arxiv_id": "2508.11350",
        "ARXIVID": "2508.11350",
        "COMMENT": "HOID-R1 is a new MLLM-based framework for open-world human-object interaction detection, integrating chain-of-thought reasoning and RL. This is a direct match to criterion 2 (new MLLMs) and also relates to spatial understanding (criterion 1).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.11167": {
        "authors": [
            "Jianhong Han",
            "Yupei Wang",
            "Liang Chen"
        ],
        "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images",
        "abstract": "arXiv:2508.11167v1 Announce Type: new  Abstract: Unsupervised domain adaptation methods have been widely explored to bridge domain gaps. However, in real-world remote-sensing scenarios, privacy and transmission constraints often preclude access to source domain data, which limits their practical applicability. Recently, Source-Free Object Detection (SFOD) has emerged as a promising alternative, aiming at cross-domain adaptation without relying on source data, primarily through a self-training paradigm. Despite its potential, SFOD frequently suffers from training collapse caused by noisy pseudo-labels, especially in remote sensing imagery with dense objects and complex backgrounds. Considering that limited target domain annotations are often feasible in practice, we propose a Vision foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised framework for SFOD in remote sensing images. VG-DETR integrates a Vision Foundation Model (VFM) into the training pipeline in a \"free lunch\" manner, leveraging a small amount of labeled target data to mitigate pseudo-label noise while improving the detector's feature-extraction capability. Specifically, we introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's semantic priors to further assess the reliability of the generated pseudo-labels. By recovering potentially correct predictions from low-confidence outputs, our strategy improves pseudo-label quality and quantity. In addition, a dual-level VFM-guided alignment method is proposed, which aligns detector features with VFM embeddings at both the instance and image levels. Through contrastive learning among fine-grained prototypes and similarity matching between feature maps, this dual-level alignment further enhances the robustness of feature representations against domain gaps. Extensive experiments demonstrate that VG-DETR achieves superior performance in source-free remote sensing detection tasks.",
        "arxiv_id": "2508.11167",
        "ARXIVID": "2508.11167",
        "COMMENT": "This paper proposes VG-DETR, a semi-supervised detection transformer for source-free object detection in remote sensing, guided by a vision foundation model. It matches criterion 4 (vision foundation models and applications) and introduces a VFM-guided pseudo-label mining strategy, which is a clever statistical trick.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10935": {
        "authors": [
            "Qi Liu",
            "Yabei Li",
            "Hongsong Wang",
            "Lei He"
        ],
        "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model",
        "abstract": "arXiv:2508.10935v1 Announce Type: new  Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of open-world applications like autonomous driving. Existing open-vocabulary 3D detection methods typically adopt a two-stage pipeline consisting of pseudo-label generation followed by semantic alignment. While vision-language models (VLMs) recently have dramatically improved the semantic accuracy of pseudo-labels, their geometric quality, particularly bounding box precision, remains commonly neglected.To address this issue, we propose a High Box Quality Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and refine high-quality pseudo-labels for open-vocabulary classes. The framework comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal Generator that utilizes cross-modality geometric consistency to generate high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA) Denoiser that progressively refines 3D proposals by leveraging geometric priors from annotated categories through a DDIM-based denoising mechanism.Compared to the state-of-the-art method, training with pseudo-labels generated by our approach achieves a 7.37% improvement in mAP on novel classes, demonstrating the superior quality of the pseudo-labels produced by our framework. HQ-OV3D can serve not only as a strong standalone open-vocabulary 3D detector but also as a plug-in high-quality pseudo-label generator for existing open-vocabulary detection or annotation pipelines.",
        "arxiv_id": "2508.10935",
        "ARXIVID": "2508.10935",
        "COMMENT": "This paper presents HQ-OV3D, a new open-vocabulary 3D detection framework using diffusion models and vision-language models for high-quality pseudo-labels. It is highly relevant to criterion 4 (vision foundation models and applications) and also touches on criterion 3 (novel methods for embodied AI benchmarks, as open-world 3D detection is crucial for embodied agents).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11032": {
        "authors": [
            "Yanwu Yang",
            "Guinan Su",
            "Jiesi Hu",
            "Francesco Sammarco",
            "Jonas Geiping",
            "Thomas Wolfers"
        ],
        "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation",
        "abstract": "arXiv:2508.11032v1 Announce Type: new  Abstract: Universal medical image segmentation models have emerged as a promising paradigm due to their strong generalizability across diverse tasks, showing great potential for a wide range of clinical applications. This potential has been partly driven by the success of general-purpose vision models such as the Segment Anything Model (SAM), which has inspired the development of various fine-tuned variants for medical segmentation tasks. However, fine-tuned variants like MedSAM are trained on comparatively limited medical imaging data that often suffers from heterogeneity, scarce annotations, and distributional shifts. These challenges limit their ability to generalize across a wide range of medical segmentation tasks. In this regard, we propose MedSAMix, a training-free model merging method that integrates the strengths of both generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical image segmentation. In contrast to traditional model merging approaches that rely on manual configuration and often result in suboptimal outcomes, we propose a zero-order optimization method to automatically discover optimal layer-wise merging solutions. Furthermore, for clinical applications, we develop two regimes to meet the demand of domain-specificity and generalizability in different scenarios by single-task optimization and multi-objective optimization respectively. Extensive evaluations on 25 medical segmentation tasks demonstrate that MedSAMix effectively mitigates model bias and consistently improves performance in both domain-specific accuracy and generalization, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations.",
        "arxiv_id": "2508.11032",
        "ARXIVID": "2508.11032",
        "COMMENT": "Matches criterion 4: Proposes a training-free model merging approach for medical image segmentation, leveraging vision foundation models (SAM, MedSAM) and zero-order optimization for layer-wise merging.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11282": {
        "authors": [
            "Muzammil Khan",
            "Enzo Kerkhof",
            "Matteo Fusaglia",
            "Koert Kuhlmann",
            "Theo Ruers",
            "Fran\\c{c}oise J. Siepel"
        ],
        "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction",
        "abstract": "arXiv:2508.11282v1 Announce Type: new  Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction significantly enhances monocular minimally invasive surgical procedures by enabling accurate navigation and improved spatial awareness. However, monocular endoscope pose estimation and tissue reconstruction face persistent challenges, including depth ambiguity, physiological tissue deformation, inconsistent endoscope motion, limited texture fidelity, and a restricted field of view. To overcome these limitations, a unified framework for monocular endoscopic tissue reconstruction that integrates scale-aware depth prediction with temporally-constrained perceptual refinement is presented. This framework incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust initialisation and Depth Anything for efficient per-frame depth prediction, in conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth estimates. These estimates are temporally refined by computing pixel correspondences using RAFT and adaptively blending flow-warped frames based on LPIPS perceptual similarity, thereby reducing artefacts arising from physiological tissue deformation and motion. To ensure accurate registration of the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module is integrated, optimising both rotation and translation. Finally, truncated signed distance function-based volumetric fusion and marching cubes are applied to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED, with ablation and comparative analyses, demonstrate the framework's robustness and superiority over state-of-the-art methods.",
        "arxiv_id": "2508.11282",
        "ARXIVID": "2508.11282",
        "COMMENT": "This paper presents a unified framework for monocular endoscopic tissue reconstruction, integrating scale-aware depth prediction and perceptual priors. It is relevant to criterion 1 (spatial understanding in embodied agents, specifically in medical robotics), with novel integration of depth prediction and perceptual refinement.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.11550": {
        "authors": [
            "Zuo Zuo",
            "Jiahao Dong",
            "Yanyun Qu",
            "Zongze Wu"
        ],
        "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model",
        "abstract": "arXiv:2508.11550v1 Announce Type: new  Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.",
        "arxiv_id": "2508.11550",
        "ARXIVID": "2508.11550",
        "COMMENT": "Related to vision foundation models and generative modeling (criterion 4): Proposes a training-free anomaly generation method using Stable Diffusion with novel dual-attention enhancements for industrial anomaly detection.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.10963": {
        "authors": [
            "Zixiang Yang",
            "Yue Ma",
            "Yinhan Zhang",
            "Shanhui Mo",
            "Dongrui Liu",
            "Linfeng Zhang"
        ],
        "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
        "abstract": "arXiv:2508.10963v1 Announce Type: new  Abstract: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.",
        "arxiv_id": "2508.10963",
        "ARXIVID": "2508.10963",
        "COMMENT": "This paper introduces EVCtrl, a plug-and-play control adapter for efficient visual generation, focusing on controllable image and video generation with foundation models. It is relevant to criterion 4 (vision foundation models and applications), especially in generative modeling and control.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.11185": {
        "authors": [
            "Abhinav Kumar",
            "Yuliang Guo",
            "Zhihao Zhang",
            "Xinyu Huang",
            "Liu Ren",
            "Xiaoming Liu"
        ],
        "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
        "abstract": "arXiv:2508.11185v1 Announce Type: new  Abstract: Monocular 3D object detectors, while effective on data from one ego camera height, struggle with unseen or out-of-distribution camera heights. Existing methods often rely on Plucker embeddings, image transformations or data augmentation. This paper takes a step towards this understudied problem by first investigating the impact of camera height variations on state-of-the-art (SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset with multiple camera heights, we observe that depth estimation is a primary factor influencing performance under height variations. We mathematically prove and also empirically observe consistent negative and positive trends in mean depth error of regressed and ground-based depth models, respectively, under camera height changes. To mitigate this, we propose Camera Height Robust Monocular 3D Detector (CHARM3R), which averages both depth estimates within the model. CHARM3R improves generalization to unseen camera heights by more than $45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at https://github.com/abhi1kumar/CHARM3R",
        "arxiv_id": "2508.11185",
        "ARXIVID": "2508.11185",
        "COMMENT": "This paper introduces CHARM3R, a monocular 3D detector robust to unseen camera heights, with a novel analysis and mitigation strategy. It is relevant to spatial understanding (criterion 1), especially in embodied/simulated environments, and offers a new method for generalization in 3D vision.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.11628": {
        "authors": [
            "Qiang Li",
            "Shansong Wang",
            "Mingzhe Hu",
            "Mojtaba Safari",
            "Zachary Eidex",
            "Xiaofeng Yang"
        ],
        "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
        "abstract": "arXiv:2508.11628v1 Announce Type: new  Abstract: Mammogram visual question answering (VQA) integrates image interpretation with clinical reasoning and has potential to support breast cancer screening. We systematically evaluated the GPT-5 family and GPT-4o model on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment, abnormality detection, and malignancy classification tasks. GPT-5 consistently was the best performing model but lagged behind both human experts and domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%), calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0% malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and specificity (52.3%). While GPT-5 exhibits promising capabilities for screening tasks, its performance remains insufficient for high-stakes clinical imaging applications without targeted domain adaptation and optimization. However, the tremendous improvements in performance from GPT-4o to GPT-5 show a promising trend in the potential for general large language models (LLMs) to assist with mammography VQA tasks.",
        "arxiv_id": "2508.11628",
        "ARXIVID": "2508.11628",
        "COMMENT": "This paper evaluates GPT-5 and GPT-4o on mammogram visual question answering, providing empirical results on their performance in clinical imaging tasks. It matches criterion 2 (VLLMs/MLLMs) and offers surprising empirical results about the limitations and progress of general LLMs in vision-language medical tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.10947": {
        "authors": [
            "Ronghao Xu",
            "Zhen Huang",
            "Yangbo Wei",
            "Xiaoqian Zhou",
            "Zikang Xu",
            "Ting Liu",
            "Zihang Jiang",
            "S. Kevin Zhou"
        ],
        "title": "MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text",
        "abstract": "arXiv:2508.10947v1 Announce Type: new  Abstract: Artificial intelligence has demonstrated significant potential in clinical decision-making; however, developing models capable of adapting to diverse real-world scenarios and performing complex diagnostic reasoning remains a major challenge. Existing medical multi-modal benchmarks are typically limited to single-image, single-turn tasks, lacking multi-modal medical image integration and failing to capture the longitudinal and multi-modal interactive nature inherent to clinical practice. To address this gap, we introduce MedAtlas, a novel benchmark framework designed to evaluate large language models on realistic medical reasoning tasks. MedAtlas is characterized by four key features: multi-turn dialogue, multi-modal medical image interaction, multi-task integration, and high clinical fidelity. It supports four core tasks: open-ended multi-turn question answering, closed-ended multi-turn question answering, multi-image joint reasoning, and comprehensive disease diagnosis. Each case is derived from real diagnostic workflows and incorporates temporal interactions between textual medical histories and multiple imaging modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to perform deep integrative reasoning across images and clinical texts. MedAtlas provides expert-annotated gold standards for all tasks. Furthermore, we propose two novel evaluation metrics: Round Chain Accuracy and Error Propagation Resistance. Benchmark results with existing multi-modal models reveal substantial performance gaps in multi-stage clinical reasoning. MedAtlas establishes a challenging evaluation platform to advance the development of robust and trustworthy medical AI.",
        "arxiv_id": "2508.10947",
        "ARXIVID": "2508.10947",
        "COMMENT": "MedAtlas introduces a new benchmark for evaluating LLMs on multi-modal, multi-turn, multi-task medical reasoning with images and text. This is a new benchmark for multi-modal LLMs, matching criterion 2 (MLLMs) and criterion 3 (new benchmark for embodied AI, though in the medical domain).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.10922": {
        "authors": [
            "Jianlong Wu",
            "Wei Liu",
            "Ye Liu",
            "Meng Liu",
            "Liqiang Nie",
            "Zhouchen Lin",
            "Chang Wen Chen"
        ],
        "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model",
        "abstract": "arXiv:2508.10922v1 Announce Type: new  Abstract: The recent advancement in video temporal grounding (VTG) has significantly enhanced fine-grained video understanding, primarily driven by multimodal large language models (MLLMs). With superior multimodal comprehension and reasoning abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing traditional fine-tuned methods. They not only achieve competitive performance but also excel in generalization across zero-shot, multi-task, and multi-domain settings. Despite extensive surveys on general video-language understanding, comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill this gap, this survey systematically examines current research on VTG-MLLMs through a three-dimensional taxonomy: 1) the functional roles of MLLMs, highlighting their architectural significance; 2) training paradigms, analyzing strategies for temporal reasoning and task adaptation; and 3) video feature processing techniques, which determine spatiotemporal representation effectiveness. We further discuss benchmark datasets, evaluation protocols, and summarize empirical findings. Finally, we identify existing limitations and propose promising research directions. For additional resources and details, readers are encouraged to visit our repository at https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.",
        "arxiv_id": "2508.10922",
        "ARXIVID": "2508.10922",
        "COMMENT": "Matches criterion 2: Survey on video temporal grounding with multimodal large language models (MLLMs), focusing on their architectures, training, and empirical findings.",
        "RELEVANCE": 8,
        "NOVELTY": 5
    },
    "2508.11165": {
        "authors": [
            "Bing Liu",
            "Le Wang",
            "Mingming Liu",
            "Hao Liu",
            "Rui Yao",
            "Yong Zhou",
            "Peng Liu",
            "Tongqiang Xia"
        ],
        "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models",
        "abstract": "arXiv:2508.11165v1 Announce Type: new  Abstract: Existing dehazing methods deal with real-world haze images with difficulty, especially scenes with thick haze. One of the main reasons is the lack of real-world paired data and robust priors. To avoid the costly collection of paired hazy and clear images, we propose an efficient semi-supervised image dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first stage, we employ the EM algorithm to decouple the joint distribution of paired hazy and clear images into two conditional distributions, which are then modeled using a unified Brownian Bridge diffusion model to directly capture the structural and content-related correlations between hazy and clear images. In the second stage, we leverage the pre-trained model and large-scale unpaired hazy and clear images to further improve the performance of image dehazing. Additionally, we introduce a detail-enhanced Residual Difference Convolution block (RDC) to capture gradient-level information, significantly enhancing the model's representation capability. Extensive experiments demonstrate that our EM-B3DM achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.",
        "arxiv_id": "2508.11165",
        "ARXIVID": "2508.11165",
        "COMMENT": "Related to generative modeling and vision (criterion 4): Proposes a semi-supervised image dehazing method using Brownian Bridge diffusion models and EM, with a novel detail-enhanced block. Not directly about spatial intelligence or embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.11339": {
        "authors": [
            "Mingxiao Ma",
            "Shunyao Zhu",
            "Guoliang Kang"
        ],
        "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection",
        "abstract": "arXiv:2508.11339v1 Announce Type: new  Abstract: Incremental object detection (IOD) aims to continuously expand the capability of a model to detect novel categories while preserving its performance on previously learned ones. When adopting a transformer-based detection model to perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning the detection performance on previously learned categories may severely degenerate. Previous typical methods mainly rely on knowledge distillation (KD) to mitigate the catastrophic knowledge forgetting of transformer-based detection models. Specifically, they utilize Hungarian Matching to build a correspondence between the queries of the last-phase and current-phase detection models and align the classifier and regressor outputs between matched queries to avoid knowledge forgetting. However, we observe that in IOD task, Hungarian Matching is not a good choice. With Hungarian Matching, the query of the current-phase model may match different queries of the last-phase model at different iterations during KD. As a result, the knowledge encoded in each query may be reshaped towards new categories, leading to the forgetting of previously encoded knowledge of old categories. Based on our observations, we propose a new distillation approach named Index-Aligned Query Distillation (IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD establishes a correspondence between queries of the previous and current phase models that have the same index. Moreover, we perform index-aligned distillation only on partial queries which are critical for the detection of previous categories. In this way, IAQD largely preserves the previous semantic and spatial encoding capabilities without interfering with the learning of new categories. Extensive experiments on representative benchmarks demonstrate that IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art performance.",
        "arxiv_id": "2508.11339",
        "ARXIVID": "2508.11339",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1) and vision foundation models (criterion 4): Proposes a new distillation approach for transformer-based incremental object detection, focusing on mitigating catastrophic forgetting.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.11265": {
        "authors": [
            "Pei He",
            "Lingling Li",
            "Licheng Jiao",
            "Ronghua Shang",
            "Fang Liu",
            "Shuang Wang",
            "Xu Liu",
            "Wenping Ma"
        ],
        "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds",
        "abstract": "arXiv:2508.11265v1 Announce Type: new  Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying models to unseen environments. Current methods mitigate the domain shift by augmenting the data distribution of point clouds. However, the model learns global geometric patterns in point clouds while ignoring the category-level distribution and alignment. In this paper, a category-level geometry learning framework is proposed to explore the domain-invariant geometric features for domain generalized 3D semantic segmentation. Specifically, Category-level Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric properties of point cloud features, which constructs the geometric properties of each class and couples geometric embedding to semantic learning. Secondly, Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D distribution and align the category-level geometric embeddings, allowing the model to focus on the geometric invariant information to improve generalization. Experimental results verify the effectiveness of the proposed method, which has very competitive segmentation accuracy compared with the state-of-the-art domain generalized point cloud methods.",
        "arxiv_id": "2508.11265",
        "ARXIVID": "2508.11265",
        "COMMENT": "Somewhat related to spatial understanding (criterion 1), as it proposes a category-level geometry learning framework for 3D point cloud segmentation, but not directly on embodied agents or spatial intelligence in agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.11272": {
        "authors": [
            "Jun Li",
            "Kai Li",
            "Shaoguo Liu",
            "Tingting Gao"
        ],
        "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering",
        "abstract": "arXiv:2508.11272v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.",
        "arxiv_id": "2508.11272",
        "ARXIVID": "2508.11272",
        "COMMENT": "This paper proposes a reasoning-augmented representation engineering framework for composed image retrieval, leveraging chain-of-thought and LVLMs. It is relevant to criterion 4 (vision foundation models and applications), with a focus on retrieval and representation engineering.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.11218": {
        "authors": [
            "Jialin Li",
            "Shuqi Wu",
            "Ning Wang"
        ],
        "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving",
        "abstract": "arXiv:2508.11218v1 Announce Type: new  Abstract: Re-Identification (ReID) is a critical technology in intelligent perception systems, especially within autonomous driving, where onboard cameras must identify pedestrians across views and time in real-time to support safe navigation and trajectory prediction. However, the presence of uncertain or missing input modalities--such as RGB, infrared, sketches, or textual descriptions--poses significant challenges to conventional ReID approaches. While large-scale pre-trained models offer strong multimodal semantic modeling capabilities, their computational overhead limits practical deployment in resource-constrained environments. To address these challenges, we propose a lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a multimodal token mapper, synthetic modality augmentation strategy, and cross-modal cue interactive learner. Together, these components enable unified feature representation, mitigate the impact of missing modalities, and extract complementary information across different data types. Additionally, UMM leverages CLIP's vision-language alignment ability to fuse multimodal inputs efficiently without extensive finetuning. Experimental results demonstrate that UMM achieves strong robustness, generalization, and computational efficiency under uncertain modality conditions, offering a scalable and practical solution for pedestrian re-identification in autonomous driving scenarios.",
        "arxiv_id": "2508.11218",
        "ARXIVID": "2508.11218",
        "COMMENT": "This paper presents a CLIP-based uncertainty modal modeling framework for pedestrian re-identification in autonomous driving, focusing on multi-modal fusion and robustness to missing modalities. It is relevant to criterion 4 (vision foundation models and applications), especially in the context of CLIP and multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.11569": {
        "authors": [
            "Zheng Wang",
            "Shihao Xu",
            "Wei Shi"
        ],
        "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
        "abstract": "arXiv:2508.11569v1 Announce Type: new  Abstract: Sports analytics has received significant attention from both academia and industry in recent years. Despite the growing interest and efforts in this field, several issues remain unresolved, including (1) data unavailability, (2) lack of an effective trajectory-based framework, and (3) requirement for sufficient supervision labels. In this paper, we present TrajSV, a trajectory-based framework that addresses various issues in existing studies. TrajSV comprises three components: data preprocessing, Clip Representation Network (CRNet), and Video Representation Network (VRNet). The data preprocessing module extracts player and ball trajectories from sports broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to learn clip representations based on these trajectories. Additionally, VRNet learns video representations by aggregating clip representations and visual features with an encoder-decoder architecture. Finally, a triple contrastive loss is introduced to optimize both video and clip representations in an unsupervised manner. The experiments are conducted on three broadcast video datasets to verify the effectiveness of TrajSV for three types of sports (i.e., soccer, basketball, and volleyball) with three downstream applications (i.e., sports video retrieval, action spotting, and video captioning). The results demonstrate that TrajSV achieves state-of-the-art performance in sports video retrieval, showcasing a nearly 70% improvement. It outperforms baselines in action spotting, achieving state-of-the-art results in 9 out of 17 action categories, and demonstrates a nearly 20% improvement in video captioning. Additionally, we introduce a deployed system along with the three applications based on TrajSV.",
        "arxiv_id": "2508.11569",
        "ARXIVID": "2508.11569",
        "COMMENT": "This paper introduces a trajectory-based framework for sports video representation, leveraging spatial-temporal information and transformer-based models. It is relevant to spatial understanding (criterion 1) and vision foundation model applications (criterion 4), especially in the context of sports analytics and video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.11524": {
        "authors": [
            "Wenkai Yu",
            "Jianhang Tang",
            "Yang Zhang",
            "Shanjiang Tang",
            "Kebing Jin",
            "Hankz Hankui Zhuo"
        ],
        "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models",
        "abstract": "arXiv:2508.11524v1 Announce Type: new  Abstract: Addressing large-scale planning problems has become one of the central challenges in the planning community, deriving from the state-space explosion caused by growing objects and actions. Recently, researchers have explored the effectiveness of leveraging Large Language Models (LLMs) to generate helpful actions and states to prune the search space. However, prior works have largely overlooked integrating LLMs with domain-specific knowledge to ensure valid plans. In this paper, we propose a novel LLM-assisted planner integrated with problem decomposition, which first decomposes large planning problems into multiple simpler sub-tasks. Then we explore two novel paradigms to utilize LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where LLM4Inspire provides heuristic guidance according to general knowledge and LLM4Predict employs domain-specific knowledge to infer intermediate conditions. We empirically validate the effectiveness of our planner across multiple domains, demonstrating the ability of search space partition when solving large-scale planning problems. The experimental results show that LLMs effectively locate feasible solutions when pruning the search space, where infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds particular promise compared with LLM4Inspire, which offers general knowledge within LLMs.",
        "arxiv_id": "2508.11524",
        "ARXIVID": "2508.11524",
        "COMMENT": "This paper explores new paradigms for integrating LLMs with classical planners, specifically for problem decomposition and search space pruning. It is relevant to embodied AI and spatial intelligence, but does not focus on embodied agents or spatial understanding per se. Closest to criterion 1, but not a direct match.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.10937": {
        "authors": [
            "Jiarui Yang",
            "Hang Guo",
            "Wen Huang",
            "Tao Dai",
            "Shutao Xia"
        ],
        "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting",
        "abstract": "arXiv:2508.10937v1 Announce Type: new  Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable progress, generally maintaining high image fidelity and identity (ID) consistency under standard settings. However, in extreme degradation scenarios (e.g., scale $> 8\\times$), critical attributes and ID information are often severely lost in the input image, making it difficult for conventional models to reconstruct realistic and ID-consistent faces. Existing methods tend to generate hallucinated faces under such conditions, producing restored images lacking authentic ID constraints. To address this challenge, we propose a novel FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID restoration under large scaling factors while mitigating hallucination effects. Our approach involves three key designs: 1) \\textbf{Masking} the facial region in the low-resolution (LR) image to eliminate unreliable ID cues; 2) \\textbf{Warping} a reference image to align with the LR input, providing style guidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT) images for fine-grained ID modeling and personalized adaptation. We first pretrain a diffusion-based model to explicitly decouple style and ID by forcing it to reconstruct masked LR face regions using both style and identity embeddings. Subsequently, we freeze most network parameters and perform lightweight fine-tuning of the ID embedding using a small set of target ID images. This embedding encodes fine-grained facial attributes and precise ID information, significantly improving both ID consistency and perceptual quality. Extensive quantitative evaluations and visual comparisons demonstrate that the proposed IDFSR substantially outperforms existing approaches under extreme degradation, particularly achieving superior performance on ID consistency.",
        "arxiv_id": "2508.10937",
        "ARXIVID": "2508.10937",
        "COMMENT": "This paper proposes a novel face super-resolution method using identity decoupling and fitting, with a diffusion-based model. While it is a generative model in vision, it does not directly address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models. It is relevant to your friend's general interest in generative modeling in vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.11502": {
        "authors": [
            "Eyad Alshami",
            "Shashank Agnihotri",
            "Bernt Schiele",
            "Margret Keuper"
        ],
        "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking",
        "abstract": "arXiv:2508.11502v1 Announce Type: new  Abstract: It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features. In this work, we propose \"Amending Inherent Interpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations. In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM enables the training of well-performing and inherently interpretable models that faithfully summarize the decision process. We validate AIM across a diverse range of challenging datasets that test both out-of-distribution generalization and fine-grained visual understanding. These include general-purpose classification benchmarks such as ImageNet100, HardImageNet, and ImageWoof, as well as fine-grained classification datasets such as Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual benefits: interpretability improvements, as measured by the Energy Pointing Game (EPG) score, and accuracy gains over strong baselines. These consistent gains across domains and architectures provide compelling evidence that AIM promotes the use of genuine and meaningful features that directly contribute to improved generalization and human-aligned interpretability.",
        "arxiv_id": "2508.11502",
        "ARXIVID": "2508.11502",
        "COMMENT": "This paper proposes AIM, a self-supervised masking method to improve interpretability and generalization in deep neural networks. While it is relevant to vision and interpretability, it does not directly match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10942": {
        "authors": [
            "Liming Xu",
            "Dave Towey",
            "Andrew P. French",
            "Steve Benford"
        ],
        "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram",
        "abstract": "arXiv:2508.10942v1 Announce Type: new  Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it is expected that our everyday environment may soon be decorating with objects connecting with virtual elements. Alerting to the presence of these objects is therefore the first step for motivating follow-up further inspection and triggering digital material attached to the objects. This work studies a special kind of these objects -- Artcodes -- a human-meaningful and machine-readable decorative markers that camouflage themselves with freeform appearance by encoding information into their topology. We formulate this problem of recongising the presence of Artcodes as Artcode proposal detection, a distinct computer vision task that classifies topologically similar but geometrically and semantically different objects as a same class. To deal with this problem, we propose a new feature descriptor, called the shape of orientation histogram, to describe the generic topological structure of an Artcode. We collect datasets and conduct comprehensive experiments to evaluate the performance of the Artcode detection proposer built upon this new feature vector. Our experimental results show the feasibility of the proposed feature vector for representing topological structures and the effectiveness of the system for detecting Artcode proposals. Although this work is an initial attempt to develop a feature-based system for detecting topological objects like Artcodes, it would open up new interaction opportunities and spark potential applications of topological object detection.",
        "arxiv_id": "2508.10942",
        "ARXIVID": "2508.10942",
        "COMMENT": "This paper introduces a new feature descriptor for detecting Artcodes, focusing on topological structure. While it is a novel computer vision method, it does not match any of the four criteria directly.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11493": {
        "authors": [
            "David H. Chan",
            "Mark Roberts",
            "Dana S. Nau"
        ],
        "title": "Landmark-Assisted Monte Carlo Planning",
        "abstract": "arXiv:2508.11493v1 Announce Type: new  Abstract: Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in every solution plan$\\unicode{x2013}$have contributed to major advancements in classical planning, but they have seldom been used in stochastic domains. We formalize probabilistic landmarks and adapt the UCT algorithm to leverage them as subgoals to decompose MDPs; core to the adaptation is balancing between greedy landmark achievement and final goal achievement. Our results in benchmark domains show that well-chosen landmarks can significantly improve the performance of UCT in online probabilistic planning, while the best balance of greedy versus long-term goal achievement is problem-dependent. The results suggest that landmarks can provide helpful guidance for anytime algorithms solving MDPs.",
        "arxiv_id": "2508.11493",
        "ARXIVID": "2508.11493",
        "COMMENT": "This paper introduces a new method for leveraging probabilistic landmarks in Monte Carlo planning for MDPs. While it is a methodological improvement in planning, it does not directly address spatial understanding or embodied agents (criterion 1), nor does it relate to VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10962": {
        "authors": [
            "Jiarong Li",
            "Imad Ali Shah",
            "Diarmaid Geever",
            "Fiachra Collins",
            "Enda Ward",
            "Martin Glavin",
            "Edward Jones",
            "Brian Deegan"
        ],
        "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving",
        "abstract": "arXiv:2508.10962v1 Announce Type: new  Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for automotive perception systems, particularly under visual ambiguity caused by metamerism, a phenomenon where distinct materials appear similar in RGB imagery. This work investigates hyperspectral imaging (HSI) to overcome this limitation by capturing unique material signatures beyond the visible spectrum, especially in the Near-Infrared (NIR). To manage the inherent high-dimensionality of HSI data, we propose a band selection strategy that integrates information theory techniques (joint mutual information maximization, correlation analysis) with a novel application of an image quality metric (contrast signal-to-noise ratio) to identify the most spectrally informative bands. Using the Hyperspectral City V2 (H-City) dataset, we identify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and reconstruct pseudo-color images for comparison with co-registered RGB. Quantitative results demonstrate increased dissimilarity and perceptual separability of VRU from the background. The selected HSI bands yield improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity (Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently outperforming RGB and confirming a marked reduction in metameric confusion. By providing a spectrally optimized input, our method enhances VRU separability, establishing a robust foundation for downstream perception tasks in Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately contributing to improved road safety.",
        "arxiv_id": "2508.10962",
        "ARXIVID": "2508.10962",
        "COMMENT": "Related to vision and perception, but not directly matching any specific criterion. Focuses on hyperspectral imaging for urban driving, not on embodied agents, VLLMs, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11173": {
        "authors": [
            "Ruobing Jiang",
            "Yang Liu",
            "Haobing Liu",
            "Yanwei Yu",
            "Chunyang Wang"
        ],
        "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery",
        "abstract": "arXiv:2508.11173v1 Announce Type: new  Abstract: Continuous category discovery (CCD) aims to automatically discover novel categories in continuously arriving unlabeled data. This is a challenging problem considering that there is no number of categories and labels in the newly arrived data, while also needing to mitigate catastrophic forgetting. Most CCD methods cannot handle the contradiction between novel class discovery and classification well. They are also prone to accumulate errors in the process of gradually discovering novel classes. Moreover, most of them use knowledge distillation and data replay to prevent forgetting, occupying more storage space. To address these limitations, we propose Independence-based Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes independent enrichment of diversity module, joint discovery of novelty module, and continuous increment by orthogonality module. In independent enrichment, the backbone is trained separately using contrastive loss to avoid it focusing only on features for classification. Joint discovery transforms multi-stage novel class discovery into single-stage, reducing error accumulation impact. Continuous increment by orthogonality module generates mutually orthogonal prototypes for classification and prevents forgetting with lower space overhead via representative representation replay. Experimental results show that on challenging fine-grained datasets, our method outperforms the state-of-the-art methods.",
        "arxiv_id": "2508.11173",
        "ARXIVID": "2508.11173",
        "COMMENT": "Related to category discovery and representation learning, but not directly matching any specific criterion. Some relevance to general interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11347": {
        "authors": [
            "Yifei Li",
            "Lingling Zhang",
            "Hang Yan",
            "Tianzhe Zhao",
            "Zihan Ma",
            "Muye Huang",
            "Jun Liu"
        ],
        "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding",
        "abstract": "arXiv:2508.11347v1 Announce Type: new  Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities and relations in a low-dimensional space, primarily focusing on static graphs. However, real-world KGs are dynamically evolving with the constant addition of entities, relations and facts. To address such dynamic nature of KGs, several continual knowledge graph embedding (CKGE) methods have been developed to efficiently update KG embeddings to accommodate new facts while maintaining learned knowledge. As KGs grow at different rates and scales in real-world scenarios, existing CKGE methods often fail to consider the varying scales of updates and lack systematic evaluation throughout the entire update process. In this paper, we propose SAGE, a scale-aware gradual evolution framework for CKGE. Specifically, SAGE firstly determine the embedding dimensions based on the update scales and expand the embedding space accordingly. The Dynamic Distillation mechanism is further employed to balance the preservation of learned knowledge and the incorporation of new facts. We conduct extensive experiments on seven benchmarks, and the results show that SAGE consistently outperforms existing baselines, with a notable improvement of 1.38% in MRR, 1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with methods using fixed embedding dimensions show that SAGE achieves optimal performance on every snapshot, demonstrating the importance of adaptive embedding dimensions in CKGE. The codes of SAGE are publicly available at: https://github.com/lyfxjtu/Dynamic-Embedding.",
        "arxiv_id": "2508.11347",
        "ARXIVID": "2508.11347",
        "COMMENT": "This paper presents SAGE, a scale-aware continual knowledge graph embedding framework. While it is a novel approach in knowledge graph embeddings, it does not directly match any of the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11212": {
        "authors": [
            "Zhangjian Ji",
            "Wenjin Zhang",
            "Shaotong Qiao",
            "Kai Feng",
            "Yuhua Qian"
        ],
        "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network",
        "abstract": "arXiv:2508.11212v1 Announce Type: new  Abstract: Human pose estimation has been widely applied in the human-centric understanding and generation, but most existing state-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. In order to obtain an accurate, robust yet lightweight human pose estimator, one feasible way is to transfer pose knowledge from a powerful teacher model to a less-parameterized student model by knowledge distillation. However, the traditional knowledge distillation framework does not fully explore the contextual information among human joints. Thus, in this paper, we propose a novel coarse-to-fine two-stage knowledge distillation framework for human pose estimation. In the first-stage distillation, we introduce the human joints structure loss to mine the structural information among human joints so as to transfer high-level semantic knowledge from the teacher model to the student model. In the second-stage distillation, we utilize an Image-Guided Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human pose obtained from the first-stage distillation and supervise the training of the IGP-GCN in the progressive way by the final output pose of teacher model. The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose datasets, show that our proposed method performs favorably against lots of the existing state-of-the-art human pose estimation methods, especially for the more complex CrowdPose dataset, the performance improvement of our model is more significant.",
        "arxiv_id": "2508.11212",
        "ARXIVID": "2508.11212",
        "COMMENT": "This paper proposes a new knowledge distillation framework for lightweight human pose estimation. While relevant to computer vision, it does not directly address spatial intelligence for embodied agents, VLLMs/MLLMs, benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11531": {
        "authors": [
            "Shilei Wang",
            "Gong Cheng",
            "Pujian Lai",
            "Dong Gao",
            "Junwei Han"
        ],
        "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction",
        "abstract": "arXiv:2508.11531v1 Announce Type: new  Abstract: Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.",
        "arxiv_id": "2508.11531",
        "ARXIVID": "2508.11531",
        "COMMENT": "Multi-State Tracker proposes a new object tracking method with multi-state specialization and interaction for efficient tracking. While it is a vision method, it does not directly address spatial intelligence, embodied agents, VLLMs/MLLMs, or vision foundation models. Not a direct match to any criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11301": {
        "authors": [
            "Jiarong Li",
            "Imad Ali Shah",
            "Enda Ward",
            "Martin Glavin",
            "Edward Jones",
            "Brian Deegan"
        ],
        "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study",
        "abstract": "arXiv:2508.11301v1 Announce Type: new  Abstract: Pedestrian segmentation in automotive perception systems faces critical safety challenges due to metamerism in RGB imaging, where pedestrians and backgrounds appear visually indistinguishable.. This study investigates the potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We compared standard RGB against two dimensionality-reduction approaches by converting 128-channel HSI data into three-channel representations: Principal Component Analysis (PCA) and optimal band selection using Contrast Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM). Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25% F1-score improvements. These improved performance results from enhanced spectral discrimination of optimally selected HSI bands effectively reducing false positives. This study demonstrates robust pedestrian segmentation through optimal HSI band selection, showing significant potential for safety-critical automotive applications.",
        "arxiv_id": "2508.11301",
        "ARXIVID": "2508.11301",
        "COMMENT": "This paper is a comparative study of hyperspectral vs. RGB for pedestrian segmentation. While it is relevant to computer vision and machine learning, it does not match any of the specific criteria (no new VLLMs/MLLMs, no embodied AI, no new spatial reasoning method, no vision foundation model application).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.10940": {
        "authors": [
            "Nirmal Gaud",
            "Krishna Kumar Jha",
            "Jhimli Adhikari",
            "Adhini Nasarin P S",
            "Joydeep Das",
            "Samarth S Deshpande",
            "Nitasha Barara",
            "Vaduguru Venkata Ramya",
            "Santu Saha",
            "Mehmet Tarik Baran",
            "Sarangi Venkateshwarlu",
            "Anusha M D",
            "Surej Mouli",
            "Preeti Katiyar",
            "Vipin Kumar Chaudhary"
        ],
        "title": "NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification",
        "abstract": "arXiv:2508.10940v1 Announce Type: new  Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional Neural Networks (CNNs) that integrates adaptive max pooling with non-linear activation function for image classification tasks. The acronym NIRMAL stands for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum, Adaptive, and Localized. By dynamically adjusting pooling parameters based on desired output dimensions and applying a Rectified Linear Unit (ReLU) activation post-pooling, NIRMAL Pooling improves robustness and feature expressiveness. We evaluated its performance against standard Max Pooling on three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on CIFAR-10, demonstrating consistent improvements, particularly on complex datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN performance in diverse image recognition tasks, offering a flexible and reliable alternative to traditional pooling methods.",
        "arxiv_id": "2508.10940",
        "ARXIVID": "2508.10940",
        "COMMENT": "This paper introduces NIRMAL Pooling, a new pooling layer for CNNs. While it is a methodological improvement in computer vision, it does not address spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}