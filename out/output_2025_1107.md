# Personalized Daily ArXiv Papers 11/07/2025
Total relevant papers: 25

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](#link0)
**Authors:** Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu

1. [Cambrian-S: Towards Spatial Supersensing in Video](#link1)
**Authors:** Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie

2. [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](#link2)
**Authors:** Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng

3. [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](#link3)
**Authors:** Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang

4. [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](#link4)
**Authors:** Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie

5. [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](#link5)
**Authors:** Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe

6. [FastGS: Training 3D Gaussian Splatting in 100 Seconds](#link6)
**Authors:** Shiwei Ren, Tianci Wen, Yongchun Fang, Biao Lu

7. [Tracking and Understanding Object Transformations](#link7)
**Authors:** Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan

8. [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](#link8)
**Authors:** Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata

9. [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](#link9)
**Authors:** Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy, Rahul Ramachandran, Chia-Yu Hsu

10. [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](#link10)
**Authors:** Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li

11. [SILVI: Simple Interface for Labeling Video Interactions](#link11)
**Authors:** Ozan Kanbertay (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen), Richard Vogg (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen, Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Elif Karakoc (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Peter M. Kappeler (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany, Department of Sociobiology/Anthropology, University of G\"ottingen, G\"ottingen, Germany), Claudia Fichtel (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Alexander S. Ecker (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen)

12. [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](#link12)
**Authors:** Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising

13. [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](#link13)
**Authors:** Chang Liu, Juan Li, Sheng Zhang, Chang Liu, Jie Li, Xu Zhang

14. [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](#link14)
**Authors:** Sam Bahrami, Dylan Campbell

15. [Text to Sketch Generation with Multi-Styles](#link15)
**Authors:** Tengjie Li, Shikui Tu, Lei Xu

16. [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](#link16)
**Authors:** Ke Du, Yimin Peng, Chao Gao, Fan Zhou, Siqiao Xue

17. [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](#link17)
**Authors:** Josef Mayr, Anna Reithmeir, Maxime Di Folco, Julia A. Schnabel

18. [Near-Lossless 3D Voxel Representation Free from Iso-surface](#link18)
**Authors:** Yihao Luo, Xianglong He, Chuanyu Pan, Yiwen Chen, Jiaqi Wu, Yangguang Li, Wanli Ouyang, Yuanming Hu, Guang Yang, ChoonHwai Yap

19. [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](#link19)
**Authors:** Nand Kumar Yadav (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), Rodrigue Rizk (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), William CW Chen (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), KC Santosh (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA)

20. [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](#link20)
**Authors:** Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar

21. [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](#link21)
**Authors:** Hyunseok Kwak, Kyeongwon Lee, Jae-Jin Lee, Woojoo Lee

22. [A Proprietary Model-Based Safety Response Framework for AI Agents](#link22)
**Authors:** Qi Li, Jianjun Xu, Pingtao Wei, Jiu Li, Peiqiang Zhao, Jiwei Shi, Xuan Zhang, Yanhui Yang, Xiaodong Hui, Peng Xu, Wenqin Shao

23. [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](#link23)
**Authors:** Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen

24. [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](#link24)
**Authors:** Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov

---
## 0. [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570) <a id="link0"></a>
**ArXiv ID:** 2511.04570
**Authors:** Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu

**Abstract:**  "Thinking with Text" and "Thinking with Images" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce "Thinking with Video", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions "thinking with video" as a unified multimodal reasoning paradigm.

**Comment:** Matches criterion 2: Shows new VLLMs/MLLMs. Also matches criterion 4: Vision foundation models and applications. Introduces 'Thinking with Video' as a new multimodal reasoning paradigm, evaluates Sora-2 (a video generation model) as a unified multimodal reasoner, and builds a new benchmark (VideoThinkBench), which is a novel angle for multimodal intelligence.
**Relevance:** 10
**Novelty:** 9

---

## 1. [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670) <a id="link1"></a>
**ArXiv ID:** 2511.04670
**Authors:** Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie

**Abstract:**  We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.

**Comment:** Matches criterion 1 (methodological improvements to spatial understanding and intelligence) and criterion 3 (new benchmark for spatial supersensing in video). Cambrian-S introduces a new paradigm and benchmark (VSI-SUPER) for spatial supersensing, and proposes predictive sensing as a novel approach.
**Relevance:** 10
**Novelty:** 9

---

## 2. [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317) <a id="link2"></a>
**ArXiv ID:** 2511.04317
**Authors:** Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng

**Abstract:**  Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user's intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at https://rise-t2v.github.io.

**Comment:** Matches criterion 2 (new VLLM/MLLM methods). The paper proposes RISE-T2V, a new framework integrating LLMs with video diffusion models for text-to-video generation, introducing a rephrasing adapter for improved semantic alignment and prompt rephrasing.
**Relevance:** 10
**Novelty:** 8

---

## 3. [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595) <a id="link3"></a>
**ArXiv ID:** 2511.04595
**Authors:** Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang

**Abstract:**  Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.

**Comment:** Matches criterion 1 (spatial intelligence in dynamic 3D scene understanding) and criterion 4 (leverages pretrained vision foundation models for 3D scene reconstruction). UniSplat introduces a unified spatio-temporal fusion framework for dynamic driving scenes, with a novel 3D latent scaffold and dual-branch decoder.
**Relevance:** 9
**Novelty:** 7

---

## 4. [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992) <a id="link4"></a>
**ArXiv ID:** 2511.03992
**Authors:** Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie

**Abstract:**  Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.

**Comment:** Matches criterion 1 (new methodological improvements to spatial understanding on embodied agents) and criterion 3 (novel methods for 3D scene understanding with potential for embodied AI). The paper introduces a new framework (CaRF) for multi-view consistent 3D referring segmentation, directly addressing spatial intelligence and geometric reasoning in 3D Gaussian fields, with explicit mention of embodied AI applications.
**Relevance:** 9
**Novelty:** 7

---

## 5. [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016) <a id="link5"></a>
**ArXiv ID:** 2511.04016
**Authors:** Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe

**Abstract:**  The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our model's effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications.

**Comment:** Matches criterion 4: Vision foundation models related and its applications. MedDChest is a foundational vision model for thoracic imaging, trained from scratch on a large, multimodal dataset, and introduces a novel content-aware data augmentation method.
**Relevance:** 8
**Novelty:** 7

---

## 6. [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283) <a id="link6"></a>
**ArXiv ID:** 2511.04283
**Authors:** Shiwei Ren, Tianci Wen, Yongchun Fang, Biao Lu

**Abstract:**  The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/

**Comment:** Matches criterion 1 (methodological improvements to spatial understanding for embodied agents) and criterion 4 (vision foundation models and applications). FastGS proposes a new acceleration framework for 3D Gaussian Splatting, with a novel densification and pruning strategy based on multi-view consistency, and demonstrates strong generality across spatial tasks including SLAM and dynamic scene reconstruction.
**Relevance:** 8
**Novelty:** 7

---

## 7. [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678) <a id="link7"></a>
**ArXiv ID:** 2511.04678
**Authors:** Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan

**Abstract:**  Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at https://tubelet-graph.github.io.

**Comment:** Matches criterion 3 (new benchmark and method for tracking and understanding object transformations, a novel angle in temporal and semantic reasoning). Introduces a new dataset (VOST-TAS) and a zero-shot system (TubeletGraph) for tracking objects through state changes.
**Relevance:** 8
**Novelty:** 7

---

## 8. [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008) <a id="link8"></a>
**ArXiv ID:** 2511.04008
**Authors:** Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata

**Abstract:**  Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.

**Comment:** Matches criterion 4: Vision foundation models and applications. Proposes a novel GNN-based routing mechanism for parameter-efficient domain generalization in ViTs, which is a clever statistical trick for robust adaptation.
**Relevance:** 7
**Novelty:** 7

---

## 9. [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474) <a id="link9"></a>
**ArXiv ID:** 2511.04474
**Authors:** Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy, Rahul Ramachandran, Chia-Yu Hsu

**Abstract:**  Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.

**Comment:** Matches criterion 4 (vision foundation models and their applications). The paper evaluates and adapts geospatial foundation models for landslide hazard mapping, showing generalizability and robustness across domains.
**Relevance:** 8
**Novelty:** 6

---

## 10. [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281) <a id="link10"></a>
**ArXiv ID:** 2511.04281
**Authors:** Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li

**Abstract:**  Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.

**Comment:** Matches criterion 4: Vision foundation models and applications. Uses DINOv2 (a vision foundation model) for gait representation learning in cross-modal person re-identification, with a novel multi-granularity enhancement module.
**Relevance:** 7
**Novelty:** 6

---

## 11. [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819) <a id="link11"></a>
**ArXiv ID:** 2511.03819
**Authors:** Ozan Kanbertay (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen), Richard Vogg (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen, Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Elif Karakoc (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Peter M. Kappeler (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany, Department of Sociobiology/Anthropology, University of G\"ottingen, G\"ottingen, Germany), Claudia Fichtel (Behavioral Ecology & Sociobiology Unit, German Primate Center, G\"ottingen, Germany), Alexander S. Ecker (Institute of Computer Science and Campus Institute Data Science, University of G\"ottingen)

**Abstract:**  Computer vision methods are increasingly used for the automated analysis of large volumes of video data collected through camera traps, drones, or direct observations of animals in the wild. While recent advances have focused primarily on detecting individual actions, much less work has addressed the detection and annotation of interactions -- a crucial aspect for understanding social and individualized animal behavior. Existing open-source annotation tools support either behavioral labeling without localization of individuals, or localization without the capacity to capture interactions. To bridge this gap, we present SILVI, an open-source labeling software that integrates both functionalities. SILVI enables researchers to annotate behaviors and interactions directly within video data, generating structured outputs suitable for training and validating computer vision models. By linking behavioral ecology with computer vision, SILVI facilitates the development of automated approaches for fine-grained behavioral analyses. Although developed primarily in the context of animal behavior, SILVI could be useful more broadly to annotate human interactions in other videos that require extracting dynamic scene graphs. The software, along with documentation and download instructions, is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

**Comment:** Matches criterion 3 (embodied AI, new benchmarks or methods with novel angles). Presents SILVI, a new open-source tool for labeling video interactions, enabling structured annotation for training and validating computer vision models in behavioral analysis, which is a novel angle for interaction annotation.
**Relevance:** 7
**Novelty:** 6

---

## 12. [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347) <a id="link12"></a>
**ArXiv ID:** 2511.04347
**Authors:** Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising

**Abstract:**  Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird's Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.

**Comment:** Matches criterion 3 (embodied AI, new benchmarks or methods with novel angles). Investigates the impact of weather-induced sensor occlusion on BEVFusion for 3D object detection, highlighting underexplored issues in sensor fusion and proposing the need for occlusion-aware evaluation.
**Relevance:** 7
**Novelty:** 6

---

## 13. [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388) <a id="link13"></a>
**ArXiv ID:** 2511.04388
**Authors:** Chang Liu, Juan Li, Sheng Zhang, Chang Liu, Jie Li, Xu Zhang

**Abstract:**  Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at https://github.com/liangxiansheng093/BoRe-Depth.

**Comment:** Matches criterion 1 (methodological improvements to spatial understanding on embodied agents). Proposes a lightweight monocular depth estimation model with enhanced boundary refinement for embedded systems, which is relevant for spatial intelligence in embodied AI.
**Relevance:** 7
**Novelty:** 6

---

## 14. [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970) <a id="link14"></a>
**ArXiv ID:** 2511.03970
**Authors:** Sam Bahrami, Dylan Campbell

**Abstract:**  Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset -- Room Envelopes -- that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene's extent, as well as the shape and location of its objects.

**Comment:** Matches criterion 3 (new benchmark for spatial layout understanding in indoor scenes). The paper introduces a synthetic dataset for indoor layout reconstruction, focusing on structural scene elements, which is a novel angle for spatial understanding benchmarks.
**Relevance:** 7
**Novelty:** 6

---

## 15. [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123) <a id="link15"></a>
**ArXiv ID:** 2511.04123
**Authors:** Tengjie Li, Shikui Tu, Lei Xu

**Abstract:**  Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at https://github.com/CMACH508/M3S.

**Comment:** Matches criterion 4 (vision foundation models and applications). Presents a training-free diffusion-based framework for text-to-sketch generation with explicit multi-style control, leveraging vision-language models and diffusion models for controllable synthesis.
**Relevance:** 6
**Novelty:** 6

---

## 16. [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394) <a id="link16"></a>
**ArXiv ID:** 2511.04394
**Authors:** Ke Du, Yimin Peng, Chao Gao, Fan Zhou, Siqiao Xue

**Abstract:**  DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across diverse scales. A single YAML-driven workflow covers classification, retrieval and metric learning; more than 1000 pretrained backbones are exposed through a timm-compatible interface, together with modular losses, augmentations and distributed-training utilities. Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products, while one-command export to ONNX or HuggingFace bridges research and deployment. By consolidating datasets, models, and training techniques into one platform, DORAEMON offers a scalable foundation for rapid experimentation in visual recognition and representation learning, enabling efficient transfer of research advances to real-world applications. The repository is available at https://github.com/wuji3/DORAEMON.

**Comment:** Matches criterion 4 (vision foundation models and applications). Introduces DORAEMON, a unified library for large-scale visual object modeling and representation learning, supporting over 1000 pretrained backbones and scalable experimentation.
**Relevance:** 6
**Novelty:** 5

---

## 17. [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190) <a id="link17"></a>
**ArXiv ID:** 2511.04190
**Authors:** Josef Mayr, Anna Reithmeir, Maxime Di Folco, Julia A. Schnabel

**Abstract:**  Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis.

**Comment:** Matches criterion 4 (vision foundation models and applications). Explores the use of pretrained general vision encoders (DINOv2, MedSAM) with covariance descriptors for medical image classification, showing improved results over handcrafted features.
**Relevance:** 5
**Novelty:** 5

---

## 18. [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029) <a id="link18"></a>
**ArXiv ID:** 2511.04029
**Authors:** Yihao Luo, Xianglong He, Chuanyu Pan, Yiwen Chen, Jiaqi Wu, Yangguang Li, Wanli Ouyang, Yuanming Hu, Guang Yang, ChoonHwai Yap

**Abstract:**  Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.

**Comment:** Somewhat relevant to criterion 4 (3D representation and reconstruction methods, which are foundational for vision models), but does not focus on foundation models or embodied AI directly.
**Relevance:** 4
**Novelty:** 6

---

## 19. [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912) <a id="link19"></a>
**ArXiv ID:** 2511.03912
**Authors:** Nand Kumar Yadav (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), Rodrigue Rizk (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), William CW Chen (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA), KC Santosh (AI Research Lab, Department of Computer Science, Biomedical, Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA)

**Abstract:**  Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.

**Comment:** Does not directly match any specific criterion, but is relevant to computer vision and machine learning in medical imaging. Focuses on anomaly detection with clever statistical tricks (uncertainty gating, SWAG), but not on spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 20. [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092) <a id="link20"></a>
**ArXiv ID:** 2511.03092
**Authors:** Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar

**Abstract:**  The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.

**Comment:** Somewhat related to criterion 2 (VLLMs/MLLMs), as it discusses efficient long sequence decoding for LLMs and vLLM frameworks, but the focus is on memory and inference optimization rather than new model architectures or multi-modal learning.
**Relevance:** 4
**Novelty:** 5

---

## 21. [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765) <a id="link21"></a>
**ArXiv ID:** 2511.03765
**Authors:** Hyunseok Kwak, Kyeongwon Lee, Jae-Jin Lee, Woojoo Lee

**Abstract:**  On-device fine-tuning of CNNs is essential to withstand domain shift in edge applications such as Human Activity Recognition (HAR), yet full fine-tuning is infeasible under strict memory, compute, and energy budgets. We present LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional layers, (ii) selectively updates only the output-side core with zero-initialization to keep the auxiliary path inactive at the start, and (iii) fuses the update back into dense kernels, leaving inference cost unchanged. This design preserves convolutional structure and reduces the number of trainable parameters by up to two orders of magnitude compared to full fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, consistently outperforming prior parameter-efficient baselines under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and selective-core training yield 1.4-3.8x faster convergence to target F1. LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms.

**Comment:** Does not directly match any specific criterion, but is relevant to efficient model adaptation for edge devices, which is tangential to vision foundation models and embodied AI.
**Relevance:** 3
**Novelty:** 5

---

## 22. [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138) <a id="link22"></a>
**ArXiv ID:** 2511.03138
**Authors:** Qi Li, Jianjun Xu, Pingtao Wei, Jiu Li, Peiqiang Zhao, Jiwei Shi, Xuan Zhang, Yanhui Yang, Xiaodong Hui, Peng Xu, Wenqin Shao

**Abstract:**  With the widespread application of Large Language Models (LLMs), their associated security issues have become increasingly prominent, severely constraining their trustworthy deployment in critical domains. This paper proposes a novel safety response framework designed to systematically safeguard LLMs at both the input and output levels. At the input level, the framework employs a supervised fine-tuning-based safety classification model. Through a fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention), it performs precise risk identification and differentiated handling of user queries, significantly enhancing risk coverage and business scenario adaptability, and achieving a risk recall rate of 99.3%. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a specifically fine-tuned interpretation model, ensuring all responses are grounded in a real-time, trustworthy knowledge base. This approach eliminates information fabrication and enables result traceability. Experimental results demonstrate that our proposed safety control model achieves a significantly higher safety score on public safety evaluation benchmarks compared to the baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk test set, the framework's components attained a perfect 100% safety score, validating their exceptional protective capabilities in complex risk scenarios. This research provides an effective engineering pathway for building high-security, high-trust LLM applications.

**Comment:** Somewhat related to criterion 2 (VLLMs/MLLMs), as it discusses safety frameworks for LLMs, but does not introduce a new VLLM or MLLM or focus on vision or multi-modal learning.
**Relevance:** 3
**Novelty:** 4

---

## 23. [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615) <a id="link23"></a>
**ArXiv ID:** 2511.04615
**Authors:** Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen

**Abstract:**  Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.

**Comment:** Somewhat related to criterion 4 (vision foundation models and applications), as it discusses evaluation metrics for deep learning models in virtual immunohistochemistry, but does not introduce a new foundation model or method for spatial intelligence.
**Relevance:** 3
**Novelty:** 4

---

## 24. [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525) <a id="link24"></a>
**ArXiv ID:** 2511.04525
**Authors:** Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov

**Abstract:**  Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.

**Comment:** Does not directly match any specific criterion. Focuses on surgical video analysis and weak supervision, which is outside the main criteria but within general computer vision.
**Relevance:** 3
**Novelty:** 4

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.